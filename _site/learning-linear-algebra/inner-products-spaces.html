<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>
<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.191">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<meta name="description" content="Learning Linear Algebra">
<title>Inner Products Spaces - Learning Linear Algebra</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>
<meta name="quarto:offset" content="./">
<link href="./determinants.html" rel="next">
<link href="./linear-transformations.html" rel="prev">
<link href="./../assets/favicon.ico" rel="icon">
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<style>html{ scroll-behavior: smooth; }</style>
<style>
@media screen and (max-width: 440px) {
    .navbar-brand-logo::after {
        content: 'DK'!important;
        margin-top: -4px!important;
    }
    .navbar-toggler-icon {
        width:26px!important;
        height:26px!important;
    }
    .navbar-toggler {
        border:0px!important;
    }
    .navbar-title {
        display:none!important;
    }
    .footer-items {
        display:block!important;
        margin:40px 0px;
    }    
}
.navbar-title:hover, .navbar a:hover {
    color:#000000!important;
}
.book-title {
    font-weight:800;
    text-transform: uppercase;
    font-size: 150%;
    margin:-20px -20px 20px -20px;
    line-height:120%;
    text-decoration: none!important;
    display:block;
}
@media screen and (max-width: 990px) {
    .book-title {
        padding-left: 20px!important;
    }    
}
</style>
<style>
    .quarto-title-block .quarto-title-banner {
      background-repeat: no-repeat!important;
      background-position: center center;
      background-size: cover!important;
      background-attachment: fixed;
    }
    .quarto-title-banner .title 
    {
        background: none!important;
    }
</style>
  
  
<meta property="og:title" content="Inner Products Spaces - Learning Linear Algebra">
<meta property="og:description" content="Learning Linear Algebra">
<meta property="og:site-name" content="Learning Linear Algebra">
</head>
<body class="nav-sidebar floating nav-fixed">
<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="./index.html" class="navbar-brand navbar-brand-logo">
    <img src="./../assets/directknowledge-logo.svg" alt="Direct Knowledge Logo" width="28" height="24"  class="navbar-logo">
    </a>
    <a aria-label="Learning Linear Algebra" class="navbar-brand" href="https://directknowledge.com">
    <span class="navbar-title">Direct Knowledge</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-books" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Books</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-books">    
        <li>
    <a class="dropdown-item" href="./../basic-set-theory/" rel="" target=""><i class="bi bi-book" role="img" aria-label="Basic Set Theory Book">
</i> 
 <span class="dropdown-text">Basic Set Theory</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./../learning-number-theory/" rel="" target=""><i class="bi bi-book" role="img" aria-label="Learning Linear Algebra Book">
</i> 
 <span class="dropdown-text">Learning Number Theory</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./../learning-linear-algebra/" rel="" target=""><i class="bi bi-book" role="img" aria-label="Learning Linear Algebra Book">
</i> 
 <span class="dropdown-text">Learning Linear Algebra</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./../single-variable-calculus/" rel="" target=""><i class="bi bi-book" role="img" aria-label="Single Variable Calculus Book">
</i> 
 <span class="dropdown-text">Single Variable Calculus</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./../multivariable-calculus/" rel="" target=""><i class="bi bi-book" role="img" aria-label="Multivariable Calculus Book">
</i> 
 <span class="dropdown-text">Multivariable Calculus</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item dropdown ">
    <a class="nav-link dropdown-toggle" href="#" id="nav-menu-articles" role="button" data-bs-toggle="dropdown" aria-expanded="false" rel="" target="">
 <span class="menu-text">Articles</span>
    </a>
    <ul class="dropdown-menu dropdown-menu-end" aria-labelledby="nav-menu-articles">    
        <li>
    <a class="dropdown-item" href="./../blog/" rel="" target=""><i class="bi bi-journal" role="img" aria-label="Personal Blog">
</i> 
 <span class="dropdown-text">Blog</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./../tutorials/" rel="" target=""><i class="bi bi-journal" role="img" aria-label="Tutorials">
</i> 
 <span class="dropdown-text">Tutorials</span></a>
  </li>  
        <li>
    <a class="dropdown-item" href="./../research/" rel="" target=""><i class="bi bi-journal" role="img" aria-label="Research Articles">
</i> 
 <span class="dropdown-text">Research</span></a>
  </li>  
    </ul>
  </li>
  <li class="nav-item">
    <a class="nav-link" href="./../about.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://www.youtube.com/channel/UCi_E35l9kKfrxoQJRMbgMsg/" rel="" target=""><i class="bi bi-youtube" role="img" aria-label="YouTube">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/directknowledge" rel="" target=""><i class="bi bi-github" role="img" aria-label="GitHub">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/directknowledge/directknowledge.com" rel="" target=""><i class="bi bi-github" role="img" aria-label="Source Code">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
  <nav class="quarto-secondary-nav" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
    <div class="container-fluid d-flex justify-content-between">
      <h1 class="quarto-secondary-nav-title"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Inner Products Spaces</span></h1>
      <button type="button" class="quarto-btn-toggle btn" aria-label="Show secondary navigation">
        <i class="bi bi-chevron-right"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse sidebar-navigation floating overflow-auto">
      <div class="mt-2 flex-shrink-0 align-items-center">
        <a class="book-title" href="https://directknowledge.com/learning-linear-algebra/">Learning Linear Algebra</a><div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
      </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./index.html" class="sidebar-item-text sidebar-link">Preface</a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./systems-of-linear-equations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">1</span>&nbsp; <span class="chapter-title">Systems of Linear Equations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./vector-spaces.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">2</span>&nbsp; <span class="chapter-title">Vector Spaces</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./linear-transformations.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Transformations</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./inner-products-spaces.html" class="sidebar-item-text sidebar-link active"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Inner Products Spaces</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./determinants.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Determinants</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./eigenvalues-and-eigenvectors.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">6</span>&nbsp; <span class="chapter-title">Eigenvalues and Eigenvectors</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./canonical-forms.html" class="sidebar-item-text sidebar-link"><span class="chapter-number">7</span>&nbsp; <span class="chapter-title">Canonical Forms</span></a>
  </div>
</li>
        <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./references.html" class="sidebar-item-text sidebar-link">References</a>
  </div>
</li>
    </ul>
    </div>
</nav>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active"><a href="https://directknowledge.com/learning-linear-algebra/"><picture><source type="image/webp" srcset="../../assets/learning-linear-algebra-cover.webp"><source type="image/webp" srcset="../../assets/learning-linear-algebra-cover-thumbnail.png"><img class="shadow border" style="margin-bottom:12px;" width="192" height="288" src="../../assets/learning-linear-algebra-cover.png" alt="Learning Linear Algebra Book Cover"></picture></a>
    <h2 id="toc-title">Chapter Contents</h2>
   
  <ul>
  <li><a href="#orthonormal-bases-and-orthogonal-projections" id="toc-orthonormal-bases-and-orthogonal-projections" class="nav-link active" data-scroll-target="#orthonormal-bases-and-orthogonal-projections"><span class="toc-section-number">4.1</span>  Orthonormal Bases and Orthogonal Projections</a></li>
  <li><a href="#gram-schmidt-process-and-qr-factorization" id="toc-gram-schmidt-process-and-qr-factorization" class="nav-link" data-scroll-target="#gram-schmidt-process-and-qr-factorization"><span class="toc-section-number">4.2</span>  Gram-Schmidt Process and QR Factorization</a></li>
  <li><a href="#orthogonal-transformations-and-orthogonal-matrices" id="toc-orthogonal-transformations-and-orthogonal-matrices" class="nav-link" data-scroll-target="#orthogonal-transformations-and-orthogonal-matrices"><span class="toc-section-number">4.3</span>  Orthogonal Transformations and Orthogonal Matrices</a></li>
  <li><a href="#inner-products" id="toc-inner-products" class="nav-link" data-scroll-target="#inner-products"><span class="toc-section-number">4.4</span>  Inner Products</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/directknowledge/directknowledge.com/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">
<header id="title-block-header">
<h1 class="title d-none d-lg-block display-7"><span class="chapter-number">4</span>&nbsp; <span class="chapter-title">Inner Products Spaces</span></h1>
</header>
<p><span class="math inline">\(\newcommand{\vlist}[2]{#1_1,#1_2,\ldots,#1_#2}\)</span> <span class="math inline">\(\newcommand{\vectortwo}[2]{\begin{bmatrix} #1 \\ #2\end{bmatrix}}\)</span> <span class="math inline">\(\newcommand{\vectorthree}[3]{\begin{bmatrix} #1 \\ #2 \\ #3\end{bmatrix}}\)</span> <span class="math inline">\(\newcommand{\vectorfour}[4]{\begin{bmatrix} #1 \\ #2 \\ #3 \\ #4\end{bmatrix}}\)</span> <span class="math inline">\(\newcommand{\vectorfive}[5]{\begin{bmatrix} #1 \\ #2 \\ #3 \\ #4 \\ #5 \end{bmatrix}}\)</span> <span class="math inline">\(\newcommand{\lincomb}[3]{#1_1 \vec{#2}_1+#1_2 \vec{#2}_2+\cdots + #1_m \vec{#2}_#3}\)</span> <span class="math inline">\(\newcommand{\norm}[1]{\left|\left |#1\right|\right |}\)</span> <span class="math inline">\(\newcommand{\ip}[1]{\left \langle #1\right \rangle}\)</span> <span class="math inline">\(\newcommand{\plim}[2]{\lim_{\footnotesize\begin{array}{c} \\[-10pt] #1 \\[0pt] #2 \end{array}}}\)</span></p>
<p>Write a content brief for the book ``Orthogonality (A Lively Introduction with Proofs).” The book covers orthonormal bases, orthogonal projections, Gram-Schmidt process, QR factorization, orthogonal transformations, and orthogonal matrices. It also includes inner-products.</p>
<p>Orthogonality is a vital topic in linear algebra and mathematics as a whole. It is often studied in the context of inner product spaces, where it plays a key role in many important results. In this book, we will give a lively introduction to orthogonality, with an emphasis on proofs and intuition. We will also discuss inner-products and their relationship to orthogonality. This book is aimed at students who are studying linear algebra or related topics. It will be a valuable resource for those who want to deepen their understanding of orthogonality and its many applications.</p>
<p>Orthonormal bases and orthogonal projections are two of the most important concepts in linear algebra. An orthonormal basis is a set of vectors that are all perpendicular to each other, and an orthogonal projection is a way of computing the projection of a vector onto an orthonormal basis.</p>
<p>In mathematics, an orthonormal basis is a special type of basis for a vector space. It consists of mutually orthogonal unit vectors that are also normalized, meaning that they have a length of 1. An orthogonal projection is a transformation of a vector onto another vector that is perpendicular to it. In other words, it is the process of projecting one vector onto another vector that is at right angles to it. Orthonormal bases and orthogonal projections are important in many areas of mathematics, including linear algebra and geometry.</p>
<p>Together, these concepts allow us to decompose any vector into a series of simple, linearly independent components. This can be helpful for many purposes, including data compression, error correction, and signal processing. In this book, we will discuss these concepts in more detail and show how they can be used in practice.</p>
<p>Orthonormal bases and orthogonal projections are closely related concepts, and understanding both can be helpful in a variety of contexts.</p>
<p>The Gram-Schmidt process is a method for orthogonalizing a set of vectors. It is commonly used in numerical analysis and in particular, in the QR factorization of a matrix.</p>
<p>The Gram-Schmidt process begins with a set of linearly independent vectors <span class="math inline">\(u_1,u_2,...,u_n\)</span> and produces a set of orthonormal vectors <span class="math inline">\(v_1,v_2,...,v_n\)</span> that span the same space as the original vectors.</p>
<p>The <span class="math inline">\(v\)</span>’s are constructed iteratively as follows: For <span class="math inline">\(k = 1,2,...,n\)</span>, let <span class="math inline">\(v_k\)</span> be the vector <span class="math inline">\(u_k - \text{proj}(v_1) - \text{proj}(v_2) - ... - \text{proj}(v_k)-1\)</span> where <span class="math inline">\(\text{proj}(v_j)\)</span> is the projection of <span class="math inline">\(u_k\)</span> onto <span class="math inline">\(v_j\)</span>.</p>
<p>Once the v’s have been computed, they can be used to obtain the QR factorization of any matrix <span class="math inline">\(A\)</span> as follows: <span class="math inline">\(A = QR\)</span> where <span class="math inline">\(Q\)</span> is an <span class="math inline">\(n×n\)</span> matrix whose columns are the <span class="math inline">\(v\)</span>’s and <span class="math inline">\(R\)</span> is an upper triangular matrix.</p>
<p>The Gram-Schmidt process is sometimes also called the QR algorithm. The name comes from its use in computing the QR factorization of a matrix. However, it should be noted that the Gram-Schmidt process can be used to orthogonalize any set of vectors, not just those that arise from a matrix.</p>
<p>In general, the Gram-Schmidt process is a reliable QR factorization method when used with care.</p>
<p>In mathematics, an orthogonal transformation is a linear transformation that preserves the inner product of vectors. In other words, it preserves angles between vectors. If a transformation is both orthogonal and preserves length, then it is also an isometry. Orthogonal transformations are also known as rigid motions or rotations.</p>
<p>An orthogonal matrix is a square matrix whose columns and rows are orthogonal unit vectors. That is, the transpose of the matrix is equal to its inverse. Every orthogonal matrix has determinant +1 or <span class="math inline">\(-1\)</span>, since the determinant of a matrix is equal to the product of its eigenvalues.</p>
<p>Orthogonal transformations arise naturally in many areas of mathematics and physics. For example, they are used to define rotations in space, and they can be used to diagonalize Hermitian matrices. In addition, they are used in many numerical algorithms, such as the QR factorization algorithm for solving linear least squares problems.</p>
<p>Orthogonal matrices have many interesting properties, including the fact that their eigenvectors are always orthogonal to one another. This makes them particularly useful for many applications involving signal processing and image compression.</p>
<p>An inner-product is a mathematical operation that takes two vectors and produces a scalar result. The most common type of inner-product is the dot-product, which simply multiplies the corresponding components of the two vectors and sums the results.</p>
<p>The dot-product can be used to calculate the magnitude of a vector, as well as the angle between two vectors. More generally, an inner-product can be any symmetric bilinear map from a vector space to its underlying field.</p>
<p>Inner-products are often used in machine learning and statistics, where they can be used to measure similarity between data points.</p>
<p>In physics, inner-products are used to define Hermitian operators, which are important in quantum mechanics.</p>
<p>Orthogonality is a powerful mathematical tool that has many applications in physics, engineering, and numerical analysis. In this book, you’ll learn how the Gram-Schmidt process can be used to orthogonalize a set of vectors, and how orthogonal matrices can be used to define rotations in space. We have also seen how inner-products can be used to measure similarity between vectors, and to define important operators in physics. With this book as your guide, you should now have a good understanding of the basics of orthogonality and its many uses.</p>
<section id="orthonormal-bases-and-orthogonal-projections" class="level2" data-number="4.1">
<h2 data-number="4.1" class="anchored" data-anchor-id="orthonormal-bases-and-orthogonal-projections"><span class="header-section-number">4.1</span> Orthonormal Bases and Orthogonal Projections</h2>
<p>The <strong>norm</strong> of a vector <span class="math inline">\(\vec v\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> is <span class="math inline">\(\norm{\vec v}=\sqrt{\vec v \cdot \vec v}\)</span>. A vector <span class="math inline">\(\vec u\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> is called a <strong>unit vector</strong> if <span class="math inline">\(\norm{\vec u}=1\)</span>.</p>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.1 </strong></span>If <span class="math inline">\(\vec v\in \mathbb{R}^n\)</span> and <span class="math inline">\(k\)</span> is a scalar, then <span class="math inline">\(\norm{k \vec v}=|k| \norm{\vec v}\)</span>, and if <span class="math inline">\(v\)</span> is nonzero then <span class="math inline">\(\vec u=\frac{1}{\norm{\vec v}} \vec v\)</span> is a unit vector. Since <span class="math inline">\(\norm{k \vec v}^2=(k \vec v)\cdot (k \vec v)=k^2(\vec v \cdot \vec v)=k^2\norm{\vec v}^2\)</span>, taking square roots provides <span class="math inline">\(\norm{k \vec v}=|k| \norm{\vec v}\)</span>. If <span class="math inline">\(v\)</span> is nonzero, then <span class="math inline">\(\frac{1}{\norm{\vec v}}\)</span> is defined and so <span class="math inline">\(\norm{\vec u}= \frac{1}{\norm{\vec v}} \norm{\vec v}=1\)</span> which follows by the first part.</p>
</div>
<p>Two vectors <span class="math inline">\(\vec v\)</span> and <span class="math inline">\(\vec w\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> are called <strong>perpendicular</strong> or <strong>orthogonal</strong> if <span class="math inline">\(\vec v \cdot \vec w=0\)</span>. The vectors <span class="math inline">\(\vec u_1,\ldots,\vec u_m\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> are called <strong>orthonormal</strong> if they are all unit vectors and orthogonal to one another. A basis of <span class="math inline">\(\mathbb{R}^n\)</span> consisting only of orthonormal vectors is called an <strong>orthonormal basis</strong>.</p>
<div id="thm-orthonormal-vectors" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.1 (Orthonormal Vectors) </strong></span>Orthonormal vectors are linearly independent, and thus orthonormal vectors <span class="math inline">\(\vec u_1,\ldots,\vec u_n\in \mathbb{R}^n\)</span> form a basis for <span class="math inline">\(\mathbb{R}^n\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\((\vec u_1,\ldots,\vec u_m)\)</span> are orthonormal vectors in <span class="math inline">\(\mathbb{R}^n\)</span>. To show linear independence suppose,<br>
<span class="math display">\[
c_1 \vec u_1+\cdots + c_m \vec u_m=\vec 0
\]</span> for some scalars <span class="math inline">\(c_1,\ldots,c_m\)</span> in <span class="math inline">\(\mathbb{R}\)</span>. Applying the dot product with <span class="math inline">\(\vec u_i\)</span>, <span class="math display">\[
\left ( c_1 \vec u_1 + \cdots +c_m \vec u_m\right ) \cdot \vec u_i =\vec 0 \cdot \vec u_i=0.
\]</span> Because the dot product is distributive, <span class="math inline">\(c_1(\vec u_1 \cdot \vec u_i )+\cdots + c_m (\vec u_m \cdot \vec u_i)=0\)</span>. We know that <span class="math inline">\(\vec u_i\cdot \vec u_i=1\)</span> and all other dot products are zero. Therefore, <span class="math inline">\(c_i=0\)</span>. Since this holds for all <span class="math inline">\(i=1,\ldots,m\)</span>, it follows that <span class="math inline">\(c_1=\cdots =c_m=0\)</span>, and therefore, <span class="math inline">\((\vec u_1,\ldots,\vec u_m)\)</span> are linearly independent. The second part follows since <span class="math inline">\(n\)</span> linearly independent vectors in <span class="math inline">\(\mathbb{R}^n\)</span> always forms a basis.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.2 </strong></span>Find three examples of an orthonormal basis for a subspace they span. The vectors <span class="math inline">\(\vec e_1,\ldots, \vec e_m\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> form an orthonormal basis of the subspace they span. For any scalar <span class="math inline">\(\theta\)</span>, the vectors <span class="math inline">\(\vectortwo{\cos \theta}{\sin \theta}\)</span>, <span class="math inline">\(\vectortwo{-\sin\theta}{\cos \theta}\)</span> form an orthonormal basis of <span class="math inline">\(\mathbb{R}^2\)</span>. The vectors <span class="math display">\[
\begin{array}{ccc}
\vec u_1=\vectorfour{1/2}{1/2}{1/2}{1/2}, &amp;
\vec u_2=\vectorfour{1/2}{1/2}{-1/2}{-1/2}, &amp;
\vec u_3=\vectorfour{1/2}{-1/2}{1/2}{-1/2}
\end{array}
\]</span> in <span class="math inline">\(\mathbb{R}^4\)</span> form an orthonormal basis of the subspace they span.</p>
</div>
<p>Let <span class="math inline">\(V\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>. The <strong>orthogonal complement</strong> <span class="math inline">\(V^\perp\)</span> of <span class="math inline">\(V\)</span> is the set of those vectors <span class="math inline">\(\vec x\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span> that are orthogonal to all vectors in <span class="math inline">\(V\)</span> namely, <span class="math display">\[\begin{equation}
V^{\perp}=\{ \vec x \in \mathbb{R}^n \mid \vec v \cdot \vec x =0, \text{ for all } \vec v \text{ in } V\}.
\end{equation}\]</span> It is easy to verify that <span class="math inline">\(V^\perp\)</span> is always a subspace and that <span class="math inline">\((\mathbb{R}^n)^\perp=\{0\}\)</span> and <span class="math inline">\(\{0\}^\perp=\mathbb{R}^n\)</span>. Also notice that if <span class="math inline">\(U_1\subseteq U_2\)</span> then <span class="math inline">\(U_2^\perp \subseteq U_1^\perp\)</span>. If <span class="math inline">\(\vec x\in V^{\perp}\)</span> then <span class="math inline">\(\vec x\)</span> is said to be <strong>perpendicular</strong> to <span class="math inline">\(V\)</span>. The vector <span class="math inline">\(\vec x^{\parallel}\)</span> in the following theorem is called the <strong>orthogonal projection</strong> of <span class="math inline">\(\vec x\)</span> on a subspace <span class="math inline">\(V\)</span> of <span class="math inline">\(\mathbb{R}^n\)</span> and is denoted by <span class="math inline">\(\text{proj}_V (\vec x)\)</span>.</p>
<div id="thm-orthogonal-projection" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.2 (Orthogonal Projection) </strong></span>&nbsp;</p>
<ul>
<li>If <span class="math inline">\(V\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span> and <span class="math inline">\(\vec x\in \mathbb{R}^n\)</span>, then <span class="math inline">\(\vec x=\vec x^\parallel + \vec x^\perp\)</span> where <span class="math inline">\(\vec x^\perp\)</span> is perpendicular to <span class="math inline">\(V\)</span>, and this representation is unique.</li>
<li>If <span class="math inline">\(V\)</span> is a subspace of <span class="math inline">\(\mathbb{R}^n\)</span> with an orthonormal basis <span class="math inline">\(\vec u_1 ,\ldots, \vec u_m\)</span>, then <span class="math display">\[\begin{equation}
\mathrm{proj}_V (\vec x):=\vec x^\parallel = (\vec u_1 \cdot \vec x) \vec u_1 + \cdots + (\vec u_m \cdot \vec x) \vec u_m
\end{equation}\]</span> for all <span class="math inline">\(\vec x\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span>.</li>
<li>Let <span class="math inline">\(\vec u_1 ,\ldots, \vec u_n\)</span> be an orthonormal basis in <span class="math inline">\(\mathbb{R}^n\)</span>, then <span class="math display">\[\begin{equation}
\vec x = (\vec u_1 \cdot \vec x) \vec u_1 + \cdots + (\vec u_n \cdot \vec x) \vec u_n
\end{equation}\]</span> for all <span class="math inline">\(\vec x\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is left for the reader.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.3 </strong></span>Find the orthogonal projection of <span class="math inline">\(\vectorthree{49}{49}{49}\)</span> onto the subspace of <span class="math inline">\(\mathbb{R}^3\)</span> spanned by <span class="math inline">\(\vectorthree{2}{3}{6}\)</span> and <span class="math inline">\(\vectorthree{3}{-6}{2}\)</span>. The two given vectors spanning the subspace are orthogonal since <span class="math inline">\(2(3)+3(-6)+6(2)=0\)</span>, but they are not unit vectors since both have length 7. To obtain an orthonormal basis <span class="math inline">\(\vec u_1, \vec u_2\)</span> of the subspace, we divide by 7: <span class="math inline">\(\vec u_1=\frac{1}{7}\vectorthree{2}{3}{6}\)</span> and <span class="math inline">\(\vec u_2=\frac{1}{7}\vectorthree{3}{-6}{2}\)</span>. Now we can use <span class="math inline">\(\eqref{Orthogonal Projection}\)</span> with <span class="math inline">\(\vec x=\vectorthree{49}{49}{49}\)</span>. Then <span class="math display">\[
\text{proj}_V(\vec x)=(\vec u_1 \cdot \vec x)\vec u_1+(u_2\cdot \vec x) \vec u_2=
11 \vectorthree{2}{3}{6}+(-1)\vectorthree{3}{-6}{2}= \vectorthree{19}{39}{64}.
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.4 </strong></span>Find the coordinates of the vector <span class="math inline">\(\vec x=\vectorfour{4}{5}{6}{7}\)</span> with respect to the orthonormal basis <span class="math display">\[
\begin{array}{cccc}
\vec u_1=\vectorfour{1/2}{1/2}{1/2}{1/2}, &amp;
\vec u_2=\vectorfour{1/2}{1/2}{-1/2}{-1/2}, &amp;
\vec u_3=\vectorfour{1/2}{-1/2}{1/2}{-1/2}, &amp;  
\vec u_4=\vectorfour{1/2}{-1/2}{-1/2}{1/2}.
\end{array}
\]</span> Normally to find the coordinates of <span class="math inline">\(\vec x\)</span> we would solve the system <span class="math display">\[
\vectorfour{4}{5}{6}{7}=
c_1 \vectorfour{1/2}{1/2}{1/2}{1/2}+
c_2 \vectorfour{1/2}{1/2}{-1/2}{-1/2}+
c_3 \vectorfour{1/2}{-1/2}{1/2}{-1/2} +
c_4 \vectorfour{1/2}{-1/2}{-1/2}{1/2}
\]</span> for <span class="math inline">\(c_1, c_2, c_3, c_4\)</span>. However we can use <span class="math inline">\(\eqref{Orthogonal Projection}\)</span> instead: <span class="math display">\[
c_1=u_1\cdot \vec x=\vectorfour{1/2}{1/2}{1/2}{1/2} \cdot \vectorfour{4}{5}{6}{7}=11,
\]</span> <span class="math display">\[
c_2=u_2\cdot \vec x=\vectorfour{1/2}{1/2}{-1/2}{-1/2} \cdot \vectorfour{4}{5}{6}{7}=-2,
\]</span> <span class="math display">\[
c_3=u_3\cdot \vec x=\vectorfour{1/2}{-1/2}{1/2}{-1/2} \cdot \vectorfour{4}{5}{6}{7}=-1,
\]</span> <span class="math display">\[
c_4=u_4\cdot \vec x=\vectorfour{1/2}{-1/2}{-1/2}{1/2} \cdot \vectorfour{4}{5}{6}{7}=0.
\]</span> Therefore the <span class="math inline">\(\mathcal{B}\)</span>-coordinate vector of <span class="math inline">\(\vec x\)</span> is <span class="math inline">\(\vectorfour{11}{-2}{-1}{0}\)</span>.</p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.3 (Properties of the Orthogonal Complement) </strong></span>Let <span class="math inline">\(V\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span>, then</p>
<ul>
<li><span class="math inline">\(\mathrm{proj}_V(\vec x)\)</span> is a linear transformation <span class="math inline">\(\mathbb{R}^n\to V\)</span> with kernel <span class="math inline">\(V^\perp\)</span>,</li>
<li><span class="math inline">\(V \cap V^\perp = \{\vec 0\}\)</span>,</li>
<li><span class="math inline">\(\text{dim} V +\text{dim} V^\perp=n\)</span>, and</li>
<li><span class="math inline">\((V^\perp)^\perp = V\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof of each part follows.</p>
<ul>
<li>To prove the linearity of <span class="math inline">\(T(x):=\mathrm{proj}_V(\vec x)\)</span> we will use the definition of a projection: <span class="math inline">\(T(\vec x)\)</span> is in <span class="math inline">\(V\)</span>, and <span class="math inline">\(\vec x-T(\vec x)\)</span> is in <span class="math inline">\(V^\perp\)</span>. To show <span class="math inline">\(T(\vec x+\vec y)=T(\vec x)+T(\vec y)\)</span>, notice <span class="math inline">\(T(\vec x)+T(\vec y)\)</span> is in <span class="math inline">\(V\)</span> (since <span class="math inline">\(V\)</span> is a subspace), and <span class="math inline">\(\vec x+\vec y-(T(\vec x)+T(\vec y))=(\vec x-T(\vec x))+(\vec y-T(\vec y))\)</span> is in <span class="math inline">\(V^\perp\)</span> (since <span class="math inline">\(V^\perp\)</span> is a subspace). To show that <span class="math inline">\(T(k \vec x)=k T(\vec x)\)</span>, note that <span class="math inline">\(k T(\vec x)\)</span> is in <span class="math inline">\(V\)</span> (since <span class="math inline">\(V\)</span> is a subspace) and <span class="math inline">\(k \vec x-k T(\vec x)=k(\vec x-T(\vec x))\)</span> is in <span class="math inline">\(V^\perp\)</span> (since <span class="math inline">\(V^\perp\)</span> is a subspace).</li>
<li>Since <span class="math inline">\(\{\vec 0\} \subseteq V\)</span> and <span class="math inline">\(\{\vec 0\} \subseteq V^\perp\)</span>, <span class="math inline">\(\{\vec 0\} \subseteq V\cap V^\perp.\)</span> If a vector <span class="math inline">\(\vec x\)</span> is in <span class="math inline">\(V\)</span> as well as in <span class="math inline">\(V^\perp\)</span>, then <span class="math inline">\(\vec x\)</span> is orthogonal to itself: <span class="math inline">\(\vec x \cdot \vec x =\norm{x}^2=0\)</span>, so that <span class="math inline">\(\vec x\)</span> must equal <span class="math inline">\(\vec 0\)</span> which shows <span class="math inline">\(V \subseteq \{\vec 0\}\)</span>. Therefore, <span class="math inline">\(V\cap V^\perp=\{\vec 0\}\)</span>.</li>
<li>Apply the Rank-Nullity Theorem to the linear transformation <span class="math inline">\(T(\vec x)=\text{proj}_V(\vec x)\)</span> yielding <span class="math inline">\(n=\text{dim} \mathbb{R}^n =\text{dim} \text{image} T+\text{dim} \ker T=\text{dim} V+\text{dim} V^\perp\)</span>.</li>
<li>Let <span class="math inline">\(\vec v\in V\)</span>. Then <span class="math inline">\(\vec v\cdot \vec x=0\)</span> for all <span class="math inline">\(\vec x\)</span> in <span class="math inline">\(V^\perp\)</span>. Since <span class="math inline">\((V^\perp)^\perp\)</span> contains all vectors <span class="math inline">\(\vec y\)</span> such that <span class="math inline">\(\vec y \cdot \vec x =0\)</span>, <span class="math inline">\(\vec v\)</span> is in <span class="math inline">\((V^\perp)^\perp\)</span>. So <span class="math inline">\(V\)</span> is a subspace of <span class="math inline">\((V^\perp)^\perp\)</span>. Using (iii) with <span class="math inline">\(V\)</span> (<span class="math inline">\(n=\text{dim} V+\text{dim} V^\perp\)</span>) and again with <span class="math inline">\(V^\perp\)</span> (<span class="math inline">\(\text{dim} V^\perp+\text{dim} (V^\perp)^\perp\)</span>) yielding <span class="math inline">\(\text{dim} V=\text{dim} (V^\perp)^\perp\)</span>; and since <span class="math inline">\(V\)</span> is a subspace of <span class="math inline">\((V^\perp)^\perp\)</span> it follows that <span class="math inline">\(V=(V^\perp)^\perp\)</span>.<br>
</li>
</ul>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.5 </strong></span>Find a basis for <span class="math inline">\(W^\perp\)</span>, where <span class="math inline">\(W=\text{span} \left( \vectorfour{1}{2}{3}{4}, \vectorfour{5}{6}{7}{8} \right)\)</span>. The orthogonal complement <span class="math inline">\(W^\perp\)</span> of <span class="math inline">\(W\)</span> consists of the vectors <span class="math inline">\(\vec x\)</span> in <span class="math inline">\(\mathbb{R}^4\)</span> such that <span class="math display">\[
\vectorfour{x_1}{x_2}{x_3}{x_4} \cdot \vectorfour{1}{2}{3}{4}
=0 \hspace{1cm}\text{and}  
\vectorfour{x_1}{x_2}{x_3}{x_4} \cdot \vectorfour{5}{6}{7}{8}=0.
\]</span> Finding these vectors amounts to solving the system <span class="math display">\[
\begin{cases}
x_1+2x_2+3x_3+4x_4&amp;=0
\\ 5x_1+6x_2+7x_3+8x_4 &amp; =0
\end{cases}
\]</span> The solutions are of the form <span class="math display">\[
\vectorfour{x_1}{x_2}{x_3}{x_4}=\vectorfour{s+2t}{-2s-3t}{s}{t}=s\vectorfour{1}{-2}{1}{0}+t\vectorfour{2}{-3}{0}{1}.
\]</span> The two vectors on the right form a basis of <span class="math inline">\(W^{\perp}\)</span>.</p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.4 </strong></span>Let <span class="math inline">\(\vec x, \vec y \in \mathbb{R}^n\)</span>. Then</p>
<ul>
<li>( <strong>Pythagorean Theorem</strong>) <span class="math display">\[
||\vec x + \vec y || ^2 = || \vec x|| ^2 + || \vec y || ^2
\]</span> holds if and only if <span class="math inline">\(\vec x \perp \vec y\)</span>,</li>
<li>( <strong>Cauchy-Schwarz</strong>) <span class="math display">\[
| \vec x \cdot \vec y | \leq || \vec x || \, || \vec y ||
\]</span> where equality holds if and only if <span class="math inline">\(\vec x\)</span> and <span class="math inline">\(\vec y\)</span> are parallel,</li>
<li>( <strong>Law of Cosines</strong>) the angle <span class="math inline">\(\theta\)</span> between <span class="math inline">\(\vec x\)</span> and <span class="math inline">\(\vec y\)</span> is defined as <span class="math display">\[
\theta = \arccos \frac{\vec x \cdot \vec y}{|| \vec x || \, || \vec y||},
\]</span></li>
<li>( <strong>Triangular Inequality</strong>) <span class="math display">\[
\norm{\vec x+\vec y}\leq \norm{\vec x}+\norm{\vec y}.
\]</span></li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof of each part follows.</p>
<ul>
<li>The verification is straightforward: <span class="math display">\[\begin{align*}
\norm{\vec x+\vec y }^2
&amp; =(\vec x + \vec y)\cdot (\vec x + \vec y)
=\vec x \cdot \vec x+2(\vec x \cdot \vec y) +\vec y \cdot \vec y \\
&amp; =\norm{\vec x}^2+2(\vec x\cdot \vec y)+\norm{\vec y}^2
=\norm{\vec x}^2+\norm{\vec y}^2
\end{align*}\]</span> where the last equality holds if and only if <span class="math inline">\(\vec x\cdot \vec y=0\)</span>.</li>
<li>Let <span class="math inline">\(V\)</span> be a one-dimensional subspace of <span class="math inline">\(\mathbb{R}^n\)</span> spanned by a nonzero vector <span class="math inline">\(\vec y\)</span>. Let <span class="math inline">\(\vec u=\frac{1}{\norm{\vec y}} \vec y\)</span>. Then <span class="math display">\[
\norm{\vec x}
\geq \norm{\text{proj}_V(\vec x)}
=\norm{(\vec x\cdot \vec u)\vec u}
=|\vec x\cdot \vec u|
=\left| \vec x \cdot \left( \frac{1}{\norm{y}}\vec y \right)\right|
= \frac{1}{\norm{y}}|\vec x \cdot \vec y|
\]</span> multiplying by <span class="math inline">\(\norm{\vec y}\)</span>, yields <span class="math inline">\(| \vec x \cdot \vec y | \leq || \vec x || \, || \vec y ||\)</span>. Notice that <span class="math inline">\(\norm{\vec x} \geq \norm{\text{proj}_V(\vec x)}\)</span> holds by applying the Pythagorean theorem to <span class="math inline">\(\vec x=\vec x^\parallel +\vec x^\perp\)</span> with <span class="math inline">\(\vec x^\perp \cdot \vec x^\parallel =0\)</span> so that <span class="math inline">\(\norm{\vec x}^2=\norm{\text{proj}_V(\vec x)}^2+\norm{\vec x^\perp}^2\)</span> which leads to <span class="math inline">\(\norm{\text{proj}_V \vec x} \leq \norm{\vec x}\)</span>.</li>
<li>We have to make sure that <span class="math inline">\(\eqref{law-of-cosines}\)</span> is defined, that is <span class="math inline">\(\theta\)</span> is between <span class="math inline">\(-1\)</span> and <span class="math inline">\(1\)</span>, or equivalently, <span class="math display">\[
\left | \frac{\vec x \cdot \vec y }{\norm{x} \, \norm{y}} \right | \leq 1.
\]</span> But this follows from the Cauchy-Schwarz inequality.</li>
<li>Using the Cauchy-Schwarz inequality, the verification is straightforward, <span class="math display">\[
\norm{\vec x+\vec y}^2=(\vec x + \vec y)\cdot (\vec x +  \vec y)=\norm{\vec x}^2+\norm{\vec y}^2+2(\vec x\cdot \vec y)
\]</span> <span class="math display">\[
\leq \norm{\vec x}^2+\norm{\vec y}^2+2\norm{\vec x}\norm{\vec y}=(\norm{\vec x}+\norm{\vec y})^2
\]</span> Taking the square root of both sides yields <span class="math inline">\(\norm{\vec x+\vec y}\leq \norm{\vec x}+\norm{\vec y}\)</span>.</li>
</ul>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.6 </strong></span>Determine whether the angle between the vectors <span class="math inline">\(\vec u=\vectorthree{2}{3}{4}\)</span>, <span class="math inline">\(\vec v=\vectorthree{2}{-8}{5}\)</span> is a right angle using the Pythagorean Theorem. Since <span class="math inline">\(\norm{\vec u}=\sqrt{2^2+3^2+4^2}=\sqrt{29}\)</span> and <span class="math inline">\(\norm{\vec v}=\sqrt{2^2+(-8)^2+5^2}=\sqrt{93}\)</span>. Then <span class="math display">\[
\norm{\vec u+\vec v}^2=\left|\left| \, \, \vectorthree{4}{-5}{9} \, \,
\right| \right|^2 =122 = 29+93= || \vec u|| ^2 + || \vec v || ^2
\]</span> shows <span class="math inline">\(\vec u \perp \vec v\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.7 </strong></span>Consider the vectors <span class="math inline">\(\vec u =\vectorfour{1}{1}{\vdots}{1}\)</span> and <span class="math inline">\(\vec v=\vectorfour{1}{0}{\vdots}{0}\)</span> in <span class="math inline">\(\mathbb{R}^n\)</span>. For <span class="math inline">\(n=2,3,4\)</span>, find the angle <span class="math inline">\(\theta\)</span> between <span class="math inline">\(\vec u\)</span> and <span class="math inline">\(\vec v\)</span>. Then find the limit of <span class="math inline">\(\theta\)</span> as <span class="math inline">\(n\)</span> approaches infinity. For any possible value of <span class="math inline">\(n\)</span>, <span class="math display">\[
\theta_n=\arccos \frac{\vec u \cdot \vec v}{\norm{\vec u}\norm{\vec v}}=\arccos \frac{1}{\sqrt{n}}.
\]</span> Then <span class="math display">\[
\begin{bmatrix}
\theta_2=\arccos \frac{1}{\sqrt{2}}=\frac{\pi}{4}, &amp;
\hspace{.5cm}  \theta_3=\arccos \frac{1}{\sqrt{3}}=\frac{\pi}{4}\sim 0.955 \text{ rads}, &amp;
\hspace{.5cm} \theta_4=\arccos \frac{1}{\sqrt{4}}=\frac{\pi}{3}.
\end{bmatrix}
\]</span> Since <span class="math inline">\(y=\arccos(x)\)</span> is a continuous function, <span class="math display">\[
\lim_{x\mapsto \infty} \theta_n
= \lim_{x\mapsto \infty} \arccos\left( \frac{1}{\sqrt{n}} \right)
= \arccos \left( \lim_{x\mapsto \infty} \frac{1}{\sqrt{n}} \right)
= \arccos(0)
=\frac{\pi}{2}.
\]</span></p>
</div>
</section>
<section id="gram-schmidt-process-and-qr-factorization" class="level2" data-number="4.2">
<h2 data-number="4.2" class="anchored" data-anchor-id="gram-schmidt-process-and-qr-factorization"><span class="header-section-number">4.2</span> Gram-Schmidt Process and QR Factorization</h2>
<p>The Gram-Schmidt process represents a change of basis from a basis <span class="math inline">\(\mathcal{B}=(\vec v_1, \vec v_2, ..,,\vec v_m)\)</span> of a subspace <span class="math inline">\(V\)</span> of <span class="math inline">\(\mathbb{R}^n\)</span> to an orthonormal basis <span class="math inline">\(\mathcal{U}=(\vec u_1, \vec u_2, \ldots,\vec u_m)\)</span> of <span class="math inline">\(V\)</span>; it is most sufficiently described in terms of the change of basis matrix <span class="math inline">\(R\)</span> from <span class="math inline">\(\mathcal{B}\)</span> to <span class="math inline">\(\mathcal{U}\)</span> via, <span class="math display">\[\begin{equation}
M:=\begin{bmatrix} \vec v_1 &amp; \vec v_2 &amp; \cdots &amp; \vec v_m \end{bmatrix}
=\begin{bmatrix} \vec u_1 &amp;\vec u_2 &amp; \cdots &amp; \vec u_m \end{bmatrix}R=:QR
\end{equation}\]</span></p>
<p>The <span class="math inline">\(QR\)</span> factorization is an effective way to organize and record the work performed in the Gram-Schmidt process; it is also useful for many computational and theoretical purposes.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.5 (Gram-Schmidt Process) </strong></span>Let <span class="math inline">\(\vec v_1, \vec v_2, \ldots, \vec v_m\)</span> be a basis of a subspace <span class="math inline">\(V\)</span> of <span class="math inline">\(\mathbb{R}^n\)</span>. Then <span class="math display">\[\begin{equation}
\begin{array}{cccc}
\vec u_1 = \frac{1}{||\vec v_1 || } \vec v_1,  &amp;
\vec u_2 = \frac{1}{||\vec v_2^\perp || } \vec v_2^\perp, &amp; \ldots, &amp;
\vec u_m = \frac{1}{||\vec v_m^\perp || } \vec v_m^\perp
\end{array}
\end{equation}\]</span> is an orthonormal basis of <span class="math inline">\(V\)</span> where <span class="math display">\[\begin{equation}
\vec v_j ^\perp
= \vec v_j - \vec v_j^\parallel
= \vec v_j - (\vec u_1 \cdot \vec v_j) \vec u_1 - (\vec u_2 \cdot \vec v_j) \vec u_2 - \cdots - (\vec u_{j-1} \cdot \vec v_j) \vec u_{j-1}.
\end{equation}\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>For each <span class="math inline">\(j\)</span>, we resolve the vectors <span class="math inline">\(\vec v_j\)</span> into its components parallel and perpendicular to the span of the preceding vectors <span class="math inline">\(\vec v_1, \vec v_2, \ldots, \vec v_{j-1}\)</span>: <span class="math display">\[\vec v_j = \vec v_j^\parallel + \vec v_j^\perp \hspace{1cm} \text{with respect to } \text{span}(\vec v_1, \vec v_2, \ldots, \vec v_{j-1}).\]</span> Then use <span class="math inline">\(\ref{Orthogonal Projection}\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.8 </strong></span>Perform the Gram-Schmidt process on the vectors <span class="math display">\[
\vec v_1=\vectorthree{4}{0}{3}, \qquad
\vec v_2=\vectorthree{25}{0}{-25}, \qquad
\vec v_3=\vectorthree{0}{-2}{0}.
\]</span> By Theorem <span class="math inline">\(\eqref{Gram-Schmidt Process}\)</span>, we determine <span class="math inline">\(\vec u_1, \vec u_2, \vec u_3\)</span> as follows: <span class="math display">\[
\vec u_1= \vectorthree{4/5}{0}{3/5},
\hspace{1cm}
\vec u_2=\frac{\vec v_2^\perp}{\norm{\vec v_2^\perp}}
=\frac{\vec v_2-\left(\vec u_1 \cdot \vec v_2\right) \vec u_1}{\norm{\vec v_2-\left(\vec u_1 \cdot \vec v_2\right)\vec u_1}}
=\vectorthree{3/5}{0}{-4/5},
\]</span> and <span class="math display">\[
\vec u_3=\frac{v_3^\perp}{\norm{v_3^\perp}}
=\frac{\vec v_3-\left(\vec u_1 \cdot \vec v_3\right) \vec u_1-\left(\vec u_2 \cdot \vec v_3\right) \vec u_2}{\norm{\vec v_3-\left(\vec u_1 \cdot \vec v_3\right) \vec u_1-\left(\vec u_2 \cdot \vec v_3\right) \vec u_2}}
=\vectorthree{0}{-1}{0}.
\]</span> Therefore <span class="math display">\[
\left( \vectorthree{4/5}{0}{3/5},  \vectorthree{3/5}{0}{-4/5}, \vectorthree{0}{-1}{0}\right).
\]</span> is an orthonormal basis for <span class="math inline">\(\text{span} (\vec v_1, \vec v_2, \vec v_3)\)</span>.</p>
</div>
<p>::: {#thm- } [QR Factorization] Let <span class="math inline">\(M\)</span> be an <span class="math inline">\(n \times m\)</span> matrix with linearly independent columns <span class="math inline">\(\vec v_1 , \vec v_2, \ldots,\vec v_m\)</span>. Then there exists an <span class="math inline">\(n\times m\)</span> matrix <span class="math inline">\(Q\)</span> whose columns <span class="math inline">\(\vec u_1, \vec u_2, \ldots, \vec u_m\)</span> are orthonormal and an upper triangular matrix <span class="math inline">\(R\)</span> with positive diagonal entries such that <span class="math inline">\(M=Q R\)</span>; and this representation is unique. Furthermore, the entries <span class="math inline">\(r_{ij}\)</span> of <span class="math inline">\(R\)</span> are given by</p>
<ul>
<li><span class="math inline">\(r_{11}=\norm{\vec v_1}\)</span>,</li>
<li><span class="math inline">\(r_{jj}=||\vec v_j^\perp||\)</span> (for <span class="math inline">\(j=2,\ldots,m\)</span>), and</li>
<li><span class="math inline">\(r_{i j}=\vec u_i \cdot \vec v_j\)</span> (for <span class="math inline">\(i&lt;j\)</span>). :::</li>
</ul>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is left for the reader.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.9 </strong></span>Find the <span class="math inline">\(QR\)</span> factorization of the matrix and display the commutative diagram. <span class="math display">\[
M=\begin{bmatrix} 4 &amp; 25 &amp; 0 \\ 0 &amp; 0 &amp; -2 \\ 3 &amp; -25 &amp; 0\end{bmatrix}
\]</span> Let <span class="math display">\[
\begin{array}{ccc}
\vec v_1=\vectorthree{4}{0}{3}, &amp;
\vec v_2=\vectorthree{25}{0}{-25}, &amp;
\vec v_3=\vectorthree{0}{-2}{0}
\end{array}
\]</span> then (as determined above) an orthonormal basis for the column vectors of <span class="math inline">\(M\)</span> is<br>
<span class="math display">\[\left(\vec u_1, \vec u_2, \vec u_3\right)=\left( \vectorthree{4/5}{0}{3/5},  \vectorthree{3/5}{0}{-4/5}, \vectorthree{0}{-1}{0}\right).\]</span> Determining the entries of <span class="math inline">\(R\)</span> (also as determined above): <span class="math display">\[
\begin{array}{ccc}
r_{11}=\norm{\vec v_1}=5, &amp;
r_{22}=\norm{\vec v_2^\perp}=35, &amp;
r_{33}=\norm{\vec v_3^\perp}=2
\end{array}
\]</span> <span class="math display">\[
\begin{array}{ccc}
r_{12}=\vec u_1 \cdot \vec v_2=5, &amp;
r_{13}=\vec u_1 \cdot \vec v_3=0, &amp;
r_{23}=\vec u_2 \cdot \vec v_3=0
\end{array}
\]</span> and therefore, the <span class="math inline">\(QR\)</span>-factorization of <span class="math inline">\(M\)</span> is <span class="math display">\[
M= \begin{bmatrix} 4 &amp; 25 &amp; 0 \\ 0 &amp; 0 &amp; -2 \\ 3 &amp; -25 &amp; 0\end{bmatrix}
= \begin{bmatrix}4/5 &amp; 3/5 &amp; 0 \\ 0 &amp; 0 &amp; -1 \\ 3/5 &amp; -4/5 &amp; 0 \end{bmatrix}
\begin{bmatrix}5 &amp; 5 &amp; 0 \\ 0 &amp; 35 &amp; 0 \\ 0 &amp; 0 &amp; 2\end{bmatrix}
=QR.
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.10 </strong></span>&nbsp;</p>
<ol type="a">
<li>Perform the Gram-Schmidt process on the vectors <span class="math display">\[
\vec v_1=\vectorfour{1}{7}{1}{7}, \qquad
\vec v_2= \vectorfour{0}{7}{2}{7}, \qquad
\vec v_3 = \vectorfour{1}{8}{1}{6}.
\]</span></li>
<li>Find the <span class="math inline">\(QR\)</span> factorization of the matrix <span class="math inline">\(M=\begin{bmatrix} \vec v_1 &amp; \vec v_2 &amp; \vec v_3\end{bmatrix}.\)</span></li>
</ol>
<p>By Theorem <span class="math inline">\(\eqref{Gram-Schmidt Process}\)</span>, we determine <span class="math inline">\(\vec u_1, \vec u_2, \vec u_3\)</span> as follows: <span class="math display">\[
\vec u_1= \vectorfour{1/10}{7/10}{1/10}{7/10},
\vec u_2=\frac{\vec v_2^\perp}{\norm{\vec v_2^\perp}}
=\frac{\vec v_2-\left(\vec u_1 \cdot \vec v_2\right) \vec u_1}{\norm{\vec v_2-\left(\vec u_1 \cdot \vec v_2\right)\vec u_1}}
=\vectorfour{-1/\sqrt{2}}{0}{1/\sqrt{2}}{0},
\]</span> and <span class="math display">\[
\vec u_3=\frac{\vec v_3^\perp}{\norm{\vec v_3^\perp}}
=\frac{\vec v_3-\left(\vec u_1 \cdot \vec v_3\right) \vec u_1-\left(\vec u_2 \cdot \vec v_3\right) \vec u_2}{\norm{\vec v_3-\left(\vec u_1 \cdot \vec v_3\right) \vec u_1-\left(\vec u_2 \cdot \vec v_3\right) \vec u_2}}
=\vectorfour{0}{1/\sqrt{2}}{0}{-1/\sqrt{2}}.
\]</span> Therefore an orthonormal basis for <span class="math inline">\(\text{span} (\vec v_1, \vec v_2, \vec v_3)\)</span> is <span class="math display">\[
\left( \vectorfour{1/10}{7/10}{1/10}{7/10},  \vectorfour{-1/\sqrt{2}}{0}{1/\sqrt{2}}{0}, \vectorfour{0}{1/\sqrt{2}}{0}{-1/\sqrt{2}}\right).
\]</span> Determining the entries of <span class="math inline">\(R\)</span> (also as determined above): <span class="math display">\[
\begin{array}{ccc}
r_{11}=\norm{\vec v_1}=10, &amp;
r_{22}=\norm{\vec v_2^\perp}=\sqrt{2}, &amp;
r_{33}=\norm{\vec v_3^\perp}=\sqrt{2}
\end{array}
\]</span> <span class="math display">\[
\begin{array}{ccc}
r_{12}=\vec u_1 \cdot \vec v_2=10, &amp;
r_{13}=\vec u_1 \cdot \vec v_3=10, &amp;
r_{23}=\vec u_2 \cdot \vec v_3=0
\end{array}
\]</span> and therefore, the <span class="math inline">\(QR\)</span>-factorization of <span class="math inline">\(M\)</span> is <span class="math display">\[
M=\begin{bmatrix} 1 &amp; 0 &amp; 1 \\ 7 &amp; 7 &amp; 8 \\ 1 &amp; 2 &amp; 1 \\ 7 &amp; 7 &amp; 6 \end{bmatrix}
= \begin{bmatrix}1/10 &amp; -1/\sqrt{2} &amp; 0 \\ 7/10 &amp; 0 &amp; 1/\sqrt{2} \\ 1/10 &amp; 1/\sqrt{2} &amp; 0 \\ 7/10 &amp; 0 &amp; -1/\sqrt{2}\end{bmatrix}
\begin{bmatrix}10 &amp; 10 &amp; 10 \\ 0 &amp; \sqrt{2} &amp; 0 \\ 0 &amp; 0 &amp; \sqrt{2}\end{bmatrix}
=QR.
\]</span></p>
</div>
</section>
<section id="orthogonal-transformations-and-orthogonal-matrices" class="level2" data-number="4.3">
<h2 data-number="4.3" class="anchored" data-anchor-id="orthogonal-transformations-and-orthogonal-matrices"><span class="header-section-number">4.3</span> Orthogonal Transformations and Orthogonal Matrices</h2>
<p>A linear transformation <span class="math inline">\(T\)</span> from <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(\mathbb{R}^n\)</span> is called an <strong>orthogonal transformation</strong> if it preserves the length of vectors: <span class="math inline">\(\norm{T(\vec x)}=\norm{x}\)</span> for all <span class="math inline">\(\vec x\in \mathbb{R}^n.\)</span> If <span class="math inline">\(T(\vec x)=A\vec x\)</span> is an orthogonal transformation, we say <span class="math inline">\(A\)</span> is an <strong>orthogonal matrix</strong> .</p>
<div id="lem-orthogonal-transformation" class="theorem lemma">
<p><span class="theorem-title"><strong>Lemma 4.1 </strong></span>Let <span class="math inline">\(T\)</span> be an orthogonal transformation from <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(\mathbb{R}^n\)</span>. If <span class="math inline">\(\vec v, \vec w \in \mathbb{R}^n\)</span> are orthogonal, then <span class="math inline">\(T(\vec v), T(\vec w) \in \mathbb{R}^n\)</span> are orthogonal.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>We want to show <span class="math inline">\(T(\vec v), T(\vec w)\)</span> are orthogonal, and by the Pythagorean theorem, we have to show <span class="math display">\[
\norm{T(\vec v)+T(\vec w)}^2=\norm{T(\vec v)}^2+\norm{T(\vec w)}^2.
\]</span> This equality follows <span class="math display">\[\begin{align*}
\norm{T(\vec v)+T(\vec w)}^2
&amp; =\norm{T(\vec v+\vec w)}^2
=\norm{\vec v+\vec w}^2 \\
&amp; =\norm{\vec v}^2+\norm{\vec w}^2
=\norm{T(\vec v)}^2+\norm{T(\vec w)}^2
\end{align*}\]</span> since <span class="math inline">\(T\)</span> is linear, orthogonal and that <span class="math inline">\(\vec v, \vec w\)</span> are orthogonal, respectively.</p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.6 </strong></span>A linear transformation <span class="math inline">\(T\)</span> from <span class="math inline">\(\mathbb{R}^n\)</span> to <span class="math inline">\(\mathbb{R}^n\)</span> is orthogonal if and only if the vectors <span class="math inline">\(T(\vec e_1)\)</span>, , <span class="math inline">\(T(\vec e_n)\)</span> form an orthonormal basis.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(T\)</span> is an orthogonal transformation, then by definition, the <span class="math inline">\(T(\vec e_i)\)</span> are unit vectors, and also, by <span class="math inline">\(\ref{orthogonal transformation}\)</span> they are orthogonal. Therefore, <span class="math inline">\(T(\vec e_1)\)</span>, , <span class="math inline">\(T(\vec e_n)\)</span> form an orthonormal basis.</p>
<p>Conversely, suppose <span class="math inline">\(T(\vec e_1)\)</span>, , <span class="math inline">\(T(\vec e_n)\)</span> form an orthonormal basis. Consider a vector <span class="math inline">\(\vec x=x_1 \vec e_1+\cdots +x_n \vec e_n\)</span>. Then <span class="math display">\[\begin{align*}
\norm{T(\vec x)}^2
&amp;=\norm{T(x_1 \vec e_1+\cdots + x_n \vec e_n)}^2
=\norm{x_1 T(\vec e_1)+\cdots + x_n T(\vec e_n)}^2 \\
&amp;=\norm{x_1T(\vec e_1)}^2+\cdots + \norm{x_nT(\vec e_n)}^2 = x_1^2+\cdots + x_n^2
=\norm{x}^2.
\end{align*}\]</span> Taking the square root of both sides shows that <span class="math inline">\(T\)</span> preserves lengths and therefore, <span class="math inline">\(T\)</span> is an orthogonal transformation.</p>
</div>
<div id="cor-oob" class="theorem corollary">
<p><span class="theorem-title"><strong>Corollary 4.1 </strong></span>An <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is orthogonal if and only if its columns form an orthonormal basis.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof is left for the reader.</p>
</div>
<p>The <strong>transpose</strong> <span class="math inline">\(A^T\)</span> of an <span class="math inline">\(n\times n\)</span> matrix <span class="math inline">\(A\)</span> is the <span class="math inline">\(n\times n\)</span> matrix whose <span class="math inline">\(ij\)</span>-th entry is the <span class="math inline">\(ji\)</span>-th entry of <span class="math inline">\(A\)</span>. We say that a square matrix <span class="math inline">\(A\)</span> is <strong>symmetric</strong> if <span class="math inline">\(A^T=A\)</span>, and <span class="math inline">\(A\)</span> is called <strong>skew-symmetric</strong> if <span class="math inline">\(A^T=-A\)</span>.</p>
<div id="thm-orthogonal-and-transpose-properties" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.7 (Orthogonal and Transpose Properties) </strong></span>&nbsp;</p>
<ul>
<li>The product of two orthogonal <span class="math inline">\(n\times n\)</span> matrices is orthogonal.</li>
<li>The inverse of an orthogonal matrix is orthogonal.</li>
<li>If the products <span class="math inline">\((A B)^T\)</span> and <span class="math inline">\(B^T A^T\)</span> are defined then they are equal.</li>
<li>If <span class="math inline">\(A\)</span> is invertible then so is <span class="math inline">\(A^T\)</span>, and <span class="math inline">\((A^T)^{-1}=(A^{-1})^T\)</span>.</li>
<li>For any matrix <span class="math inline">\(A\)</span>, <span class="math inline">\(\text{rank}\,(A) = \text{rank} \,(A^T)\)</span>.</li>
<li>If <span class="math inline">\(\vec v\)</span> and <span class="math inline">\(\vec w\)</span> are two column vectors in <span class="math inline">\(\mathbb{R}^n\)</span>, then <span class="math inline">\(\vec v \cdot \vec w = \vec v^T \vec w\)</span>.</li>
<li>The <span class="math inline">\(n \times n\)</span> matrix <span class="math inline">\(A\)</span> is orthogonal if and only if <span class="math inline">\(A^{-1}=A^T\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof of each part follows.</p>
<ul>
<li>Suppose <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are orthogonal matrices, then <span class="math inline">\(AB\)</span> is an orthogonal matrix since <span class="math inline">\(T(\vec x)=AB \vec x\)</span> preserves length because <span class="math inline">\(\norm{T(\vec x)}=\norm{AB \vec x}=\norm{A(B \vec x)}=\norm{B \vec x}=\norm{\vec x}.\)</span></li>
<li>Suppose <span class="math inline">\(A\)</span> is an orthogonal matrix, then <span class="math inline">\(A^{-1}\)</span> is orthogonal an matrix since <span class="math inline">\(T(\vec x)=A^{-1} \vec x\)</span> preserves length because <span class="math inline">\(\norm{A^{-1}\vec x}=\norm{A(A^{-1}\vec x)}=\norm{\vec x}\)</span>.</li>
<li>We will compare entries in the matrices <span class="math inline">\((AB)^T\)</span> and <span class="math inline">\(B^T A^T\)</span> as follows: <span class="math display">\[
\begin{array}{rl}
i j \text{-th entry of }(AB)^T &amp;= ji \text{-th entry of }AB\\
&amp; = (j \text{-th row of } A) \cdot (i \text{-th column of } B)\\ \\
i j \text{-th entry of }B^TA^T &amp;=(i \text{-th row of } B^T) \cdot (j \text{th column of } A^T)\\
&amp; = (i \text{-th column of } B) \cdot (j \text{-th row of } A)\\
&amp; = (j \text{-th row of } A) \cdot (i \text{-th column of } B).
\end{array}
\]</span> Therefore, the <span class="math inline">\(ij\)</span>-th entry of <span class="math inline">\((AB)^T\)</span> is the same of the <span class="math inline">\(ij\)</span>-th entry of <span class="math inline">\(B^T A^T\)</span>.</li>
<li>Suppose <span class="math inline">\(A\)</span> is invertible, then <span class="math inline">\(A A^{-1}=I_n\)</span>. Taking the transpose of both sides along with (iii) it yields, <span class="math inline">\((A A^{-1})^T=(A^{-1})^T A^T=I_n\)</span>. Thus <span class="math inline">\(A^T\)</span> is invertible and since inverses are unique, it follows <span class="math inline">\((A^T)^{-1}=(A^{-1})^T\)</span>.</li>
<li>Exercise.</li>
<li>If <span class="math inline">\(\vec v=\vectorthree{a_1}{\vdots}{a_n}\)</span> and <span class="math inline">\(\vec w=\vectorthree{b_1}{\vdots}{b_n}\)</span>, then <span class="math display">\[
\vec v \cdot \vec w=\vectorthree{a_1}{\vdots}{a_n}\cdot \vectorthree{b_1}{\vdots}{b_n}=a_1b_1+\cdots +a_n b_n
=\begin{bmatrix}a_1 &amp; \cdots &amp; a_n\end{bmatrix}  \vectorthree{b_1}{\vdots}{b_n}
=\vectorthree{a_1}{\vdots}{a_n}^T \vec w=\vec v^T \vec w.
\]</span></li>
<li>Let’s write <span class="math inline">\(A\)</span> in terms of its columns: <span class="math inline">\(A=\begin{bmatrix} \vec v_1 &amp; \cdots &amp; \vec v_n \end{bmatrix}\)</span>. Then <span class="math display">\[\begin{equation}
\label{ata}
A^T A=
\begin{bmatrix}  \vec v_1^T  \\  \vdots \\  \vec v_n^T \end{bmatrix}
\begin{bmatrix} \vec v_1 &amp; \cdots &amp; \vec v_n  \end{bmatrix}
=\begin{bmatrix}\vec v_1 \cdot \vec v_1 &amp; &amp; \vec v_1 \cdot \vec v_n \\ \vdots &amp; \cdots &amp; \vdots \\ \vec v_n \cdot \vec v_1 &amp; &amp; \vec v_n \cdot \vec v_n\end{bmatrix}.
\end{equation}\]</span> Now <span class="math inline">\(A\)</span> is orthogonal, by <span class="math inline">\(\ref{oob}\)</span>, if and only if <span class="math inline">\(A\)</span> has orthonormal columns, meaning <span class="math inline">\(A\)</span> is orthogonal if and only if <span class="math inline">\(A^TA=I_n\)</span> by <span class="math inline">\(\ref{ata}\)</span>. Therefore, <span class="math inline">\(A\)</span> is orthogonal if and only if <span class="math inline">\(A^{-1}=A^T\)</span>.</li>
</ul>
</div>
<div id="thm-orthogonal-projection-matrix" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.8 (Orthogonal Projection Matrix) </strong></span>&nbsp;</p>
<ul>
<li>Let <span class="math inline">\(V\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span> with orthonormal basis <span class="math inline">\(\vec u_1\)</span>, , <span class="math inline">\(\vec u_m\)</span>. The matrix of the orthogonal projection onto <span class="math inline">\(V\)</span> is <span class="math inline">\(Q Q^T\)</span> where <span class="math inline">\(Q= \begin{bmatrix} \vec u_1 &amp; \cdots &amp; \vec u_m \end{bmatrix}.\)</span></li>
<li>Let <span class="math inline">\(V\)</span> be a subspace of <span class="math inline">\(\mathbb{R}^n\)</span> with basis <span class="math inline">\(\vec v_1,\ldots,\vec v_m\)</span> and let <span class="math inline">\(A=\begin{bmatrix}\vec v_1 &amp; \cdots \vec v_m \end{bmatrix}\)</span>, then the orthogonal projection matrix onto <span class="math inline">\(V\)</span> is <span class="math inline">\(A(A^T A)^{-1}A^T\)</span>.</li>
</ul>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>The proof of each part follows.</p>
<ul>
<li><p>Since <span class="math inline">\(\vec u_1\)</span>, , <span class="math inline">\(\vec u_m\)</span> is an orthonormal basis of <span class="math inline">\(V\)</span> we can, by <span class="math inline">\(\ref{Orthogonal Projection}\)</span>, write, <span class="math display">\[\begin{align*}
\text{proj}_V (\vec x)
&amp; =(\vec u_1 \cdot \vec x) \vec u_1 + \cdots + (\vec u_m \cdot \vec x) \vec u_m
=\vec u_1 \vec u_1^T \vec x + \cdots +\vec u_m \vec u_m^T \vec x &amp;  \\
&amp;=(\vec u_1 \vec u_1^T  + \cdots +\vec u_m \vec u_m^T) \vec x
= \begin{bmatrix} \vec u_1 &amp; \cdots &amp; \vec u_m  \end{bmatrix}
\begin{bmatrix} \vec u_1^T  \\  \vdots \\  \vec u_m^T  \end{bmatrix} \vec x
=QQ^T\vec x.
\end{align*}\]</span></p></li>
<li><p>Since <span class="math inline">\(\vec v_1,\ldots,\vec v_m\)</span> form a basis of <span class="math inline">\(V\)</span>, there exists unique scalars <span class="math inline">\(c_1,\ldots,c_m\)</span> such that <span class="math inline">\(\text{proj}_V(\vec x)=c_1 \vec v_1+\cdots +c_m \vec v_m\)</span>. Since <span class="math inline">\(A=\begin{bmatrix}\vec v_1 &amp; \cdots &amp; \vec v_m \end{bmatrix}\)</span> we can write <span class="math inline">\(\text{proj}_V(\vec x)=A \vec c\)</span>. Consider the system <span class="math inline">\(A^TA\vec c =A^T \vec x\)</span> where <span class="math inline">\(A^TA\)</span> is the coefficient matrix and <span class="math inline">\(\vec c\)</span> is the unknown. Since <span class="math inline">\(\vec c\)</span> is the coordinate vector of <span class="math inline">\(\text{proj}_V(\vec x)\)</span> with respect to the basis <span class="math inline">\((v_1,\ldots,v_m)\)</span>, the system has a unique solution. Thus, <span class="math inline">\(A^TA\)</span> must be invertible, and so we can solve for <span class="math inline">\(\vec c\)</span>, namely <span class="math inline">\(\vec c=(A^T A)^{-1}A^T\vec x\)</span>. Therefore, <span class="math inline">\(\text{proj}_V(\vec x)=A \vec c =A (A^T A)^{-1}A^T\)</span> as desired. Notice it suffices to consider the system <span class="math inline">\(A^TA\vec c =A^T \vec x\)</span>, or equivalently <span class="math inline">\(A^T(\vec x-A \vec c)=\vec 0\)</span>, because <span class="math display">\[
A^T(\vec x -A \vec c)=A^T(\vec x-c_1 \vec v_1-\cdots - c_m \vec v_m)
\]</span> is the vector whose <span class="math inline">\(i\)</span>th component is <span class="math display">\[
(\vec v_i)^T(\vec x-c_1 \vec v_1-\cdots -c_m \vec v_m)=\vec v_i\cdot(\vec x-c_1\vec v_1-\cdots -c_m \vec v_m)
\]</span> which we know to be zero since <span class="math inline">\(\vec x-\text{proj}_V(\vec x)\)</span> is orthogonal to <span class="math inline">\(V\)</span>.</p></li>
</ul>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.11 </strong></span>Is there an orthogonal transformation <span class="math inline">\(T\)</span> from <span class="math inline">\(\mathbb{R}^3\)</span> to <span class="math inline">\(\mathbb{R}^3\)</span> such that <span class="math display">\[
T\vectorthree{2}{3}{0}=\vectorthree{3}{0}{2} \hspace{1cm} \text{and}  \hspace{1cm} T\vectorthree{-3}{2}{0}=\vectorthree{2}{-3}{0}?
\]</span> No, since the vectors <span class="math inline">\(\vectorthree{2}{3}{0}\)</span> and <span class="math inline">\(\vectorthree{-3}{2}{0}\)</span> are orthogonal, whereas <span class="math inline">\(\vectorthree{3}{0}{2}\)</span> and <span class="math inline">\(\vectorthree{2}{-3}{0}\)</span> are not, by <span class="math inline">\(\ref{orthogonal transformation}\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.12 </strong></span>Find an orthogonal transformation <span class="math inline">\(T\)</span> from <span class="math inline">\(\mathbb{R}^3\)</span> to <span class="math inline">\(\mathbb{R}^3\)</span> such that <span class="math display">\[
T\vectorthree{2/3}{2/3}{1/3}=\vectorthree{0}{0}{1}.
\]</span> Let’s think about the inverse of <span class="math inline">\(T\)</span> first. The inverse of <span class="math inline">\(T\)</span>, if it exists, must satisfy <span class="math inline">\(T^{-1}(\vec e_3)=\vectorthree{2/3}{2/3}{1/3}=\vec v_3\)</span>. Furthermore, the vectors <span class="math inline">\(\vec v_1, \vec v_2, \vec v_3\)</span> must form an orthonormal basis of <span class="math inline">\(\mathbb{R}^3\)</span> where <span class="math inline">\(T^{-1}\vec x=\begin{bmatrix}\vec v_1 &amp; \vec v_2 &amp; \vec v_3\end{bmatrix} \vec x\)</span>. We require a vector <span class="math inline">\(\vec v_1\)</span> with <span class="math inline">\(\vec v_1\cdot \vec v_3=0\)</span> and <span class="math inline">\(\norm{\vec v_1}=1\)</span>. By inspection, we find <span class="math inline">\(\vec v_1=\vectorthree{-2/3}{1/3}{2/3}\)</span>. Then <span class="math display">\[
\vec v_2=\vec v_1\times \vec v_3=\vectorthree{-2/3}{1/3}{2/3} \times \vectorthree{2/3}{2/3}{1/3}=\vectorthree{1/9-4/9}{-(-2/9-4/9)}{-4/9-2/9}=\vectorthree{-1/3}{2/3}{-2/3}
\]</span> does the job since <span class="math inline">\(\norm{v_1}=\norm{v_2}=\norm{v_3}=1\)</span> and <span class="math inline">\(\vec v_1\cdot \vec v_2=\vec v_1\cdot \vec v_3=\vec v_2\cdot \vec v_3=0\)</span>. In summary <span class="math display">\[
T^{-1}=\begin{bmatrix}-2/3 &amp; -1/3 &amp; 2/3 \\ 1/3 &amp; 2/3 &amp; 2/3 \\\ 2/3 &amp; -2/3 &amp; 1/3\end{bmatrix}\vec x.
\]</span> By <span class="math inline">\(\ref{Orthogonal and Transpose Properties}\)</span> the matrix of <span class="math inline">\(T^{-1}\)</span> is orthogonal and the matrix <span class="math inline">\(T=(T^{-1})^{-1}\)</span> is the transpose of the matrix of <span class="math inline">\(T^{-1}\)</span>. Therefore, it suffices to use <span class="math display">\[
T=\begin{bmatrix}-2/3 &amp; -1/3 &amp; 2/3 \\ 1/3 &amp; 2/3 &amp; 2/3 \\\ 2/3 &amp; -2/3 &amp; 1/3\end{bmatrix}^T\vec x=\begin{bmatrix}-2/3 &amp; 1/3 &amp; 2/3 \\ -1/3 &amp; 2/3 &amp; -2/3 \\\ 2/3 &amp; 2/3 &amp; 1/3 \end{bmatrix} \vec x.
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.13 </strong></span>Show that a matrix with orthogonal columns need not be an orthogonal matrix. For example <span class="math inline">\(A=\begin{bmatrix}4 &amp; -3 \\ 3 &amp; 4 \end{bmatrix}\)</span> is not an orthogonal matrix <span class="math inline">\(T\vec x=A\vec x\)</span> does not preserve length by comparing the lengths of <span class="math inline">\(\vec x\)</span> and <span class="math inline">\(T\vec x\)</span> with <span class="math inline">\(\vectortwo{-3}{4}\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.14 </strong></span>Find all orthogonal <span class="math inline">\(2\times 2\)</span> matrices. Write <span class="math inline">\(A=\begin{bmatrix}\vec v_1 &amp; \vec v_2\end{bmatrix}\)</span>. The unit vector <span class="math inline">\(\vec v_1\)</span> can be expressed as <span class="math inline">\(\vec v_1=\vectortwo{\cos \theta}{\sin \theta}\)</span>, for some <span class="math inline">\(\theta\)</span>. Then <span class="math inline">\(v_2\)</span> will be one of the two unit vectors orthogonal to <span class="math inline">\(\vec v_1\)</span>, namely <span class="math inline">\(\vec v_2=\vectortwo{-\sin \theta}{\cos \theta}\)</span> or <span class="math inline">\(\vec v_2=\vectortwo{\sin \theta}{-\cos \theta}\)</span>. Therefore, an orthogonal <span class="math inline">\(2\times 2\)</span> matrix is either of the form <span class="math display">\[
A=\begin{bmatrix}\cos \theta &amp; -\sin \theta \\ \sin \theta &amp; \cos \theta \end{bmatrix}\hspace{1cm} \text{or} \hspace{1cm} A=\begin{bmatrix}\cos \theta &amp; \sin \theta \\ \sin \theta &amp; -\cos \theta \end{bmatrix}
\]</span> representing a rotation or a reflection, respectively.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.15 </strong></span>Given <span class="math inline">\(n\times n\)</span> matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> which of the following must be symmetric?</p>
<ul>
<li><span class="math inline">\(B B^T\)</span></li>
<li><span class="math inline">\(A^T B^TB A\)</span></li>
<li><span class="math inline">\(B(A+A^T)B^T\)</span></li>
</ul>
<p>The solution to each part follows.</p>
<ul>
<li><span class="math inline">\(\ref{Orthogonal and Transpose Properties}\)</span>, <span class="math inline">\(B B^T\)</span> is symmetric because <span class="math display">\[
(B B^T)^T=(B^T)^TB^T=B B^T.
\]</span></li>
<li><span class="math inline">\(\ref{Orthogonal and Transpose Properties}\)</span>, <span class="math inline">\(A^T B^TB A\)</span> is symmetric because <span class="math display">\[
(A^TB^TBA)^T=A^TB^T(B^T)^T(A^T)^T=A^TB^TBA.
\]</span></li>
<li><span class="math inline">\(\ref{Orthogonal and Transpose Properties}\)</span>, <span class="math inline">\(B(A+A^T)B^T\)</span> is symmetric because <span class="math display">\[
(B(A+A^T)B^T)^T=((A+A^T)B^T)^TB^T=B(A+A^T)^TB^T
\]</span> <span class="math display">\[
=B(A^T+A)^TB^T=B((A^T)^T+A^T)B^T=B(A+A^T)B^T.
\]</span></li>
</ul>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.16 </strong></span>If the <span class="math inline">\(n\times n\)</span> matrices <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are symmetric which of the following must be symmetric as well?</p>
<ul>
<li><span class="math inline">\(2I_n+3A-4 A^2\)</span>,</li>
<li><span class="math inline">\(A B^2 A\)</span>.</li>
</ul>
<p>The solution to each part follows.</p>
<ul>
<li>First note that <span class="math inline">\((A^2)^T=(A^T)^2=A^2\)</span> for a symmetric matrix <span class="math inline">\(A\)</span>. Now we can use the linearity of the transpose, <span class="math display">\[
(2I_n+3A-4 A^2)^T=2I_n^T+3A^T-4 (A^2)^T=2I_n+3A-4 A^2
\]</span> showing that the matrix <span class="math inline">\(2I_n+3A-4 A^2\)</span> is symmetric.</li>
<li>The matrix <span class="math inline">\(A B^2 A\)</span> is symmetric since, <span class="math display">\[
(AB^2A)^T=(ABBA)^T=(BA)^T(AB)^T=A^TB^TB^TA^T=AB^2A.
\]</span></li>
</ul>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.17 </strong></span>Use <span class="math inline">\(\ref{Orthogonal Projection Matrix}\)</span> to find the matrix <span class="math inline">\(A\)</span> of the orthogonal projection onto <span class="math display">\[
W=\text{span} \left(\vectorfour{1}{1}{1}{1},\vectorfour{1}{9}{-5}{3}\right).
\]</span> Then find the matrix of the orthogonal projection onto the subspace of <span class="math inline">\(\mathbb{R}^4\)</span> spanned by the vectors <span class="math inline">\(\vectorfour{1}{1}{1}{1}\)</span> and <span class="math inline">\(\vectorfour{1}{2}{3}{4}\)</span>. First we apply <span class="math inline">\(\ref{Gram-Schmidt Process}\)</span>, the Gram-Schmidt process, to <span class="math inline">\(W=\text{span}(\vec v_1, \vec v_2)\)</span>, to find that the vectors <span class="math display">\[
\vec u_1=\frac{\vec v_1}{\norm{\vec v_1}}
=\vectorfour{1/2}{1/2}{1/2}{1/2}, \hspace{1cm}\vec u_2
=\frac{\vec v_2^\perp}{\norm{\vec v_2^\perp}}
=\frac{\vec v_2-\left(\vec u_1 \cdot \vec v_2\right) \vec u_1}{\norm{\vec v_2-\left(\vec u_1 \cdot \vec v_2\right)\vec u_1}}
=\vectorfour{-1/10}{7/10}{-7/10}{1/10}
\]</span> form an orthonormal basis of <span class="math inline">\(W\)</span>. By <span class="math inline">\(\ref{Orthogonal Projection Matrix}\)</span>, the matrix of the projection onto <span class="math inline">\(W\)</span> is <span class="math inline">\(A=Q Q^T\)</span> where <span class="math inline">\(Q=\begin{bmatrix}\vec u_1 &amp; \vec u_2\end{bmatrix}\)</span>. Therefore the orthogonal projection onto <span class="math inline">\(W\)</span> is <span class="math display">\[
A=
\begin{bmatrix} 1/2 &amp; -1/10 \\ 1/2 &amp; 7/10 \\ 1/2 &amp; -7/10 \\ 1/2 &amp; 1/10 \end{bmatrix}
\begin{bmatrix} 1/2 &amp; 1/2 &amp; 1/2 &amp; 1/2  \\ -1/10 &amp; 7/10 &amp; -7/10 &amp; 1/10 \end{bmatrix}
=\frac{1}{100}
\begin{bmatrix}
26 &amp; 18 &amp; 32 &amp; 24 \\
18 &amp; 74 &amp; -24 &amp; 32 \\
32 &amp; -24 &amp; 74 &amp; 18 \\
24 &amp; 32 &amp; 18 &amp; 26
\end{bmatrix}.
\]</span> Let <span class="math inline">\(A=\begin{bmatrix}1 &amp; 1 \\ 1 &amp; 2 \\1 &amp; 3 \\1 &amp; 4 \end{bmatrix}\)</span> and then the orthogonal projection matrix is <span class="math display">\[
A(A^TA)^{-1}A^T
=\frac{1}{10}\begin{bmatrix}7 &amp; 4 &amp; 1 &amp; -2 \\4 &amp; 3 &amp; 2 &amp; 1 \\ 1 &amp; 2 &amp; 3 &amp; 4 \\ -2 &amp; 1 &amp; 4 &amp; 7 \end{bmatrix}.
\]</span></p>
</div>
</section>
<section id="inner-products" class="level2" data-number="4.4">
<h2 data-number="4.4" class="anchored" data-anchor-id="inner-products"><span class="header-section-number">4.4</span> Inner Products</h2>
<p>Recall that the norm of <span class="math inline">\(x\in \mathbb{R}^n\)</span> defined by <span class="math inline">\(\norm{x}=\sqrt{x_1^2+x_2^2}\)</span> is not linear. To injective linearity into the discussion we introduce the dot product: for <span class="math inline">\(x,y\in \mathbb{R}^n\)</span> the <strong>dot product</strong> of <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> is defined as <span class="math inline">\(x \cdot y=x_1 y_1+\cdots +x_n y_n\)</span>. Obviously <span class="math inline">\(x \cdot x=\norm{x}^2\)</span>, and with the dot product being so useful, so we generalize the dot product into an inner product on a vector space <span class="math inline">\(V\)</span>.</p>
<p>An <strong>inner product</strong> on <span class="math inline">\(V\)</span> is a function that takes each ordered pair <span class="math inline">\((u,v)\)</span> of elements of <span class="math inline">\(V\)</span> to a number <span class="math inline">\(\ip{u,v} \in\mathbb{F}\)</span> and has the following properties:</p>
<ul>
<li>() <strong>positivity</strong> <span class="math inline">\(\ip{ v,v }\geq 0\)</span> for all <span class="math inline">\(v\in V\)</span>;</li>
<li>() <strong>definiteness</strong> <span class="math inline">\(\ip{ v,v }=0\)</span> if and only if <span class="math inline">\(v=0\)</span>;</li>
<li>( <strong>additivity</strong> in the first slot) <span class="math inline">\(\ip{ u+v,w } = \ip{u,w} +\ip{ v,w }\)</span> for all <span class="math inline">\(u,v,w\in V\)</span>;</li>
<li>( <strong>homogeneity</strong> in the first slot) <span class="math inline">\(\ip{av,w} =a\ip{v,w}\)</span> for all <span class="math inline">\(a\in \mathbb{F}\)</span> and all <span class="math inline">\(v,w,\in V\)</span>;</li>
<li>( <strong>conjugate symmetry</strong>) <span class="math inline">\(\ip{v,w} = \overline{\ip{w,v} }\)</span> for all <span class="math inline">\(v,w\in V\)</span>.</li>
</ul>
<p>Recall that for <span class="math inline">\(z\in \mathbb{C}^n\)</span>, we define the norm of <span class="math inline">\(z\)</span> by <span class="math display">\[
\norm{z}=\sqrt{|z_1|^2+\cdots + |z_n|^2}
\]</span> where the absolute values are needed because we want <span class="math inline">\(\norm{z}\)</span> to be a non-negative number. Then <span class="math display">\[\begin{equation}\label{cp}
\norm{z}^2=z_1 \overline{z_1}+\cdots + z_n \overline{z_n}
\end{equation}\]</span> because every <span class="math inline">\(\lambda\in\mathbb{C}\)</span> satisfies <span class="math inline">\(|\lambda|^2 =\lambda \overline{\lambda}\)</span>. Since <span class="math inline">\(\norm{z^2}\)</span> is the inner-product of <span class="math inline">\(z\)</span> with itself, as in <span class="math inline">\(\mathbb{R}^n\)</span>, the Equation <span class="math inline">\(\eqref{cp}\)</span> suggests that the inner product of <span class="math inline">\(w\in \mathbb{C}^n\)</span> with <span class="math inline">\(z\)</span> should equal <span class="math display">\[
w_1 \overline{z_1}+\cdots+w_n \overline{z_n}.
\]</span> We should expect that the inner product of <span class="math inline">\(w\)</span> with <span class="math inline">\(z\)</span> equals the complex conjugate of the inner product of <span class="math inline">\(z\)</span> with <span class="math inline">\(w\)</span>, thus motivating the definition of conjugate symmetry.</p>
<p>An <strong>inner-product space</strong> is a vector space <span class="math inline">\(V\)</span> along with an inner product on <span class="math inline">\(V\)</span>. The inner product defined on <span class="math inline">\(\mathbb{F}^n\)</span> by <span class="math display">\[
\ip{ (w_1,\ldots,w_n),(z_1,\ldots,z_n) } = w_1 \overline{z_1}+\cdots + w_n \overline{z_n}
\]</span> is called the <strong>Euclidean inner product</strong>.</p>
<p>Continue to let <span class="math inline">\(V\)</span> denote a complex or real vector space. In this section we develop the basic theorems for norms. For <span class="math inline">\(v\in V\)</span>, the <strong>norm</strong> of <span class="math inline">\(v\)</span> is defined by <span class="math inline">\(||v||=\sqrt{\ip{v,v} }\)</span>. Two vectors <span class="math inline">\(u,v\in V\)</span> are <strong>orthogonal</strong> if <span class="math inline">\(\ip{u,v}= 0\)</span>.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.9 </strong></span>[Pythagorean Theorem] If <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are orthogonal vectors in <span class="math inline">\(V\)</span>, then <span class="math display">\[
\norm{u + v}^2 = \norm{u}^2 + \norm{y}^2.
\]</span></p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose that <span class="math inline">\(u,v\)</span> are orthogonal vectors in <span class="math inline">\(V\)</span>. Then <span class="math display">\[
\norm{u+v}^2=\ip{u+v,u+v}=\norm{u}^2+\norm{v}^2+\ip{u,v}+\ip{v,u}=\norm{u}^2+\norm{v}^2,
\]</span> as desired.</p>
</div>
<p>::: {#thm- } [Orthogonal Decomposition] If <span class="math inline">\(v\)</span> is a nonzero vector in <span class="math inline">\(V\)</span>, then <span class="math inline">\(u\)</span> can be written as a scalar multiple of <span class="math inline">\(v\)</span> plus a vector orthogonal to <span class="math inline">\(v\)</span>. :::</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(a\in \mathbb{F}\)</span>. Then <span class="math display">\[
u=a v+(u-av).
\]</span> Thus we need to choose <span class="math inline">\(a\)</span> so that <span class="math inline">\(v\)</span> is orthogonal to <span class="math inline">\((u-a v)\)</span>. In other words, we want <span class="math display">\[
0=\ip{u-av,v}=\ip{u,v}-a\ip{v,v}=\ip{u,v}-a\norm{v}^2.
\]</span> The equation above shows that we should choose <span class="math inline">\(a\)</span> to be <span class="math inline">\(\ip{u,v}/\norm{v}^2\)</span> (assume that <span class="math inline">\(v\ne 0\)</span> to avoid division by 0). Making this choice of <span class="math inline">\(a\)</span>, we can write <span class="math display">\[
u=\frac{\ip{u,v}}{\norm{v}^2} v+\left(u-\frac{\ip{u,v}}{\norm{v}^2}v\right).
\]</span> Thus, if <span class="math inline">\(v\neq 0\)</span> then the equation above writes <span class="math inline">\(u\)</span> as a scalar multiple of <span class="math inline">\(v\)</span> plus a vector orthogonal to <span class="math inline">\(v\)</span>.</p>
</div>
<p>::: {#thm- } [Cauchy-Schwarz] If <span class="math inline">\(u, v \in V\)</span>, then <span class="math display">\[
| \ip{u,v} | \leq \norm{u} \, \norm{v}
\]</span> where equality holds if and only if one of <span class="math inline">\(u, v\)</span> is a scalar multiple of the other. :::</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(u,v\in V\)</span>. If <span class="math inline">\(v=0\)</span>, then both sides and the desired inequality holds. Thus we can assume that <span class="math inline">\(v\neq 0\)</span>. Consider the orthogonal decomposition <span class="math display">\[
u=\frac{\ip{u,v}}{\norm{v}^2}v+w
\]</span> where <span class="math inline">\(w\)</span> is orthogonal to <span class="math inline">\(v\)</span>. By the Pythagorean theorem, <span class="math display">\[\begin{align*}
\norm{u}^2
=\norm{\frac{\ip{u,v}}{\norm{v}^2} v}^2+\norm{w}^2\\
=\frac{|\ip{u,v}|^2}{\norm{v}^2}+\norm{w}^2\\
\geq \frac{|\ip{u,v}|^2}{\norm{v}^2}
\end{align*}\]</span> Multiplying both sides by <span class="math inline">\(\norm{v}^2\)</span> and then taking square roots gives the Cauchy-Schwarz inequality. Notice that there is equality if and only if <span class="math inline">\(w=0\)</span>, that is, if and only if <span class="math inline">\(u\)</span> is a multiple of <span class="math inline">\(v\)</span>.</p>
</div>
<p>::: {#thm- } [Triangular Inequality] If <span class="math inline">\(u, v \in V\)</span>, then <span class="math display">\[
\norm{u+v}\leq \norm{u}+\norm{v}
\]</span> where equality holds if and only if one of <span class="math inline">\(u, v\)</span> is a nonnegative multiple of the other. :::</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(u,v\in V\)</span>. Then <span class="math display">\[\begin{align}
\norm{u+v}^2 \notag
&amp;=\ip{u+v,u+v}  \notag\\
&amp;=\ip{u,u}+\ip{v,v}+\ip{u,v}+\ip{v,u}  \notag \\
&amp;=\ip{u,u}+\ip{v,v}+\ip{u,v}+\overline{\ip{u,v}}  \notag \\
&amp;=\norm{u,u}^2+\norm{v,v}^2+2 \text{remark}\ip{u,v}  \notag \\
&amp;\leq \norm{u,u}^2+\norm{v,v}^2+2 | \ip{u,v} | \label{ti1} \\
&amp;\leq \norm{u,u}^2+\norm{v,v}^2+2 \norm{u}\norm{v} \label{ti2}\\
&amp;= \left(\norm{u}+\norm{v}\right)^2  \notag
\end{align}\]</span> and so by taking square root of both sides yields the triangular inequality. This proof shows that the triangle inequality is an equality if and only if we have equality in <span class="math inline">\(\eqref{ti1}\)</span> and <span class="math inline">\(\eqref{ti2}\)</span>. Thus we have equality in the triangular inequality if and only if <span class="math display">\[\begin{equation}\label{ti3}
\ip{u,v}=\norm{u}\norm{v}.
\end{equation}\]</span> If one of <span class="math inline">\(u,v\)</span> is a nonnegative multiple of the other, then <span class="math inline">\(\eqref{ti3}\)</span> holds. Conversely, suppose <span class="math inline">\(\eqref{ti3}\)</span> holds. The the condition for equality in the Cauchy-Schwarz inequality implies that one of <span class="math inline">\(u,v\)</span> must be a scalar multiple of the other. Clearly, then <span class="math inline">\(\eqref{ti3}\)</span> forces the scalar in question to be nonnegative, as desired.</p>
</div>
<p>::: {#thm- } [Parallelogram Equality] If <span class="math inline">\(u, v \in V\)</span>, then <span class="math display">\[
\norm{u+v}^2+\norm{u-v}^2= 2 \left( \norm{u}^2+\norm{v}^2 \right).
\]</span> :::</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>If <span class="math inline">\(u, v \in V\)</span>, then<br>
<span class="math display">\[\begin{align*}
\norm{u+v}^2+\norm{u-v}^2
&amp;= \ip{u+v,u+v}+\ip{u-v,u-v} \\
&amp; = \norm{u}^2+\norm{v}^2+\ip{u,v}+\ip{v,u}+\norm{u}^2+\norm{v}^2-\ip{u,v}-\ip{v,u} \\
&amp; =2 \left( \norm{u}^2+\norm{v}^2 \right )
\end{align*}\]</span> as desired.</p>
</div>
<p>A list of vectors is called <strong>orthonormal</strong> if the vectors in it are pairwise orthogonal and each vector has norm 1.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.10 </strong></span>If <span class="math inline">\((e_1,\ldots,e_m)\)</span> is an orthonormal list of vectors in <span class="math inline">\(V\)</span>, then <span class="math display">\[
\norm{a_1 e_1+\cdots +a_m e_m}^2=|a_1|^2+\cdots + |a_n|^2
\]</span> for all <span class="math inline">\(a_1,\ldots,a_m\in\mathbb{F}\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Because each <span class="math inline">\(e_j\)</span> has norm 1, this follows easily from repeated by application of the Pythagorean theorem.</p>
</div>
<p>::: {#thm- } Every orthonormal list of vectors is linearly independent. :::</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\((e_1,\ldots,e_n)\)</span> is an orthonormal list of vectors in <span class="math inline">\(V\)</span> and <span class="math inline">\(a_1,\ldots,a_n\in \mathbb{F}\)</span> are such that <span class="math inline">\(a_1 e_1+\cdots + a_n e_n=0\)</span>. Then <span class="math inline">\(|a_1|^2+\cdots + |a_n|^2=0\)</span>, which means that all the <span class="math inline">\(a_j\)</span>’s are 0, as desired.</p>
</div>
<p>An <strong>orthonormal basis</strong> of <span class="math inline">\(V\)</span> is an orthonormal list of vectors in <span class="math inline">\(V\)</span> that is also a basis of <span class="math inline">\(V\)</span>.</p>
<p>The importance of orthonormal bases stems mainly from the following proposition.</p>
<p>::: {#thm- } Suppose <span class="math inline">\((e_1,\ldots,e_n)\)</span> is an orthonormal basis of <span class="math inline">\(V\)</span>. Then <span class="math display">\[\begin{equation} \label{inim1}
v=\ip{ v,e_1 } e_1+\cdots + \ip{ v,e_n } e_n
\end{equation}\]</span> and <span class="math display">\[\begin{equation}\label{inim2}
\norm{v}=|\ip{ v,e_1 } |^2+\cdots + |\ip{ v,e_n } |^2
\end{equation}\]</span> for every <span class="math inline">\(v\in V\)</span>. :::</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Let <span class="math inline">\(v\in V\)</span>. Because <span class="math inline">\((e_1,\ldots,e_n)\)</span> is a basis of <span class="math inline">\(V\)</span>, there exist scalars <span class="math inline">\(a_1,\ldots,a_n\)</span> such that <span class="math inline">\(v=a_1 e_1+\cdots + a_n e_n\)</span>. Take the inner product of both sides of this equation with <span class="math inline">\(e_j\)</span>, getting <span class="math inline">\(\ip{v,e_j}=a_j\)</span>. Thus <span class="math inline">\(\eqref{inim1}\)</span> holds. Clearly <span class="math inline">\(\eqref{inim2}\)</span> holds by <span class="math inline">\(\eqref{inim1}\)</span> and <span class="math inline">\(\eqref{innorm}\)</span>.</p>
</div>
<p>::: {#thm- } [Gram-Schmidt] If <span class="math inline">\((v_1,\ldots,v_m)\)</span> is a linearly independent list of vectors in <span class="math inline">\(V\)</span>, then there exists an orthonormal list <span class="math inline">\((e_1,\ldots,e_m)\)</span> of vectors in <span class="math inline">\(V\)</span> such that <span class="math display">\[\begin{equation} \label{gmeq}
\text{span}(v_1,\ldots,v_j)=\text{span}(e_1,\ldots,e_j)
\end{equation}\]</span> for <span class="math inline">\(j=1,\ldots,m\)</span> :::</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\((v_1,\ldots,v_m)\)</span> is a linearly independent list of vectors in <span class="math inline">\(V\)</span>. To construct the <span class="math inline">\(e\)</span>’s, start by setting <span class="math inline">\(e_1=\frac{v_1}{\norm{v_1}}\)</span>. This satisfies <span class="math inline">\(\eqref{gmeq}\)</span> for <span class="math inline">\(j=1\)</span>. We will choose <span class="math inline">\(e_2,\ldots e_m\)</span> inductively, as follows. Suppose <span class="math inline">\(j&gt;1\)</span> and an orthonormal list <span class="math inline">\((e_1,\ldots,e_{j-1})\)</span> has been chosen so that <span class="math display">\[\begin{equation}\label{gspan}
\text{span}(v_1,\ldots,v_{j-1})=\text{span}(e_1,\ldots,e_{j-1}).
\end{equation}\]</span> Let <span class="math display">\[\begin{equation}\label{gsproj}
e_j=\frac{v_j-\ip{v_j,e_1}e_1-\cdots - \ip{v_j,e_{j-1}} e_{j-1} }{\norm{v_j-\ip{v_j,e_1}e_1-\cdots - \ip{v_j,e_{j-1}} e_{j-1}}}.
\end{equation}\]</span> Note that <span class="math inline">\(v_j \not\in \text{span}(v_1,\ldots,v_{j-1})\)</span> (because <span class="math inline">\((v_1,\ldots,v_m)\)</span> is linearly independent) and thus <span class="math inline">\(v_j\not \in \text{span}(e_1,\ldots,e_{j-1})\)</span>. Hence we are not dividing by 0 in the equation above, and so <span class="math inline">\(e_j\)</span> is well-defined. Dividing a vector by its norm produces a new vector with norm 1; thus <span class="math inline">\(\norm{e_j}=1\)</span>.</p>
<p>Let <span class="math inline">\(1\leq k &lt;j\)</span>. Then <span class="math display">\[\begin{align*}
\ip{e_j,e_k}
&amp;=\ip{\frac{v_j-\ip{v_j,e_1}e_1-\cdots - \ip{v_j,e_{j-1}} e_{j-1} }{\norm{v_j-\ip{v_j,e_1}e_1-\cdots - \ip{v_j,e_{j-1}} e_{j-1}}},e_k} \\
&amp;= \frac{\ip{v_j,e_k}-\ip{v_j,e_k}\ip{e_k,e_k}}{\norm{v_j-\ip{v_j,e_1}e_1-\cdots - \ip{v_j,e_{j-1}} e_{j-1}}} \\
&amp;=0.
\end{align*}\]</span> Thus <span class="math inline">\((e_1,\ldots,e_j)\)</span> is an orthonormal list.</p>
<p>From <span class="math inline">\(\eqref{gsproj}\)</span>, we see that <span class="math inline">\(v_j\in \text{span}(e_1,\ldots,e_j)\)</span>. Combining this information with <span class="math inline">\(\eqref{gspan}\)</span> shows that <span class="math display">\[
\text{span}(v_1,\ldots,v_{j-1})\subset \text{span}(e_1,\ldots,e_{j}).
\]</span> Both lists above are linearly independent (the <span class="math inline">\(v\)</span>’s by hypothesis, the <span class="math inline">\(e\)</span>’s by orthonormality and <span class="math inline">\(\eqref{oind}\)</span>. Thus both subspaces above have dimension <span class="math inline">\(j\)</span>, and hence must be equal, completing the proof.</p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.11 </strong></span>Every finite-dimensional inner-product space has an orthonormal basis.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Choose a basis of <span class="math inline">\(V\)</span>. Apply the Gram-Schmidt procedure to it, producing an orthonormal list. This list is linearly independent and it spans <span class="math inline">\(V\)</span>. Thus it is an orthonormal basis.</p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.12 </strong></span>Every orthonormal list of vectors in <span class="math inline">\(V\)</span> can be extended to an orthonormal basis of <span class="math inline">\(V\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\((e_1, \ldots ,e_m)\)</span> is an orthonormal list of vectors in <span class="math inline">\(V\)</span>. Then <span class="math inline">\((e_1, \ldots , e_m)\)</span> is linearly independent and so can be extended to a basis <span class="math display">\[
\mathcal{B}=(e_1, \ldots, e_m, v_1, \ldots, v_n)
\]</span> of <span class="math inline">\(V\)</span>. Now apply the Gram-Schmidt procedure to <span class="math inline">\(\mathcal{B}\)</span> producing an orthonormal list <span class="math inline">\((e_1,\ldots,e_m,f_1,\ldots,f_n)\)</span>; here the Gram-Schmidt procedure leaves the first <span class="math inline">\(m\)</span> vectors unchanged because they are already orthonormal. Clearly <span class="math inline">\(\mathcal{B}\)</span> is an orthonormal basis of <span class="math inline">\(V\)</span> because it is linearly independent and its span equals <span class="math inline">\(V\)</span>. Hence we have our extension of <span class="math inline">\((e_1,\ldots,e_m)\)</span> to an orthonormal basis of <span class="math inline">\(V\)</span>.</p>
</div>
<p>Recall that if <span class="math inline">\(V\)</span> is a complex vector space, then for each operator on <span class="math inline">\(V\)</span> there is a basis with respect to which the matrix of the operator is upper-triangular. Now for inner-product spaces we would like to know the same question.</p>
<p>::: {#thm- } Suppose <span class="math inline">\(T\in\mathcal{L}(V)\)</span>. If <span class="math inline">\(T\)</span> has an upper-triangular matrix with respect to some basis of <span class="math inline">\(V\)</span>, then <span class="math inline">\(T\)</span> has an upper-triangular matrix with respect to some orthonormal basis of <span class="math inline">\(V\)</span>. :::</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\(T\)</span> has upper-triangular matrix with respect to some basis <span class="math inline">\((v_1,\ldots,v_n)\)</span> of <span class="math inline">\(V\)</span>. Thus <span class="math inline">\(\text{span}(v_1,\ldots,v_j)\)</span> is invariant under <span class="math inline">\(T\)</span> for each <span class="math inline">\(j=1,\ldots,n\)</span>. Apply the Gram-Schmidt procedure to <span class="math inline">\((v_1,\ldots,v_n)\)</span>, producing an orthonormal basis <span class="math inline">\((e_1,\ldots,e_n)\)</span> of <span class="math inline">\(V\)</span>. Because <span class="math display">\[
\text{span}(e_1,\ldots,e_j)=\text{span}(v_1,\ldots,v_j)
\]</span> for each <span class="math inline">\(j\)</span>, we conclude that <span class="math inline">\(\text{span}(e_1,\ldots,e_j)\)</span> is invariant under <span class="math inline">\(T\)</span> for each <span class="math inline">\(j=1,\ldots,n\)</span>. Thus, by <span class="math inline">\(\eqref{utm}\)</span>, <span class="math inline">\(T\)</span> has an upper-triangular matrix with respect to the orthonormal basis <span class="math inline">\((e_1,\ldots,e_n)\)</span>.</p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.13 </strong></span>Suppose <span class="math inline">\(V\)</span> is a complex vector space and <span class="math inline">\(T\in \mathcal{L}(V)\)</span>. Then <span class="math inline">\(T\)</span> has an upper-triangular matrix with respect to some orthonormal basis of <span class="math inline">\(V\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>This follows immediately from <span class="math inline">\(\eqref{cutm}\)</span> and <span class="math inline">\(\eqref{outm}\)</span>.</p>
</div>
<p>If <span class="math inline">\(U\)</span> is a subset of an inner-product space <span class="math inline">\(V\)</span>, then the <strong>orthogonal complement</strong> of <span class="math inline">\(U\)</span> is defined as <span class="math inline">\(U^\bot=\{v\in V\, : \, \ip{v,u} =0 \text{ for all } u\in U\}.\)</span></p>
<p>::: {#thm- } [Orthogonal Decomposition] If <span class="math inline">\(U\)</span> is a subspace of an inner-product space <span class="math inline">\(V\)</span>, then <span class="math inline">\(v=U\oplus U^\perp\)</span>. :::</p>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose that <span class="math inline">\(U\)</span> is a subspace of <span class="math inline">\(V\)</span>. First we will show that <span class="math display">\[\begin{equation}\label{sumfirst}
V=U + U^\perp.
\end{equation}\]</span> To do this, suppose <span class="math inline">\(v\in V\)</span>. Let <span class="math inline">\((e_1,\ldots,e_m)\)</span> be an orthonormal basis of <span class="math inline">\(U\)</span>. Obviously, <span class="math display">\[
v=\underbrace{ \ip{v,e_1}e_1+\cdots +\ip{v,e_m}e_m}_u+\underbrace{v-\ip{v,e_1}e_1-\cdots -\ip{v,e_m}e_m}_w.
\]</span> Clearly, <span class="math inline">\(u\in U\)</span>. Because <span class="math inline">\((e_1,\ldots,e_m)\)</span> is an orthonormal list, for each <span class="math inline">\(j\)</span> we have <span class="math display">\[
\ip{w,e_j}=\ip{v,e_j}-\ip{v,e_j}=0.
\]</span> Thus <span class="math inline">\(w\)</span> is orthogonal to every vector in <span class="math inline">\(\text{span}(e_1,\ldots,e_m)\)</span>. In other words, <span class="math inline">\(w\in U^\perp\)</span>, completing the proof of <span class="math inline">\(\eqref{sumfirst}\)</span>.</p>
<p>If <span class="math inline">\(v\in U\cap U^\perp\)</span>, then <span class="math inline">\(v\)</span> (which is in <span class="math inline">\(U\)</span>) is orthogonal to every vector in <span class="math inline">\(U\)</span> (including <span class="math inline">\(v\)</span> itself), which implies that <span class="math inline">\(\ip{v,v}=0\)</span>, which implies that <span class="math inline">\(v=0\)</span>. Thus <span class="math display">\[\begin{equation}\label{orthogonalss}
U\cap U^\perp=\{0\}.
\end{equation}\]</span> Now <span class="math inline">\(\eqref{sumfirst}\)</span> and <span class="math inline">\(\eqref{orthogonalss}\)</span> imply that <span class="math inline">\(U\oplus U^\perp\)</span>.</p>
</div>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.14 </strong></span>If <span class="math inline">\(U\)</span> is a subspace of an inner-product space <span class="math inline">\(V\)</span>, then <span class="math inline">\(U=(U^\perp)^\perp\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose that <span class="math inline">\(U\)</span> is a subspace of <span class="math inline">\(V\)</span>. First we will show that <span class="math display">\[\begin{equation}\label{subsetorth}
U\subseteq (U^\perp)^\perp.
\end{equation}\]</span> To do this, suppose that <span class="math inline">\(u\in U\)</span>. Then <span class="math inline">\(\ip{u,v}=0\)</span> for every <span class="math inline">\(v\in U^\perp\)</span> (by definition of <span class="math inline">\(U^\perp\)</span>). Because <span class="math inline">\(u\)</span> is orthogonal to every vector in <span class="math inline">\(U^\perp\)</span>, we have <span class="math inline">\(u\in (U^\perp)^\perp\)</span>, completing the proof of <span class="math inline">\(\eqref{subsetorth}\)</span>.</p>
<p>To prove the inclusion in the other direction, suppose <span class="math inline">\(v\in (U^\perp)^\perp\)</span>. By <span class="math inline">\(\eqref{ip-orthogonal-decomposition}\)</span>, we can write <span class="math inline">\(v=u+w\)</span>, where <span class="math inline">\(u\in U\)</span> and <span class="math inline">\(w\in U^\perp\)</span>. We have <span class="math inline">\(v-u=w\in U^\perp\)</span>. Because <span class="math inline">\(v\in (U^\perp)^\perp\)</span> and <span class="math inline">\(u\in(U^\perp)^\perp\)</span> (from <span class="math inline">\(\eqref{subsetorth}\)</span>), we have <span class="math inline">\(v-u\in (U^\perp)^\perp\)</span>. Thus <span class="math inline">\(v-u\in U^\perp\cap (U^\perp)^\perp\)</span>, which implies that <span class="math inline">\(v=u\)</span>, which implies that <span class="math inline">\(v\in U\)</span>. Thus <span class="math inline">\((U^\perp)^\perp \subseteq U\)</span>, which along with <span class="math inline">\(\eqref{subsetorth}\)</span> completes the proof.</p>
</div>
<p>Let <span class="math inline">\(V=U \oplus U ^\bot\)</span> and for <span class="math inline">\(v\in V\)</span> let <span class="math inline">\(v=u+w\)</span> where <span class="math inline">\(w\in U ^\bot\)</span>. Then <span class="math inline">\(u\)</span> is called the <strong>orthogonal projection</strong> of <span class="math inline">\(V\)</span> onto <span class="math inline">\(U\)</span> and is denoted by <span class="math inline">\(P_U v\)</span>.</p>
<div id="thm-" class="theorem">
<p><span class="theorem-title"><strong>Theorem 4.15 </strong></span>If <span class="math inline">\(U\)</span> is a subspace of an inner-product space <span class="math inline">\(V\)</span> and <span class="math inline">\(v\in V\)</span>. Then <span class="math inline">\(\norm{v-P_U v}\leq \norm{v-u}\)</span> for every <span class="math inline">\(u\in U\)</span>. Furthermore, if <span class="math inline">\(u\in U\)</span> and the inequality above is an equality; then <span class="math inline">\(u=P_U v\)</span>.</p>
</div>
<div class="proof">
<p><span class="proof-title"><em>Proof</em>. </span>Suppose <span class="math inline">\(u\in U\)</span>. Then <span class="math display">\[\begin{align}
\norm{v-P_U v}^2
&amp; \leq \norm{v-P_Uv}+ \norm{P_U v-u}^2 \label{mp1} \\
&amp; = \norm{v-P_U v+P_Uv-u}^2 = \norm{v-u}^2   \label{mp2}
\end{align}\]</span> where <span class="math inline">\(\eqref{mp2}\)</span> comes from the Pythagorean theorem, which applies because <span class="math inline">\(v-P_U v\in U^\perp\)</span> and <span class="math inline">\(P_U v-u\in U\)</span>. Taking the square root gives the desired inequality. The inequality is an equality if and only if <span class="math inline">\(\eqref{mp1}\)</span> is an equality, which happens if and only if <span class="math inline">\(\norm{P_U v-u}=0\)</span>, which happens if and only if <span class="math inline">\(u=P_u v\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.18 </strong></span>Show that if <span class="math inline">\(c_1,\ldots,c_n\)</span> are positive numbers, then <span class="math display">\[
\ip{ (w_1,\ldots,w_n),(z_1,\ldots,z_n) } = c_1 w_1 \overline{z_1}+\cdots + c_n w_n \overline{z_n}
\]</span> defines an inner product on <span class="math inline">\(\mathbb{F}^n\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.19 </strong></span>Show that if <span class="math inline">\(p,q\in \mathcal{P}_m(\mathbb{F})\)</span>, then <span class="math display">\[
\ip{ p ,q } = \int_0^1 p(x)\overline{q(x)}dx
\]</span> is an inner product on the vector space <span class="math inline">\(\mathcal{P}_m(\mathbb{F})\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.20 </strong></span>Show that every inner product is a linear map in the first slot, as well as a linear map in the second slot.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.21 </strong></span>If <span class="math inline">\(v\in V\)</span> and <span class="math inline">\(a\in \mathbb{F}\)</span>, then <span class="math inline">\(\norm{av}=|a| \norm{v}\)</span>, and if <span class="math inline">\(v\)</span> is nonzero then <span class="math inline">\(u=\frac{1}{\norm{v}} v\)</span> is a unit vector. Since <span class="math inline">\(\norm{a v}^2=\ip{av,av} =a \overline{a} \ip{v,v} =|a|^2 \norm{v}^2\)</span>, taking square roots provides <span class="math inline">\(\norm{a v}=|a| \norm{v}\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.22 </strong></span>Prove that if <span class="math inline">\(x, y\)</span> are nonzero vectors in <span class="math inline">\(\mathbb{R}^2\)</span>, then <span class="math display">\[
\ip{x,y} =\norm{x}\norm{y} \cos \theta,
\]</span> where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span>. The law of cosines gives <span class="math display">\[
\norm{x-y}^2=\norm{x}^2+\norm{y}^2-2\norm{x}\norm{y} \cos \theta.
\]</span> The left hand side of this equation is <span class="math display">\[
\norm{x-y}^2=(x-y)\cdot (x-y)=\norm{x}^2-2 (x \cdot y) +\norm{y}^2
\]</span> so <span class="math display">\[
x\cdot y= \norm{x}\norm{y}\cos \theta.
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.23 </strong></span>Suppose <span class="math inline">\(u,v \in V\)</span>. Prove that <span class="math inline">\(\ip{ u,v } =0\)</span> if and only if <span class="math inline">\(||u||\leq ||u+a v||\)</span> for all <span class="math inline">\(a\in F\)</span>. If <span class="math inline">\(\ip{u,v}=0\)</span>, then by the Pythagorean theorem <span class="math display">\[
\norm{u+\alpha v}^2=\norm{u}^2+\norm{\alpha v}^2\geq \norm{u}.
\]</span> Conversely, we will prove the contrapositive, that is we will prove: if <span class="math inline">\(\ip{u,v}\neq0\)</span> then there exists <span class="math inline">\(a \in \mathbb{F}\)</span> such that <span class="math inline">\(\norm{u}&gt;\norm{u+av}\)</span>. Suppose <span class="math inline">\(\ip{u,v}\neq 0\)</span> then <span class="math inline">\(u\)</span> and <span class="math inline">\(v\)</span> are both nonzero vectors. By the orthogonal decomposition, we can write <span class="math display">\[\begin{equation} \label{ex3}
u=\alpha v+w
\end{equation}\]</span> for some <span class="math inline">\(\alpha \in \mathbb{F}\)</span> and where <span class="math inline">\(\ip{w,v}=0\)</span>. Notice <span class="math inline">\(\alpha \neq 0\)</span> since <span class="math inline">\(\ip{u,v}\neq 0\)</span>. Since <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> are orthogonal <span class="math display">\[
\norm{u}^2=|\alpha|^2\norm{v}^2+\norm{w}^2
\]</span> Let <span class="math inline">\(a=-\alpha\)</span>. Then by equation <span class="math inline">\(\eqref{ex3}\)</span> <span class="math display">\[
\norm{u+a v}^2=\norm{w}^2
\]</span> and so <span class="math display">\[
\norm{u}^2=|a|^2\norm{v}^2 +\norm{u+a v}^2 &gt;  \norm{u+a v}^2
\]</span> which implies <span class="math display">\[
\norm{u}&gt;\norm{u+a v}
\]</span> as desired.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.24 </strong></span>Prove that <span class="math display">\[
\left (\sum_{k=1}^{n} a_k b_k \right )^2
\leq \left ( \sum_{k=1}^n k a_k^2 \right ) \left ( \sum_{k=1}^n \frac{b_k^2}{k}\right )
\]</span> for all real numbers <span class="math inline">\(a_1,\ldots,a_n\)</span> and <span class="math inline">\(b_1,\ldots,b_n\)</span>. This is a simple trick. <span class="math display">\[
\left (\sum_{k=1}^{n} a_k b_k \right )^2
= \left ( \sum_{k=1}^n \sqrt{k} a_k \frac{b_k}{\sqrt{k}} \right )^2
\leq \left ( \sum_{k=1}^n k a_k^2 \right ) \left ( \sum_{k=1}^n \frac{b_k^2}{k}\right )
\]</span> where the last inequality is from the Cauchy-Schwarz inequality.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.25 </strong></span>Suppose <span class="math inline">\(u, v\in V\)</span> are such that <span class="math inline">\(||u||=3\)</span>, <span class="math inline">\(||u+v||=4\)</span>, and <span class="math inline">\(||u-v||=6\)</span>. What number must <span class="math inline">\(||v||\)</span> equal? Using the parallelogram equality <span class="math display">\[
\norm{u+v}^2+\norm{u-v}^2=2 \left(\norm{u}^2+\norm{v}^2\right)
\]</span> to get <span class="math display">\[
16+36=2(9+\norm{v}^2), \qquad \norm{v}=\sqrt{17}.
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.26 </strong></span>Prove or disprove: there is an inner product on <span class="math inline">\(\mathbb{R}^2\)</span> such that the associated norm is given by <span class="math inline">\(||(x_1,x_2)||=|x_1|+|x_2|\)</span> for all <span class="math inline">\((x_1,x_2)\in \mathbb{R}^2\)</span>. There is no such inner product. Take for instance, <span class="math display">\[
u=(1/4,0), \qquad v=(0,3/4), \qquad u+v=(1/4,3/4).
\]</span> Then we have equality in the triangular inequality <span class="math display">\[
1=\norm{u+v}\leq \norm{u}+\norm{v}=1/4+3/4.
\]</span> By the triangular inequality, we must have <span class="math inline">\(u=a v\)</span> or <span class="math inline">\(v=a v\)</span>, with <span class="math inline">\(a\geq 0\)</span>. But clearly no such <span class="math inline">\(a\in \mathbb{F}\)</span> exists.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.27 </strong></span>Prove that if <span class="math inline">\(V\)</span> is a real inner-product space, then <span class="math display">\[
\ip{ u,v } =\frac{||u+v||^2-||u-v||^2}{4}
\]</span> for all <span class="math inline">\(u,v\in V\)</span>. Expressing the norms as inner products <span class="math display">\[\begin{align*}
%\ip{ u,v }
\frac{\norm{u+v}^2- \norm{u-v}^2}{4}
&amp;=\frac{\ip{u+v,u+v}-\ip{u-v,u-v} }{4} \\
&amp;=\frac{\ip{u,u}+\ip{v,v}+\ip{u,v}+\ip{v,u} - \ip{u,u}-\ip{v,v}+\ip{u,v}+\ip{v,u}}{4} \\
&amp;=\frac{2\ip{u,v}+2\ip{v,u} }{4} \\
&amp;=\frac{2\ip{u,v}+2\ip{u,v} }{4} \quad \text{(because $V$ is a real inner product space)} \\
&amp;=\ip{u,v}
\end{align*}\]</span> as desired.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.28 </strong></span>Prove that if <span class="math inline">\(V\)</span> is a complex inner-product space, then <span class="math display">\[
\ip{ u,v }
\]</span> is <span class="math display">\[
\frac{||u+v||^2-||u-v||^2 + ||u+i v||^2 i - ||u-i v||^2 i}{4}
\]</span> for all <span class="math inline">\(u,v\in V\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.29 </strong></span>A norm on a vector space <span class="math inline">\(U\)</span> is a function <span class="math inline">\(||\text{  }|| : U\rightarrow [0,\infty)\)</span> such that <span class="math inline">\(||u||=0\)</span> if and only if <span class="math inline">\(u=0\)</span>, <span class="math inline">\(||\alpha u ||= |\alpha | ||u||\)</span> for all <span class="math inline">\(\alpha \in F\)</span> and all <span class="math inline">\(u \in U\)</span>, and <span class="math inline">\(||u+v|| \leq ||u||+||v||\)</span> for all <span class="math inline">\(u,v \in U\)</span>. Prove that a norm satisfying the parallelogram equality comes from an inner product (in other words, show that if <span class="math inline">\(|| \text{  }||\)</span> is a norm on <span class="math inline">\(U\)</span> satisfying the parallelogram equality, then there is an inner product <span class="math inline">\(\ip{ \text{ } , \text{ } }\)</span> on <span class="math inline">\(U\)</span> such that <span class="math inline">\(||u||=\ip{ u, u } ^{1/2}\)</span> for all <span class="math inline">\(u \in U\)</span>).</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.30 </strong></span>Suppose <span class="math inline">\(n\)</span> is a positive integer. Prove that <span class="math display">\[
\left (
\frac{1}{\sqrt{2\pi}},\frac{\sin x}{\sqrt{\pi}},\frac{\sin 2x}{\sqrt{\pi}},\ldots,\frac{\sin n x}{\sqrt{\pi}}, \frac{\cos x}{\sqrt{\pi}},\frac{\cos 2x}{\sqrt{\pi}},\ldots,\frac{\cos n x}{\sqrt{\pi}}
\right )
\]</span> is an orthonormal list of vectors in <span class="math inline">\(\mathcal{C}[-\pi,\pi]\)</span>, the vector space of continuous real-valued functions on <span class="math inline">\([-\pi,\pi]\)</span> with inner product <span class="math display">\[
\ip{ f,g } = \int_{-\pi}^{\pi} f(x)g(x) \, d x .
\]</span> Computation of these integrals is based on the product-to-sum formulas from trigonometry: <span class="math display">\[\begin{align*}
\sin(A)\sin(B)&amp;=\frac{1}{2}\cos(A-B)-\frac{1}{2}\cos(A+B) \\
\cos(A)\cos(B)&amp;=\frac{1}{2}\cos(A-B)+\frac{1}{2}\cos(A+B) \\
\sin(A)\cos(B)&amp;=\frac{1}{2}\sin(A-B)+\frac{1}{2}\sin(A+B).
\end{align*}\]</span> Here is a sample computation, valid for <span class="math inline">\(m,n=1,2,3,,\ldots\)</span> when <span class="math inline">\(m\neq n\)</span>. <span class="math display">\[\begin{align*}
\ip{\sin(mx),\cos(nx)}
&amp;= \int_{-\pi}^{\pi}\sin(mx)\cos(nx) \, dx \\
&amp;= \int_{-\pi}^{\pi}\left(\frac{1}{2}\sin((m-n)x)+\frac{1}{2}\sin((m+n)x)\right)\, dx \\
&amp;= \left. \frac{-1}{2(m-n)}\cos\left((m-n)x\right)+\frac{-1}{2(m+n)}\cos\left((m+n)x\right) \right|^{\pi}_{-\pi}, \\
&amp;= \frac{-1}{2(m-n)}[-1-(-1)]+\frac{-1}{2(m-n)}[-1-(-1)] \\
&amp;=0.
\end{align*}\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.31 </strong></span>On <span class="math inline">\(\mathcal{P}_2(\mathbb{R})\)</span>, consider the inner product given by <span class="math display">\[
\ip{ p,q } = \int_o^1 p(x) q(x) \, d x.
\]</span> Apply the Gram-Schmidt procedure to the basis <span class="math inline">\((1,x,x^2)\)</span> to produce an orthonormal basis of <span class="math inline">\(\mathcal{P}_2(\mathbb{R})\)</span>. Computing <span class="math inline">\(e_1\)</span>: A calculation gives <span class="math display">\[
\ip{1,1}=\int_0^1 1 \, dx=1,
\]</span> so <span class="math inline">\(e_1=1\)</span>. Computing <span class="math inline">\(e_2\)</span>: A calculation gives <span class="math display">\[
\ip{x,1}=\int_0^1 x \, dx =\frac{1}{2}.
\]</span> Let <span class="math display">\[
f_2=x-1/2.
\]</span> Then <span class="math display">\[
\norm{f_2}^2=\ip{x-1/2,x-1/2}=\int_0^1\left(x^2-x+\frac{1}{4}\right)\, dx =\frac{1}{12},
\]</span> so <span class="math display">\[
e_2=\frac{f_2}{\norm{f_2}}=2\sqrt{3}\left(x-\frac{1}{2}\right).
\]</span> Computing <span class="math inline">\(e_3\)</span>: A calculation gives <span class="math display">\[
\ip{x^2,1}=\int_0^1 x^2 \, dx =\frac{1}{3},
\]</span> and <span class="math display">\[
\ip{x^2,2\sqrt{3}\left(x-\frac{1}{2}\right)}=2\sqrt{3}\int_0^1\left(x^3-\frac{x^2}{2}\right) \, dx=\frac{1}{2\sqrt{3}}.
\]</span> Let <span class="math display">\[
f_3=x^2-\frac{1}{2\sqrt{3}}\left(x-\frac{1}{2}\right)-\frac{1}{3}=x^2-x+\frac{1}{6}.
\]</span> Then <span class="math display">\[
\norm{f_3}^2=\int_0^1\left(x^2-x+\frac{1}{6}\right)^2 \, dx
\]</span> and <span class="math display">\[
e_3=\frac{f_3}{\norm{f_3}}.
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.32 </strong></span>What happens if the Gram-Schmidt procedure is applied to a list of vectors that is not linearly independent? By examining the proof, notice that the numerator in the Gram-Schmidt formula is the difference between <span class="math inline">\(v_j\)</span> and the orthogonal projection <span class="math inline">\(P_u\)</span> of <span class="math inline">\(v_j\)</span> onto the subspace <span class="math display">\[
U=\text{span}(v_1,\ldots,v_{j-1})=\text{span}(e_1,\ldots,e_{j-1}).
\]</span> If <span class="math inline">\(v_j\in U\)</span>, then <span class="math inline">\(v_j-P_U v_j=0\)</span>, so the numerator has norm 0 and division by the denominator is not defined. The algorithm can be adapted to handle this case by testing for 0 in the denominator. If 0 is found, throw <span class="math inline">\(v_j\)</span> out of the list and continue. The result will be an orthonormal basis for <span class="math inline">\(\text{span}(v_1,\ldots,v_N)\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.33 </strong></span>Suppose <span class="math inline">\(V\)</span> is a real inner-product space and <span class="math inline">\((v_1,\ldots,v_m)\)</span> is a linearly independent list of vectors in <span class="math inline">\(V\)</span>. Prove that there exist exactly <span class="math inline">\(2^m\)</span> orthonormal lists <span class="math inline">\((e_1,\ldots,e_m)\)</span> of vectors in <span class="math inline">\(V\)</span> such that span <span class="math inline">\((v_1,\ldots,v_j)=\)</span> span <span class="math inline">\((e_1,\ldots,e_j)\)</span> for all <span class="math inline">\(j\in \{1,\ldots,m\}\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.34 </strong></span>Suppose <span class="math inline">\((e_1,\ldots,e_M)\)</span> is an orthonormal list of vectors in <span class="math inline">\(V\)</span>. Let <span class="math inline">\(v \in V\)</span>. Prove that <span class="math display">\[
||v||^2=\sum_{n=1}^M \left|\ip{v,e_n}\right|^2
\]</span> if and only if $v $ span <span class="math inline">\((e_1,\ldots,e_M)\)</span>. Extend <span class="math inline">\((e_1,\ldots,e_M)\)</span> to an orthonormal basis for <span class="math inline">\(V\)</span>. Then <span class="math display">\[
\sum_{n=1}^N\ip{v,e_n} e_n
\]</span> and <span class="math display">\[
\norm{v}^2=\sum_{n=1}^N \left|\ip{v,e_n}\right|^2.
\]</span> If <span class="math inline">\(v\in \text{span}(e_1,\ldots,e_M)\)</span> then <span class="math inline">\(\ip{v,e_n}=0\)</span> for <span class="math inline">\(n&gt;M\)</span>, and <span class="math display">\[
\norm{v}^2=\sum_{m=1}^M \left|\ip{v,e_n}\right|^2.
\]</span> If <span class="math inline">\(v \not\in\text{span}(e_1,\ldots,e_M)\)</span> then for some <span class="math inline">\(n&gt;M\)</span> we have <span class="math inline">\(\ip{v,e_n}\neq 0\)</span>. This gives <span class="math display">\[
\norm{v}^2=\sum_{n=1}^N\left|\ip{v,e_n}\right|^2&gt;\sum_{m=1}^M\left|v,e_n\right|^2.
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.35 </strong></span>Find an orthonormal basis of <span class="math inline">\(\mathcal{P}_2(\mathbb{R})\)</span>, such that the differentiation operator on <span class="math inline">\(\mathcal{P}_2(\mathbb{R})\)</span> has an upper-triangular matrix with respect to this basis.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.36 </strong></span>Suppose <span class="math inline">\(U\)</span> is a subspace of <span class="math inline">\(V\)</span>. Prove that <span class="math inline">\(\text{dim} U^{\bot} = \text{dim} V - \text{dim} U\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.37 </strong></span>Suppose <span class="math inline">\(U\)</span> is a subspace of <span class="math inline">\(V\)</span>. Prove that <span class="math inline">\(U^{\bot} = \{0\}\)</span> if and only if <span class="math inline">\(U=V\)</span>. If <span class="math inline">\(U=V\)</span> and <span class="math inline">\(v\in U^\perp\)</span> then <span class="math inline">\(v\in U\cap U^\perp\)</span>, and <span class="math inline">\(\ip{v,v}=0\)</span>, so <span class="math inline">\(v=0\)</span>. Therefore, <span class="math inline">\(V=U\oplus U^\perp\)</span>. If <span class="math inline">\(U^\perp=\{0\}\)</span>, then <span class="math inline">\(U=V\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.38 </strong></span>Prove that if <span class="math inline">\(P\in \mathcal{L}(V)\)</span> is such that <span class="math inline">\(P^2=P\)</span> and every vector in null <span class="math inline">\(P\)</span> is orthogonal to every vector in range <span class="math inline">\(P\)</span>, then <span class="math inline">\(P\)</span> is an orthogonal projection.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.39 </strong></span>Prove that if <span class="math inline">\(P\in \mathcal{L}(V)\)</span> is such that <span class="math inline">\(P^2=P\)</span> and <span class="math inline">\(|| P v ||\leq ||v||\)</span> for every <span class="math inline">\(v\in V\)</span>, then <span class="math inline">\(P\)</span> is an orthogonal projection.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.40 </strong></span>Suppose <span class="math inline">\(T\in \mathcal{L}(V)\)</span> and <span class="math inline">\(U\)</span> is a subspace of <span class="math inline">\(V\)</span>. Prove that <span class="math inline">\(U\)</span> is invariant under <span class="math inline">\(T\)</span> if and only if <span class="math inline">\(P_U T= T P_U\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.41 </strong></span>Suppose <span class="math inline">\(T\in \mathcal{L}(V)\)</span> and <span class="math inline">\(U\)</span> is a subspace of <span class="math inline">\(V\)</span>. Prove that <span class="math inline">\(U\)</span> and <span class="math inline">\(U^\bot\)</span> are both invariant under <span class="math inline">\(T\)</span> if and only if <span class="math inline">\(P_U T= T P_U\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.42 </strong></span>In <span class="math inline">\(\mathbb{R}^4\)</span>, let <span class="math inline">\(U=\text{span} \left ( (1,1,0,0),(1,1,1,2) \right )\)</span>. Find <span class="math inline">\(u\in U\)</span> such that <span class="math inline">\(||u-(1,2,3,4)||\)</span> is as small as possible. in <span class="math inline">\(\mathbb{R}^4\)</span> let <span class="math inline">\(U=\text{span}((1,1,0,0),(1,1,1,2)).\)</span> Find <span class="math inline">\(u\in U\)</span> such that <span class="math inline">\(\norm{u-(1,2,3,4)}\)</span> is as small as possible. We want the orthogonal projection <span class="math inline">\(P_U(1,2,3,4)\)</span>. Notice that <span class="math inline">\(U=\text{span}((1,1,0,0),(0,0,1,2)).\)</span> An orthonormal basis for <span class="math inline">\(U\)</span> is <span class="math display">\[
\left( \frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}} ,0,0\right ),\left(0,0,\frac{1}{\sqrt{5}},\frac{2}{\sqrt{5}} \right)
\]</span> Thus the desired vector is <span class="math display">\[
P_U(1,2,3,4)=\left(\frac{3}{2},\frac{3}{2},0,0\right)+\left(0,0,\frac{11}{5},\frac{2}{5}\right).
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.43 </strong></span>Find a polynomial <span class="math inline">\(p\in \mathcal{P}_3(\mathbb{R})\)</span> such that <span class="math inline">\(p(0)=0\)</span>, <span class="math inline">\(p'(0)=0\)</span>, and <span class="math display">\[
\int_0^1 |2+3x-p(x) |^2 \, dx
\]</span> is as small as possible.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.44 </strong></span>Find a polynomial <span class="math inline">\(p\in \mathcal{P}_5(\mathbb{R})\)</span> that makes <span class="math display">\[
\int_{-\pi}^{\pi} |\sin x - p(x) |^2 \, dx
\]</span> is as small as possible.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.45 </strong></span>Find a polynomial <span class="math inline">\(p\in \mathcal{P}_2(\mathbb{R})\)</span> such that <span class="math display">\[
\phi(p)=p \left( \frac{1}{2} \right) = \int_{0}^{1} p(x) \, q(x) \, dx
\]</span> for every <span class="math inline">\(p\in \mathcal{P}_2(\mathbb{R})\)</span>. Here is the direct approach. Every <span class="math inline">\(q\in\mathcal{P}_2(\mathbb{R})\)</span> can be expressed as <span class="math display">\[
\alpha+\beta(x-1/2)+\gamma(x-1/2)^2.
\]</span> The desired polynomial <span class="math inline">\(q\)</span> must satisfy <span class="math display">\[
p(x)=1: \qquad \phi(1)=p(1/2)=1=\int_0^1\left( \alpha+\beta(x-1/2)+\gamma(x-1/2)^2 \right) \, dx = \alpha+\gamma \frac{1}{12}.
\]</span> Moving to <span class="math inline">\(p(x)=x-1/2\)</span> we find <span class="math display">\[\begin{align*}
p(x)&amp;=x-1/2: \qquad \phi(x-1/2)=p(1/2)=0 \\
&amp;=\int_0^1(x-1/2)[\alpha+\beta(x-1/2)+\gamma(x-1/2)^2] \, dx =\beta \frac{1}{12},
\end{align*}\]</span> so <span class="math inline">\(\beta=0\)</span>. Finally <span class="math display">\[
p(x)=(x-1/2)^2: \qquad \phi((x-1/2)^2)=p(1/2)=0
\]</span> <span class="math display">\[
=\int_0^1(x-1/2)^2[\alpha+\beta(x-1/2)+\gamma(x-1/2)^2] \, dx =\alpha \frac{1}{12}+\gamma\frac{1}{80}.
\]</span> Solving gives <span class="math display">\[
\alpha=\frac{27}{12}, \qquad  \beta=0, \qquad \gamma=-15.
\]</span> Thus <span class="math display">\[
q(x)=\frac{27}{12}-15(x-1/2)^2.
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.46 </strong></span>Find a polynomial <span class="math inline">\(q\in \mathcal{P}_2(\mathbb{R})\)</span> such that <span class="math display">\[
\phi(p)=\int_{0}^{1} p(x) \, (\cos \pi x)  \, dx= \int_{0}^{1} p(x) \, q(x) \, dx
\]</span> for every <span class="math inline">\(p\in \mathcal{P}_2(\mathbb{R})\)</span>. Taking the same approach as in the previous example. We compute <span class="math display">\[
p(x)=1: \qquad \phi(1)=\int_0^1\cos(\pi x) \, dx=0
\]</span> <span class="math display">\[
=\int_0^1[\alpha+\beta(x-1/2)+\gamma(x-1/2)^2]\,dx=\alpha+\gamma\frac{1}{12}.
\]</span> Moving to <span class="math inline">\(p(x)=x-1/2\)</span> we find <span class="math display">\[
p(x)=x-1/2: \qquad \phi(x-1/2)=\int_0^1(x-1/2)\cos(\pi x)\, dx=\int_0^1 x \cos (\pi x) \, dx
\]</span> <span class="math display">\[
=-\frac{2}{\pi^2}=\int_0^1(x-1/2)[\alpha+\beta(x-1/2)+\gamma(x-1/2)^2]\, dx=\beta\frac{1}{12},
\]</span> so <span class="math inline">\(\beta=\frac{-24}{\pi^2}\)</span>. Finally, since <span class="math inline">\(\cos(\pi x)\)</span> is odd about <span class="math inline">\(x=1/2\)</span>, <span class="math display">\[
p(x)=(x-1/2)^2: \qquad \phi((x-1/2)^2)=\int_0^1(x-1/2)^2\cos(\pi x)\, dx =0
\]</span> <span class="math display">\[
=\int_0^1(x-1/2)^2[\alpha+\beta(x-1/2)+\gamma(x-1/2)^2]=\alpha \frac{1}{12} +\gamma\frac{1}{80}.
\]</span> Solving gives <span class="math display">\[
\alpha=\gamma=0, \qquad \beta=\frac{-24}{\pi^2}.
\]</span> Thus <span class="math display">\[
q(x)=\frac{-24}{\pi^2}(x-1/2).
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.47 </strong></span>Give an example of a real vector space <span class="math inline">\(V\)</span> and <span class="math inline">\(T\in \mathcal{L}(V)\)</span> such that trace<span class="math inline">\((T^2) &lt; 0\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.48 </strong></span>Suppose <span class="math inline">\(V\)</span> is a real vector space, <span class="math inline">\(T\in \mathcal{L}(V)\)</span>, and <span class="math inline">\(V\)</span> has a basis consisting of eignvectors of <span class="math inline">\(T\)</span>. Prove that trace<span class="math inline">\((T^2)\geq 0\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.49 </strong></span>Suppose <span class="math inline">\(V\)</span> is an inner-product space and <span class="math inline">\(v, w\in V\)</span>. Define <span class="math inline">\(T\in \mathcal{L}(V)\)</span> by <span class="math inline">\(T u=\langle u,v \rangle w\)</span>. Find a formula for trace <span class="math inline">\(T\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.50 </strong></span>Prove that if <span class="math inline">\(P\in \mathcal{L}(V)\)</span> satisfies <span class="math inline">\(P^2=P\)</span>, then trace <span class="math inline">\(P\)</span> is a nonnegative integer.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.51 </strong></span>Prove that if <span class="math inline">\(V\)</span> is an inner-product space and <span class="math inline">\(T\in \mathcal{L}(V)\)</span>, then trace <span class="math inline">\(T^*=\overline{\text{trace} T}.\)</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.52 </strong></span>Suppose <span class="math inline">\(V\)</span> is an inner-product space and <span class="math inline">\(T\in \mathcal{L}(V)\)</span> is a positive operator with trace <span class="math inline">\(T=0\)</span>, then <span class="math inline">\(T=0\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.53 </strong></span>Suppose <span class="math inline">\(T\in \mathcal{L}(\mathbb{C}^3)\)</span> is the operator whose matrix is <span class="math display">\[
\begin{bmatrix}
51 &amp; -12 &amp; -21 \\
60 &amp; -40 &amp; -28 \\
57 &amp; -68 &amp; 1
\end{bmatrix} .
\]</span> If <span class="math inline">\(-48\)</span> and <span class="math inline">\(24\)</span> are eigenvalues of <span class="math inline">\(T\)</span>, find the third eigenvalue of <span class="math inline">\(T\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.54 </strong></span>Prove or give a counterexample: if <span class="math inline">\(T\in \mathcal{L}(V)\)</span> and <span class="math inline">\(c\in F\)</span>, then trace<span class="math inline">\((c T ) = c\)</span> trace <span class="math inline">\(T\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.55 </strong></span>Prove or give a counterexample,: if <span class="math inline">\(S, T\in \mathcal{L}(V)\)</span>, then trace <span class="math inline">\((S T)=(\text{trace} S)(\text{trace} T)\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.56 </strong></span>Suppose <span class="math inline">\(T\in \mathcal{L}(V)\)</span>. Prove that if <span class="math inline">\(\text{trace} (ST)=0\)</span> for all <span class="math inline">\(S\in \mathcal{L}(V)\)</span>, then <span class="math inline">\(T=0\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.57 </strong></span>Suppose <span class="math inline">\(V\)</span> is an inner-product space and <span class="math inline">\(T\in \mathcal{L}(V)\)</span>. Prove that if <span class="math inline">\((e_1,\ldots.,e_n)\)</span> is an orthonormal basis of <span class="math inline">\(V\)</span>, then <span class="math display">\[
\text{trace}(T^* T)=|| T e_1||^2+\cdots + ||T e_n||^2.
\]</span> Conclude that the right side of the equation above is independent of which orthonormal basis <span class="math inline">\((e_1,\ldots,e_n)\)</span> is chosen for <span class="math inline">\(V\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.58 </strong></span>Suppose <span class="math inline">\(V\)</span> is a complex inner-product space and <span class="math inline">\(T\in \mathcal{L}(V)\)</span>. Let <span class="math inline">\(\lambda_1,\ldots.,\lambda_n\)</span> be the eigenvalues of <span class="math inline">\(T\)</span>, repeated according to multiplicity.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.59 </strong></span>Suppose <span class="math display">\[
\begin{bmatrix}
a_{1,1} &amp; \cdots &amp; a_{1,n} \\
\vdots &amp;   &amp; \vdots \\
a_{n,1} &amp; \cdots &amp; a_{n,n} \\
\end{bmatrix}
\]</span> is the matrix of <span class="math inline">\(T\)</span> with respect to some orthonormal basis of <span class="math inline">\(V\)</span>. Prove that <span class="math display">\[
|\lambda_1|^2+\cdots + |\lambda_n|^2\leq \sum^n_{k=1} \sum_{j=1}^n |a_{j,k}|^2.
\]</span></p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.60 </strong></span>Suppose <span class="math inline">\(V\)</span> is an inner-product space. Prove that <span class="math inline">\(\langle S, T\rangle=\text{trace}(S T^*)\)</span> defines an inner-product on <span class="math inline">\(\mathcal{L}(V)\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.61 </strong></span>Suppose <span class="math inline">\(V\)</span> is an inner-product space and <span class="math inline">\(T\in \mathcal{L}(V)\)</span>. Prove that if <span class="math inline">\(||T^* v ||\leq ||T v||\)</span> for every <span class="math inline">\(v \in V\)</span>, then <span class="math inline">\(T\)</span> is normal.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.62 </strong></span>Prove or give a counterexample: if <span class="math inline">\(T\in \mathcal{L}(V)\)</span> and <span class="math inline">\(c\in F\)</span>, then <span class="math inline">\(\det(cT)=c^{\text{dim} V} \det T\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.63 </strong></span>Prove or give a counterexample: if <span class="math inline">\(T\in \mathcal{L}(V)\)</span>, then <span class="math inline">\(\det(S+T)=\det S+ \det T\)</span>.</p>
</div>
<div id="exm-" class="theorem example">
<p><span class="theorem-title"><strong>Example 4.64 </strong></span>Suppose <span class="math inline">\(A\)</span> is a block upper-triangular matrix <span class="math display">\[
A=
\begin{bmatrix}
A_1 &amp; &amp; * \\
&amp; \ddots &amp; \\
0 &amp; &amp; A_m
\end{bmatrix} ,
\]</span> where each <span class="math inline">\(A_j\)</span> along the diagonal is a square matrix. Prove that <span class="math display">\[
\det A =(\det A_1) \cdots (\det A_m).
\]</span></p>
</div>
</section>
</main> <!-- /main -->
<script type="application/ld+json">
    {
    "@context": "https://schema.org",
    "@type": "Organization",
    "name": "Direct Knowledge",
    "url": "https://diretcknowledge.com",
    "logo": "https://directknowledge.com/assets/directknowledge-logo.svg",
    "foundingDate": "2017",
    "founders": [
    {
    "@type": "Person",
    "name": "David Andrew Smith"
    },
    {
    "@type": "Person",
    "name": "David A. Smith"
    } ],
    "address": {
    "@type": "PostalAddress",
    "addressLocality": "Fort Worth",
    "addressRegion": "Texas",
    "addressCountry": "United States"
    },
    "contactPoint": {
    "@type": "ContactPoint",
    "contactType": "customer support",
    "email": "contact@directknowledge.com"
    },
    "sameAs": [
    "https://youtube.com/@directknowledge",
    "https://github.com/directknowledge/"
    ]
    }
</script>
<script type="application/ld+json">
    {
    "@context": "https://schema.org",
    "@type": "WebSite",
          "name": "Direct Knowledge",
        "url": "https://directknowledge.com"
    }
</script>
<script>
MathJax = {
  tex: {
    tags: 'ams',
    inlineMath: [['$', '$'], ['\\(', '\\)']],
    processRefs: true,
    processEnvironments: true,
    packages: {'[+]': ['newcommand']},
    processEnvironments: true,
  },
  svg: {
    fontCache: 'global'
  }
};
console.log("MaxJax Initalizing");
</script>
  
<script>
var xtimes = 1;
console.log(xtimes);
ActiveMaxJax = function() {
  if( xtimes == 1 ) {
    var script = document.createElement('script');
    script.src = 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
    script.async = true;
    document.head.appendChild(script);
    xtimes = 0;
    console.log(xtimes);
  } 
};
window.addEventListener("scroll", function() { ActiveMaxJax(); });
window.addEventListener("click", function() { ActiveMaxJax(); });
window.addEventListener("mousemove", function() { ActiveMaxJax(); });
window.addEventListener("keydown", function() { ActiveMaxJax(); });
window.addEventListener("touchstart", function() { ActiveMaxJax(); });
window.addEventListener("keydown", function() { ActiveMaxJax(); });
console.log("MaxJax Initialization Complete");
</script>
<script>
// Load the script after the user scrolls, moves the mouse, or touches the screen
document.addEventListener('scroll', initGTMOnEvent);
document.addEventListener('mousemove', initGTMOnEvent);
document.addEventListener('touchstart', initGTMOnEvent);
// Or, load the script after 2 seconds
document.addEventListener('DOMContentLoaded', () => { setTimeout(initGTM, 2000); });
// Initializes Google Tag Manager in response to an event
function initGTMOnEvent (event) {
	initGTM();
	event.currentTarget.removeEventListener(event.type, initGTMOnEvent);
}
// Initializes Google Tag Manager
function initGTM () {
	if (window.gtmDidInit) {
	  // Don't load again
	  return false;
	}
	window.gtmDidInit = true;
	
	// Create the script
	const script = document.createElement('script');
	script.type = 'text/javascript';
	script.onload = () => { 
	  window.dataLayer = window.dataLayer || [];
	  function gtag(){ dataLayer.push(arguments); }
	  gtag('js', new Date());
	  gtag('config', 'G-899QW82HQV');
	}
	script.src = 'https://www.googletagmanager.com/gtag/js?id=G-899QW82HQV';
	
	// We are still deferring the script
	script.defer = true;
	
	// Append the script to the body of the document
	document.getElementsByTagName('body')[0].appendChild(script);
}
</script>
<style>
    .theorem {
        background: linear-gradient(90deg,rgba(200, 217, 234, .5), rgba(200, 217, 234, .8));
        padding:6px 6px 2px 6px;
        border-left:1px solid rgba(0, 0, 255, 0.22);
        margin:12px 0px;
    }
    .theorem-title strong::after {
        content:'. ';
        margin-left:-4px;
        padding-right:5px;
    }
    .lemma {
        background: linear-gradient(90deg,rgba(188, 201, 168, 0.5), rgba(188, 201, 168, .8));
    }
    .corollary {
        background: linear-gradient(90deg,rgba(248, 197, 240, 0.5), rgba(248, 197, 240, .8));
    }
    .proposition {
        background: linear-gradient(90deg,rgba(199, 209, 156, 0.5), rgba(198, 230, 185, 0.8));
    }
    .definition {
        background: linear-gradient(90deg,rgba(210, 180, 140, .5), rgba(210, 180, 140, .8));
    }
    .example {
        background: linear-gradient(90deg,rgba(255, 255, 204, .5), rgba(255, 255, 204, .8));
    }
    .proof-title {
        font-weight:600;
    }
    #quarto-content ol li::marker {
        content: "(" counter(list-item, lower-arabic) ") ";
     }
     .solution:after, .proof:after {
        margin-top:-44px;
        margin-bottom:64px;
        float:right;
        content: '\25A0';
     }
     .blockquote {
        font-weight: 500!important;
        color:#000000!important;
     }
</style>
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
      var filterRegex = new RegExp(/https:\/\/directknowledge\.com\/learning-linear-algebra\//);
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
          // target, if specified
          link.setAttribute("target", "_blank");
      }
    }
});
</script>
<nav class="page-navigation">
  <div class="nav-page nav-page-previous">
      <a href="./linear-transformations.html" class="pagination-link">
        <i class="bi bi-arrow-left-short"></i> <span class="nav-page-text"><span class="chapter-number">3</span>&nbsp; <span class="chapter-title">Linear Transformations</span></span>
      </a>          
  </div>
  <div class="nav-page nav-page-next">
      <a href="./determinants.html" class="pagination-link">
        <span class="nav-page-text"><span class="chapter-number">5</span>&nbsp; <span class="chapter-title">Determinants</span></span> <i class="bi bi-arrow-right-short"></i>
      </a>
  </div>
</nav>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
      <div class="nav-footer-center">
        <ul class="footer-items list-unstyled">
    <li class="nav-item">
    <a class="nav-link" href="https://directknowledge.com">© Copyright 2023, All Rights Reserved. DirectKnowledge.com</a>
  </li>  
</ul>
      </div>
  </div>
</footer>
<script src="site_libs/quarto-html/zenscroll-min.js"></script>
<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
</body></html>