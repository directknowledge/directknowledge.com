[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Linear Algebra",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#short-description",
    "href": "index.html#short-description",
    "title": "Learning Linear Algebra",
    "section": "Short Description",
    "text": "Short Description\nIn this book, we cover the basics of linear algebra. Topics include systems of linear equations, finite-dimensional vector spaces, linear transformations, and inner-product spaces. Determinants, eigenvalues, and eigenvectors are also covered. While we explore these topics we’ll also learn how to use python when doing linear algebra. In the second part, we detail the structure of a linear operator by providing an algorithm for the Jordan canonical form of a linear transformation using python."
  },
  {
    "objectID": "index.html#other-resources",
    "href": "index.html#other-resources",
    "title": "Learning Linear Algebra",
    "section": "Other Resources",
    "text": "Other Resources\nMy YouTube channel, which contains videos discussing the main ideas of this book, is a great companion to accompany your reading experience. Read and write along as you easily access video embeds directly on this site!"
  },
  {
    "objectID": "index.html#update-history",
    "href": "index.html#update-history",
    "title": "Learning Linear Algebra",
    "section": "Update History",
    "text": "Update History\nHere is where I keep a log of the majors changes as I finish writing the book.\n\nFirst draft of preface published on 1/27/2023.\nFirst draft published on 1/18/2023."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Learning Linear Algebra",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nA hearty debt of gratitude is owed to those who worked tirelessly and selflessly to see this book through its final version. Their contributions were immense, making the publication process a true success - I am extremely thankful for their efforts!\nDavid A. Smith \\ Fort Worth, Texas"
  },
  {
    "objectID": "systems-of-linear-equations.html#the-elimination-method",
    "href": "systems-of-linear-equations.html#the-elimination-method",
    "title": "1  Systems of Linear Equations",
    "section": "1.1 The Elimination Method",
    "text": "1.1 The Elimination Method\nThe elimination method is an algebraic way to find the solution to a system of linear equations. To use the elimination method, add or subtract the equations so that one of the unknowns cancels out. This will give you a new equation with one fewer unknown. Repeat this process until you are left with an equation that has only one unknown. Solve this equation for the unknown. Now, substitute the value of the unknown back into one of the original equations. This will give you the solution to the system.\nThe elimination method is usually easiest when the coefficients (the numbers in front of the unknowns) are already opposites. However, it is sometimes necessary to multiply one or both equations by a constant before you can eliminate an unknown.\nThere are many ways to solve a system of linear equations. The best way to choose a method is to look at the equations and decide which one will be the easiest to work with. In general, the substitution method is best when one equation is already solved for one unknown. The elimination method is best when the coefficients of one unknown are opposites. The graphing method is a good way to check your work, but it can be difficult to find the intersection of two lines."
  },
  {
    "objectID": "systems-of-linear-equations.html#row-operations-and-similar-matrices",
    "href": "systems-of-linear-equations.html#row-operations-and-similar-matrices",
    "title": "1  Systems of Linear Equations",
    "section": "1.2 Row Operations and Similar Matrices",
    "text": "1.2 Row Operations and Similar Matrices\nIn solving systems of linear equations, we will often need to use row operations. Row operations are mathematical operations that can be performed on the rows of a matrix. There are three types of row operations:\n\nInterchange two rows\nMultiply a row by a nonzero constant\nAdd a multiple of one row to another row\n\nRow operations do not change the solution set of a system of linear equations. This means that if we perform row operations on a matrix, the new matrix will have the same solution set as the original matrix. We can use row operations to transform a matrix into an equivalent matrix. Two matrices are equivalent if they have the same solution set.\nWe can use row operations to solve systems of linear equations. In general, we will use row operations to transform the coefficient matrix into an upper triangular matrix. An upper triangular matrix is a matrix where all the elements below the main diagonal are zero. Once we have transformed the coefficient matrix into an upper triangular matrix, we can use back substitution to solve the system.\nWe can use row operations to transform a matrix into any equivalent matrix. However, some matrices are more difficult to work with than others. In general, it is easier to work with matrices that are in reduced row echelon form. A matrix is in reduced row echelon form if it is upper triangular and if the leading entry in each nonzero row is a 1. The leading entry in a row is the first nonzero entry in the row.\nWe can use row operations to transform any matrix into an equivalent matrix that is in reduced row echelon form. Once we have transformed the matrix into reduced row echelon form, we can use back substitution to solve the system of linear equations."
  },
  {
    "objectID": "systems-of-linear-equations.html#gaussian-elimination-method",
    "href": "systems-of-linear-equations.html#gaussian-elimination-method",
    "title": "1  Systems of Linear Equations",
    "section": "1.3 Gaussian Elimination Method",
    "text": "1.3 Gaussian Elimination Method\nThe Gaussian elimination method is another way to solve a system of linear equations. To use the Gaussian elimination method, you must first arrange the equations into row form. This will make it easier to see how to eliminate an unknown. Next, multiply each equation by a constant so that one of the unknowns cancels out. Then, add or subtract these equations so that another unknown cancels out. Finally, solve the equation resulting from this process for the last unknown. Now, substitute this value back into one of the original equations and you have the solution to the system!\nThe Gaussian Elimination Method is a procedure used to solve systems of linear equations. The method is named after German mathematician Carl Friedrich Gauss and Irish mathematician William Rowan Hamilton, who developed it independently in the early 19th century."
  },
  {
    "objectID": "systems-of-linear-equations.html#gauss-jordan-elimination-method",
    "href": "systems-of-linear-equations.html#gauss-jordan-elimination-method",
    "title": "1  Systems of Linear Equations",
    "section": "1.4 Gauss-Jordan Elimination Method",
    "text": "1.4 Gauss-Jordan Elimination Method\nThe Gauss-Jordan Elimination Method is similar to the traditional Gaussian Elimination Method, but it is more efficient and easier to use. The method works by first transforming the system of equations into an equivalent system with a triangular form. Then, the solution is obtained by a backward substitution.\nThis method is called Gauss-Jordan Elimination because it was developed by two mathematicians, Carl Friedrich Gauss and Wilhelm Jordan. Although it is not the quickest or most intuitive method, it is a robust way to solve linear equations. When done correctly, it will always give you the correct answer."
  },
  {
    "objectID": "systems-of-linear-equations.html#conclusion",
    "href": "systems-of-linear-equations.html#conclusion",
    "title": "1  Systems of Linear Equations",
    "section": "1.5 Conclusion",
    "text": "1.5 Conclusion\nThere are many ways to solve a system of linear equations. The best way to choose a method is to look at the equations and decide which one will be the easiest to work with. In general, the substitution method is best when one equation is already solved for one unknown. The elimination method is best when the coefficients of one unknown are opposites. The Gaussian elimination method is best when there are a lot of equations and unknowns. Lastly, the Gauss-Jordan elimination method is the most robust way to solve linear equations. It is not always the quickest or easiest method, but it will always give you the correct answer.\nNo matter which method you choose, make sure you understand the steps involved and why they work. This will help you not only solve the equations correctly but also to understand what is happening behind the scenes. With a little practice, you will be solving systems of linear equations like a pro!\nA linear system of equations in two variables \\(x\\) and \\(y\\) has the form \\[\n\\begin{cases}\na x + b y =c\\\\\nd x+ e y =f\n\\end{cases}\n\\] where \\(a, b, c, d, e, f\\) are given numbers such as real numbers or complex numbers. Sometimes a linear system is also written using indices\n\\[\\begin{equation}\n\\label{2by2sys}\n\\begin{cases}\na_{11} x+a_{12} y=b_1\\\\\na_{21}x+ a_{22} y =b_2\n\\end{cases}\n\\end{equation}\\]\nto help reduce on the number of letters used. For simplicity let’s temporally assume that the coefficients are nonzero real numbers and ask the question: how many solutions can there be to a \\(2\\times 2\\) system? The key idea is to realize that each linear equation in \\(\\ref{2by2sys}\\) represents a line in the Cartesian plane. If we consider the possible ways lines can intersect in the plane we come to the conclusion that there must either be no solutions, one unique solution, or infinitely many points \\((x,y)\\) that solves the system in \\(\\ref{2by2sys}\\).\n\nExample 1.1  Determine whether the linear system \\[\n\\begin{cases}\n2x+3y =0 \\\\\n4x+5y=0.\n\\end{cases}\n\\] has no solutions, exactly one solution, or infinitely many solutions. If we multiple the first equation, namely \\(2x+3y=0\\) by 2 and subtract from the second equation, \\(4x+5y=0\\), we obtain \\(y=0\\). Therefore the solution is unique and is \\((x,y)=(0,0)\\).\n\nA \\(3\\times 3\\) linear system has the form \\[\\begin{equation}\n\\label{3by3sys}\n\\begin{cases}\na_{11} x+a_{12} y+a_{13}z =b_1\\\\\na_{21} x+a_{22} y+a_{23}z =b_2\\\\\na_{31} x+a_{32} y+a_{33}z =b_3\\\\\n\\end{cases}\n\\end{equation}\\] where \\(a_{ij}\\) and \\(b_1, b_2, b_3\\) are numbers and \\(x, y, z\\) are variables. Geometrically, linear equations in three variables are just planes in three dimensions. So what are the different types of solution sets for the system in \\(\\ref{3by3sys}\\)? By considering the possible ways three planes might intersect in three dimensions we come to the conclusion that there must either be no solutions, one unique solution, or infinitely many points \\((x,y,z)\\) that solves the system in \\(\\ref{3by3sys}\\).\n\nExample 1.2  Determine whether the linear system \\[\\begin{equation}\n\\label{linesysex1}\n\\begin{cases}\nx+2y+3z=0 \\\\\n4x+5y+6z=3 \\\\\n7x+8y+9z =0\n\\end{cases}\n\\end{equation}\\] has no solutions, exactly one solution, or infinitely many solutions. Multiply the first equation by \\(-2\\) and add to the second equation obtaining the equation \\(2x+y=3\\). Eliminating \\(z\\) from the first and third equations we obtain \\(4x+2y=0\\) by multiplying the first equation by \\(-3\\) and adding to the third equation. The \\(x\\) and \\(y\\) that satisfies \\(\\ref{linesysex1}\\) must also satisfy the system \\[\\begin{equation}\n\\label{linesysex1b}\n\\begin{cases}\n2x+y=3 \\\\\n4x+2y=0.\n\\end{cases}\n\\end{equation}\\] Multiply the first equation of \\(\\ref{linesysex1b}\\) by \\(2\\) yields \\(4x+2y=6\\). Notice there are no \\(x\\) and \\(y\\) that satisfies both \\(4x+2y=6\\) and \\(4x+2y=0\\). Thus the system in \\(\\ref{linesysex1b}\\) has no solutions; therefore the system in \\(\\ref{linesysex1}\\) also has no solutions.\n\n\nExample 1.3 Given numbers \\(a, b\\), and \\(c\\), show that the linear system \\[\n\\begin{cases}\nx+2y +3z = a\\\\\nx+3y +8z = b\\\\\nx+2y +2z = c\n\\end{cases}\n\\] either has no solutions, exactly one solution, or infinitely many solutions. We choose to eliminate \\(x\\) first and we obtain the system \\[\n\\begin{cases}\n-y-5z  =a-b\\\\\nz  =a-c.\n\\end{cases}\n\\] Next we eliminate \\(z\\) obtaining \\(y=-6a+b+5c\\). Therefore the unique solution is \\((x,y,z)=(10a-2b-7c,-6a+b+5c,a-c)\\).\n\nLet’s consider a set of linear equations that involves \\(n\\) unknown quantities represented by \\(\\vlist{x}{n}\\). Let \\(a_{ij}\\) represent the number that is the coefficient of \\(x_j\\) in the \\(i\\)th equation. Let \\(\\vlist{b}{m}\\) be given numbers. The linear system of equations \\[\\begin{equation}\n\\label{syseq}\n\\begin{cases}\na_{11} x_1+a_{12} x_2+\\cdots +a_{1n} x_n =  b_1 \\\\\na_{21} x_1+a_{22} x_2+\\cdots +a_{2n} x_n =  b_2 \\\\\n\\hfill \\vdots \\hfill \\\\\na_{m1} x_1+a_{m2} x_2+\\cdots +a_{mn} x_n =  b_m \\\\\n\\end{cases}\n\\end{equation}\\] is called a system of simultaneous linear algebraic equations. A solution of this system is an ordered set of \\(n\\) numbers that satisfies each of the \\(m\\) statements in the system. A linear system of equations with no solution is called and a system with at least one solution is called inconsistent . The array\n\\[\n\\left[\n\\begin{array}{l|l}\n\\begin{matrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n}\\\\\n& & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots  & a_{mn}\\\\\n\\end{matrix}\n&\n\\begin{matrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m\n\\end{matrix}\n\\end{array}\n\\right]\n\\] is called the corresponding to the linear system in \\(\\ref{syseq}\\) augmented matrix.\nFor example, the linear system in \\(\\ref{example:2by2system}\\) was shown to be consistent and the linear system in \\(\\ref{example:3by3system}\\) was shown to be inconsistent. The augmented matrices for these systems are as follows. \\[\n\\left[\n\\begin{array}{l|l}\n\\begin{matrix}\n2 & 3 \\\\\n4 & 5\n\\end{matrix}\n&\n\\begin{matrix}\n0 \\\\ 0\n\\end{matrix}\n\\end{array}\n\\right]\n\\qquad\n%\\text{and}\n\\qquad\n\\left[\n\\begin{array}{l|l}\n\\begin{matrix}\n1& 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{matrix}\n&\n\\begin{matrix}\n0 \\\\ 3 \\\\0\n\\end{matrix}\n\\end{array}\n\\right]\n\\]\n\nExample 1.4  Find all solutions to the following linear system. \\[\n\\begin{cases}\n-150 x+500y=z \\\\\n50x+100y+z =200\n\\end{cases}\n\\] The given system is equivalent to \\[\n\\begin{cases}\n-150 x+500y-z=0 \\\\\n50x+100y+z =200.\n\\end{cases}\n\\] Adding these equations yields \\(-100x+600y=200\\). Since we have one equation with two variables, one of the variables is free. We choose to let \\(y\\) be free. Let \\(y=t\\) for an arbitrary number \\(t\\). Then solving for \\(x\\) we obtain \\(-100x=200y-600y\\), or equivalently \\(x=-2+6t\\). By substituting into the original system, we find \\(z=-150(-2+6t)+500t=300-400t.\\) Therefore, there are an infinitely many solutions, which can be represented by the set \\[\n\\{(x,y,z) \\mid x=-2+6t, y=t, z=300-400t \\text{ where $t\\in \\mathbb{R} $} \\}.\n\\]\n\nThe augmented matrix for the system in \\(\\ref{example:2by3system}\\) is \\[\n\\left[\n\\begin{array}{l|l}\n\\begin{matrix}\n-150 & 500 & -1 \\\\\n50 & 100 & 1\n\\end{matrix}\n&\n\\begin{matrix}\n0 \\\\ 200\n\\end{matrix}\n\\end{array}\n\\right]\n\\] Do you think it is possible to decide, by inspection of an augmented matrix, whether or not the corresponding linear system will be consistent or inconsistent?\nIn our examples so far we have seen linear systems having no solutions, a unique solution, or perhaps an infinite number of solutions. These examples suggest the following definition.\n\nDefinition 1.1 Two linear systems are called equivalent if they have the same solution set.\n\n\nExample 1.5 Find a system of linear equations with three unknowns \\(x,y,z\\) whose solutions are \\(x=6+5t\\), \\(y=4+3t\\), \\(z=2+t\\) where \\(t\\) is arbitrary. We want to eliminate \\(t\\). Solving for \\(t\\) yields \\(t=z-2\\). By substitution, \\(x=6+5(z-2)\\) and \\(y=4+3(z-2)\\). Thus we have a linear system \\[\n\\begin{cases}\nx-5z=-4\\\\\ny-3z =-2\n\\end{cases}\n\\] which has an infinitely many solutions.\n\n\nTheorem 1.1  Linear systems of equations are equivalent if each can be obtained from the other by one or more of the following operations.\n\nInterchange the order of the equations.\nMultiply (or divide) one equation by a nonzero scalar.\nAdd one multiple of one equation to another.\n\n\n\nProof. If we consider the solution set as a geometric object, it should be very easy to understand that they way in which we write the equations that represents the object does not change the object. Thus it should be obvious that interchanging the order of the equations will not change the solutions to a linear system. Neither will multiplying (or dividing) both sides of an equation by a nonzero constant.\nLet system \\(A\\) be an \\(m\\times n\\) system represented by \\[\\begin{equation}\n\\label{syseqA}\n\\begin{cases}\na_{11} x_1+a_{12} x_2+\\cdots +a_{1n} x_n =  b_1 \\\\\n\\hfill \\vdots \\hfill \\\\\na_{i1} x_1+a_{i2} x_2+\\cdots +a_{in} x_n =  b_i  \\\\\n\\hfill \\vdots \\hfill \\\\\na_{m1} x_1+a_{m2} x_2+\\cdots +a_{mn} x_n =  b_m & \\\\\n\\end{cases}\n\\end{equation}\\] Consider system \\(B\\) obtained from system \\(A\\) by adding \\(k\\) times equation \\(i\\) to equation \\(j\\) as follows: \\[\\begin{equation}\n\\label{syseqB}\n\\begin{cases}\na_{11} x_1+a_{12} x_2+\\cdots +a_{1n} x_n =  b_1  \\\\\n\\qquad \\qquad \\vdots \\qquad \\qquad \\qquad  \\vdots \\\\\n(a_{j1}+k a_{i1}) x_1+(a_{j2}+k a_{i2}) x_2+\\cdots +(a_{jn}+k a_{in}) x_n =  b_j +k b_i\\\\ \\qquad \\qquad  \\vdots \\qquad \\qquad \\qquad  \\vdots \\\\\na_{m1} x_1+a_{m2} x_2+\\cdots +a_{mn} x_n =  b_m \\\\\n\\end{cases}\n\\end{equation}\\] Let \\(S_A\\) and \\(S_B\\) be the solutions sets of systems \\(A\\) and \\(B\\), respectively. We will show \\(S_A=S_B\\). Let \\(x_0=(\\vlist{x}{n})\\) be a solution of system \\(A\\). Thus \\(x_0\\) satisfies every linear equation in \\(\\ref{syseqA}\\). So \\(x_0\\) satisfies every equation in system B except possibly the \\(j\\)th equation. Working with the \\(j\\)th equation in system \\(B\\) we find \\[\\begin{align*}\n& (a_{j1}+k a_{i1}) x_1+(a_{j2}+k a_{i2}) x_2+\\cdots +(a_{jn}+k a_{in}) x_n \\\\\n& \\qquad = a_{j1}x_1+\\cdots +a_{jn} x_n+k (a_{i1}x_1+\\cdots a_{jn}x_n)\\\\\n& \\qquad =  b_j +k b_i.\\end{align*}\\] This shows \\(x_0\\) also satisfies the \\(j\\)th equation of system \\(B\\). Since \\(x_0\\) satisfies every equation in \\(\\ref{syseqB}\\), \\(x_0\\) is also a member of \\(S_B\\). So far we have shown \\(S_A\\subseteq S_B\\). Conversely, assume \\(x_0\\) is a solutions to every equation in \\(\\ref{syseqB}\\). Working with the \\(j\\)th equation of system \\(A\\) we find, \\[\\begin{align}\n& a_{j1} x_1+a_{j2} x_2+\\cdots +a_{jn}x_n=b_j \\notag \\\\\n\\Longleftrightarrow \\quad & a_{j1} x_1+ (k a_{i1}-ka_{i1})x_1\n%+ a_{j2} x_2+ (k a_{21}-ka_{21})x_2\n+\\cdots +a_{jn} + (k a_{i1}-ka_{i1})x_n=b_j \\label{eqsys}\\notag \\\\\n\\Longleftrightarrow \\quad & (a_{j1}+k a_{i1}) x_1+(a_{j2}+k a_{i2}) x_2+\\cdots +(a_{jn}+k a_{in}) x_n = b_j+k b_i.\n\\end{align}\\] Notice \\(\\ref{eqsys}\\) holds since \\(x_0\\) satisfies system \\(B\\); and thus \\(x_0\\) satisfies the \\(j\\)th equation of system \\(A\\). Therefore \\(S_A=S_B\\) as desired.\n\nWhile working through the details of the following two examples, notice how the operations in \\(\\ref{proprowoperations}\\) are being used.\n\nExample 1.6 Let \\(a, b\\), and \\(c\\) be constants. Solve the linear system \\[\n\\begin{cases}\ny+z  =a \\\\\nx+z =b \\\\\nx+y = c.\n\\end{cases}\n\\] Eliminating \\(z\\) from the first and second equations we obtain the system \\[\n\\begin{cases}\nx-y = b-a \\\\\nx+y=c\n\\end{cases}\n\\] Solving for \\(x\\) yields \\(x=(c+b-a)/2\\). Using the original system we find \\(y\\) and \\(z\\) to be \\[\ny= c-x=c-(c+b-a)/2=(c-b+a)/2\n\\] \\[\nz=a-y=a-(c-b+a)/2=(a+b-c)/2\n\\] Therefore, the solution to the system is \\[\n(x,y,z)=\\left(\\frac{c+b-a}{2},\\frac{a+c-b}{2},\\frac{a+b-c}{2}\\right).\n\\]\n\n\nExample 1.7 Find the smallest positive integer \\(C\\) such that \\(x, y, z\\) are integers and satisfies the linear system of equations \\[\n\\begin{cases}\n2x+y =C \\\\\n3y+z =C \\\\\nx+4z = C.\n\\end{cases}\n\\] Multiply the third equation by \\(-2\\) and add to the first equation, obtaining \\(y-8z=-C\\). Multiplying the second equation by 8 and adding to \\(y-8z=-C\\), yields \\(25y=7C\\). Solving for \\(y\\) we obtain \\(y=(7/25)C\\). By substitution, \\(x=(9/25)C\\) and \\(z=(4/25)C\\). Therefore, 25 is the smallest integer \\(C\\) such that \\(x,y,z\\) are integers and solves the system.\n\nWe will solve a linear system of equations using elementary row operations on matrices using a procedure known as Gaussian elimination . The solution set will be a collection of vectors. Here is the overall strategy to solving linear systems using matrices. \\[\n\\small\n\\begin{tabular}{c}\n$\\left\\{\\parbox[c]{1.75cm}{\\begin{center}linear system of equations\\end{center}}\\right\\}$  $\\rightarrow$  \n$\\left\\{\\parbox[c]{1.75cm}{\\begin{center}matrix representation of system\\end{center}}\\right\\}$\n$\\rightarrow$\n$\\left\\{\\parbox[c]{1.75cm}{\\begin{center}row echelon form of system\\end{center}}\\right\\}$\n$\\rightarrow$  \n$\\left\\{\\parbox[c]{1.75cm}{\\begin{center}solutions of system as a set of vectors\\end{center}}\\right\\}$\n\\end{tabular}\\]\n\nDefinition 1.2 The following operations are collectively known as elementary row operations .\n\nInterchange two rows.\nMultiply a row by a nonzero scalar.\nAdd a multiple of a row from another row.\n\n\nFrom \\(\\ref{proprowoperations}\\) it is obvious that applying elementary row-operations to a linear system of equations leads to an equivalent system. Reducing a linear system of equations, while preserving the solutions set, is an extremely useful idea that we will develop extensively.\nFor example, in \\(\\ref{gjeunique}\\) (see below) we find that the linear systems \\[\\begin{equation}\n\\label{refex}\n\\begin{cases}\nx+2y+z =3 \\\\\n2x+5y-z =-4 \\\\\n3x-2y-z =5\n\\end{cases}\n\\qquad\n\\qquad\n\\begin{cases}\nx=2 \\\\\ny=-1 \\\\\nz=3\n\\end{cases}\n\\end{equation}\\] are equivalent. In \\(\\ref{gjeunique}\\) we will show how to apply elementary row-operations to obtain the system on the right. While the system on the left might be a given linear system of equations, the system on the right is solved.\n\nDefinition 1.3 A matrix is said to be in row echelon form if it satisfies all the following conditions.\n\nAll rows with at least one nonzero coefficient are above any rows of all zeroes.\nThe first nonzero number from the left, (called the leading coefficient ) of a nonzero row is always strictly to the right of the leading coefficient of the row above it.\nAll entries in a column below a leading coefficient are zeroes.\n\nFurther, a matrix is said to be in reduced row echelon form if it is in row echelon form and the additional condition holds.\n\nEvery leading coefficient is 1 and is the only nonzero entry in its column.\n\n\nFor example, the augmented matrices for the linear systems in \\(\\ref{refex}\\) are \\[\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 1 & 3 \\\\\n2 & 5 & -1 & -4 \\\\\n3 & -2 & -1 & 5\n\\end{array}\n\\end{bmatrix}\\qquad\n\\qquad\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 0 & 0 & 2 \\\\\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1& 3\n\\end{array}\n\\end{bmatrix}.\n\\] By checking conditions (i)-(iv) we can see that the matrix on the right is in reduced row echelon form. For more examples, consider the following matrices. \\[\nA = \\begin{bmatrix} 0 & 5 \\\\ 2 & 3 \\end{bmatrix}\n\\qquad\nB = \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\end{bmatrix}\n\\qquad\nC = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\] \\[\nD = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}\n\\qquad\nE = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\qquad\nF = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}\n\\] Notice that matrices \\(D\\) and \\(E\\) are in reduced row echelon form and the others are not.\n\nExample 1.8  Use Gauss-Jordan elimination to solve the system. \\[\n\\begin{cases}\nx+2y+z =3 \\\\\n2x+5y-z =-4 \\\\\n3x-2y-z =5\n\\end{cases}\n\\] Using row operations on the augmented matrix we obtain the reduced row echelon form. \\[\\begin{align*}\n& \\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 1 & 3 \\\\\n2 & 5 & -1 & -4 \\\\\n3 & -2 & -1 & 5\n\\end{array}\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_1+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle -3R_1+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 1 & 3 \\\\\n0 & 1 & -3 & -10 \\\\\n0 & -8 & -4 & -4\n\\end{array}\n\\end{bmatrix}\n\\\\ & \\qquad \\qquad\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle 8 R_2+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 1 & 3 \\\\\n0 & 1 & -3 & -10 \\\\\n0 & 0 & -28 & -84\n\\end{array}\n\\end{bmatrix}\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -\\frac{1}{28}R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 1 & 3 \\\\\n0 & 1 & -3 & -10\\\\\n0 & 0 & 1 & 3\n\\end{array}\n\\end{bmatrix}\n\\\\ &  \\qquad \\qquad\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle 3R_3+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle -R_3+R_1}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 0 & 0 \\\\\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1 & 3\n\\end{array}\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_2+R_1}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 0 & 0 & 2 \\\\\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1& 3\n\\end{array}\n\\end{bmatrix}.\n\\end{align*}\\] Therefore the unique solution is \\(x=2\\), \\(y=-1\\), and \\(z=3\\).\n\n\nExample 1.9 Use the Gauss-Jordan elimination method to solve the following linear system. \\[\\begin{equation}\n\\label{gjee1}\n\\begin{cases}\nx+y -2z+4t =5 \\\\\n2x+2y-3z+t =3 \\\\\n3x+3y-4z-2t  = 1\n\\end{cases}\n\\end{equation}\\] Using row operations on the augmented matrix we obtain the reduced row echelon form. \\[\\begin{align*}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & -2 & 4 & 5 \\\\\n2 & 2 & -3 & 1 & 3 \\\\\n3 & 3 & -4 & -2 & 1\n\\end{array}\n\\end{bmatrix}\n& \\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_1+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle -3R_1+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & -2 & 4 & 5 \\\\\n0 & 0 & 1 & -7  & -7 \\\\\n0 & 0 & 2 & -14 & -14\n\\end{array}\n\\end{bmatrix}\n\\\\ &\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_2+R_3}\n\\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle 2R_2+R_1}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & 0 & -10 & -9 \\\\\n0 & 0 & 1 & -7 & -7 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{array}\n\\end{bmatrix}\n\\end{align*}\\] The system in \\(\\ref{gjee1}\\) is equivalent to \\[\n\\begin{cases}\nx+y-10t =-9 \\\\\nz-7t  = -7\n\\end{cases}\n\\quad \\text{ or more simply } \\quad\n\\begin{cases}\nx =-9-y+10t \\\\\nz =-7+7t.\n\\end{cases}\n\\] Therefore \\[\n\\{(x,y,z,w)\\in \\mathbb{R}^4 \\mid x=-9-y+10t, y=s, z=-7+7t, w=t,\n\\text{ for } s,t\\in \\mathbb{R}\\}.\n\\] is the solution set.\n\n\nExample 1.10 Use the Gauss-Jordan elimination method to solve the following linear system. \\[\n\\begin{cases}\nx_1+x_2-2x_3+3x_4 =4 \\\\\n2x_1+3x_2+3x_3-x_4=3 \\\\\n5 x_1+7 x_2+4 x_3+x_4 = 5\n\\end{cases}\n\\] Using row operations on the augmented matrix we obtain the reduced row echelon form. \\[\\begin{align*}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & -2 & 3 & 4 \\\\\n2 & 3 & 3 & -1 & 3 \\\\\n5 & 7 & 4 & 1 & 5\n\\end{array}\n\\end{bmatrix}\n& \\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_1+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle -5R_1+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & -2 & 3 & 4 \\\\\n0 & 1 & 7 & -7 & -5 \\\\\n0 & 2 & 14 & -14 & 15\n\\end{array}\n\\end{bmatrix}\n\\\\ & \\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_2+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & -2 & 3 & 4\\\\\n0 & 1 & 7 & -7 & -5 \\\\\n0 & 0 & 0 & 0 & 25\n\\end{array}\n\\end{bmatrix}\n\\end{align*}\\] Notice the last row corresponds to the equation \\(0x_1+0x_2+0x_3+0x_4=25\\). Therefore there are no solutions to this system. Actually using Gauss-Jordan elimination means reaching reduced row echelon form; however, once we encounter the row \\([0 \\, \\, \\, 0 \\, \\, \\, 0 \\, \\, \\, 0 \\, \\, \\, 25 ]\\) we immediately stop and reach the correct conclusion: no solutions to the system.\n\n\nExample 1.11 Find the polynomial of degree 2 whose graph goes through the points \\((1,-1)\\), \\((2,3)\\), and \\((3,13)\\). Let \\(f(t)=a+b t+ c t^2\\) be such a polynomial. Using the given points we setup a system and solve. \\[\\begin{align*}\n&\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & b & c\\\\\na & 2b & 4c \\\\\na & 3b & 9c\n\\end{matrix} &  \n\\begin{matrix}\n-1 \\\\ 3 \\\\ 13\n\\end{matrix}\n\\end{array} \\right]\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -R_1+R_2} \\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle -R_1+R_3}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & b & c\\\\\n0 & b & 3c \\\\\n0 & 2b & 8c\n\\end{matrix} &  \n\\begin{matrix}\n1 \\\\ 4 \\\\ 14\n\\end{matrix}\n\\end{array} \\right]\n\\\\ & \\qquad \\qquad\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_2+R_3}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & b & c\\\\\n0 & b & 3c \\\\\n0 & 0 & 2c\n\\end{matrix} &  \n\\begin{matrix}\n-1 \\\\ 4 \\\\ 6\n\\end{matrix} \\end{array} \\right]\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle \\frac{1}{2}R_3}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & b & c\\\\\n0 & b & 3c \\\\\n0 & 0 & c\n\\end{matrix} &  \n\\begin{matrix}\n-1 \\\\ 4 \\\\ 3\n\\end{matrix}\n\\end{array} \\right]\n\\\\ & \\qquad \\qquad\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -3R_3+R_2}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & b & c\\\\\n0 & b & 0 \\\\\n0 & 0 & c\n\\end{matrix} &  \n\\begin{matrix}\n-1 \\\\ -5 \\\\ 3\n\\end{matrix}\n\\end{array} \\right]\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -R_2+R_1}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na& 0 & c\\\\\n0 & b & 0 \\\\\n0 & 0 & c\n\\end{matrix} &  \n\\begin{matrix}\n4 \\\\ -5 \\\\ 3\n\\end{matrix}\n\\end{array} \\right]\n\\\\ & \\qquad \\qquad\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -R_3+R_1}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & 0 & 0 \\\\\n0 & b & 0 \\\\\n0 & 0 & c\n\\end{matrix} &  \n\\begin{matrix}\n1 \\\\ -5 \\\\ 3\n\\end{matrix}\n\\end{array} \\right].\n\\end{align*}\\] Therefore the required polynomial is \\(f(t)=3t^2-5t+1\\)."
  },
  {
    "objectID": "systems-of-linear-equations.html#exercises",
    "href": "systems-of-linear-equations.html#exercises",
    "title": "1  Systems of Linear Equations",
    "section": "1.6 Exercises",
    "text": "1.6 Exercises\n\nExercise 1.1 Verify that \\((2,3,-1)\\) is a solution of the linear system.\n\n\\(\\begin{cases}x+2y+z=7 \\\\ x-y=-1 \\\\ 4x+y+z=10 \\end{cases}\\)\n\\(\\begin{cases} x+y=5 \\\\ x-z=3 \\\\ y+z=2 \\end{cases}\\)\n\n\n\nExercise 1.2 Verify that every triple of the form \\((7-2k,8+6k,k)\\) is a solution of the linear system of equations\n\n\\(\\begin{cases} x_1+2x_3=7 \\\\ x_2-6x_3=8 \\end{cases}\\)\n\\(\\begin{cases} 2x_1+4x_3=14 \\\\ x_1+3x_2-16x_3=31 \\end{cases}\\)\n\n\n\nExercise 1.3 Do the following systems have no solutions, exactly one solution, or infinitely many solutions? Justify your answer.\n\n\\(\\begin{cases}x+y=1\\\\ x+2y=1 \\end{cases}\\)\n\\(\\begin{cases} 3x+y=1\\\\ y=-2 \\end{cases}\\)\n\\(\\begin{cases}x+y=1\\\\ 2x+2y=2 \\end{cases}\\)\n\\(\\begin{cases}3x+y=2 \\\\ 6x+2y=4 \\end{cases}\\)\n\n\n\nExercise 1.4 Sketch the graph of each equation of the linear system and decide whether it has no solutions, exactly one solution, or infinitely many solutions.\n\n\\(\\begin{cases} x+y=2\\\\ 2x+3y=0 \\end{cases}\\)\n\\(\\begin{cases} -x+3y=2\\\\ 2x-6y=-4 \\end{cases}\\)\n\n\n\nExercise 1.5 Find the solutions, if any, of the following linear systems of equations without using matrices. If an equation has more than one solution, write the general solution.\n\n\\(\\begin{cases} 3x-2y= 7 \\\\ 2x+y=15 \\end{cases}\\)\n\\(\\begin{cases} x+y=7\\\\ 3x+4y=12 \\end{cases}\\)\n\\(\\begin{cases} 2x_1-3x_2=1\\\\ 4x_1-6x_2=-2\\\\ x_1+x_2=1 \\end{cases}\\)\n\\(\\begin{cases} 2x_1-5x_2=12 \\end{cases}\\)\n\\(\\begin{cases} i x_1-3ix_2= 1\\\\ (2+i)x_1-x_2=-1 \\end{cases}\\)\n\\(\\begin{cases} (1+i)x_1-2ix_2=2\\\\ 2x_1-3i x_2=4-3i \\end{cases}\\)\n\n\n\nExercise 1.6 If possible, find the point(s) of intersection.\n\n\\(x-4y=11\\) and \\(7x-2y=9\\)\n\\(x-4y+3z=11\\), \\(7x-2y-z=-1\\), \\(7x-2y+z=-2\\)\n\n\n\nExercise 1.7 Let \\(u=(1,1,2,-1)\\) and \\(v=(1,1,1,0)\\). For which scalars \\(a\\) and \\(b\\) is it true that \\(a u+b v\\) is a solution to the following system?\n\n\\(\\begin{cases} 4x-2y-z-w=1 \\\\ x+3y-2z-2w=2 \\end{cases}\\)\n\\(\\begin{cases}x-4y-z-2w=4 \\\\ 7x+y-5z-2w=12 \\end{cases}\\)\n\n\n\nExercise 1.8 Find all solutions, if any exist, for the following linear systems of equations.\n\n\\(\\begin{cases} x-y=1\\\\ 2x=4 \\end{cases}\\)\n\\(\\begin{cases} 2x+3y-z=19\\\\ 3x-2y+3z=7\\end{cases}\\)\n\\(\\begin{cases}x-3y=2 \\\\-2x+6y=-4\\end{cases}\\)\n\\(\\begin{cases}x-y+2z-2w=1\\\\2x+y+3w=4 \\\\2x+y+3w=6\\end{cases}\\)\n\n\n\nExercise 1.9 Find all solutions, if any exist, for the following linear systems of equations.\n\n\\(\\begin{cases}2x-z=-1\\\\x+y-z=0\\\\2x-y+2z=3\\end{cases}\\)\n\\(\\begin{cases}2x-3y+2z=1\\\\x-6y+z=2\\\\-x-3y-z=1\\end{cases}\\)\n\n\n\nExercise 1.10 Find all solutions, if any exist, for the following linear systems of equations.\n\n\\(\\begin{cases} x_1+2x_2-x_3=0\\\\ 2x_1+5x_2+5x_3=0\\\\ x_1+4x_2+7x_3=0\\\\ x_1+3x_2+3x_3=0 \\end{cases}\\)\n\\(\\begin{cases} x+y+z+w=1\\\\ 2x-2y+z+2w=3\\\\ 2x+6y+3z+2w=1\\\\ 5x-3y+3z+5w=8 \\end{cases}\\)\n\n\n\nExercise 1.11 For what values of the constant \\(k\\) do the systems have no solution, exactly one solution, or infinity many solutions.\n\n\\(\\begin{cases} x+y=1\\\\ 3x+3y=k \\end{cases}\\)\n\\(\\begin{cases} x+ky=1\\\\ 2x-y=k \\end{cases}\\)\n\n\n\nExercise 1.12 Find \\(a, b, c\\) such that \\[\n\\frac{x^2-x+3}{(x^2+2)(2x-1)}=\\frac{a x+b}{x^2+2}+\\frac{c}{2x-1}.\n\\]\n\n\nExercise 1.13 Let \\(a\\) and \\(b\\) be arbitrary constants. Find all solutions to the linear system.\n\n\\(\\begin{cases} x+2y=a\\\\ 3x+5y=b \\end{cases}\\)\n\\(\\begin{cases} a x+2y=1\\\\ 3x+by=4 \\end{cases}\\)\n\n\n\nExercise 1.14 Let \\(a\\) and \\(b\\) be arbitrary constants. Find all solutions to the linear system.\n\n\\(\\begin{cases}x+2y+3z=a\\\\ x+3y+8z=b\\\\ x+2y+2z=c \\end{cases}\\)\n\\(\\begin{cases} x-2y+4z=a\\\\ x-3y+5z=b\\\\ x-2y+6z=c \\end{cases}\\)\n\n\n\nExercise 1.15 A system of linear equations all of whose constant terms are zero is called a homogenous system.\n\nShow that a homogenous system always has at least one solution.\nGive examples to show that a homogenous system may have more than one solution or exactly one solution.\n\n\n\nExercise 1.16 Write the system corresponding to each augmented matrix.\n\n\\(\\left[\\begin{array}{ccc|c} 1 & 2 & 1 & 1 \\\\ 0 & 4 & 0 & 1 \\\\ 0 & 0 & 3 & 3 \\end{array}\\right]\\)\n\\(\\left[\\begin{array}{cccc|c} 1 & 0 & 0 & 0 & 0 \\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ccc|c} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{array}\\right]\\)\n\\(\\left[\\begin{array}{c|c} 2 & 1 \\\\ 0 & 1 \\\\ 1 & 2 \\end{array}\\right]\\)\n\\(\\left[\\begin{array}{c|c} 1 & 1 \\\\ 1 & 0 \\end{array}\\right]\\)\n\\(\\left[\\begin{array}{cccc|c} 1 & 1 & 1 & 1 & 0 \\end{array}\\right]\\)\n\n\n\nExercise 1.17 Which of the following matrices are in reduced row echelon form?\n\n\\(\\begin{bmatrix} 0 & 1 & 2 & 0 & 3 \\\\ 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 0 & 0 & 2 & 1 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 0 & 1\\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 0 & 0 & 1\\\\ 0 & 0 & 1\\\\ 0 & 0 & 1\\\\ \\end{bmatrix}\\)\n\n\n\nExercise 1.18 Which of the following matrices are in reduced row echelon form?\n\n\\(\\begin{bmatrix} 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0\\\\ \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 0 & 0 & 3 & 1 \\\\ 0 & 0 & 0 & 1 & 1 \\\\ 0 &0 & 0 & 0 & 1 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 0 & 2 & 1 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\)\n\n\n\nExercise 1.19 Find all solutions of the linear system using the Gauss-Jordan elimination method.\n\n\\(\\begin{cases} x_2+2x_4+3x_5=0\\\\ 4x_4+8x_5=0 \\end{cases}\\)\n\\(\\begin{cases} 3x+4y =0\\\\ -2x+7y=0 \\end{cases}\\)\n\n\n\nExercise 1.20 Find all solutions of the linear system using the Gauss-Jordan elimination method.\n\n\\(\\begin{cases} x_4+2x_5-x_6=2 \\\\ x_1+2x_2+x_5-x_6=0\\\\ x_1+2x_2+2x_3-x_5+x_6=2 \\end{cases}\\)\n\\(\\begin{cases} 3x_1+3x_2-4x_3+x_4=2 \\\\ x_1+x_2-x_3-x_4=5 \\end{cases}\\)\n\\(\\begin{cases} 2 x_1+3x_2+4x_3=6-x_4 \\\\ 3x_1-2x_2-x_4=1+4x_3 \\\\ 3x_1+3x_3+x_4=4-x_2 \\\\ x_2+x_3-4x_4=-3-4x_1\\end{cases}\\)\n\\(\\begin{cases} 4x_1+3x_2+2x_3-x_4=4 \\\\ 5x_1+4x_2+3x_3-x_4=4\\\\ -2x_1-2x_2-x_3+2x_4=-3\\\\ 11x_1+6x_2+4x_3+x_4=11 \\end{cases}\\)\n\\(\\begin{cases} 2x_1-3x_2+x_3=5 \\\\ x_1+x_2-x_3=3\\\\ 4x_1-x_2-x_3=1 \\end{cases}\\)\n\\(\\begin{cases} x_1-x_2=4\\\\ 2x_1+x_2=7\\\\ 5x_1-2x_2=19 \\end{cases}\\)\n\n\n\nExercise 1.21 Find all \\(4\\times 1\\) matrices in reduced row echelon form.\n\n\nExercise 1.22 How many types of \\(3\\times 2\\) matrices in reduced row echelon form are there? - How many types of \\(2\\times 3\\) matrices in reduced row echelon form are there?\n\n\nExercise 1.23  \n\nDescribe the possible reduced row echelon forms for a matrix with two rows and two columns.\nDescribe the possible reduced row echelon forms for a matrix with three rows and three columns.\n\n\n\nExercise 1.24 Find the polynomial of degree 3 whose graph passes through the points \\((0,1)\\), \\((1,0)\\), \\((-1,0)\\), and \\((2,-15)\\). Sketch the graph of this cubic.\n\n\nExercise 1.25 Find the polynomial of degree 4 whose graph passes through the points \\((1,1)\\), \\((2,-1)\\), \\((3,-59)\\), and \\((-2, -29)\\). Sketch the graph of this quartic.\n\n\nExercise 1.26 Find the values of \\(k\\), if any, for which the system has\n\nonly one solution,\nno solutions,\nan infinite number of solutions\n\\(\\begin{cases} x_1-x_2=3 \\\\ 3x_1+kx_2=6 \\end{cases}\\)\n\\(\\begin{cases}x_1+2k x_2 =7\\\\ x_1+(k^2+1)x_2=3 \\end{cases}\\)\n\\(\\begin{cases}k x_1-x_2=1 \\\\ x_1+k x_2=i\\end{cases}\\)\n\\(\\begin{cases} k x+|k| y=1 \\\\ |k|x-ky=-1 \\end{cases}\\)\n\n\n\nExercise 1.27 Find values for \\(a\\) such that the system of linear equations \\[\\begin{cases} x_1-3x_2+x_3=1 \\\\ 2x_1+x_2-x_3=-1 \\\\ 5x_1-8x_2+(a^2-2)x_3=a  \\end{cases}\\] will have\n\ninfinitely many solutions,\nno solutions\nexactly one solution.\n\n\n\nExercise 1.28 Use Gauss-Jordan elimination to solve the linear system \\[\n\\left\\{ \\begin{array}{lll}\na x_1+ b x_2 & & =r \\quad (a\\neq 0) \\\\\nc x_1+ d x_2 & & =s \\quad (e\\neq 0)\\\\\n& e x_3+f x_4 & =t \\\\\n& g x_3+h x_4 & =u \\\\\n\\end{array}\\right.\\] State conditions on \\(a, b, c, d, e, f, g\\) and \\(h\\) which guarantee a unique solution.\n\n\nExercise 1.29 Show that the system \\[\\begin{cases}a_1 x+ b_1 y+c_1 z=0 \\\\ a_2 x+b_2 y +c_2 z=0 \\end{cases}\\] always has a solution other than \\(x=0\\) and \\(y=0\\).\n\n\nExercise 1.30 What can you say about the last row of the reduced row echelon form of the following matrix? \\[\n\\begin{bmatrix}\n1 & 2 & 3 & \\cdots & n\\\\\nn+1 & n+2 & n+3 & \\cdots & 2n \\\\\n\\vdots & \\vdots & \\vdots & \\vdots &  \\vdots \\\\\nn^2-n+1 & n^2-n+2 & n^2-n+3 & \\cdots & n^2\n\\end{bmatrix}\n\\]\n\n\nExercise 1.31 Show that the linear system of equations \\[\\begin{cases} 3x_1+2x_2-4x_3=a \\\\ -4x_1+x_2-x_3 =b \\\\ 7x_1+12x_2-22x_3 =c \\end{cases}\\] will have infinitely many solutions if \\(c=5a+2b\\).\n\n\nExercise 1.32 Find an equation of the form \\(x^2+y^2+a x+by+c=0\\) of the circle passing through the following points.\n\n\\((-2,1)\\), \\((5,0)\\), and \\((4,1)\\)\n\\((1,1)\\), \\((5,-3)\\), and \\((-3,-3)\\)"
  },
  {
    "objectID": "systems-of-linear-equations.html#matrices-and-vectors",
    "href": "systems-of-linear-equations.html#matrices-and-vectors",
    "title": "1  Systems of Linear Equations",
    "section": "1.7 Matrices and Vectors",
    "text": "1.7 Matrices and Vectors\nThe entries of an \\(n\\times m\\) matrix \\(A\\) are denoted by \\(a_{i,j}\\) where \\(1\\leq i\\leq n\\) and \\(1\\leq j \\leq m\\). For example, the matrix \\[\n\\begin{bmatrix}a_{11} & a_{12} & a_{13} \\\\ a_{21} & a_{22} & a_{23}\\end{bmatrix}\n\\] is a \\(2\\times 3\\) matrix with \\(i=2\\) rows and \\(j=3\\) columns. If \\(i=j\\), then the matrix is called a square matrix .\n\nDefinition 1.4 Two matrices \\(A=[a_{ij}]\\) and \\(B=[b_{ij}]\\) of the same size are equal when \\(a_{ij}=b_{ij}\\) for all \\(i,j\\).\n\nA square matrix \\(A=[a_{ij}]\\) is called a diagonal matrix if \\(a_{ij}=0\\) whenever \\(i\\neq j\\). A square matrix \\(A=[a_{ij}]\\) is called upper triangular ( lower triangular) when \\(a_{ij}=0\\) whenever \\(i>j\\) (\\(i<j\\)). The zero matrix has all entries zero and the identity matrix has \\(a_{ii}=1\\), \\(i=1, 2, 3...,n\\) and all other entries zero. We remark that when \\(m\\) and \\(n\\) are understood from discussion they are usually left off. \\[\n0_{m\\times n}=\n\\begin{bmatrix}\n0 & \\cdots & 0 \\\\\n0 & \\cdots & 0 \\\\\n\\vdots & \\cdots & \\vdots \\\\\n0 & \\cdots &0\n\\end{bmatrix}_{m\\times n}\n\\qquad\nI_n=\n\\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0\\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{bmatrix}_{n\\times n}\n\\]\nConsider the following matrices. \\[\n\\begin{array}{lllll}\nA=\\begin{bmatrix}1 & 2 & 3 \\\\ 4 & 5 & 6 \\end{bmatrix}\n&\nB=\\begin{bmatrix} 2 & 0 & 0 \\\\ 0 & 3 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\n&\nC=\\begin{bmatrix} 2 & 3 \\\\ 0 & 4 \\end{bmatrix}\n&\nD=\\begin{bmatrix}5 & 0 & 0 \\\\ 4 & 0 & 0 \\\\ 3 & 2 & 1\\end{bmatrix}\n\\end{array}\n\\] Notice matrices \\(B\\), \\(C\\) and \\(D\\) are square and \\(A\\) is not. Also notice the only diagonal matrix is \\(B\\), matrices \\(B\\) and \\(C\\) are upper triangular, and the only lower triangular matrix is \\(D\\).\nWe will see that matrix addition has many of the same properties that scalar addition has; for example, commutativity, associativity, additive inverses, and the additive identity property.\n\nDefinition 1.5 Let \\(A=[a_{ij}]\\) and \\(B=[b_{ij}]\\) be matrices of the same size. The matrix sum of \\(A\\) and \\(B\\) is the matrix \\(C\\) defined by \\[\nC=A+B=[a_{ij}]+[b_{ij}]=[a_{ij}+b_{ij}].\n\\]\n\n\nTheorem 1.2 [Properties of Matrix Addition] Let \\(A\\), \\(B\\), and \\(C\\) denote arbitrary matrices such that the following operations are defined. Then the following hold.\n\n\\(A+B=B+A\\),\n\\(A+(B+C)=(A+B)+C\\),\nfor each \\(A\\), there exists \\(0\\) such that \\(0+A=A\\), and\nfor each \\(A\\), there exists a matrix \\(-A\\) such that \\(A+(-A)=0\\).\n\n\n\n\nProof. \nLet \\(A\\) and \\(B\\) denote matrices of the same size with entries \\(A_{ij}\\) and \\(B_{ij}\\), respectively. Then \\[\\begin{align*}\n(A+B)_{ij} & =A_{ij}+B_{ij} & \\text{definition of matrix addition}  \\\\\n& =B_{ij}+A_{ij} & \\text{scalar commutativity}  \\\\\n& =(B+A)_{ij}  & \\text{definition of matrix addition}\n\\end{align*}\\] Therefore, by the definition of matrix equality, \\(A+B=B+A\\) follows.\nIf matrix \\(C\\) is also of the same size as \\(A\\) and \\(B\\) with entries \\(C_{ij}\\) then, \\[\\begin{align*}\n(A+(B+C))_{ij} & =A_{ij}+\\left(B+C\\right)_{ij} &  \\text{definition of matrix addition}   \\\\\n& =A_{ij}+\\left(B_{ij}+C_{ij}\\right) &  \\text{definition of matrix addition}   \\\\\n& =\\left(A_{ij}+B_{ij}\\right)+C_{ij} &  \\text{scalar associativity}   \\\\\n& =\\left(A+B\\right)_{ij}+C_{ij} &  \\text{definition of matrix addition}   \\\\\n& =(\\left(A+B\\right)+C)_{ij} &  \\text{definition of matrix addition}\n\\end{align*}\\] Therefore, by the definition of matrix equality, \\(A+(B+C)=(A+B)+C\\) follows.\nLet \\(A\\) be an \\(m\\times n\\) matrix with entires \\(A_{ij}\\) and let \\(0_{m\\times n}\\) denote the \\(m\\times n\\) matrix with all entires equal to the scalar additive identity denoted by \\(0\\). Denoting the entries of \\(0_{m\\times n}\\) by \\(0_{ij}\\) we find\n\n\\[\\begin{align*}\n(0+A)_{ij}=0_{ij}+A_{ij} = A_{ij}.\n\\end{align*}\\]\nTherefore, by the definition of matrix equality, \\(0+A=A\\) follows. - The proof of this part is left for the reader as Exercise \\(\\ref{ex:PropertiesofMatrixAddition}\\).\n\nThe matrix properties listed in \\(\\ref{PropertiesofMatrixAddition}\\) are called the\n\n commutative ,\n associative ,\n additive identity , and\n\n additive inverse\nproperties for matrix addition, respectively.\n\nJust as matrix addition has several properties in common with the underlying field of scalars so too does scalar multiplication.\n\nDefinition 1.6 If \\(A\\) is any matrix and \\(k\\) is any scalar, the scalar multiple \\(kA\\) is the matrix obtained by multiplying each entry of \\(A\\) by \\(k\\), that is if \\(A=[a_{ij}]\\), then \\(k A=[k a_{ij}].\\)\n\nThe following properties say that scalers and matrices distribute over each other and are associative with each other.\n\nTheorem 1.3 [Properties of Scalar Multiplication] Let \\(A\\) and \\(B\\) denote arbitrary matrices such that the following operations are defined. Let \\(k\\) and \\(p\\) denote arbitrary scalars. Then the following hold.\n\n\\(k(A+B)=k A+k B\\)\n\\((k+p)A=k A+p A\\)\n\\((kp)A=k(pA)\\)\n\\(A(kB)=(kA)B\\)\n\n\n\n\nProof. \nLet \\(A\\) and \\(B\\) be \\(n\\times m\\) matrices. Then, for \\(1\\leq i \\leq n\\) and \\(1\\leq j\\leq m\\) and an arbitrary scalar \\(k\\) we find, \\[\\begin{align*}\n\\left(k(A+B)\\right)_{ij}& =k\\left((A+B)_{ij}\\right) &  \\text{definition of scalar multiplication}   \\\\\n& =k(A_{ij}+B_{ij})&  \\text{definition matrix addition}   \\\\\n& =(kA)_{ij}+(kB)_{ij}&  \\text{distributive property of scalar multiplication}   \\\\\n& =(kA+kB)_{ij}&  \\text{definition of matrix addition}\n\\end{align*}\\] Therefore, by the definition of matrix equality, \\(k(A+B)=k A+k B\\) follows.\n\nLet \\(A\\) be an \\(n\\times m\\) matrix and let \\(k\\) and \\(p\\) be scalars. Then we find, \\[\\begin{align*}\n((kp)A)_{ij}& =(kp)A_{ij}&  \\text{definition of scalar multiplication}   \\\\\n& =k(pA_{ij})&  \\text{associative property of scalars}   \\\\\n& =k(pA)_{ij}&  \\text{definition of scalar multiplication}   \\\\\n& =(k(pA))_{ij}&  \\text{definition of scalar multiplication}\n\\end{align*}\\] Therefore, by the definition of matrix equality, \\((kp)A=k(pA)\\) follows. The proof of the remaining parts are left for the reader as Exercise \\(\\ref{ex:Properties of Scalar Multiplication}\\).\n\n\nExample 1.12 If \\(A=\\begin{bmatrix} 3 & -1 & 4 \\\\ 2 & 0 & 6 \\end{bmatrix}\\) and \\(B=\\begin{bmatrix} 1 & 2 & -1\\\\ 0 & 3 & 2 \\end{bmatrix},\\) determine \\(5A\\), \\(\\frac{1}{2}B\\), and \\(3A-2B\\). A quick computation reveals the following. \\[\n5A=\n\\begin{bmatrix}\n15 & -5 & 20 \\\\\n10 & 0 & 30\n\\end{bmatrix}\n\\quad\n\\frac{1}{2}B=\n\\begin{bmatrix}\n\\frac{1}{2} & 1 & -\\frac{1}{2} \\\\\n0 & \\frac{3}{2} & 1\n\\end{bmatrix}\n\\quad\n3A-2B=\n\\begin{bmatrix}\n7 & -7 & 14 \\\\\n6 & -6 & 14\n\\end{bmatrix}\n\\]\n\n\nExample 1.13 Let \\(A\\), \\(B\\), and \\(C\\) be matrices of the same size. Simplify \\[\n2(A+3C)-3(2C-B)-3[2(2A+B-4C)-4(A-2C)].\n\\] We obtain \\(2 A - 3 B\\).\n\n\nExample 1.14 If \\(kA=0\\), show that either \\(k=0\\) or \\(A=0\\). Suppose \\(k\\neq 0\\) and \\(A=[a_{ij}]\\), then \\(kA=[k a_{ij}]=0\\) and notice each entry \\(k a_{ij}\\) is zero if and only if \\(a_{ij}=0\\) for every \\(i\\) and \\(j\\). Therefore, either \\(k=0\\) or every entry in \\(A\\) is zero.\n\n\nExample 1.15 Find \\(1\\times 3\\) matrices \\(X\\) and \\(Y\\) such that \\[\n\\begin{cases}\nX+2Y= \\begin{bmatrix} 1 & 3 & -2 \\end{bmatrix} \\\\\nX+Y= \\begin{bmatrix} 2 & 0 & 1 \\end{bmatrix}.\n\\end{cases}\n\\] Subtracting these equations together yields \\(Y=\\begin{bmatrix} -1 & 3 & -3 \\end{bmatrix}.\\) Then \\(X=\\begin{bmatrix} 2 & 0 & 1 \\end{bmatrix}\\) and \\(-Y= \\begin{bmatrix} 3 & -3 & 4 \\end{bmatrix}.\\)\n\nLet \\(A\\) be an \\(m\\times n\\) matrix and \\(B\\) a \\(n\\times p\\) matrix, their matrix product \\(C=AB\\) is defined as the the \\(m\\times p\\) matrix whose \\((i,j)\\)th entry is the dot product of the \\(i\\)th row of \\(A\\) with the \\(j\\)th column of \\(B\\), that is, the product of two matrices is the matrix \\(C\\) where \\(c_{ij}=\\sum_{k=1}^n a_{ik}b_{kj}\\). Matrix multiplication is illustrated by the following equation. \\[\\begin{equation*}\n\\label{matprod}\nAB=\n\\begin{bmatrix}\na_{ij}\n\\end{bmatrix}\n\\begin{bmatrix}\nb_{ij}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\displaystyle{\\sum_{k=1}^n a_{1k}b_{k1}} & \\cdots & \\displaystyle{\\sum_{k=1}^n a_{1k}}b_{kp} \\\\\n\\vdots & \\displaystyle{\\sum_{k=1}^n a_{ik}b_{kj}} & \\vdots \\\\\n\\displaystyle{\\sum_{k=1}^n a_{mk}b_{k1}} & \\cdots & \\displaystyle{\\sum_{k=1}^n a_{mk}}b_{kn}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nc_{ij}\n\\end{bmatrix}\n=C\n\\end{equation*}\\]\n\nExample 1.16  Let \\(A\\) and \\(B\\) be defined as follows. \\[\nA=\n\\begin{bmatrix}\n4 & -5 & 6 \\\\\n1 & 8 & -9\n\\end{bmatrix}\n\\qquad\n\\qquad\nB=\n\\begin{bmatrix}\n1 & 2 & -3 \\\\\n-2 & -1 & 5 \\\\\n-2 & 5 & -1\n\\end{bmatrix}\n\\] If possible, find \\(AB\\) and \\(BA\\). Since \\(A\\) is a \\(2\\times 3\\) matrix and \\(B\\) is a \\(3\\times 3\\) matrix we see that \\(AB\\) is defined as a \\(2\\times 3\\) matrix and \\(BA\\) is not defined. \\[\\begin{align*}\nAB &\n=\n\\begin{bmatrix}\n\\begin{bmatrix} 4 & -5 & 6 \\end{bmatrix}\\cdot\\vectorthree{1}{-2}{-2}\n\\vspace{5pt}\n&\n\\begin{bmatrix} 4 & -5 & 6 \\end{bmatrix}\\cdot\\vectorthree{2}{-1}{5}\n&\n\\begin{bmatrix} 4 & -5 & 6 \\end{bmatrix}\\cdot\\vectorthree{-3}{5}{-1}\n\\hspace{5pt}\n\\\\\n\\begin{bmatrix} 1 & 8 & -9 \\end{bmatrix}\\cdot\\vectorthree{1}{-2}{-2}\n&\n\\begin{bmatrix} 1 & 8 & -9 \\end{bmatrix}\\cdot\\vectorthree{2}{-1}{5}\n&\n\\begin{bmatrix} 1 & 8 & -9 \\end{bmatrix}\\cdot\\vectorthree{-3}{5}{-1}\n\\hspace{5pt}\n\\end{bmatrix}\n&\n= \\begin{bmatrix}\n2 & 43 & -19 \\\\\n3  &-51 & 52\n\\end{bmatrix}\n\\end{align*}\\]\n\n\\(\\ref{MatrixMult}\\) is a counterexample showing that matrix multiplication is not commutative. Even so, matrix multiplication has many useful properties such as associativity, multiplicative inverses, and distributive properties.\n::: {#thm- } [Properties of Matrix Multiplication] Let \\(A\\), \\(B\\), and \\(C\\) denote arbitrary matrices such that the following operations are defined. Then the following hold.\n\n\\(A(BC)=(AB)C\\)\n\\(A(B+C)=AB+AC\\)\n\\((B+C)A=BA+CA\\)\n\\(AI=A\\)\n\\(IA=A\\) :::\n\n\n\nProof. \nSuppose \\(B\\) is an \\(n\\times p\\) matrix. Since \\(BC\\) is defined, \\(C\\) is a matrix with \\(p\\) rows, and \\(BC\\) has \\(n\\) rows. Because \\(A(BC)\\) is defined we may assume \\(A\\) is an \\(m\\times n\\) matrix. Thus the product \\(AB\\) exists and it is an \\(m\\times p\\) matrix, from which it follows that the product \\((AB)C\\) exists. To show that \\(A(BC)=A(BC)\\) we must show that \\[\\begin{equation}\n\\label{matass}\n[A(BC)]_{ij}=[(AB)C]_{ij}.\n\\end{equation}\\]\nfor arbitrary \\(i\\) and \\(j\\). By definition of matrix multiplication, \\[\\begin{align*}\n[A(BC)]_{ij} &\n= \\sum_t A_{it}(BC)_{tj}\n= \\sum_t A_{it} \\sum_s B_{t s} C_{sj} \\\\\n& = \\sum_t \\sum_s A_{it} B_{ts} C_{sj}\n=\\sum_s \\sum_t A_{it} B_{ts} C_{sj} \\\\\n& =\\sum_s \\left( \\sum_t A_{it} B_{ts} \\right) C_{sj}\n=\\sum_s (AB_{i s})  C_{sj}\n=[(AB)C]_{ij}.\n\\end{align*}\\]\nLet \\(A_{ij}\\), \\(B_{ij}\\), and \\(C_{ij}\\) denote the entries of matrices \\(A\\), \\(B\\) and \\(C\\) of sizes \\(m\\times n\\), \\(n\\times p\\), and \\(n\\times p\\), respectively. Then \\(B+C=[B_{ij}+C_{ij}]\\), so the \\((i,j)\\)th entry of \\(A(B+C)\\) is \\[\n\\sum_{k=1}^n A_{ik}\\left(B_{kj}+C_{kj}\\right)\n=\\sum_{k=1}^n \\left(A_{ik} B_{kj}+A_{ik} C_{kj}\\right)\n=\\sum_{k=1}^n A_{ik} B_{kj}+\\sum_{k=1}^n A_{ik} C_{kj}\n\\] this is the \\((i,j)\\)th entry of \\(AB+AC\\) because the sums on the right are the \\((i,j)\\)th entries of \\(AB\\) and \\(AC\\), respectively. Hence \\(A(B+C)=AB+AC\\).\n\nLet \\(A=[a_{ij}]_{m\\times n}\\). Then \\[\nAI=\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n\\vdots & \\ddots & \\ddots & \\vdots \\\\\na_{m_1} & a_{m2} & \\cdots & a_{mm}\n\\end{bmatrix}_{m\\times n}\n\\begin{bmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & 1 & 0 & \\cdots & 0\\\\\n0 & 0 & 1 & \\cdots & 0 \\\\\n\\vdots & \\ddots & \\ddots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1\n\\end{bmatrix}_{n\\times n}.\n\\] and the \\((i,j)\\)th entry of \\(AI\\) is the \\(i\\)th row of \\(A\\) times the \\(j\\)th column of \\(I\\), that is \\[\n\\fbox{$\\begin{matrix}\na_{i1} & a_{i2} & \\cdots & a_{ij} & \\cdots & a_{in}\n\\end{matrix}$}\n\\quad\n\\begin{array}{rl}\n\\fbox{$\n\\begin{matrix}\n0 \\\\ 0 \\\\ \\vdots \\\\ 1 \\\\ \\vdots \\\\ 0\n\\end{matrix}\n$} &\n\\begin{matrix}\n\\text{ } \\\\ \\text{ } \\\\ \\text{ } \\\\ \\leftarrow \\text{Position $j$} \\\\  \\text{ } \\\\ \\text{ }\n\\end{matrix}\n\\end{array}\n\\] So the \\((i,j)\\)th entry of \\(AI\\) is \\(a_{ij}\\), the \\((i,j)\\)th entry of \\(A\\). Thus, \\(AI=A\\). The proof of the remaining parts are left for the reader as Exercise \\(\\ref{ex:Properties of Matrix Multiplication}\\).\n\nThe matrix properties listed in \\(\\ref{Properties of Matrix Multiplication}\\) are called the\n\n associative ,\n left distributive ,\n right distributive ,\n left identity , and\n\n right identity\nproperty for matrix multiplication, respectively.\n\n\nExample 1.17 Find all matrices that commute with \\(A=\\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}\\). Let \\(B=\\begin{bmatrix} a & b \\\\ c & d\\end{bmatrix}\\) be a matrix that commutes with \\(A\\). Matrix multiplication yields\n\\[\nAB=\\begin{bmatrix}  a+2c & b+2d \\\\ c & d\\end{bmatrix}\n\\qquad \\text{and} \\qquad\nBA=\\begin{bmatrix} a & 2a+b \\\\ c & 2c+d \\end{bmatrix}.\n\\] Since \\(a+2c=a\\) it follows that \\(c=0\\). Since \\(b+2d=2a+b\\) it follows that \\(a=d\\). Thus all matrices that commute with \\(A\\) have the form \\(\\begin{bmatrix} a & b \\\\ 0 & a \\end{bmatrix}.\\)\n\n\nExample 1.18 Write the matrix equation for the system \\[\n\\begin{cases}\n3x_1+x_2+7x_3+2x_4 =13 \\\\\n2x_1-4x_2+14x_3-x_4 =-10 \\\\\n5x_1+11x_2-7x_3+8x_4 =59 \\\\\n2x_1+5x_2-4x_3-3x_4=39.\n\\end{cases}\n\\] Find the reduced row echelon form using Gauss-Jordan elimination. Solve the system. The matrix equation is \\[\n\\begin{bmatrix}\n3 & 1 & 7 & 2 \\\\\n2 & -4 & 14 & -1 \\\\\n5 & 11 & -7 & 8 \\\\\n2 & 5 & -4 & -3\n\\end{bmatrix}\n\\vectorfour{x_1}{x_2}{x_3}{x_4}\n=\\vectorfour{13}{-10}{59}{39}.\n\\] The reduced row echelon form of the coefficient matrix and the solution set are as follows \\[\n\\begin{bmatrix}\n1 & 0 & 3 & 0 & 4 \\\\\n0 & 1 & -2 & 0 & 5 \\\\\n0 & 0 & 0 & 1 & -2 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{bmatrix}\n\\qquad\n%\\text{and}\n\\qquad\n\\left\\{\\vectorfour{4-3t}{5+2c}{t}{-2} \\mid t\\in \\mathbb{R}\\right\\}\n\\]\n\nA matrix with a single column is called a column vector or just and a matrix with a single row is called a row vector.\n\nDefinition 1.7 A vector \\(\\vec{b}\\) is called a linear combination of the vectors \\(\\vlist{v}{m}\\) if there exists scalars \\(\\vlist{a}{m}\\) such that \\[\n\\vec{b}=\\lincomb{a}{v}{m}.\n\\]\n\n\nExample 1.19 Given the following vectors, is \\(\\vec{u}\\) a linear combination of the vectors \\(\\vec{v}_1\\) and \\(\\vec{v}_2\\)? \\[\n\\vec{u}=\\begin{bmatrix} 7 \\\\8 \\\\9 \\end{bmatrix}\n\\qquad\n\\vec{v}_1=\\begin{bmatrix} 1 \\\\ 2 \\\\3 \\end{bmatrix}\n\\qquad\n\\vec{v}_2=\\begin{bmatrix} 4 \\\\ 5 \\\\6 \\end{bmatrix}\n\\] We want to find scalars \\(x_1\\) and \\(x_2\\) such that \\(\\vec{u}=x_1\\vec{v}_1+x_2\\vec{v}_2.\\) We solve for \\(x_1\\) and \\(x_2\\) using an augmented matrix and row-operations as follows. \\[\n\\begin{bmatrix}\n1 & 4 & 7 \\\\\n2 & 5 & 8 \\\\\n3 & 6 & 9\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{-2R_1+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{-3R_1+R_3}\n\\end{array}\n\\begin{bmatrix}\n1 & 4 & 7 \\\\\n0 & -3 & -6 \\\\\n0 & -6 & -12\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{-\\frac{1}{3}R_2}\n\\\\\n\\stackrel{\\longrightarrow}{-\\frac{1}{6}R_3}\n\\end{array}\n\\begin{bmatrix}\n1 & 4 & 7 \\\\\n0 & 1 & 2 \\\\\n0 & 1 & 2\n\\end{bmatrix}\n\\] \\[\n\\stackrel{\\longrightarrow}{-R_2+R_3}\n\\begin{bmatrix}\n1 & 4 & 7 \\\\\n0 & 1 & 2 \\\\\n0 & 0 & 0\n\\end{bmatrix}\\stackrel{\\longrightarrow}{-4R_2+R_1}\n\\begin{bmatrix}\n1 & 0 & -1 \\\\\n0 & 1 & 2 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\] Thus we find \\(x_1=-1\\) and \\(x_2=2\\). Therefore, yes \\(\\vec{u}\\) is a linear combination of \\(\\vec{v}_1\\) and \\(\\vec{v}_2\\), namely \\(\\vec{u}=2\\vec{v}_2-\\vec{v}_1\\).\n\n\nExample 1.20 Given the following vectors, determine whether \\(\\vec{u}\\) or \\(\\vec{w}\\) is a linear combination of the vectors \\(\\vec v_1\\) and \\(\\vec v_2\\). \\[\n\\vec{u}=\\vectorthree{1}{1}{4}\n\\qquad\n\\vec{w}=\\vectorthree{1}{5}{1}\n\\qquad\n\\vec v_1=\\vectorthree{1}{2}{-1}\n\\qquad\n\\vec v_2=\\vectorthree{3}{5}{2}\n\\] First, \\(\\vec{u}\\) is a linear combination of \\(\\vec v_1\\) and \\(\\vec v_2\\) since \\[\n\\vectorthree{1}{1}{4}\n=(-2)\\vectorthree{1}{2}{-1}+(1)\\vectorthree{3}{5}{2}.\n\\] To determine whether \\(\\vec{w}\\) is a linear combination of \\(\\vec{v}_1\\) and \\(\\vec{v}_2\\) we consider the equation \\[\n\\vec{w}=x \\vectorthree{1}{2}{-1}+y\\vectorthree{3}{5}{2}\n\\] which leads to the system \\[\n\\begin{cases}\nx+3y= 1 \\\\\n2x+5y =5 \\\\\n-x+2y =1.\n\\end{cases}\n\\] This linear system can be shown to have an empty solution set; and thus \\(\\vec{w}\\) is not a linear combination of \\(\\vec v_1\\) and \\(\\vec v_2\\).\n\n\nExample 1.21 For which values of the constant \\(c\\) is \\(\\vectorthree{1}{c}{c^2}\\) a linear combination of \\(\\vectorthree{1}{a}{a^2}\\) and \\(\\vectorthree{1}{b}{b^2}\\), where \\(a\\) and \\(b\\) are arbitrary constants? We need to solve the following linear system \\[\n\\vectorthree{1}{c}{c^2}=x\\vectorthree{1}{a}{a^2}+y\\vectorthree{1}{b}{b^2}\n\\qquad\n\\text{ with augmented matrix}\n\\qquad\n\\begin{bmatrix}\n\\begin{array}{cc|c}\n1 & 1 & 1 \\\\\na & b & c \\\\\na^2 & b^2 & c^2\n\\end{array}\n\\end{bmatrix}.\n\\] using row operations the augmented matrix reduces to \\[\n\\begin{bmatrix}\n\\begin{array}{cc|c}\n1 & 1 & 1 \\\\\n0 & b-a & c-a \\\\\n0 & 0 & (c-a)(c-b)\n\\end{array}\n\\end{bmatrix}.\n\\] This system is consistent if and only if \\(c=a\\) or \\(c=b\\). Thus the vector is a linear combination if \\(c=a\\) or \\(c=b\\).\n\n\nExample 1.22 Express the vector \\(\\begin{bmatrix} 7 \\\\11 \\end{bmatrix}\\) as the sum of a vector on the line \\(y=3x\\) and a vector on the line \\(y=x/2\\). We wish to find \\(x_1\\) and \\(x_2\\) such that \\[\n\\begin{bmatrix} 7 \\\\ 11 \\end{bmatrix}=\n\\begin{bmatrix} x_1 \\\\ 3x_1 \\end{bmatrix}+\n\\begin{bmatrix} x_2 \\\\ \\frac{1}{2}x_2 \\end{bmatrix}.\n\\] We solve this linear system using row-operations as follows. \\[\n\\begin{bmatrix}1 & 1 & 7 \\\\\n3 & \\frac{1}{2} & 11\n\\end{bmatrix}\n\\stackrel{\\longrightarrow}{\\scriptstyle -3R_1+R_2}\n\\begin{bmatrix}1 & 1 & 7\\\\\n0 & \\frac{-5}{2} & -10\n\\end{bmatrix}\n%\\]\\[\\qquad \\qquad\n\\stackrel{\\longrightarrow}{\\scriptstyle(-2/5)R_2}\n\\begin{bmatrix}1 & 1 & 7 \\\\\n0 & 1 & 4\n\\end{bmatrix}\\stackrel{\\longrightarrow}{\\scriptstyle-R_2+R_1}\n\\begin{bmatrix}1 & 0 & 7 \\\\\n0 & 1 & 4\n\\end{bmatrix}\n\\] The solution is \\(x_1=3\\) and \\(x_2=4\\). Therefore the desired sum is\n\\[\n\\begin{bmatrix} 7 \\\\ 11 \\end{bmatrix}=\n\\begin{bmatrix} 3 \\\\ 9 \\end{bmatrix}+\n\\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}\n\\] since \\(\\begin{bmatrix} 3 \\\\ 9 \\end{bmatrix}\\) is on the line \\(y=3x\\) and \\(\\begin{bmatrix} 4 \\\\ 1 \\end{bmatrix}\\) is on the line \\(y=x/2.\\)\n\n\nTheorem 1.4 Let \\(B\\) be an \\(n\\times p\\) matrix and \\(A\\) and \\(p \\times m\\) matrix with columns \\(\\vec v_1\\), …, \\(\\vec v_m\\). Then the product \\(B A\\) is \\[\nB A = B\n\\begin{bmatrix}\n\\vec v_1 & \\cdots & \\vec v_m\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nB \\vec v_1 & \\cdots & B \\vec v_m\n\\end{bmatrix} .\n\\] ::: {.proof } Notice that \\(\\vec v_1\\), …, \\(\\vec v_m\\) are all \\(p\\times 1\\) column vectors consisting of the columns of \\(A\\). So the product of \\(B\\) and each \\(\\vec v_i\\) (\\(0\\leq i \\leq m\\)) are the \\(n\\times 1\\) column vectors constituting \\(BA\\).\n\n:::\n\nTheorem 1.5  If the column vectors of an \\(n\\times m\\) matrix \\(A\\) are \\(\\vec{v}_1,\\vec{v}_2,...,\\vec{v}_n\\) and \\(\\vec{x}\\) is a vector with entries \\(\\vlist{x}{m}\\), then \\[\nA \\vec x\n%=\\begin{bmatrix} | & & | \\\\ \\vec v_1 & \\cdots & \\vec v_m \\\\ | & & | \\end{bmatrix}\\vec x\n=\\lincomb{x}{v}{m}.\n\\]\n\n\nProof. The proof follows from the following equation. \\[\\begin{align*}\nA \\vec x\n& =\\begin{bmatrix} | & & | \\\\ \\vec v_1 & \\cdots & \\vec v_m \\\\ | & & | \\end{bmatrix}\\vec x\n=\n\\begin{bmatrix}\nv_{11} & \\cdots & v_{1m} \\\\\n\\vdots & \\ddots & \\vdots \\\\\nv_{n1} & \\cdots & v_{nm} \\\\\n\\end{bmatrix}\n\\vectorfour{x_1}{x_2}{\\vdots}{x_m} \\\\\n& =\n\\begin{bmatrix}\nv_{11} x_1 + v_{12} x_2 + \\cdots + v_{1m}x_m \\\\\n\\vdots  \\\\\nv_{n1} x_1 + v_{n2} x_2 + \\cdots + v_{nm} x_m\n\\end{bmatrix}_{n\\times 1}\n= \\lincomb{x}{v}{m}\n\\end{align*}\\]\n\n\nCorollary 1.1 Let \\(A\\) be an \\(n\\times m\\) matrix, \\(\\vec x\\) and \\(\\vec y\\) be \\(m\\times 1\\) column vectors, and let \\(k\\) be a scalar. Then the following hold.\n\n\\(A(\\vec x + \\vec y)=A(\\vec x) + A(\\vec y)\\)\n\\(A(k \\vec x )=k A(\\vec x)\\)\n\n\n\nProof. Since the column vectors \\(\\vec x\\) and \\(\\vec y\\) are matrices and the matrices \\(A(\\vec x + \\vec y)\\), \\(A(\\vec x) + A(\\vec y)\\), \\(A(k \\vec x )\\), and \\(k A(\\vec x)\\) are defined we can apply \\(\\ref{Properties of Matrix Multiplication}\\) to obtain \\(A(\\vec x + \\vec y)=A(\\vec x) + A(\\vec y)\\) and \\(A(k \\vec x )=k A(\\vec x)\\)."
  },
  {
    "objectID": "systems-of-linear-equations.html#exercises-1",
    "href": "systems-of-linear-equations.html#exercises-1",
    "title": "1  Systems of Linear Equations",
    "section": "1.8 Exercises",
    "text": "1.8 Exercises\n\nExercise 1.33 Let \\(A\\) be a matrix of size \\(4\\times 5\\), let \\(B\\) be a matrix of size \\(6\\times 4\\), let \\(C\\) be a matrix of size \\(4\\times 5\\), and let \\(D\\) be a matrix of size \\(4\\times 2\\). Which of the following are defined, and for those that are, what is their size?\n\n\\(BA\\)\n\\(DA\\)\n\\(B+A\\)\n\\(C+A\\)\n\\(C(AB)\\)\n\\((BA)C\\)\n\\(C+DB\\)\n\\(D(C+A)\\)\n\\(A+CB\\)\n\n\n\nExercise 1.34 Let \\(A=\\begin{bmatrix} 2 & 1 & 3 \\\\ 4 & 0 & -1 \\\\ -1 & 1 & 0 \\end{bmatrix}\\) and \\(B=\\begin{bmatrix} 1 & -1 & 1 \\\\ 0 & 2 & 0 \\\\ 4 & 3 & 2 \\end{bmatrix}.\\) Where possible, find the following matrices.\n\n\\(A+B\\)\n\\(B-A\\)\n\\(3B\\)\n\\(-2A\\)\n\\(AB-BA\\)\n\\(-2BA\\)\n\\(2A-4B\\)\n\\(B^2-A^2\\)\n\n\n\nExercise 1.35 Find \\(a, b, c\\), and \\(d\\) that satisfies the equation.\n\n\\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} c-3d & -d \\\\ 2a+d & a+b \\end{bmatrix}\\)\n\\(3 \\begin{bmatrix} a \\\\ b \\end{bmatrix} + 2 \\begin{bmatrix} b \\\\ a \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 2 \\end{bmatrix}\\)\n\n\n\nExercise 1.36 Let \\(A=\\begin{bmatrix}2 & 1 \\\\ 0 & -1\\end{bmatrix},\\) \\(B=\\begin{bmatrix}3 & -1 & 2\\\\ 0 & 1 & 4\\end{bmatrix},\\) \\(C=\\begin{bmatrix}3 & -1 \\\\ 2 & 0\\end{bmatrix},\\) and \\(D=\\begin{bmatrix}1 & 3 \\\\ -1 & 0 \\\\1 & 4\\end{bmatrix}.\\) Where possible, find the following\n\n\\(3A-2B\\)\n\\(5C\\)\n\\(B+D\\)\n\\(2B-3D\\)\n\\(A-D\\)\n\\(A-2C\\)\n\n\n\nExercise 1.37 Find \\(A\\) in terms of \\(B\\).\n\n\\(A+B=3A+2B\\)\n\\(2A-B=5(A+2B)\\)\n\n\n\nExercise 1.38 Simpify the following expressions where \\(A\\), \\(B\\), and \\(C\\) are matrices.\n\n\\(2[9(A-B)+7(2B-A)]-2[3(2B+A)-2(A+3B)]\\)\n\\(5[3(A-B+2C)-2(3C-B)-A]+2[3(3A-B+C)+2(B-2A)-2C]\\)\n\n\n\nExercise 1.39 Show that if \\(Q+A=A\\) holds for every \\(m\\times n\\) matrix \\(A\\), then \\(Q=0_{mn}\\).\n\n\nExercise 1.40 Show that if \\(A\\) is an \\(m\\times n\\) matrix with \\(A+A'=0_{mn}\\), then \\(A'=-A\\).\n\n\nExercise 1.41 Show that if \\(A\\) denotes an \\(m\\times n\\) matrix, then \\(A=-A\\) if and only if \\(A=0\\).\n\n\nExercise 1.42 Given the following vectors, determine the values of \\(a\\) and \\(b\\) such that \\(\\vec{u}\\) is a linear combination of \\(\\vec{v_1}\\) and\n\\(\\vec{v_2}\\). \\[\n\\vec{u}=\\vectorfour{5}{7}{a}{b}\n\\qquad\n\\vec{v_1}=\\vectorfour{1}{1}{1}{1}\n\\qquad\n\\vec{v_2}=\\vectorfour{4}{3}{2}{1}\n\\]\n\n\nExercise 1.43 Find all solutions \\(x_1, x_2, x_3\\) of the equation \\(\\vec b = x_1 \\vec v_1 + x_2 \\vec v_2 + x_3 \\vec v_3\\) given the following vectors. \\[\n\\vec b = \\vectorfour{-8}{-1}{2}{15}\n\\qquad\n\\vec v_1 = \\vectorfour{1}{4}{7}{5}\n\\qquad\n\\vec v_2 = \\vectorfour{2}{5}{8}{3}\n\\qquad\n\\vec v_3 = \\vectorfour{4}{6}{9}{1}\n\\]\n\n\nExercise 1.44 Determine the value of \\(a\\) such that \\(\\vec{u}\\) is a linear combination of \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\) given the following vectors. \\[\n\\vec{u}=\\vectorthree{1}{a}{a^2}\n\\qquad\n\\vec{v_1}=\\vectorthree{1}{2}{4}\n\\qquad\n\\vec{v_2}=\\vectorthree{1}{3}{9}\n\\]\n\n\nExercise 1.45 Find the product of the given matrices.\n\n\\(\\begin{bmatrix} 1 & -1 & 2 \\\\ 2 & 0 & 4 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 2 & 3 & 1 \\\\ 1 & 9 & 7\\\\ -1 & 0 & 2 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 3 & -3 \\\\ \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 3 & 0 \\\\ -2 & 1 \\\\ 0 & 6 \\end{bmatrix}\\)\n\n\n\nExercise 1.46 Let \\(A=\\begin{bmatrix}2 & 3-i \\\\ 1 & i \\end{bmatrix},\\) \\(B=\\begin{bmatrix}i & 1-1 \\\\ 0 & i\\end{bmatrix},\\) and \\(C=\\begin{bmatrix} 2+1 & 1 \\\\ 3 & i+1 \\end{bmatrix}.\\) Find each of the following.\n\n\\(A+B+C\\)\n\\(AB+AC\\)\n\\(A+BC\\)\n\\(CB\\)\n\\(A^2 C\\)\n\\(C^2 A\\)\n\n\n\nExercise 1.47 Find all matrices that commute with the given matrix.\n\n\\(\\begin{bmatrix}0 & 1 \\\\ 0 & 0 \\end{bmatrix}\\)\n\\(\\begin{bmatrix}0 & 0 \\\\ 1 & 0\\end{bmatrix}\\)\n\\(\\begin{bmatrix}2 & 3 \\\\ 1 & 0\\end{bmatrix}\\)\n\n\n\nExercise 1.48 For what values of the constants \\(a, b, c\\) and \\(d\\) is\n\\(\\vec b\\) a linear combination of the vectors \\(\\vec v_1\\), \\(\\vec v_2\\), \\(\\vec v_3\\) given \\[\n\\vec b = \\begin{bmatrix} a \\\\ b  \\\\ c \\\\ d \\end{bmatrix}\n\\qquad\n\\vec v_1 =\\vectorfour{0}{0}{3}{0}\n\\qquad\n\\vec v_2 = \\vectorfour{1}{0}{4}{0}\n\\qquad\n\\vec v_3 = \\vectorfour{2}{0}{5}{6}?\n\\]\n\n\nExercise 1.49 If \\(A=\\begin{bmatrix}a & b \\\\c & d\\end{bmatrix}\\) where \\(a\\neq 0\\), show that \\(A\\) factors in the form \\[\nA=\n\\begin{bmatrix}\n1 & 0 \\\\\nx & 1\n\\end{bmatrix}\n\\begin{bmatrix}\ny & z \\\\\n0 & w\n\\end{bmatrix}.\n\\]\n\n\nExercise 1.50 Find all vectors in \\(\\mathbb{R}^4\\) that are perpendicular to the following three vectors. \\[\nu=\\vectorfour{1}{1}{1}{1} \\qquad\nv=\\vectorfour{1}{2}{3}{4} \\qquad  \nw=\\vectorfour{1}{9}{9}{7}\n\\]\n\n\nExercise 1.51 Determine a scalar \\(t\\) such that \\(A X=t X\\) where \\(A=\\begin{bmatrix}2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\) and \\(X=\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}.\\)\n\n\nExercise 1.52 Show that if \\(A\\) and \\(B\\) commute with \\(C\\), then so does \\(A+B\\).\n\n\nExercise 1.53 Show that if \\(A\\) and \\(B\\) are \\(n\\times n\\) matrices, then \\(A B=BA\\) if and only if \\((A+B)^2=A^2+2A B+B^2\\).\n\n\nExercise 1.54 Let \\(A\\) and \\(B\\) be the \\(2\\times 2\\) matrices \\[\nA=\n\\begin{bmatrix}\n\\begin{bmatrix}\n1 & 3 \\\\ 2& 1\n\\end{bmatrix}\n&\n\\begin{bmatrix}\n0 & 3 \\\\ 1 & 3\n\\end{bmatrix}\n\\\\ \\vspace{-10pt} & \\\\\n\\begin{bmatrix}\n3 & 4 \\\\\n1 & 1\n\\end{bmatrix}\n&\n\\begin{bmatrix}\n2 & 0 \\\\ 0 & 1\n\\end{bmatrix}\n\\end{bmatrix}\n\\quad\nB=\n\\begin{bmatrix}\n\\begin{bmatrix}\n3 & 3 \\\\ 1 & 1\n\\end{bmatrix}\n&\n\\begin{bmatrix}\n2 & 6 \\\\\n4 & 3\n\\end{bmatrix}\n\\\\ \\vspace{-10pt} & \\\\\n\\begin{bmatrix}\n1 & 3 \\\\ 1 & 3\n\\end{bmatrix}\n&\n\\begin{bmatrix}\n2 & 1 \\\\ 1 & 1\n\\end{bmatrix}\n\\end{bmatrix}\n\\] whose entries are themselves \\(2\\times 2\\) matrices. Where possible, find each of the following.\n\n\\(A+B\\)\n\\(-2B\\)\n\\(3A\\)\n\\(2A-3B\\)\n\\(B^2\\)\n\\(AB\\)\n\\(BA\\)\n\\(AB-BA\\)\n\\(A^2-B^2\\)\n\n\n\nExercise 1.55 Let \\(A=\\begin{bmatrix}\\cos \\theta & \\sin \\theta \\\\ -\\sin \\theta & \\cos \\theta \\end{bmatrix}\\). Find an expression for\n\n\\(A^2\\),\n\\(A^3\\), and\n\\(A^n\\) where \\(n\\) is a positive integer.\n\n\n\nExercise 1.56 The Pauli spin matrices \\[\nP_1\n=\n\\begin{bmatrix}\n0 & 1 \\\\ 1 & 0\n\\end{bmatrix},\n\\quad\nP_2\n=\n\\begin{bmatrix}\n0 & -i \\\\ i & 0\n\\end{bmatrix},\n\\quad\nP_3\n=\n\\begin{bmatrix}\n1 & 0 \\\\ 0 &-1\n\\end{bmatrix}\n\\] are used when studying electron spin. Find\n\n$P_1 P_2, $\n\n\\(P_2 P_1\\),\n\\(P_2^2\\),\n\\(P_1 P_3\\),\n\n\\(P_3 P_1\\), and\n\\(P_1+i P_2\\).\n\n\n\nExercise 1.57 Let \\(A\\) denote an arbitrary matrix and let \\(k\\) denote an arbitrary scalar. Prove the following properties hold.\n\n\\(A-A=0\\)\n\\(0A=0\\)\n\\(0A=0\\)\n\\(k I A=k A\\)\n\n\n\nExercise 1.58 Find \\(A\\vec e_1\\), \\(A\\vec e_2\\), and \\(A\\vec e_3\\) given the following. \\[\n\\vec e_1=\\vectorthree{1}{0}{0} \\quad\n\\vec e_2=\\vectorthree{0}{1}{0} \\quad\n\\vec e_3=\\vectorthree{0}{0}{1} \\qquad\n\\text{and} \\qquad\nA=\n\\begin{bmatrix}\na_1 & a_2 & a_3 \\\\\nb_1 & b_2 & b_3 \\\\\nc_1 & c_2 & c_3\n\\end{bmatrix}\n\\]\n\n\nExercise 1.59 Find a \\(3\\times 3\\) matrix \\(A\\) that satisfies all of the following. \\[\nA \\vectorthree{1}{0}{0}= \\vectorthree{1}{2}{3} \\qquad\nA \\vectorthree{0}{1}{0}= \\vectorthree{4}{5}{6}  \n\\qquad \\text{and} \\qquad\nA \\vectorthree{0}{0}{1}= \\vectorthree{7}{8}{9}\n\\]\n\n\nExercise 1.60 Find all vectors \\(\\vec x\\) such that \\(A\\vec x=\\vec b\\) given the following. \\[\nA=\n\\begin{bmatrix}\n1 & 2  & 0\\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0 \\end{bmatrix}\n\\quad \\text{ and } \\quad\n\\vec b=\\vectorthree{2}{1}{0}.\n\\]\n\n\nExercise 1.61 Write the system \\[\n\\begin{cases}\n2x_1-3x_2+5x_3 =7 \\\\\n9x_1+4x_2-6x_3 =8\n\\end{cases}\n\\] in matrix form and write \\(\\begin{bmatrix} 7 \\\\ 8 \\end{bmatrix}\\) as a linear combination of the columns vectors of the coefficient matrix.\n\n\nExercise 1.62 Show that the sum of any two diagonal matrices is diagonal.\n\n\nExercise 1.63 Show that the sum of any two upper triangular matrices is upper triangular.\n\n\nExercise 1.64 Let \\(A\\) be an \\(m\\times n\\) matrix and let \\[\nB=\\vectorfour{b_1}{b_2}{\\vdots}{b_n}\n\\qquad \\text{and} \\qquad\nC=\\begin{bmatrix} c_1 & c_2 & \\cdots & c_m\\end{bmatrix}\n\\] be a column vector and a row vector, respectively. Prove that \\(AB=\\sum_{j=1}^n b_j A_j\\) and \\(CA=\\sum_{j=1}^n c_j A_j\\).\n\n\nExercise 1.65 Let \\(A\\) and \\(B\\) denote \\(n\\times n\\) matrices. The Jordan product , \\(A\\star B\\) is defined by \\(A\\star B=\\frac{1}{2}(AB+BA)\\). Determine whether this product is commutative, associative, and/or distributive.\n\n\nExercise 1.66  Complete the proof of \\(\\ref{PropertiesofMatrixAddition}\\).\n\n\nExercise 1.67  Complete the proof of \\(\\ref{Properties of Scalar Multiplication}\\).\n\n\nExercise 1.68  Complete the proof of \\(\\ref{Properties of Matrix Multiplication}\\)."
  },
  {
    "objectID": "systems-of-linear-equations.html#on-the-solutions-of-linear-systems",
    "href": "systems-of-linear-equations.html#on-the-solutions-of-linear-systems",
    "title": "1  Systems of Linear Equations",
    "section": "1.9 On the Solutions of Linear Systems",
    "text": "1.9 On the Solutions of Linear Systems\nRecall a system of linear equations is called consistent if it has at least one solution and is called inconsistent if it has no solutions. In this section we completely characterize when a linear system of equations has solutions; and we do so using the notion of . Basically, the rank of a linear system is the number of leading coefficients in the reduced row echelon form of the augmented matrix of the given linear system. Our first goal will be to show the notion of rank is well-defined; that is, we wish to show that every matrix has a unique reduced row echelon form. Thus the rank of a linear system will be a unique number.\nFind a \\(2\\times 3\\) linear system whose augmented matrix has two different row echelon forms.\n\nDefinition 1.8 If an matrix \\(A\\) can be obtained from another matrix \\(B\\) by a finite number of elementary row operations, then we say \\(B\\) is row equivalent to \\(A\\).\n\n\nTheorem 1.6  Every matrix is row equivalent to a matrix in row echelon form.\n\n\nProof. Let \\(A\\) be an \\(m\\times n\\) nonzero matrix, with entries \\(a_{ij}\\), say \\[\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n& & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots  & a_{mn} \\\\\n\\end{bmatrix}\n\\] Either all entries in the first column of \\(A\\) are nonzero or not. If all entries are zero then the first column of \\(A\\) satisfies the conditions of row echelon form. Otherwise, assume \\(i\\) is the least such that \\(a_i\\) is nonzero. Interchanging rows 1 and \\(i\\) (if needed) we obtain a column where the first entry is nonzero. Now we have a matrix of the following form. \\[\n\\begin{array}{ll}\n\\parbox{1.7cm}{\\vspace{.5cm} $i$-th row $\\rightarrow$}\n&\n\\begin{bmatrix}\na_{i1} & a_{i2} & \\cdots & a_{in} \\\\\na_{21} & a_{22} & \\cdots & a_{2n}\\\\\n& & \\vdots \\\\\na_{11} & a_{12} & \\cdots & a_{1n}  \\\\\n& & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots  & a_{mn}\\\\\n\\end{bmatrix}\n\\end{array}\n\\] Next we multiply the first row by the multiplicative inverse of its nonzero entry, obtaining 1. For the remaining first column entries \\(a_{k,m}\\) (where \\(k>1\\)) multiply the row by the multiplicative inverse of \\(a_{k,m}\\) and add to the first row replacing the \\(k\\)-th row. These steps lead to all zero entries in the first column below the leading coefficient. Our matrix is now in the form \\[\n\\begin{bmatrix}\n1 & a'_{i2} & \\cdots & a'_{in} \\\\\n0 & a'_{22} & \\cdots & a'_{2n}\\\\\n& & \\vdots \\\\\n0 & a'_{m2} & \\cdots  & a'_{mn}\\\\\n\\end{bmatrix}\n\\] where the \\(a'_{ij}\\) are the scalars obtain from completing the row operations. We repeat this process on the remaining columns taking into account that applying row operations will not change the fact that the previous columns will continue to satisfy the conditions of row echelon form.\n\n\nTheorem 1.7  Every matrix is row equivalent to a unique matrix in reduced row echelon form.\n\n\nProof. Let \\(A\\) be an \\(m\\times n\\) matrix. We apply mathematical induction on \\(n\\) for an arbitrary \\(m\\). Let \\(n=1\\). Now \\(A\\) is just a matrix with one column and is row equivalent to one of the following matrices. \\[\n\\vectorfour{1}{0}{\\vdots}{0}\n\\qquad \\text{or} \\qquad\n\\vectorfour{0}{0}{\\vdots}{0}\n\\] So the case for \\(n=1\\) is clear. Now assume \\(n>1\\) and let \\(A\\) and \\(A'\\) denote the following matrices. \\[\nA=\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n} \\\\\n& & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots  & a_{mn} \\\\\n\\end{bmatrix}\n\\qquad \\text{and} \\qquad\nA'=\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1,n-1} \\\\\na_{21} & a_{22} & \\cdots & a_{2,n-1} \\\\\n& & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots  & a_{m,n-1} \\\\\n\\end{bmatrix}\n\\] Notice \\(A'\\) is just \\(A\\) with the \\(n\\)-th column deleted. By the way \\(A'\\) is defined, any sequence of elementary row operations that takes \\(A\\) into reduced row echelon form also takes \\(A'\\) into reduced row echelon form. By the induction hypothesis, any two matrices \\(B\\) and \\(C\\) that are reduced row echelon forms of \\(A\\) can only differ in the \\(n\\) column. Assume, for a contradiction, that \\(B\\) and \\(C\\) differ only in the \\(n\\)-th column. Then there exists an integer \\(j\\) such that the \\(j\\)-th row of \\(B\\) is not equal to the \\(j\\)-th row of \\(C\\).\n\nDue to \\(\\ref{rowequiv}\\) the following definition is well-defined; meaning if a matrix \\(A\\) is reduced to the unique matrix in reduced row echelon form\n\nDefinition 1.9 The rank of a matrix \\(A\\) is the number of leading coefficients in \\(\\text{rref}(A)\\).\n\n\nExample 1.23 Let \\(a, d, f\\) be nonzero constants and let \\(b, c, e\\) be arbitrary constants. Find the rank of the following matrices. \\[\nA=\\begin{bmatrix} 0 & 0 & a \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix} \\qquad\nB=\\begin{bmatrix} 0 & a & 0 \\\\ 0 & 0 & d \\\\ 0 & 0 & 0 \\end{bmatrix} \\qquad\nC=\\begin{bmatrix} a & b & c \\\\ 0 & d & e \\\\ 0 & 0 & f \\end{bmatrix}\n\\] Since \\(a\\), \\(f\\), and \\(d\\) are all nonzero, \\[\n\\text{rref}(A)=\\begin{bmatrix} 0 & 0 & 1 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}, \\qquad\n\\text{rref}(B)=\\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 0 \\end{bmatrix}, \\qquad\n\\text{rref}(C)=\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}.\n\\] Thus the rank of \\(A\\), \\(B\\), and \\(C\\) is 1, 2, and 3, respectively.\n\n\nLemma 1.1 Let \\(A\\vec{x}=\\vec{b}\\) and \\(C\\vec{x}=\\vec{d}\\) be two linear systems of the same size. If the augmented matrices \\(\\begin{bmatrix} A \\, | \\, \\vec{b} \\end{bmatrix}\\) and \\(\\begin{bmatrix}C \\, | \\, \\vec{d} \\end{bmatrix}\\) are row equivalent, then the linear systems are equivalent.\n\n\nProof. The proof follows immediately from \\(\\ref{proprowoperations}\\).\n\n\nTheorem 1.8 [Fundamental Theorem of Linear Systems] Let A be the coefficient matrix of an \\(n\\times m\\) system. Then\n\nIf \\(\\text{rank}(A)=n\\), then the system is consistent.\nIf \\(\\text{rank}(A)=m\\), then the system has at most one solution.\nIf \\(\\text{rank}(A)<m\\), then the system has either infinitely many solutions or none.\n\n\n\nProof. First let’s make two observations. By definition of the reduced row echelon form of a matrix, there is at most one leading 1 in each of the \\(m\\) columns and in each of the \\(n\\) columns. Let A be the coefficient matrix of an \\(n\\) by \\(m\\) system. Then \\[\n\\text{rank}(A) \\leq n \\qquad \\text{and}\\qquad \\text{rank}(A)\\leq m.\n\\] A linear system is inconsistent if and only if the reduced row echelon form of its augmented matrix has a row of the form \\[\\begin{equation}\n\\label{inconsirow}\n\\left[\n\\begin{array}{cccc|c}\n0 & 0 & \\cdots & 0 & 1\n\\end{array}\n\\right].\n\\end{equation}\\] Moreover, for any linear system with \\(m\\) variables, \\[\\begin{equation*}\n\\begin{tabular}{ccccccc} \\small\n$\\left(\\parbox[c]{1.55cm}{\\begin{center}\nnumber of free variables\n\\end{center}}\\right)$ & $=$  &\n$\\left(\\parbox[c]{1.55cm}{\\begin{center}\ntotal number of variables\n\\end{center}}\\right)$ & $-$  &\n$\\left(\\parbox[c]{1.55cm}{\\begin{center}\nnumber of leading variables\n\\end{center}}\\right)$ & $=$  \n& $\\parbox[c]{2.15cm}{\\begin{center}\n$m-\\text{rank}(A)$\n\\end{center}}$\n\\end{tabular}\n\\end{equation*}\\]\n\nIf \\(\\text{rank} A=n\\) then each row must contain a leading 1. Thus there is no row of the form in \\(\\ref{inconsirow}\\) and so the system is consistent.\nIf \\(\\text{rank}(A)=m\\) then there are no free variables. So the only possible choice is for there to be no solutions or exactly one solution.\n\nIf \\(\\text{rank}(A)=t<m\\) then there \\(m-t\\) free variables. So the only possible choice is for there to be no solutions or infinitely many solutions.\n\n\nThe following two corollaries are immediate consequences of \\(\\ref{systemchar}\\).\n\nCorollary 1.2  A linear system with fewer equations than unknowns has either no solutions or infinity many solutions.\n\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:linsystmecor1}\\).\n\n\nCorollary 1.3  A linear system of \\(n\\) equations in \\(n\\) variables has a unique solution if and only if the rank of its coefficients matrix \\(A\\) is \\(n\\), and in this case \\(\\text{rref}(A)=I_n\\).\n\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:linsystmecor2}\\).\n\n\nExample 1.24 Find the rank of the coefficient matrix and solve the linear system of equations \\[\n\\begin{cases}\nx_1-x_2+x_3=4\\\\\n3x_1+4x_2-x_3=8\\\\\n5x_1+9x_2-4x_3=13.\n\\end{cases}\n\\] We use Gaussian elimination with the augmented matrix to find the rank of the coefficient matrix. \\[\\begin{align*}\n&\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & -1 & 2 & 4\\\\\n3 & 4 & -1 & 8\\\\\n5 & 9 & -4 & 13\n\\end{array}\n\\end{bmatrix}\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{-3R_1+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{-5R_1+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & -1 & 2 & 4 \\\\\n0 & 7 & -7 & -4 \\\\\n0 & 14 & -14 & -7\n\\end{array}\n\\end{bmatrix}\n\\\\\n&\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{-\\frac{1}{7}R_2} \\\\\n\\stackrel{\\longrightarrow}{-14R_2+R_3}\n\\end{array}\n\\begin{bmatrix}\n1 & -1 & 2 & 4 \\\\\n0 & 1 & -1 & -\\frac{4}{7} \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\frac{4}{7}R_3+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{-4R_3+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{R_2+R_1}\n\\end{array}\n\\begin{bmatrix}\n1 & 0 & 1 & 0 \\\\\n0 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\end{align*}\\] The number of leading 1’s is 2 and thus \\(\\text{rank}(A)=2\\). By \\(\\ref{systemchar}\\) the system either has no solutions or infinitely many solutions. The last row represents \\(0=1\\), which means that the system has no solution.\n\n\nExample 1.25 Consider the system \\[\n\\begin{cases}\ny+2k z =0 \\\\\nx+2y+6z  =2 \\\\\nk x+2 z  =1\n\\end{cases}\n\\] where \\(k\\) is a constant.\n\nFor which values of \\(k\\) does the system have a unique solution?\nWhen are there no solutions?\nWhen are there infinitely many solutions?\n\nUsing Gaussian elimination: \\[\\begin{align*}\n\\stackrel{\\longrightarrow}{R_1 \\leftrightarrow R_2}\n\\left[\n\\begin{array}{ccc|c}\n1 & 2 & 6 & 2 \\\\\n0 & 1 & 2k & 0 \\\\\nk & 0 & 2 & 1\n\\end{array}\n\\right]\n&\n\\stackrel{\\longrightarrow}{-k R_1+R_2}\n\\left[\n\\begin{array}{ccc|c}\n1 & 2 & 6 & 2 \\\\\n0 & 1 & 2k & 0 \\\\\n0 & -2k & -6k+2 & -2k+1\n\\end{array}\n\\right]\n\\\\\n&\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{-2R_2+R_1} \\\\\n\\stackrel{\\longrightarrow}{2kR_2+R_3}\n\\end{array}\n\\left[\n\\begin{array}{ccc|c}\n1 & 0 & -4k+6 & 2 \\\\\n0 & 1 & 2k & 0 \\\\\n0 & 0 & 4k^2-6k+2 & -2k+1\n\\end{array}\n\\right]\n\\end{align*}\\] Notice $ 4k^2-6k+2=-2(-2k+1)(k-1)=0 $ when \\(k=1/2\\) and \\(k=1\\). (a) When \\(k\\neq 1/2\\) and \\(k\\neq 1\\) there is a unique solution. (b) When \\(k \\neq 1/2\\) and \\(k=1\\), this system has no solutions. (c) When \\(k=1/2\\) this system has infinitely many solutions.\n\n\nExample 1.26 Solve the linear system of equations \\[\n\\begin{cases}\n(3+i)x_1+(1+i)x_2=4+4i\\\\\nx_1-x_2=2i\n\\end{cases}\n\\] where \\(x_1\\) and \\(x_2\\) are complex variables. We convert the system into a linear system with real variables. Let \\(x_1=y_1+i z_1\\) and \\(x_2=y_2+i z_2\\). Now substation into the original system leads to the system \\[\n\\begin{cases}\n3y_1-z_1+(y_1+3z_1)i+(y_2-z_2)+(y_2+z_2)i=4+4i \\\\\n(y_1-y_2)+(z_1-z_2)i=0+2i\n\\end{cases}\\] Equating real and imaginary parts leads to the system \\[\n\\begin{cases}\n3y_1+y_2-z_1-z_2=4\\\\\ny_1+y_2+3z_1+z_2 =4\\\\\ny_1-y_2=0\\\\\nz_1-z_2=2\n\\end{cases}\n\\] The solutions are \\(y_1=1\\), \\(y_2=1\\), \\(z_1=1\\), and \\(z_2=-1\\). Thus the solutions to the original system are \\(x_1=1+i\\) and \\(x_2=1-i\\).\n\n\nDefinition 1.10 A linear system of the form \\(A \\vec x = \\vec 0\\) is called a homogeneous linear system; that is, if all of the constant terms in the linear system are zero.\n\nIn particular, if \\(A\\) and \\(B\\) are row equivalent \\(m \\times n\\) matrices, then the homogenous systems \\(A\\vec{x}=\\vec{0}\\) and \\(B\\vec{x}=\\vec{0}\\) are equivalent.\n\nLemma 1.2 If \\(A\\) is an \\(n\\times n\\) matrix and the system \\(A \\vec{x}=\\vec{0}\\) has no nontrivial solution, then \\(A\\) is row equivalent to \\(I_n\\).\n\n\nProof. If \\(A\\vec{x}=\\vec{0}\\) has no nontrivial solutions, then the trivial solution is its unique solution. By \\(\\ref{cor:linsystmecor2}\\), \\(\\text{rref}(A)=I_n\\) and by \\(\\ref{rowequiv}\\), \\(A\\) is row equivalent to a unique matrix in reduced row echelon form, thus \\(A\\) is row equivalent to \\(I_n\\).\n\n\nLemma 1.3  Let \\(A \\vec{x} = \\vec{0}\\) be a linear homogeneous system.\n\nAll homogeneous systems are consistent.\nA homogeneous system with fewer equations than unknowns has infinitely many solutions.\nIf \\(\\vec{x}_1\\) and \\(\\vec{x}_2\\) are solutions, then \\(\\vec{x}_1+\\vec{x}_2\\) is also a solution.\nIf \\(\\vec{x}_1\\) is a solution, then \\(k \\vec{x}_1\\) is also a solution.\n\n\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:homosys}\\).\n\nThere is a strong relationship between the solutions to a linear system \\(A\\vec{x} = \\vec{b}\\) and the solutions to the corresponding homogeneous system, \\(A\\vec{x}= \\vec{0}\\).\n\nTheorem 1.9 If \\(\\vec{u}\\) is any specific solution to the linear system \\(A\\vec{x}=\\vec{b}\\), then the entire solution set of \\(A\\vec{x}=\\vec{b}\\) can be described as \\[\n\\{\\vec{u}+\\vec{v} \\, | \\, \\vec{v}\n\\text{ is any solution to } A \\vec{x}=\\vec{0}\\}.\n\\]\n\n\nProof. Let \\(\\vec{u}\\) be any particular solution to the system \\(A\\vec{x}=\\vec{b}\\) and let \\(\\vec{v}\\) be any solution to the system \\(A\\vec{x}=\\vec{0}\\). Then \\[\nA(\\vec{u}+\\vec{v})\n=A\\vec{u}+A\\vec{v}\n=\\vec{b}+\\vec{0}\n=\\vec{b},\n\\] which shows that every vector of the form \\(\\vec{u}+\\vec{v}\\) is a solution to the system \\(A\\vec{x}=\\vec{b}\\). Conversely, let \\(\\vec{w}\\) be an arbitrary solution to the system \\(A\\vec{x}=\\vec{b}\\). Notice \\[\nA(\\vec{w}-\\vec{u})\n=A\\vec{w} - A\\vec{u}\n=\\vec{b}-\\vec{b}\n=\\vec{0},\n\\] which shows \\(\\vec{w}-\\vec{u}\\) is a solution to the system \\(A\\vec{x}=\\vec{0}\\). Set \\(\\vec{v}=\\vec{w}-\\vec{u}\\), then \\(\\vec{w}=\\vec{u}+\\vec{w}-\\vec{u}=\\vec{u}+\\vec{v}\\) where \\(\\vec{v}\\) is a solution to the system \\(A\\vec{x}=\\vec{0}\\).\n\n\nCorollary 1.4  If \\(A\\) is an \\(n\\times n\\) and \\(A\\vec{x}=\\vec{0}\\) has no nontrivial solutions, then the system \\(A\\vec{x}=\\vec{b}\\) has a unique solution.\n\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:unsol}\\)."
  },
  {
    "objectID": "systems-of-linear-equations.html#exercises-2",
    "href": "systems-of-linear-equations.html#exercises-2",
    "title": "1  Systems of Linear Equations",
    "section": "1.10 Exercises",
    "text": "1.10 Exercises\n\nExercise 1.69 Find an inconsistent system of two linear equations in three unknowns. Describe the situation geometrically.\n\n\nExercise 1.70 Find the rank of the matrix.\n\n\\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 3 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 2 & 2 & 2 \\\\ 2 & 2 & 2 \\\\ 2 & 2 & 2 \\\\ 2 & 2 & 2 \\\\ \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 4 & 7 \\\\ 2 & 5 & 8 \\\\ 3 & 6 & 9 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 4 & 7 \\\\ 2 & 5 & 8 \\\\ -3 & -6 & -9\\end{bmatrix}\\)\n\n\n\nExercise 1.71  \n\nIf the rank of a \\(4\\times 4\\) matrix \\(A\\) is 4, what is \\(\\text{rref}(A)\\)?\nIf the rank of a \\(5\\times 3\\) matrix \\(A\\) is 3, what is \\(\\text{rref}(A)\\)?\nIf the rank of a \\(2\\times 5\\) matrix \\(A\\) is 1, what is \\(\\text{rref}(A)\\)?\n\n\n\nExercise 1.72 Find the rank of the following matrices.\n\n\\(\\begin{bmatrix} a & 0 & 0 \\\\ 0 & c & 0\\\\ 0 & 0 & f\\end{bmatrix}\\)\n\\(\\begin{bmatrix}a & b & c \\\\ 0 & c & d \\\\ 0 & 0 & f \\end{bmatrix}\\)\n\\(\\begin{bmatrix} a & 0 & 0 \\\\ b & c & 0\\\\ d & e & f\\end{bmatrix}\\)\n\nwhere \\(a\\), \\(d\\), \\(f\\) are nonzero and \\(b\\), \\(c,\\) and \\(e\\) are arbitrary scalars.\n\n\nExercise 1.73 Find the rank of the system of equations.\n\n\\(\\begin{cases} x+2y+3z=0 \\\\ 2x+3y+4z=0 \\\\ 3x+4y+6z=0 \\end{cases}\\)\n\\(\\begin{cases} x+2y+3z=2\\\\ 2x+3y+4z=-2\\\\ 3x+4y+6z=2 \\end{cases}\\)\n\\(\\begin{cases} x+2y+3z=0 \\\\ 2x+3y+4z=1\\\\ 3x+4y+6z=3 \\end{cases}\\)\n\\(\\begin{cases} x+2y+3z=a\\\\ 2x+3y+4z=b \\\\ 3x+4y+6z=c \\end{cases}\\)\n\n\n\nExercise 1.74 Find all solutions to the homogenous system.\n\n\\(\\begin{cases} 4x_1-x_2=0 \\\\ 7x_1+3x_2=0\\\\ -8x_1+6x_2=0 \\end{cases}\\)\n\\(\\begin{cases}x_1-2x_2+x_3=0\\\\3x_1+2x_3+x_4=0\\\\ 4x_2-x_3-x_4=0\\\\ 5x_1+3x_3-x_4=0\\end{cases}\\)\n\\(\\begin{cases} x_1-3x_2=0\\\\ -2x_1+6x_2=0\\\\ 4x_1-12x_2=0 \\end{cases}\\)\n\\(\\begin{cases}x_1+x_2-x_3=0\\\\ 4x_1-x_2+5x_3=0\\\\ 2x_1-x_2-2x_3=0\\\\ 3x_1+2x_2-x_3=0\\end{cases}\\)\n\n\n\nExercise 1.75 Show that the homogenous system of linear equations \\[\n\\begin{cases}\na x+by=0 \\\\\ncx+dy =0\n\\end{cases}\n\\] has an infinite number of solutions if and only if \\(ad-bc=0\\).\n\n\nExercise 1.76 For any positive integer \\(n\\), find a system of \\(n\\) equations in two variables that has infinitely many solutions.\n\n\nExercise 1.77 If possible, find a condition of the coefficients of the homogenous system of linear equations so that (i) the system only has the zero solution, and (ii) the system has infinitely many solutions.\n\n\\(\\begin{cases} a_{11}x_1+a_{12}x_2+a_{13}x_3=0 \\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3=0 \\end{cases}\\)\n\\(\\begin{cases} a_{11}x_1+a_{12}x_2+a_{13}x_3=0 \\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3=0 \\\\ a_{31}x_1+a_{32}x_2+a_{33}x_3=0\\end{cases}\\)\n\n\n\nExercise 1.78 Show that if a system of linear equations is inconsistent, then its reduced row echelon form has a row of the form\n\\[\n\\left[\n\\begin{array}{cccc|c}\n0 & 0 & \\cdots & 0 & 1\n\\end{array}\n\\right].\n\\]\n\n\nExercise 1.79  Prove \\(\\ref{cor:linsystmecor1}\\).\n\n\nExercise 1.80  Prove \\(\\ref{cor:linsystmecor2}\\).\n\n\nExercise 1.81 Determine the values of \\(k\\) for which the system has nontrivial solutions.\n\n\\(\\begin{cases} 2x_1-3x_2+5x_3=0\\\\ -x_1+7x_2-x_3=0\\\\ 4x_1-11x_2+k x_3=0 \\end{cases}\\)\n\\(\\begin{cases} x_1-2x_2-5x_3=0\\\\ 2x_1+3x_2+x_3=0\\\\ x_1-7x_2-k x_3=0\\end{cases}\\)\n\n\n\nExercise 1.82 Show that if \\(AX=\\vec 0\\) is a homogenous system with an infinite number of solutions and the system \\(AX=B\\) has a solution, then \\(AX=B\\) must have an infinite number of solutions.\n\n\nExercise 1.83 Find an example, where possible, for each of the following.\n\nA linear system of four equations in four unknowns that has a line as a solution set.\nA linear system of four equations in four unknowns that has a plane as a solution set.\nA linear system of four equations in three unknowns that has a line as a solution set.\nA linear system of four equations in two unknowns that has a plane as a solution set.\n\n\n\nExercise 1.84 Determine whether or not the following system is consistent. \\[\n\\begin{bmatrix}\n0 & i & 1-i \\\\\n-i & 0 & i\\\\\n1-i & -i & 0\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-1 \\\\ 0 \\\\ 1\n\\end{bmatrix}\n\\]\n\n\nExercise 1.85 Show that if \\(AX=0\\) for all vectors \\(X\\), then \\(A=0\\).\n\n\nExercise 1.86 Under what conditions will \\(k\\) planes \\(a_j x +b_j y+c_j z=d_j\\) for \\(j=1, 2, ..., k\\) intersect in exactly one point?\n\n\nExercise 1.87 Given that \\(AX=B\\) is consistent and of rank \\(r\\), for what sets of \\(r\\) unknowns can one solve?\n\n\nExercise 1.88 If possible, write the matrix \\(A\\) as a linear combination of the matrices \\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\qquad \\text{and} \\qquad\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}.\n\\]\n\n\\(A=\\begin{bmatrix}3 & 0 \\\\ 0 & 2\\end{bmatrix}\\)\n\\(A=\\begin{bmatrix} 4 & 1 \\\\ 0 & -3 \\end{bmatrix}\\)\n\\(A=\\begin{bmatrix}11 & 2 \\\\ 0 & -4\\end{bmatrix}\\)\n\n\n\nExercise 1.89 Let \\(A Z=B\\) be a given system of linear equations where \\(A_{n\\times n}\\) and \\(B_{n\\times 1}\\) are complex matrices. Let \\(Z=X+i Y\\). Show that the original complex \\(n\\times n\\) system is equivalent to the \\(2n\\times 2n\\) real system \\[\n\\begin{cases}\nCX-DY=S \\\\\nCX+DY=T\n\\end{cases}\n\\] where \\(B=S+iT\\).\n\n\nExercise 1.90  Prove \\(\\ref{lem:homosys}\\).\n\n\nExercise 1.91  Prove \\(\\ref{cor:unsol}\\)."
  },
  {
    "objectID": "vector-spaces.html#introduction-to-vector-spaces",
    "href": "vector-spaces.html#introduction-to-vector-spaces",
    "title": "2  Vector Spaces",
    "section": "2.1 Introduction to Vector Spaces",
    "text": "2.1 Introduction to Vector Spaces\n\nDefinition 2.1 Let \\(\\mathbb{F}\\) be a field. The collection of vectors, as \\(n\\times 1\\) matrices over \\(\\mathbb{F}\\), is called a vector space over the field of scalars \\(\\mathbb{F}\\). We denote a vector space by \\(V\\) where \\(\\mathbb{F}\\) is a positive integer \\(n\\).\n\nSince vectors are just \\(n\\times 1\\) matrices, the proof of the next two propositions follow immediately from \\(\\ref{PropertiesofMatrixAddition}\\) and \\(\\ref{Properties of Scalar Multiplication}\\).\n\nTheorem 2.1  Let \\(V\\) be a vector space. Then the following hold.\n\nFor all \\(\\vec{u}, \\vec{v}\\in V\\), \\(\\vec{u}+\\vec{v}=\\vec{v}+\\vec{u}\\).\nFor all \\(\\vec{u}, \\vec{v}, \\vec{w} \\in V\\), \\((\\vec{u}+\\vec{v})+\\vec{w}=\\vec{u}+(\\vec{v}+\\vec{w})\\).\nFor all \\(\\vec{v}\\in V\\), there exists \\(\\vec{0} \\in V\\) such that \\(\\vec{v}+\\vec{0}=\\vec{v}\\).\nFor every \\(\\vec{v}\\in V\\), there exists \\(\\vec{w}\\in V\\) such that \\(\\vec{v}+\\vec{w}=\\vec{0}\\).\n\n\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:PropertiesofVectorAddition1}\\).\n\n\nTheorem 2.2  Let \\(V\\) be a vector space. Then the following hold.\n\nFor all \\(\\vec{v}\\in V\\), \\(1\\vec{v}=\\vec{v}\\).\nFor all \\(a, b\\in k\\) and \\(\\vec{u}\\in V\\), \\((a b) \\vec{v}=a (b \\vec{v})\\).\nFor all \\(a \\in k, u, v\\in V\\), \\(a (\\vec{u}+\\vec{v})=a \\vec{u}+ a\\vec{v}\\).\n\nFor all $a, b k $ and \\(\\vec{u}\\in V\\), \\((a+b)\\vec{u}=a \\vec{u}+ b \\vec{u}\\).\n\n\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:PropertiesofVectorAddition2}\\).\n\n\nTheorem 2.3 Let \\(V\\) be a vector space. Then the following hold.\n\nThere exists a unique additive identity (denoted by \\(\\vec{0}\\)).\nEvery \\(\\vec{v}\\in V\\) has a unique additive inverse, (denoted by \\(-\\vec{v}\\)).\n\n\n\n\nProof. \nLet \\(\\vec{u}_1\\) and \\(\\vec{u}_2\\) be additive identities in \\(V\\), then \\(\\vec{v}+\\vec{u}_1=\\vec{v}\\) and \\(\\vec{v}+\\vec{u}_2=\\vec{v}\\) for every \\(\\vec{v}\\in V\\). Then, \\[\n\\vec{u}_1\n=\\vec{u}_1+\\vec{u}_2\n=\\vec{u}_2+\\vec{u}_1\n=\\vec{u}_2\n\\] as desired.\nLet \\(\\vec{v}_1\\) and \\(\\vec{v}_2\\) be additive inverses of \\(\\vec{w}\\) in \\(V\\), then \\(\\vec{w}+\\vec{v}_1=\\vec{0}\\) and \\(\\vec{w}+\\vec{v}_2=\\vec{0}\\). Then, \\[\n\\vec{v}_1\n=\\vec{v}_1+\\vec{0}\n=\\vec{v}_1+(\\vec{w}+\\vec{v}_2)\n=(\\vec{v}_1+\\vec{w})+\\vec{v}_2\n=(\\vec{w}+\\vec{v}_1)+\\vec{v}_2\n=\\vec{0}+\\vec{v}_2=\\vec{v}_2\n\\] as desired.\n\n\n\nTheorem 2.4 Let \\(V\\) be a vector space. Then the following hold.\n\nIf \\(\\vec{v}\\in V\\), then \\(0\\, \\vec{v}=\\vec{0}\\).\nIf \\(a\\in k\\), then \\(a\\, \\vec{0}=\\vec{0}\\).\n\n\n\n\nProof. \nLet \\(\\vec{v}\\in V\\), then \\[\n\\vec{v}=1 \\vec{v}=(1+0) \\vec{v}= 1 \\vec{v}+0 \\vec{v}= \\vec{v}+0\\vec{v}\n\\] which shows that \\(0 \\vec{v}\\) is the additive identity of \\(V\\), namely \\(0 \\vec{v}=\\vec{0}\\).\nLet \\(a\\in k\\), then \\[\na \\vec{0}\n=a(\\vec{0}+\\vec{0})\n=a\\vec{0}+a\\vec{0}\n\\] which shows that \\(a \\vec{0}\\) is the additive identity of \\(V\\), namely \\(a \\vec{0}=\\vec{0}\\).\n\n\n\nTheorem 2.5 Let \\(V\\) be a vector space. Then the following hold.\n\nIf \\(\\vec{v}\\in V\\), then \\(-(-\\vec{v})=\\vec{v}\\).\nIf \\(\\vec{v}\\in V\\), then \\((-1)\\, \\vec{v}=-\\vec{v}\\).\n\n\n\n\nProof. \nLet \\(\\vec{v}\\in V\\), then \\[\n\\vec{v}+(-1)\\vec{v}\n=1 \\vec{v}+(-1) \\vec{v}\n=(1+(-1)) \\vec{v}\n=0 \\vec{v}\n=\\vec{0}\n\\] which shows that \\((-1)\\vec{v}\\) is the unique additive inverse of \\(\\vec{v}\\) namely, \\((-1)\\vec{v}=-\\vec{v}\\).\nSince \\(-\\vec{v}\\) is the unique additive inverse of \\(\\vec{v}\\), \\(\\vec{v}+(-\\vec{v})=\\vec{0}\\). Then \\((-\\vec{v})+\\vec{v}=\\vec{0}\\) shows that \\(\\vec{v}\\) is the unique additive inverse of \\(-\\vec{v}\\), namely, \\(\\vec{v}=-(-\\vec{v})\\) as desired.\n\n\n\nTheorem 2.6 Let \\(V\\) be a vector space with \\(a\\in k\\) and \\(\\vec{v}\\in V\\). If \\(a\\,\\vec{v}=\\vec{0}\\), then \\(a=0\\) or \\(\\vec{v}=\\vec{0}\\).\n\n\nProof. Suppose \\(a\\neq 0\\). If \\(a v =\\vec{0}\\) then \\[\n\\vec{v}=1 \\vec{v}\n=(a^{-1} a) \\vec{v}\n=a^{-1} (a \\vec{v})\n=a^{-1} \\vec{0}\n=\\vec{0}.\n\\] Otherwise \\(a=0\\) as desired.\n\n\nExercise 2.1 Determine whether the following collection of vectors in \\(\\mathbb{R}^3\\) are linearly independent or linearly dependent.\n\n\\((0,1,1), (1,2,1), (0,4,6), (1,0,-1)\\)\n\\((0,1,0), (1,2,1), (0,-4,6), (-1,1,-1)\\)\n\n\n\nExercise 2.2 Determine whether the following collection of vectors in \\(\\mathbb{R}^4\\) are linearly independent or linearly dependent.\n\n\\((0,1,1,1), (1,2,1,1), (0,4,6,2), (1,0,-1, 2)\\)\n\\((0,1,0,1), (1,2,1,3), (0,-4,6,-2), (-1,1,-1, 2)\\)\n\n\n\nExercise 2.3 Show that the given vectors do not form a basis for the vector space \\(V\\).\n\n\\((21,-7), (-6, 1)\\); \\(V=\\mathbb{R}^2\\)\n\\((21,-7,14), (-6, 1,-4), (1,0,0)\\); \\(V=\\mathbb{R}^3\\)\n\\((48,24,108,-72), (-24, -12,-54,36), (1,0,0,0), (1,1,0,0)\\); \\(V=\\mathbb{R}^4\\)\n\n\n\nExercise 2.4 Reduce the vectors to a basis of the vector space \\(V\\).\n\n\\((1,0), (1,2), (2,4)\\), \\(V=\\mathbb{R}^2\\)\n\\((1,2,3), (-1, -10, 15), (1, 2, -3), (2,0,6), (1, -2, 3)\\), \\(V=\\mathbb{R}^3\\)\n\n\n\nExercise 2.5 Which of the following collection of vectors in \\(\\mathbb{R}^3\\) are linearly dependent? For those that are express one vector as a linear combination of the rest.\n\n\\((1,1,0), (0,2,3), (1,2,3)\\)\n\\((1,1,0), (3,4,2), (0,2,3)\\)\n\n\n\nExercise 2.6  Prove \\(\\ref{PropertiesofVectorAddition1}\\).\n\n\nExercise 2.7  Prove \\(\\ref{PropertiesofVectorAddition2}\\).\n\n\nExercise 2.8 Let \\(S=\\{v_1, v_2, ..., v_k\\}\\) be a set of vectors in a a vector space \\(V\\). Prove that \\(S\\) is linearly dependent if and only if one of the vectors in \\(S\\) is a linear combination of all other vectors in \\(S\\).\n\n\nExercise 2.9 Suppose that \\(S=\\{v_1, v_2, v_3\\}\\) is a linearly independent set of vector in a vector space \\(V\\). Prove that \\(T=\\{u_1, u_2, u_3\\}\\) is also linearly independent where \\(u_1=v_1\\), \\(u_2=v_1+v_2\\), and \\(u_3=v_1+v_2+v_3\\).\n\n\nExercise 2.10 Which of the following sets of vectors form a basis for the vector space \\(V\\).   - \\((1,3), (1,-1)\\); \\(V=\\mathbb{R}^2\\) - \\((1,3),(-2,6)\\); \\(V=\\mathbb{R}^2\\) - \\((3,2,2), (-1,2,1), (0,1,0)\\); \\(V=\\mathbb{R}^3\\) - \\((3,2,2), (-1,2,0), (1,1,0)\\); \\(V=\\mathbb{R}^3\\) - \\((2,2,2,2), (3,3,3,2), (1,0,0,0), (0,1,0,0)\\); \\(V=\\mathbb{R}^4\\) - $(1,1,2,0), (2,2,4,0), (1,2,3,1), (2,1,3,-1), (1,2,3,-1) $; \\(V=\\mathbb{R}^4\\)\n\n\nExercise 2.11 Find a basis for the subspace of the vector space \\(V\\).\n\nAll vectors of the form \\((a,b,c)\\) where \\(b=a+c\\) where \\(V=\\mathbb{R}^3\\).\nAll vectors of the form \\((a,b,c)\\) where \\(b=a-c\\) where \\(V=\\mathbb{R}^3\\).\nAll vectors of the form \\(\\vectorfour{b-a}{a+c}{b+c}{c}\\) where \\(V=\\mathbb{R}^4\\).\n\n\n\nExercise 2.12  Let \\(\\vec{v}_1=\\vectorthree{0}{1}{1}\\), \\(\\vec{v}_2=\\vectorthree{1}{0}{0}\\) and \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2)\\).\n\nIs \\(S\\) a subspace of \\(\\mathbb{R}^3\\)?\nFind a vector \\(\\vec{u}\\) in \\(S\\) other than \\(\\vec{v}_1\\), \\(\\vec{v}_2\\).\nFind scalars which verify that \\(3\\vec{u}\\) is in \\(S\\).\nFind scalars which verify that \\(\\vec{0}\\) is in \\(S\\).\n\n\n\nExercise 2.13 Let \\(\\vec{u}_1=\\vectorthree{0}{2}{2}\\), \\(\\vec{u}_2=\\vectorthree{2}{0}{0}\\) and \\(T=\\text{span}(\\vec{u}_1,\\vec{u}_2)\\). Show \\(S=T\\) by showing \\(S\\subseteq T\\) and \\(T\\subseteq S\\) where \\(S\\) is defined in Exercise \\(\\ref{vecex1}\\).\n\n\nExercise 2.14 Prove that the non-empty intersection of two subspaces of \\(\\mathbb{R}^3\\) is a subspace of \\(\\mathbb{R}^3\\).\n\n\nExercise 2.15 Let \\(S\\) and \\(T\\) be subspaces of \\(\\mathbb{R}^3\\) defined by \\[\nS=\\text{span}\\left(\\vectorthree{1}{0}{2},\\vectorthree{0}{2}{1}\\right)\n\\qquad \\text{and} \\qquad\nT=\\text{span}\\left(\\vectorthree{2}{-2}{3},\\vectorthree{3}{-4}{4}\\right).\n\\] Show they are the same subspace of \\(\\mathbb{R}^3\\).\n\n\nExercise 2.16 Let \\({\\vec{v}_1,\\vec{v}_2, \\vec{v}_3}\\) be a linearly independent set of vectors. Show that if \\(\\vec{v}_4\\) is not a linear combination of \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_3\\), then \\({\\vec{v}_1,\\vec{v}_2, \\vec{v}_3},\\vec{v}_4\\) is a linearly independent set of vectors.\n\n\nExercise 2.17 If\n\\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) is a linearly independent set of vectors in \\(V\\), show that\n\\(\\{\\vec{v}_1,\\vec{v}_1+\\vec{v}_2, \\vec{v}_1+\\vec{v}_2+\\vec{v}_3\\}\\) is also a linearly independent set of vectors in \\(V\\).\n\n\nExercise 2.18 If\n\\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) is a linearly independent set of vectors in \\(V\\), show that\n\\(\\{\\vec{v}_1+\\vec{v}_2,\\vec{v}_2+\\vec{v}_3, \\vec{v}_3+\\vec{v}_1\\}\\) is also a linearly independent set of vectors in \\(V\\).\n\n\nExercise 2.19 Let \\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) be a linearly dependent set. Show that at least one of the \\(\\vec{v}_i\\) is a linear combination of the others.\n\n\nExercise 2.20 Prove or provide a counterexample to the following statement. If a set of vectors \\(T\\) spans the vector space \\(V\\), then \\(T\\) is linearly independent.\n\n\nExercise 2.21 Which of the following are not a basis for \\(\\mathbb{R}^3\\)?\n\n\\(\\vec{v_1}=\\vectorthree{1}{0}{0}, \\vec{v_2}=\\vectorthree{0}{1}{1}, \\vec{v_3}=\\vectorthree{1}{-1}{-1}\\)\n\\(\\vec{u_1}=\\vectorthree{0}{0}{1}, \\vec{u_2}=\\vectorthree{1}{0}{1}, \\vec{u_3}=\\vectorthree{2}{3}{4}\\)\n\n\n\nExercise 2.22 Let \\(S\\) be the space spanned by the vectors \\[\n\\vec{v}_1=\\vectorfour{1}{0}{1}{1}\n\\quad\n\\vec{v}_1=\\vectorfour{-1}{-3}{1}{0}\n\\quad\n\\vec{v}_1=\\vectorfour{2}{3}{0}{1}\n\\quad\n\\vec{v}_1=\\vectorfour{2}{0}{2}{2}\n\\] Find the dimension of \\(S\\) and a subset of \\(T\\) which could serve as a basis for \\(S\\).\n\n\nExercise 2.23 Let \\(\\{\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\}\\) be a basis for \\(V\\), and suppose that \\(\\vec{u} =a_1 \\vec{v_1}+a_2 \\vec{v_2}+\\cdots + a_n \\vec{v_n}\\) with \\(a_1\\neq 0\\). Prove that \\(\\{\\vec{u}, \\vec{v}_2, ..., \\vec{v}_n\\}\\) is also a basis for \\(V\\).\n\n\nExercise 2.24 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) and \\(T=\\text{span}(\\vec{u}_1,\\vec{u}_2,\\vec{u}_3)\\) where \\(\\vec{v}_i\\) and \\(\\vec{u}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{2}{0}\n\\quad\n\\vec{v}_2=\\vectorfour{2}{1}{1}{1}\n\\quad\n\\vec{v}_3=\\vectorfour{3}{-1}{2}{-1}\n\\qquad\n\\vec{u}_1=\\vectorfour{3}{0}{3}{1}\n\\quad\n\\vec{u}_2=\\vectorfour{1}{2}{-1}{1}\n\\quad\n\\vec{u}_3=\\vectorfour{4}{-1}{5}{1}\n\\] Is one of these two subspaces strictly contained in the other or are they equal?\n\n\nExercise 2.25 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) where \\(\\vec{v}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{2}{3}{1}\n\\qquad\n\\vec{v}_2=\\vectorfour{2}{-1}{1}{-3}\n\\qquad\n\\vec{v}_3=\\vectorfour{1}{3}{4}{2}\n\\qquad \\text{and}\\qquad\n\\vec{u}=\\vectorfour{1}{2}{3}{1}\n\\] Is the vector \\(\\vec{u}\\) in \\(S\\)?\n\n\nExercise 2.26 If possible, find a value of \\(a\\) so that the vectors \\[\n\\vectorthree{1}{2}{a}\n\\qquad\n\\vectorthree{0}{1}{a-1}\n\\qquad\n\\vectorthree{3}{4}{5}\n\\qquad\n\\] are linearly independent.\n\n\nExercise 2.27 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) where \\(\\vec{v}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{2}{3}\n\\qquad\n\\vec{v}_2=\\vectorfour{1}{0}{1}{0}\n\\qquad\n\\vec{v}_3=\\vectorfour{3}{-2}{5}{7}\n\\qquad \\text{and}\\qquad\n\\vec{u}=\\vectorfour{1}{1}{0}{-1}\n\\] Find a basis of \\(S\\) which includes the vector \\(\\vec{u}\\).\n\n\nExercise 2.28 Find a vector \\(\\vec{u}\\) in \\(\\mathbb{R}^4\\) such that \\(\\vec{u}\\) and the vectors \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{-1}{1}\n\\qquad\n\\vec{v}_2=\\vectorfour{1}{0}{1}{1}\n\\qquad\n\\vec{v}_3=\\vectorfour{1}{2}{1}{1}\n\\] for a basis of \\(\\mathbb{R}^4\\).\n\n\nExercise 2.29 Show that every subspace of \\(V\\) has no more than \\(n\\) linearly independent vectors.\n\n\nExercise 2.30 Find two bases of \\(\\mathbb{R}^4\\) that have only the vectors \\(\\vec{e}_3\\) and \\(\\vec{e}_4\\) in common.\n\n\nExercise 2.31 Prove that if a list of vectors is linearly independent so is any sublist.\n\n\nExercise 2.32 Suppose \\(\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\) and \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_4\\) are two sets of linearly dependent vectors, and suppose that \\(\\vec{v}_1\\) and \\(\\vec{v}_2\\) are linearly independent. Prove that any set of three vectors chosen from \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_3, \\vec{v}_4\\) is linearly dependent.\n\n\nExercise 2.33 If \\(\\vec{u}\\) and \\(\\vec{v}\\) are linearly independent vectors in \\(V\\), prove that the vectors \\(a\\vec{u}+b\\vec{v}\\) and \\(c\\vec{u}+d\\vec{v}\\) are also linearly independent if and only if \\(ad-bc\\neq 0\\).\n\n\nExercise 2.34  Complete the proof of \\(\\ref{prop:spnlinbasis}\\).\n\n\nExercise 2.35 Let \\(U\\) be the collection of vectors that satisfy the equations \\(x+y+z=0\\) and \\(x+2y-z=0\\). Show \\(U\\) is a subspace of \\(\\mathbb{R}^3\\), find a basis for \\(U\\), and find \\(\\dim(U)\\).\n\n\nExercise 2.36 Let \\(U\\) be the collection of vectors that satisfy the equations \\(x+y+z=0\\), \\(x+2y-z=0\\), and \\(y-2z=0\\). Show \\(U\\) is a subspace of \\(\\mathbb{R}^3\\), find a basis for \\(U\\), and find \\(\\dim(U)\\).\n\n\nExercise 2.37 Show that the only subspaces of \\(\\mathbb{R}\\) are \\(\\{\\vec{0}\\}\\) and \\(\\{\\mathbb{R}\\}\\).\n\n\nExercise 2.38 Show that the only subspaces of \\(\\mathbb{R}^2\\) are \\(\\{\\vec{0}\\}\\), \\(\\{\\mathbb{R}^2\\}\\), and any set consisting of all scalar multiples of a nonzero vector. Describe these subspaces geometrically.\n\n\nExercise 2.39 Determine the various types of subspaces of \\(\\mathbb{R}^3\\) and describe them geometrically.\n\n\nExercise 2.40 For \\(\\vec{b}\\neq\\vec{0}\\), show that the set of solutions of the \\(n\\times m\\) linear system \\(A \\vec{x}=\\vec{b}\\), is not a subspace of \\(V\\).\n\n\nExercise 2.41 Suppose that \\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\) are linearly independent in \\(\\mathbb{R}^n\\). Show that if \\(A\\) is an \\(n\\times n\\) matrix with \\(\\text{rref}(A)=I_n\\), then \\(A\\vec{v}_1, A\\vec{v}_2, ..., A\\vec{v}_n\\) are also linearly independent in \\(\\mathbb{R}^n\\).\n\n\nExercise 2.42 Let \\(S=\\{\\vlist{v}{s}\\}\\) and \\(T=\\{\\vlist{u}{t}\\}\\) be two sets of vectors in \\(V\\) where each \\(\\vec{u}_i\\), \\((i=1,2,...,t)\\) is a linear combination of the vectors in \\(S\\). Show that \\(\\vec{w}=\\lincomb{a}{u}{t}\\) is a linear combination of the vectors in \\(S\\).\n\n\nExercise 2.43 Let \\(S=\\{\\vlist{v}{m}\\}\\) be a set of non-zero vectors in a vector space \\(V\\) such that every vector in \\(V\\) can be uniquely as a linear combination of the vectors in \\(S\\). Prove that \\(S\\) is a basis for \\(V\\).\n\n\nExercise 2.44 Find a basis for the solution space of the homogeneous system \\((\\lambda I_n-A)\\vec{x}=\\vec{0}\\) for the given \\(\\lambda\\) and \\(A\\).\n\n\\(\\lambda=1, A=\\begin{bmatrix} 0 & 0 & 1 \\\\ 1 & 0 & -3 \\\\ 0 & 1 & 3 \\end{bmatrix}\\)\n\\(\\lambda=2, A=\\begin{bmatrix} -2 & 0 & 0 \\\\ 0 & -2 & -3 \\\\ 0 & 4 & 5 \\end{bmatrix}\\)\n\n\n\nExercise 2.45  Prove \\(\\ref{prop:roweqtoidn}\\).\n\n\nExercise 2.46  Prove \\(\\ref{cor:explincomb}\\).\n\n\nExercise 2.47  Prove \\(\\ref{sumprop}\\)."
  },
  {
    "objectID": "vector-spaces.html#subspaces",
    "href": "vector-spaces.html#subspaces",
    "title": "2  Vector Spaces",
    "section": "2.2 Subspaces",
    "text": "2.2 Subspaces\n\nDefinition 2.2  A subset \\(U\\) of a vector space \\(V\\) is called a subspace of \\(V\\) if it has the following three properties\n\n\\(U\\) contains the zero vector in \\(V\\),\n\\(U\\) is closed under addition: if \\(\\vec{u}\\) and \\(\\vec{v}\\) are in \\(U\\) then so is \\(\\vec{u}+\\vec{v}\\), and\n\\(U\\) is closed under scalar multiplication: if \\(\\vec{v}\\) is in \\(U\\) and \\(a\\) is any scalar, then \\(a\\vec{v}\\) is in \\(U\\).\n\n\n\nExample 2.1 Let \\(U\\) be the subset of \\(\\mathbb{R}^5\\) defined by \\[\nU=\\{(x_1,x_2,x_3,x_4,x_5)\\in\\mathbb{R}^5 \\, \\mid \\,x_1=3x_2 \\text{ and } x_3=7x_4\\}.\n\\] Show \\(U\\) is a subspace of \\(\\mathbb{R}^5.\\) The zero vector \\(\\vec{0}=(0,0,0,0,0,0)\\) is in \\(U\\) since \\(0=3(0)\\) and \\(0=7(0)\\). Let \\(\\vec{u}=(u_1, u_2, u_3, u_4, u_5)\\) and \\(\\vec{v}=(v_1, v_2, v_3, v_4, v_5)\\) be vectors in \\(U\\) and let \\(a\\) be a scalar. Then \\[\n\\vec{u}+\\vec{v}=(u_1+v_1, u_2+v_2, u_3+v_3, u_4+v_4, u_5+v_5)\n\\] is in \\(U\\) since \\[\nu_1=3 u_2 \\text{ and } v_1=3 v_2 \\text{ imply } u_1+v_1=3 (u_2+v_2)\\] and \\[\nu_3=7 u_4 \\text{ and } v_3=7 v_4 \\text{ imply } u_3+v_3=7 (u_4+v_4).\\] Also, \\(a \\vec{u}\\) is in \\(U\\) since \\(u_1=3u_2\\) implies \\(a u_1=3 (a u_2)\\). By \\(\\ref{subdef}\\), \\(U\\) is a subspace of \\(\\mathbb{R}^5\\).\n\n\nExample 2.2 Give an example of a nonempty subset \\(U\\) of \\(\\mathbb{R}^2\\) such that \\(U\\) is closed under addition and under taking additive inverses, but \\(U\\) is not a subspace of \\(\\mathbb{R}^2\\). The subset \\(\\mathbb{Z}^2\\) of \\(\\mathbb{R}^2\\) is closed under additive inverses and addition, however \\(\\mathbb{Z}^2\\) is not a subspace of \\(\\mathbb{R}^2\\) since \\(\\sqrt{2} \\in \\mathbb{R}, (1,1)\\in \\mathbb{Z}^2\\) however \\((\\sqrt{2} , \\sqrt{2}) \\not \\in \\mathbb{Z}^2\\).\n\n\nExample 2.3 Give an example of a nonempty subset \\(U\\) of \\(\\mathbb{R}^2\\) such that \\(U\\) is closed under scalar multiplication, but \\(U\\) is not a subspace of \\(\\mathbb{R}^2\\). The set \\(\\{(x_1,x_2)\\in \\mathbb{R}^2 \\mid x_1 x_2=0\\}=M\\) is closed under scalar multiplication because, if \\(\\lambda\\in \\mathbb{R}\\) and \\((x_1,x_2)\\in M\\), then \\((\\lambda x_1, \\lambda x_2)\\in M\\) holds since \\(\\lambda x_1 \\lambda x_2=0\\). However, \\(M\\) is not a subspace because \\((0,1)+(1,0)=(1,1)\\not \\in M\\) even though \\((0,1),(1,0)\\in M\\).\n\n\nExample 2.4 Show that the set of all solutions of an \\(m\\times n\\) homogenous linear system of equations is a subspace of \\(V\\) (called the null space ). Let \\(A\\vec{x}=\\vec{0}\\) be an \\(m\\times n\\) homogenous system of linear equations and let \\(U\\) be the set of solutions to this system. Of course \\(A\\vec{0}=\\vec{0}\\) and so the zero vector is in \\(U\\). Let \\(\\vec{u}\\) and \\(\\vec{v}\\) be in \\(U\\) and let \\(a\\) be a scalar. Then \\[\nA(\\vec{u}+\\vec{v})=A\\vec{u}+A\\vec{v}=\\vec{0}+\\vec{0}=\\vec{0}\n\\] and \\[\nA(a \\vec{u})=a(A\\vec{u})=a\\vec{0}=\\vec{0}\n\\] shows \\(\\vec{u}+\\vec{v}\\) and \\(a\\vec{u}\\) are in \\(U\\). By \\(\\ref{subdef}\\), \\(U\\) is a subspace of \\(V\\).\n\n\nDefinition 2.3  Let \\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_m\\) be vectors in the vector space \\(V\\). The set of all linear combinations \\[\n\\text{span}(\\vlist{v}{m})\n=\\left\\{\n\\lincomb{c}{v}{m}\n\\mid \\vlist{c}{m}\\in k \\right\\}\n\\] is called the spanning set of the vectors \\(\\vlist{v}{m}\\).\n\n\nExample 2.5 Show that the spanning set of the vectors \\(\\vlist{v}{m}\\) in \\(V\\) is a subspace of \\(V\\).\nLet \\(U=\\text{span}(\\vlist{v}{m})\\). Notice \\(\\vec{0}\\in U\\) since \\(\\vec{0}=\\lincomb{0}{v}{m}\\) where \\(0\\in k\\). Let \\(\\vec{u}\\) and \\(\\vec{v}\\) be vectors in \\(U\\) and let \\(a\\) be a scalar. By \\(\\ref{spandef}\\), there exists scalars \\(\\vlist{c}{m}\\) and scalars\n\\(\\vlist{d}{m}\\) such that \\[\n\\vec{u}\n=\\lincomb{c}{v}{m}\n\\quad \\text{ and } \\quad\n\\vec{v}\n=\\lincomb{d}{v}{m}\n\\] Then \\[\n\\vec{u}+\\vec{v}=\\sum_{i=1}^m c_i \\vec{v}_i+\\sum_{i=1}^m d_i \\vec{v}_i=\\sum_{i=1}^m (c_i+d_i) \\vec{v}_i\n\\] and \\[\na\\vec{u}=a\\left(\\sum_{i=1}^m c_i \\vec{v}_i\\right)=\n\\sum_{i=1}^m (a c_i) \\vec{v}_i\n\\] show \\(\\vec{u}+\\vec{v}\\) and \\(a\\vec{u}\\) are in \\(U\\); and thus \\(U\\) is a subspace of \\(V\\).\n\nWe say that a nonzero \\(\\vec{v}_i\\) is in the list \\(\\vec{v}_1 , \\ldots, \\vec{v}_i, ..., \\vec{v}_m\\) if \\(\\vec v_i\\) can be written as a linear combination of the other nonzero vectors in the list. An equation of the form \\(a_1 \\vec{v}_1 + \\cdots +a_m \\vec{v}_m = \\vec{0}\\) is called a linear relation among the vectors \\(\\vec{v}_1 , ..., \\vec{v}_m\\); and is called a nontrivial relation if at least one of the \\(a_i\\)’s is nonzero.\n\nDefinition 2.4  The vectors \\(\\vlist{v}{m}\\) in \\(V\\) are called linear independent if the only choice for \\[\n\\lincomb{a}{v}{m}= \\vec{0}\n\\] is \\(a_1 = a_2=\\cdots =a_m = 0\\). Otherwise the vectors \\(\\vlist{v}{m}\\) are called linear dependent.\n\n\nLemma 2.1  Show that any set \\(S=\\{\\vlist{v}{m}\\}\\) of vectors in \\(V\\) is a linearly dependent set of vectors if and only if at least one of the vectors in the set can be written as a linear combination of the others.\n\n\nProof. Assume the vectors in the set \\(S\\) are linearly dependent. By \\(\\ref{lindepdef}\\), there exists scalars \\(c_1, c_2, ..., c_m\\) (not all zero) such that \\(\\lincomb{c}{v}{m}=\\vec{0}\\). Let \\(i\\) be the least index such that \\(c_i\\) is nonzero. Thus \\(c_1=c_2=\\cdots =c_{i-1}=0\\). So \\[\nc_i \\vec{v}_i=-c_{i+1}\\vec{v}_{i+1}-\\cdots -c_m \\vec{v}_m\n\\] for some \\(i\\). Since \\(c_i\\neq 0\\) and \\(c_i\\in k\\), \\(c_i^{-1}\\) exists and thus \\[\n\\vec{v}_i\n=\\left(\\frac{-c_{i+1}}{c_i}\\right)\\vec{v}_{i+1}\n+ \\cdots +\n\\left(\\frac{-c_m}{c_i}\\right)\\vec{v}_m\n\\] which shows \\(\\vec{v}_i\\) is a linear combination of the others, via \\[\n\\vec{v}_i= 0\\vec{v}_1+\\cdots+0\\vec{v}_{i-1}+\\left(\\frac{-c_{i+1}}{c_i}\\right)\\vec{v}_{i+1}\n+ \\cdots +\n\\left(\\frac{-c_m}{c_i}\\right)\\vec{v}_m.\n\\] Now assume one of the vectors in the set \\(S\\) can be written as a linear combination of the others, say \\[\n\\vec{v}_k\n=c_1 \\vec{v}_1+\\cdots\n+c_{k-1} \\vec{v}_{k-1}\n+c_{k+1} \\vec{v}_{k+1}\n+\\cdots\n+ c_m \\vec{v}_m\n\\] where \\(c_1, c_2, \\ldots, c_m\\) are scalars. Thus, \\[\n\\vec{0}=c_1 \\vec{v}_1+\\cdots\n+ c_{k-1} \\vec{v}_{k-1}\n+(-1) \\vec{v}_k\n+c_{k+1} \\vec{v}_{k+1}\n+\\cdots + c_m \\vec{v}_m\n\\] and so by \\(\\ref{lindepdef}\\), \\(\\vec{v}_1, ..., \\vec{v}_m\\) are linearly dependent.\n\nFor a list of vectors \\(\\vlist{v}{m}\\) in \\(V\\) the following equivalent statements follow from the appropriate definitions:\n\nvectors \\(\\vlist{v}{m}\\) are linearly independent,\nnone of the vectors \\(\\vlist{v}{m}\\) are redundant,\nnone of the vectors \\(\\vlist{v}{m}\\) can be written as a linear combination of the other vectors in the list,\nthere is only the trivial relation among the vectors \\(\\vlist{v}{m}\\),\nthe only solution to the equation \\(\\lincomb{a}{v}{m}\\) is \\(a_1 = a_2=\\cdots =a_m= 0\\), and\n\\(\\text{rank}(A)=n\\) where \\(A\\) is the \\(n\\times m\\) matrix whose columns are the vectors \\(\\vlist{v}{m}\\).\n\n\nExample 2.6 Determine whether the following vectors \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{w}\\) are linearly independent. \\[\n\\vec{u}=\\vectorfour{1}{1}{1}{1}\n\\qquad\n\\vec{v}=\\vectorfour{1}{2}{3}{4}\n\\qquad\n\\vec{w}=\\vectorfour{1}{4}{7}{10}\n\\] Without interchanging rows, we use elementary row operations to find \\[\n\\text{rref}\n\\begin{bmatrix}\n\\vec{u} & \\vec{v} & \\vec{w}\n\\end{bmatrix}\n=\\begin{bmatrix}1 & 0 & -2 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}.\n\\] From this we infer the nontrivial relation \\(\\vec 0=(-2)\\vec{u}+(3)\\vec{v}+(-1)\\vec{w}.\\) Therefore the given vectors are linearly dependent.\n\n\nTheorem 2.7 Let \\[\n\\vec{v}_1=\\vectorfour{a_{11}}{a_{21}}{\\vdots}{a_{n1}}\n\\quad\n\\vec{v}_2=\\vectorfour{a_{12}}{a_{22}}{\\vdots}{a_{n2}}\n\\quad\n\\cdots\n\\quad\n\\vec{v}_m=\\vectorfour{a_{1s}}{a_{2m}}{\\vdots}{a_{nm}}\n\\] be \\(s\\) vectors in \\(V\\). These vectors are linearly dependent if and only if there exists a solution to the system of linear equations \\[\\begin{equation}\n\\label{lincomsys}\n\\begin{cases}\na_{11}x_1+a_{12}x_2+\\cdots+a_{1m}x_m=0 \\\\\na_{21}x_1+a_{22}x_2+\\cdots+a_{2m}x_m=0 \\\\\n\\qquad \\qquad \\vdots \\\\\na_{n1}x_1+a_{n2}x_2+\\cdots+a_{nm}x_m=0 \\\\\n\\end{cases}\n\\end{equation}\\] different from \\(x_1=x_2=\\cdots=x_m=0\\).\n\n\nProof. Assume \\(\\vlist{v}{m}\\) are linear dependent. By \\(\\ref{lindepdef}\\), there exists scalars \\(\\vlist{c}{m}\\) such that \\[\\begin{equation}\n\\label{lincomeq}\n\\lincomb{c}{v}{m}=\\vec{0}\n\\end{equation}\\] and not all \\(c_i\\)’s are zero. \\(\\ref{lincomeq}\\) yields a system \\[\n\\begin{cases}\na_{11}c_1+a_{12}c_2+\\cdots+a_{1m}c_m=0 \\\\\na_{21}c_1+a_{22}c_2+\\cdots+a_{2m}c_m=0 \\\\\n\\qquad \\qquad \\vdots \\\\\na_{n1}c_1+a_{n2}c_2+\\cdots+a_{nm}c_m=0 \\\\\n\\end{cases}\n\\] with solution \\(x_1=c_1\\), \\(x_2=c_2\\), …, \\(x_m=c_m\\). Since not all \\(c_i\\)’s are zero we have a solution different from \\(x_1=x_2=\\cdots=x_m=0\\). Assume the system in \\(\\ref{lincomsys}\\) has a solution \\(\\vec{x}^*\\) with \\(\\vec{x}^*\\neq \\vec{0}\\), say \\(x_i^*\\). By \\(\\ref{colvecmat}\\) we can write \\[\\begin{equation}\n\\vec{0}=A\\vec{x}=\n\\lincomb{x^*}{v}{m}\n\\end{equation}\\] Isolating the term \\(x_i^*\\vec{v}_i\\) yields \\[\\begin{equation}\nx_i^* \\vec{v}_i=-x_i^*\\vec{v}_1-\\cdots -x_{i-1}\\vec{v}_{i-1}-x_{i+1}\\vec{v}_{i+1} -\\cdots -x_m^*\\vec{v}_m\n\\end{equation}\\] and since \\(x_i^*\\neq 0\\), \\((x_i^*)^{-1}\\) exists. Therefore, \\[\\begin{equation}\n\\vec{v}_1=\\left(-\\frac{x_1^*}{x^*_i}\\right)\\vec{v}_1-\\cdots -\\left(-\\frac{x_{i-1}^*}{x^*_i}\\right)\\vec{v}_{i+1}-\\cdots - \\left(-\\frac{x_{m}^*}{x^*_i}\\right)\\vec{v}_{m}\n\\end{equation}\\] shows the vectors \\(\\vlist{v}{m}\\) are linearly dependent.\n\n\nTheorem 2.8 The \\(n\\times m\\) linear system of equations \\(A\\vec{x}=\\vec{b}\\) has a solution if and only if the vector \\(\\vec{b}\\) is contained in the subspace of \\(V\\) generated by the column vectors of \\(A\\).\n\n\nProof. Let \\(\\vec{x}\\) be a solution to \\(A\\vec{x}=\\vec{b}\\) with \\(A=\\begin{bmatrix}\\vec{v}_1 & \\vec{v}_2 & \\cdots & \\vec{v}_m \\end{bmatrix}\\). By \\(\\ref{colvecmat}\\), \\(\\vec{b}=A\\vec{x}=\\lincomb{x}{v}{m}\\) and thus \\(\\vec{b}\\in \\text{span}(\\vlist{v}{m})\\) as needed. Conversely, assume \\(\\vec{b}\\) is in the subspace generated by the column vectors of \\(A\\); that is assume \\(\\vec{b}\\in \\text{span}(\\vlist{v}{m})\\). By \\(\\ref{spandef}\\), there exists scalars \\(\\vlist{c}{m}\\) such that \\(\\vec{b}=\\lincomb{c}{v}{m}\\). By \\(\\ref{spandef}\\), \\(\\vec{b}=\\lincomb{c}{v}{m}=A\\vec{c}\\) where the components of \\(\\vec{c}\\) are the \\(c_i\\)’s. Thus the system \\(A\\vec{x}=\\vec{b}\\) has a solution, namely \\(\\vec{c}\\).\n\n\nExample 2.7 Let \\(U\\) and \\(V\\) be finite subsets of a vector space \\(V\\) with \\(U\\subseteq V\\).\n\nIf \\(U\\) is linear dependent, then so is \\(V\\).\nIf \\(V\\) is linear independent, then so is \\(U\\).\n\nLet \\(U=\\{\\vlist{u}{s}\\}\\) \\(V=\\{\\vlist{v}{t}\\}\\).\n\nIf \\(U\\) is linear dependent, then thee exists a vector, say \\(\\vec{u}_k\\) such that \\(\\vec{u}_k\\) is a linear combination of the other \\(\\vec{u}_i\\)’s. Since \\(U\\subseteq V\\) all \\(\\vec{u}_i\\)’s are in \\(V\\). Thus we have a vector \\(\\vec{u}_k\\) in \\(V\\) that is a linear combination of other vectors in \\(V\\). Therefore, by \\(\\ref{lindepothers}\\), \\(V\\) is linear dependent.\nLet \\(\\vlist{c}{s}\\) be scalars such that \\[\\begin{equation}\n\\label{lincombcus}\n\\lincomb{c}{u}{s}=\\vec{0}.\n\\end{equation}\\] Since \\(U\\subseteq V\\), we know \\(u_i\\in V\\) for \\(1\\leq i \\leq s\\). Since \\(V\\) is linear independent, \\(\\ref{lincombcus}\\) implies \\(c_1=c_2=\\cdots =c_m=0\\). By \\(\\ref{lindepdef}\\), \\(U\\) is linear independent as well.\n\n\n\nCorollary 2.1  Any vector in \\(V\\), written as a column matrix, can be expressed (uniquely) as a linear combination of \\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_m\\) if and only if \\(A \\vec{x}=\\vec{v}\\) has unique solution, where \\(A=\\begin{bmatrix}\\vec{v}_1 & \\vec{v}_2 & \\cdots & \\vec{v}_m\\end{bmatrix}\\). When there is a solution, the components \\(x_1, x_2, ...., x_m\\) of \\(\\vec{x}\\) give the coefficients for the linear combination.\n\n\nProof. This proof is left for the reader as Exercise \\(\\ref{ex:explincomb}\\).\n\n\nTheorem 2.9  The vectors \\(\\vlist{v}{n}\\) in \\(V\\) form a linearly independent set of vectors if and only if \\(\\begin{bmatrix}\\vec{v}_1& \\vec{v}_2 & \\cdots & \\vec{v}_n \\end{bmatrix}\\) is row equivalent to \\(I_n\\).\n\n\nProof. This proof is left for the reader as Exercise \\(\\ref{ex:roweqtoidn}\\).\n\n\nTheorem 2.10  Let \\(V\\) be a vector space and assume that the vectors \\(\\vlist{v}{n}\\) are linearly independent and \\(\\text{span}(\\vlist{s}{m})=V\\). Then \\(n\\leq m\\).\n\n\nProof. We are given \\[\n\\text{span}(\\vlist{s}{m})=V\n\\quad \\text{and} \\quad\n\\vlist{v}{n} \\text{ are linearly independent.}\n\\] Since \\(\\vec{v}_1\\) is a linear combination of the vectors \\(\\vec{s}_1\\), \\(\\vec{s}_2\\), …., \\(\\vec{s}_m\\) we obtain \\[\n\\text{span}(\\vec{v}_1,\\vec{s}_2,...,\\vec{s}_m)=V\n\\quad \\text{and} \\quad\n\\vec{v}_2, ..., \\vec{v}_n \\text{ are linearly independent,}\n\\] respectively. Since \\(\\vec{v}_2\\) is a linear combination of \\(\\vec{v}_1\\), \\(\\vec{s}_2\\), …, \\(\\vec{s}_m\\) we can obtain \\[\n\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{s}_3,...,\\vec{s}_m)=V\n\\quad \\text{and} \\quad\n\\vec{v}_3, ..., \\vec{v}_n \\text{ are linearly independent,}\n\\] respectively. Now if \\(m<n\\) then repeating this process will eventually exhaust the \\(\\vec{s}_i\\)’s and lead to \\[\n\\text{span}(\\vec{v}_1,\\vec{v}_2,...,\\vec{v}_m)=V\n\\quad \\text{and} \\quad\n\\vec{v}_{m+1}, ..., \\vec{v}_n \\text{ are linearly independent.}\n\\] This is a contradiction since \\(\\vec{v}_n\\) is not in \\(\\text{span}(\\vlist{v}{m});\\) and whence \\(n\\leq m\\).\n\n\nTheorem 2.11 A set \\(S=\\{\\vlist{v}{m}\\}\\) of vectors in \\(V\\) is linearly independent if and only if for any vector \\(\\vec{u}\\), if \\(\\vec{u}=\\lincomb{u}{v}{m}\\), then this representation is unique.\n\n\nProof. Assume the vectors in \\(S\\) are linearly independent and assume \\(\\vec{u}\\) is an arbitrary vector with \\[\\begin{equation}\n\\vec{u}=\\lincomb{a}{v}{m}\n\\qquad \\text{and} \\qquad\n\\vec{u}=\\lincomb{b}{v}{m}\n\\end{equation}\\] as both representations of \\(\\vec{u}\\) as linear combinations of the vectors in \\(S\\). Then \\[\n\\vec{0}=\\vec{u}-\\vec{u}\n=(a_1-b_1)\\vec{v}_1+(a_2-b_2)\\vec{v}_2+\\cdots +(a_m-b_m)\\vec{v}_m.\n\\] Since \\(S\\) is linearly independent \\(a_1-b_1=a_2-b_2=\\cdots =a_m-b_m=0\\) and thus \\(a_1=b_1\\), \\(a_2=b_2\\), …, \\(a_m=b_m\\). Therefore, the representation of \\(\\vec{u}\\) as a linear combination of the vectors in \\(S\\) is unique. Conversely, assume for nay vector \\(\\vec{u}\\) which can be written as a linear combination of the vectors in \\(S\\), the representation is unique. If \\(\\vlist{c}{m}\\) are scalars such that \\(\\lincomb{c}{v}{m}=\\vec{0}\\) then \\(c_1=c_2=\\cdots =c_m=0\\) much hold since \\(0\\vec{v}_1+0\\vec{v}_2+\\cdots +0\\vec{v}_m=\\vec{0}\\) and this representation is unique. Therefore, the vectors in \\(S\\) are linearly independent.\n\n\nDefinition 2.5  The vectors \\(\\vlist{v}{m}\\) in \\(V\\) are called a basis of a linear subspace \\(V\\) if they span \\(V\\) and are linearly independent.\n\n\nExample 2.8 Find a basis for \\(V\\) for \\(n=1,2,3,...m\\). For \\(n=2\\), the vectors \\(\\vectortwo{1}{0}\\), \\(\\vectortwo{0}{1}\\) form a basis for \\(k^2\\). For \\(n=3\\), the vectors \\(\\vectorthree{1}{0}{0}\\), \\(\\vectorthree{0}{1}{0}\\), \\(\\vectorthree{0}{0}{1}\\) form a basis for \\(k^3\\). In general, for a positive integer \\(n\\), the following \\(n\\) vectors of \\(V\\) form a basis (called the standard basis ) of \\(V\\). \\[\\begin{equation}\n\\label{stba}\n\\vec{e}_1=\\vectorfour{1}{0}{\\vdots}{0}\n\\qquad\n\\vec{e}_2=\\vectorfour{0}{1}{\\vdots}{0}\n\\qquad\n\\cdots\n\\qquad\n\\vec{e}_n=\\vectorfour{0}{0}{\\vdots}{1}\n\\end{equation}\\] The vectors in a standard basis are linearly independent by \\(\\ref{prop:roweqtoidn}\\). Given any vector \\(\\vec{v}\\) in \\(V\\) with components \\(v_i\\), we can write \\[\n\\vec{v}=\\lincomb{v}{e}{n},\n\\] and thus \\(k^n=\\text{span}(\\vlist{e}{n})\\) which shows that any standard basis is in fact a basis.\n\n\nExample 2.9 Show the following vectors \\(\\vec{v}_1\\), \\(\\vec{v}_2\\), \\(\\vec{v}_3\\), and \\(\\vec{v}_4\\) form a basis for \\(\\mathbb{R}^4\\). \\[\n\\vec v_1=\\begin{bmatrix} 1 \\\\ 1\\\\ 1 \\\\ 1 \\end{bmatrix}\n\\qquad\n\\vec v_2=\\begin{bmatrix} 1 \\\\ -1\\\\ 1 \\\\ -1 \\end{bmatrix}\n\\qquad\n\\vec v_3=\\begin{bmatrix} 1 \\\\ 2\\\\ 4 \\\\ 8 \\end{bmatrix}\\qquad\n\\vec v_4=\\begin{bmatrix} 1 \\\\ -2\\\\ 4 \\\\ -8 \\end{bmatrix}\n\\] We determine \\(\\text{rref}(A)=I_4\\) where \\(A\\) is the matrix with column vectors \\(\\vec v_1, \\vec v_2, \\vec v_3, \\vec v_4\\). By \\(\\ref{prop:roweqtoidn}\\), \\(\\vec v_1, \\vec v_2, \\vec v_3, \\vec v_4\\) are linearly independent. Since \\(\\vec v_1, \\vec v_2, \\vec v_3, \\vec v_4\\) also span \\(\\mathbb{R}^4\\), they form a basis of \\(\\mathbb{R}^4\\).\n\n\nExample 2.10 Let \\(U\\) be the subspace of \\(\\mathbb{R}^5\\) defined by \\[\nU=\\{(x_1,x_2,x_3,x_4,x_5)\\in\\mathbb{R}^5  \\mid x_1=3x_2 \\text{ and } x_3=7x_4\\}.\n\\] Find a basis of \\(U\\). The following vectors belong to \\(U\\) and are linearly independent in \\(\\mathbb{R}^5\\). \\[\nv_1=\\vectorfive{3}{1}{0}{0}{0}\n\\qquad\nv_2=\\vectorfive{0}{0}{7}{1}{0}\n\\qquad\nv_3=\\vectorfive{0}{0}{0}{0}{1}\n\\] If \\(u\\in U\\), then the representation \\[\nu\n=\\vectorfive{u_1}{u_2}{u_3}{u_4}{u_5}\n=\\vectorfive{3u_2}{u_2}{7u_4}{u_4}{u_5}\n=u_2\\vectorfive{3}{1}{0}{0}{0}+u_4\\vectorfive{0}{0}{7}{1}{0}+u_5\\vectorfive{0}{0}{0}{0}{1}\n\\] shows that they also span \\(U\\), and thus form a basis of \\(U\\) by \\(\\ref{basisdef}\\).\n\n\nTheorem 2.12  Let \\(S=\\{\\vlist{v}{n}\\}\\) be a set of vectors in a vector space \\(V\\) and let \\(W=\\text{span}(S)\\). Then some subset of \\(S\\) is a basis for \\(W\\).\n\n\nProof. Assume \\(W=\\text{span}(S)\\) and suppose \\(S\\) is a linearly independent set of vectors. Thus, in this case, \\(S\\) is a basis of \\(W\\), by \\(\\ref{basisdef}\\). So we can assume \\(S\\) is a linearly dependent set of vectors. By \\(\\ref{lindepothers}\\), there exists \\(i\\) such that \\(1\\leq i \\leq m\\) and \\(\\vec{v}_i\\) is a linear combination of the other vectors in \\(S\\). It is left for Exercise \\(\\ref{ex:spnlinbasis}\\) to show that \\[\nW=\\text{span}(S)=\\text{span}(S_1)\n\\] where \\(S_1=S/\\{\\vec{v}_i\\}\\). If \\(S_1\\) is linearly independent set of vectors, then \\(S_1\\) is a basis of \\(W\\). Otherwise, \\(S_1\\) is a linear dependent set and we can delete a vector from \\(S_1\\) that is a linear combination of the other vectors in \\(S_1\\). We obtain another subset \\(S_2\\) of \\(S\\) with \\[\nW=\\text{span}(S)=\\text{span}(S_1)=\\text{span}(S_2).\n\\] Since \\(S\\) is finite, if we continue, we find a linearly independent subset of \\(S\\) and thus a basis of \\(W\\).\n\n\nCorollary 2.2 All bases of a subspace \\(U\\) of a vector space \\(V\\) consists of the same number of vectors.\n\n\nProof. Let \\(S=\\{\\vlist{v}{n}\\}\\) and \\(T=\\{\\vlist{w}{m}\\}\\) be bases of a subspace \\(U\\). Then \\(\\text{span}(S)=V\\) and \\(T\\) is a lineal indecent set of vectors. By \\(\\ref{inpspanine}\\), \\(m\\leq n\\). Similarly, since \\(\\text{span}(T)=V\\) and \\(S\\) is a linearly independent set of vectors, \\(n\\leq m\\). Therefore, \\(m=n\\) as desired.\n\n\nCorollary 2.3  The vectors \\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\) form a basis of \\(V\\) if and only if the reduced row echelon form of the \\(n\\times n\\) matrix \\(\\begin{bmatrix}\\vec{v}_1& \\vec{v}_2 & \\cdots & \\vec{v}_n \\end{bmatrix}\\) is \\(I_n\\).\n\n\nProof. Suppose the vectors \\(\\vlist{v}{n}\\) form a basis of \\(V\\) and consider the \\(n\\times n\\) linear system \\[\n\\begin{cases}\nv_{11} x_1+v_{12} x_2+\\cdots +v_{1n} x_n=0 \\\\\nv_{21} x_1+v_{22} x_2+\\cdots +v_{2n} x_n=0 \\\\\n\\qquad \\qquad \\vdots \\\\\nv_{n1} x_1+v_{n2} x_2+\\cdots +v_{nn} x_n=0\n\\end{cases}\n\\] where the \\(v_{ij}\\)’s are the components of the \\(\\vec{v}_j\\)’s. Since \\(\\{\\vlist{v}{n}\\}\\) is a basis, \\(\\vec{v}_1=\\vec{v}_2=\\cdots =\\vec{v}_n=\\vec{0}\\) and this linear system can not have another solution. By \\(\\ref{cor:linsystmecor2}\\), \\(\\text{rref}(A)=I_n\\) where \\(A=\\begin{bmatrix}\\vec{v}_1& \\vec{v}_2 & \\cdots & \\vec{v}_n \\end{bmatrix}\\).\n\n\nDefinition 2.6 The number of vectors in a basis of a subspace \\(U\\) of \\(V\\) is called the dimension of \\(U\\), and is denoted by \\(\\text{dim} U\\).\n\n\nExample 2.11 Find a basis of the subspace of \\(\\mathbb{R}^4\\) that consists of all vectors perpendicular to both of the following vectors \\(\\vec{v}_1\\) and \\(\\vec v_2\\). \\[\n\\vec v_1=\\vectorfour{1}{0}{-1}{1}\n\\qquad\n\\vec v_2=\\vectorfour{0}{1}{2}{3}\n\\] We need to find all vectors \\(\\vec x\\) in \\(\\mathbb{R}^4\\) such that \\(\\vec x \\cdot \\vec v_1=0\\) and \\(\\vec x \\cdot \\vec v_2=0\\). We solve both \\[\n\\vectorfour{x_1}{x_2}{x_3}{x_4}\\cdot \\vectorfour{1}{0}{-1}{1}=0\n\\qquad \\text{and} \\qquad\n\\vectorfour{x_1}{x_2}{x_3}{x_4}\\cdot \\vectorfour{0}{1}{2}{3}=0\n\\] which leads to the system and matrix \\[\n\\begin{cases}\nx_1-x_3+x_4 =0 \\\\\nx_2+2x_3+3x_4 =0\n\\end{cases}\n\\qquad\n\\text{and}\n\\qquad\nA=\n\\begin{bmatrix}\n1 & 0 & -1 & 1 \\\\\n0 & 1 & 2 & 3\n\\end{bmatrix}.\n\\] All solutions are given by \\[\n\\vectorfour{x_1}{x_2}{x_3}{x_4}\n=t \\vectorfour{-1}{-3}{0}{1}+u\\vectorfour{1}{-2}{1}{0} \\quad \\text{ where $t, u\\in\\mathbb{R}$}.\n\\] It follows the vectors \\(\\vectorfour{-1}{-3}{0}{1}\\), \\(\\vectorfour{1}{-2}{1}{0}\\) form a basis of the desired subspace.\n\n\nTheorem 2.13  Let \\(U\\) be a subspace of \\(k^m\\) with \\(\\dim U=n\\), then\n\nany list of linearly independent vectors contains \\(n\\) elements,\nany list of vectors that spans \\(U\\) contains at least \\(n\\) elements,\nif \\(n\\) vectors are linearly independent then they form a basis, and\nif \\(n\\) vectors span \\(U\\), then they form a basis of \\(U\\).\n\n\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:sumprop}\\)\n\n\nExample 2.12 Determine the values of \\(a\\) for which the following vectors \\(\\vec{u}_1\\), \\(\\vec{u}_2\\), \\(\\vec{u}_3\\), and \\(\\vec{u}_4\\) form a basis of \\(\\mathbb{R}^4\\). \\[\n\\vec{u}_1=\\vectorfour{1}{0}{0}{4} \\qquad\n\\vec{u}_2=\\vectorfour{0}{1}{0}{6} \\qquad\n\\vec{u}_3=\\vectorfour{0}{0}{1}{8} \\qquad\n\\vec{u}_4=\\vectorfour{4}{5}{6}{a}\\] Let \\(A=\\begin{bmatrix}\\vec{u}_1 & \\vec{u}_2 & \\vec{u}_3& \\vec{u}_4\\end{bmatrix}\\). Using row operations we find the row-echelon form of \\(A\\) to be the following matrix. \\[\n\\begin{bmatrix}\n1 & 0 & 0 & 4 \\\\\n0 & 1 & 0 & 5 \\\\\n0 & 0 & 1 & 6\\\\\n0 & 0 & 8 & a-94\n\\end{bmatrix}\n\\] Thus, \\(\\text{rref}(A)=I_4\\) if and only if \\(a=95\\). Therefore, by \\(\\ref{rrefbasis}\\), \\(B=\\{\\vec{u}_1, \\vec{u}_2, \\vec{u}_3, \\vec{u}_4\\}\\) is a basis if and only if \\(a=95\\).\n\n\nTheorem 2.14 The dimension of the row space of a matrix \\(A\\) is equal to the dimension of the column space of \\(A\\).\n\n\nProof. Gerber pg 226.\n\n\nExercise 2.48 Determine whether the following collection of vectors in \\(\\mathbb{R}^3\\) are linearly independent or linearly dependent.   - \\((0,1,1), (1,2,1), (0,4,6), (1,0,-1)\\) - \\((0,1,0), (1,2,1), (0,-4,6), (-1,1,-1)\\)\n\n\nExercise 2.49 Determine whether the following collection of vectors in \\(\\mathbb{R}^4\\) are linearly independent or linearly dependent.   - \\((0,1,1,1), (1,2,1,1), (0,4,6,2), (1,0,-1, 2)\\) - \\((0,1,0,1), (1,2,1,3), (0,-4,6,-2), (-1,1,-1, 2)\\)\n\n\nExercise 2.50 Show that the given vectors do not form a basis for the vector space \\(V\\).   - \\((21,-7), (-6, 1)\\); \\(V=\\mathbb{R}^2\\) - \\((21,-7,14), (-6, 1,-4), (1,0,0)\\); \\(V=\\mathbb{R}^3\\) - \\((48,24,108,-72), (-24, -12,-54,36), (1,0,0,0), (1,1,0,0)\\); \\(V=\\mathbb{R}^4\\)\n\n\nExercise 2.51 Reduce the vectors to a basis of the vector space \\(V\\).   - \\((1,0), (1,2), (2,4)\\), \\(V=\\mathbb{R}^2\\) - \\((1,2,3), (-1, -10, 15), (1, 2, -3), (2,0,6), (1, -2, 3)\\), \\(V=\\mathbb{R}^3\\)\n\n\nExercise 2.52 Which of the following collection of vectors in \\(\\mathbb{R}^3\\) are linearly dependent? For those that are express one vector as a linear combination of the rest.\n\n\\((1,1,0), (0,2,3), (1,2,3)\\)\n\\((1,1,0), (3,4,2), (0,2,3)\\)\n\n\n\nExercise 2.53  Prove \\(\\ref{PropertiesofVectorAddition1}\\).\n\n\nExercise 2.54  Prove \\(\\ref{PropertiesofVectorAddition2}\\).\n\n\nExercise 2.55 Let \\(S=\\{v_1, v_2, ..., v_k\\}\\) be a set of vectors in a a vector space \\(V\\). Prove that \\(S\\) is linearly dependent if and only if one of the vectors in \\(S\\) is a linear combination of all other vectors in \\(S\\).\n\n\nExercise 2.56 Suppose that \\(S=\\{v_1, v_2, v_3\\}\\) is a linearly independent set of vector in a vector space \\(V\\). Prove that \\(T=\\{u_1, u_2, u_3\\}\\) is also linearly independent where \\(u_1=v_1\\), \\(u_2=v_1+v_2\\), and \\(u_3=v_1+v_2+v_3\\).\n\n\nExercise 2.57 Which of the following sets of vectors form a basis for the vector space \\(V\\).   - \\((1,3), (1,-1)\\); \\(V=\\mathbb{R}^2\\) - \\((1,3),(-2,6)\\); \\(V=\\mathbb{R}^2\\) - \\((3,2,2), (-1,2,1), (0,1,0)\\); \\(V=\\mathbb{R}^3\\) - \\((3,2,2), (-1,2,0), (1,1,0)\\); \\(V=\\mathbb{R}^3\\) - \\((2,2,2,2), (3,3,3,2), (1,0,0,0), (0,1,0,0)\\); \\(V=\\mathbb{R}^4\\) - $(1,1,2,0), (2,2,4,0), (1,2,3,1), (2,1,3,-1), (1,2,3,-1) $; \\(V=\\mathbb{R}^4\\)\n\n\nExercise 2.58 Find a basis for the subspace of the vector space \\(V\\).   - All vectors of the form \\((a,b,c)\\) where \\(b=a+c\\) where \\(V=\\mathbb{R}^3\\). - All vectors of the form \\((a,b,c)\\) where \\(b=a-c\\) where \\(V=\\mathbb{R}^3\\). - All vectors of the form \\(\\vectorfour{b-a}{a+c}{b+c}{c}\\) where \\(V=\\mathbb{R}^4\\).\n\n\nExercise 2.59 Let \\(\\vec{v}_1=\\vectorthree{0}{1}{1}\\), \\(\\vec{v}_2=\\vectorthree{1}{0}{0}\\) and \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2)\\).\n\nIs \\(S\\) a subspace of \\(\\mathbb{R}^3\\)?\nFind a vector \\(\\vec{u}\\) in \\(S\\) other than \\(\\vec{v}_1\\), \\(\\vec{v}_2\\).\nFind scalars which verify that \\(3\\vec{u}\\) is in \\(S\\).\nFind scalars which verify that \\(\\vec{0}\\) is in \\(S\\).\n\n\n\n\nExercise 2.60 Let \\(\\vec{u}_1=\\vectorthree{0}{2}{2}\\), \\(\\vec{u}_2=\\vectorthree{2}{0}{0}\\) and \\(T=\\text{span}(\\vec{u}_1,\\vec{u}_2)\\). Show \\(S=T\\) by showing \\(S\\subseteq T\\) and \\(T\\subseteq S\\) where \\(S\\) is defined in Exercise \\(\\ref{vecex1}\\).\n\n\nExercise 2.61 Prove that the non-empty intersection of two subspaces of \\(\\mathbb{R}^3\\) is a subspace of \\(\\mathbb{R}^3\\).\n\n\nExercise 2.62 Let \\(S\\) and \\(T\\) be subspaces of \\(\\mathbb{R}^3\\) defined by \\[\nS=\\text{span}\\left(\\vectorthree{1}{0}{2},\\vectorthree{0}{2}{1}\\right)\n\\qquad \\text{and} \\qquad\nT=\\text{span}\\left(\\vectorthree{2}{-2}{3},\\vectorthree{3}{-4}{4}\\right).\n\\] Show they are the same subspace of \\(\\mathbb{R}^3\\).\n\n\nExercise 2.63 Let \\({\\vec{v}_1,\\vec{v}_2, \\vec{v}_3}\\) be a linearly independent set of vectors. Show that if \\(\\vec{v}_4\\) is not a linear combination of \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_3\\), then \\({\\vec{v}_1,\\vec{v}_2, \\vec{v}_3},\\vec{v}_4\\) is a linearly independent set of vectors.\n\n\nExercise 2.64 If\n\\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) is a linearly independent set of vectors in \\(V\\), show that\n\\(\\{\\vec{v}_1,\\vec{v}_1+\\vec{v}_2, \\vec{v}_1+\\vec{v}_2+\\vec{v}_3\\}\\) is also a linearly independent set of vectors in \\(V\\).\n\n\nExercise 2.65 If\n\\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) is a linearly independent set of vectors in \\(V\\), show that\n\\(\\{\\vec{v}_1+\\vec{v}_2,\\vec{v}_2+\\vec{v}_3, \\vec{v}_3+\\vec{v}_1\\}\\) is also a linearly independent set of vectors in \\(V\\).\n\n\nExercise 2.66 Let \\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) be a linearly dependent set. Show that at least one of the \\(\\vec{v}_i\\) is a linear combination of the others.\n\n\nExercise 2.67 Prove or provide a counterexample to the following statement. If a set of vectors \\(T\\) spans the vector space \\(V\\), then \\(T\\) is linearly independent.\n\n\nExercise 2.68 Which of the following are not a basis for \\(\\mathbb{R}^3\\)?   - \\(\\vec{v_1}=\\vectorthree{1}{0}{0}, \\vec{v_2}=\\vectorthree{0}{1}{1}, \\vec{v_3}=\\vectorthree{1}{-1}{-1}\\) - \\(\\vec{u_1}=\\vectorthree{0}{0}{1}, \\vec{u_2}=\\vectorthree{1}{0}{1}, \\vec{u_3}=\\vectorthree{2}{3}{4}\\)\n\n\nExercise 2.69 Let \\(S\\) be the space spanned by the vectors \\[\n\\vec{v}_1=\\vectorfour{1}{0}{1}{1}\n\\quad\n\\vec{v}_1=\\vectorfour{-1}{-3}{1}{0}\n\\quad\n\\vec{v}_1=\\vectorfour{2}{3}{0}{1}\n\\quad\n\\vec{v}_1=\\vectorfour{2}{0}{2}{2}\n\\] Find the dimension of \\(S\\) and a subset of \\(T\\) which could serve as a basis for \\(S\\).\n\n\nExercise 2.70 Let \\(\\{\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\}\\) be a basis for \\(V\\), and suppose that \\(\\vec{u} =a_1 \\vec{v_1}+a_2 \\vec{v_2}+\\cdots + a_n \\vec{v_n}\\) with \\(a_1\\neq 0\\). Prove that \\(\\{\\vec{u}, \\vec{v}_2, ..., \\vec{v}_n\\}\\) is also a basis for \\(V\\).\n\n\nExercise 2.71 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) and \\(T=\\text{span}(\\vec{u}_1,\\vec{u}_2,\\vec{u}_3)\\) where \\(\\vec{v}_i\\) and \\(\\vec{u}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{2}{0}\n\\quad\n\\vec{v}_2=\\vectorfour{2}{1}{1}{1}\n\\quad\n\\vec{v}_3=\\vectorfour{3}{-1}{2}{-1}\n\\qquad\n\\vec{u}_1=\\vectorfour{3}{0}{3}{1}\n\\quad\n\\vec{u}_2=\\vectorfour{1}{2}{-1}{1}\n\\quad\n\\vec{u}_3=\\vectorfour{4}{-1}{5}{1}\n\\] Is one of these two subspaces strictly contained in the other or are they equal?\n\n\nExercise 2.72 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) where \\(\\vec{v}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{2}{3}{1}\n\\qquad\n\\vec{v}_2=\\vectorfour{2}{-1}{1}{-3}\n\\qquad\n\\vec{v}_3=\\vectorfour{1}{3}{4}{2}\n\\qquad \\text{and}\\qquad\n\\vec{u}=\\vectorfour{1}{2}{3}{1}\n\\] Is the vector \\(\\vec{u}\\) in \\(S\\)?\n\n\nExercise 2.73 If possible, find a value of \\(a\\) so that the vectors \\[\n\\vectorthree{1}{2}{a}\n\\qquad\n\\vectorthree{0}{1}{a-1}\n\\qquad\n\\vectorthree{3}{4}{5}\n\\qquad\n\\] are linearly independent.\n\n\nExercise 2.74 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) where \\(\\vec{v}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{2}{3}\n\\qquad\n\\vec{v}_2=\\vectorfour{1}{0}{1}{0}\n\\qquad\n\\vec{v}_3=\\vectorfour{3}{-2}{5}{7}\n\\qquad \\text{and}\\qquad\n\\vec{u}=\\vectorfour{1}{1}{0}{-1}\n\\] Find a basis of \\(S\\) which includes the vector \\(\\vec{u}\\).\n\n\nExercise 2.75 Find a vector \\(\\vec{u}\\) in \\(\\mathbb{R}^4\\) such that \\(\\vec{u}\\) and the vectors \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{-1}{1}\n\\qquad\n\\vec{v}_2=\\vectorfour{1}{0}{1}{1}\n\\qquad\n\\vec{v}_3=\\vectorfour{1}{2}{1}{1}\n\\] for a basis of \\(\\mathbb{R}^4\\).\n\n\nExercise 2.76 Show that every subspace of \\(V\\) has no more than \\(n\\) linearly independent vectors.\n\n\nExercise 2.77 Find two bases of \\(\\mathbb{R}^4\\) that have only the vectors \\(\\vec{e}_3\\) and \\(\\vec{e}_4\\) in common.\n\n\nExercise 2.78 Prove that if a list of vectors is linearly independent so is any sublist.\n\n\nExercise 2.79 Suppose \\(\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\) and \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_4\\) are two sets of linearly dependent vectors, and suppose that \\(\\vec{v}_1\\) and \\(\\vec{v}_2\\) are linearly independent. Prove that any set of three vectors chosen from \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_3, \\vec{v}_4\\) is linearly dependent.\n\n\nExercise 2.80 If \\(\\vec{u}\\) and \\(\\vec{v}\\) are linearly independent vectors in \\(V\\), prove that the vectors \\(a\\vec{u}+b\\vec{v}\\) and \\(c\\vec{u}+d\\vec{v}\\) are also linearly independent if and only if \\(ad-bc\\neq 0\\).\n\n\nExercise 2.81  Complete the proof of \\(\\ref{prop:spnlinbasis}\\).\n\n\nExercise 2.82 Let \\(U\\) be the collection of vectors that satisfy the equations \\(x+y+z=0\\) and \\(x+2y-z=0\\). Show \\(U\\) is a subspace of \\(\\mathbb{R}^3\\), find a basis for \\(U\\), and find \\(\\dim(U)\\).\n\n\nExercise 2.83 Let \\(U\\) be the collection of vectors that satisfy the equations \\(x+y+z=0\\), \\(x+2y-z=0\\), and \\(y-2z=0\\). Show \\(U\\) is a subspace of \\(\\mathbb{R}^3\\), find a basis for \\(U\\), and find \\(\\dim(U)\\).\n\n\nExercise 2.84 Show that the only subspaces of \\(\\mathbb{R}\\) are \\(\\{\\vec{0}\\}\\) and \\(\\{\\mathbb{R}\\}\\).\n\n\nExercise 2.85 Show that the only subspaces of \\(\\mathbb{R}^2\\) are \\(\\{\\vec{0}\\}\\), \\(\\{\\mathbb{R}^2\\}\\), and any set consisting of all scalar multiples of a nonzero vector. Describe these subspaces geometrically.\n\n\nExercise 2.86 Determine the various types of subspaces of \\(\\mathbb{R}^3\\) and describe them geometrically.\n\n\nExercise 2.87 For \\(\\vec{b}\\neq\\vec{0}\\), show that the set of solutions of the \\(n\\times m\\) linear system \\(A \\vec{x}=\\vec{b}\\), is not a subspace of \\(V\\).\n\n\nExercise 2.88 Suppose that \\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\) are linearly independent in \\(\\mathbb{R}^n\\). Show that if \\(A\\) is an \\(n\\times n\\) matrix with \\(\\text{rref}(A)=I_n\\), then \\(A\\vec{v}_1, A\\vec{v}_2, ..., A\\vec{v}_n\\) are also linearly independent in \\(\\mathbb{R}^n\\).\n\n\nExercise 2.89 Let \\(S=\\{\\vlist{v}{s}\\}\\) and \\(T=\\{\\vlist{u}{t}\\}\\) be two sets of vectors in \\(V\\) where each \\(\\vec{u}_i\\), \\((i=1,2,...,t)\\) is a linear combination of the vectors in \\(S\\). Show that \\(\\vec{w}=\\lincomb{a}{u}{t}\\) is a linear combination of the vectors in \\(S\\).\n\n\nExercise 2.90 Let \\(S=\\{\\vlist{v}{m}\\}\\) be a set of non-zero vectors in a vector space \\(V\\) such that every vector in \\(V\\) can be uniquely as a linear combination of the vectors in \\(S\\). Prove that \\(S\\) is a basis for \\(V\\).\n\n\nExercise 2.91 Find a basis for the solution space of the homogeneous system \\((\\lambda I_n-A)\\vec{x}=\\vec{0}\\) for the given \\(\\lambda\\) and \\(A\\).\n\n\\(\\lambda=1, A=\\begin{bmatrix} 0 & 0 & 1 \\\\ 1 & 0 & -3 \\\\ 0 & 1 & 3 \\end{bmatrix}\\)\n\\(\\lambda=2, A=\\begin{bmatrix} -2 & 0 & 0 \\\\ 0 & -2 & -3 \\\\ 0 & 4 & 5 \\end{bmatrix}\\)\n\n\n\nExercise 2.92  Prove \\(\\ref{prop:roweqtoidn}\\).\n\n\nExercise 2.93  Prove \\(\\ref{cor:explincomb}\\).\n\n\nExercise 2.94  Prove \\(\\ref{sumprop}\\)."
  },
  {
    "objectID": "vector-spaces.html#introduction-to-linear-spaces",
    "href": "vector-spaces.html#introduction-to-linear-spaces",
    "title": "2  Vector Spaces",
    "section": "2.3 Introduction to Linear Spaces",
    "text": "2.3 Introduction to Linear Spaces\n\nExample 2.13 Show that the set of solution to a homogenous system form a linear space with standard operations.\n\n\nExample 2.14 Show that the set of vectors for which a particular linear system has a solution is a linear space.\n\n\nDefinition 2.7 Let \\(\\mathbb{F}\\) be a field (whose elements are called scalars ) and let \\(V\\) be a nonempty set (whose elements are called vectors) on which two operations, called addition and scalar multiplication, have been defined. The addition operation (denoted by \\(+\\)), assigns to each pair \\((u,v)\\in V\\times V\\), a unique vector \\(u+v\\) in \\(V\\). The scalar multiplication operation (denoted by juxtaposition), assigns to each pair \\((a,v)\\in \\mathbb{F}\\times V\\) a unique vector \\(a v\\) in \\(V\\).\nWe call \\(V\\) a linear space if the following axioms (A1)-(A8) are also satisfied.\n\nFor all \\(u, v\\in V\\), \\(u+v=v+u\\).\nFor all \\(u, v, w \\in V\\), \\((u+v)+w=u+(v+w)\\).\nThere exists \\(0\\in V\\) such that \\(v+0=v\\) for all \\(v\\in V\\).\nFor every \\(v\\in V\\), there exists \\(w\\in V\\) such that \\(v+w=0\\).\nFor all \\(v\\in V\\), \\(1 v=v\\).\nFor all \\(a, b\\in \\mathbb{F}\\) and \\(u\\in V\\), \\((a b) v=a (b v)\\).\nFor all \\(a \\in \\mathbb{F}\\) and \\(u, v\\in V\\), \\(a(u+v)=a u+av\\).\nFor all \\(a, b \\in \\mathbb{F}\\) and \\(u\\in V\\), \\((a+b)u=a u+ b u\\).\n\nIf \\(\\mathbb{F}=\\mathbb{R}\\) then \\(V\\) is a called a real linear space . If \\(\\mathbb{F}=\\mathbb{C}\\) then \\(V\\) is called a complex linear space . We denote the zero vector (A3) as \\(0\\), to distinguish between the zero vector and the zero \\(0\\) in the field of scalars.\n\n\nExample 2.15 Let \\(V=\\{(x,y)\\mid y=mx\\}\\), where \\(m\\) is a fixed real number and \\(x\\) is an arbitrary real number. Show that \\(V\\) is a linear space.\n\n\nExample 2.16 Let \\(V=\\{(x,y,x)\\mid ax+by+cz=0\\}\\) where \\(a, b\\) and \\(c\\) are fixed real numbers. Show that \\(V\\) is a linear space with the standard operations.\n\n::: {#exm- } [Matrix Space] Show that the set \\(M_{m\\times n}\\) of all \\(m\\times n\\) matrices, with ordinary addition of matrices and scalar multiplication, forms a linear space.\n:::\n::: {#exm- } [Polynomial Space] Show that the set \\(P(t)\\) of all polynomials with real coefficients, under the ordinary operations of addition of polynomials and multiplication of a polynomial by a scalar, forms a linear space. Show that the set of all polynomials with real coefficients of degree less than or equal to \\(n\\), under the ordinary operations of addition of polynomials and multiplication of a polynomial by a scalar, forms a linear space.\n:::\n::: {#exm- } [Function Space] Show that the set \\(F(x)\\) of all functions that map the real numbers into itself is a linear space. Show that the set \\(F[a,b]\\) of all functions on the interval \\([a,b]\\) using the standard operations is a linear space.\n:::\n::: {#exm- } [The Space of Infinite Sequences] Show that the set of all infinite sequences of real numbers is a linear space, where addition and scale multiplication are defined term by term.\n:::\n::: {#exm- } [The Space of Linear Equations] Show that the set \\(L_n\\) of all linear equations with \\(n\\) variables, forms a linear space.\n:::\n\nLemma 2.2 Every linear space \\(V\\) has a unique additive identity (denoted by \\(0\\)).\n\n\nProof. Let \\(u_1\\) and \\(u_2\\) be additive identities in \\(V\\), then \\(v+u_1=v\\) and \\(v+u_2=v\\) for every \\(v\\in V\\). Thus, \\(u_1=u_1+u_2=u_2+u_1=u_2\\) as desired.\n\n\nLemma 2.3 Every \\(v\\in V\\) has a unique additive inverse, (denoted by \\(-v\\)).\n\n\nProof. Let \\(v_1\\) and \\(v_2\\) be additive inverses of \\(w\\) in \\(V\\), then \\(w+v_1=\\mathbf{0}\\) and \\(w+v_2=\\mathbf{0}\\). Thus, \\[\nv_1=v_1+\\mathbf{0}=v_1+(w+v_2)=(v_1+w)+v_2=(w+v_1)+v_2=\\mathbf{0}+v_2=v_2\n\\] as desired.\n\n\nLemma 2.4 If \\(v\\in V\\), then \\(0\\, v=0\\).\n\n\nProof. Let \\(v\\in V\\), then \\(v=1 v=(1+0) v= 1 v+0 v= v+0v\\) which shows that \\(0 v\\) is the additive identity of \\(V\\), namely \\(0 v=\\mathbf{0}\\).\n\n\nLemma 2.5 If \\(a\\in \\mathbb{F}\\), then \\(a\\, 0=0\\).\n\n\nProof. Let \\(a\\in \\mathbb{F}\\), then \\[\na \\mathbf{0}=a(\\mathbf{0}+\\mathbf{0})=a\\mathbf{0}+a\\mathbf{0}\n\\] which shows that \\(a \\mathbf{0}\\) is the additive identity of \\(V\\), namely \\(a \\mathbf{0}=\\mathbf{0}\\).\n\n\nLemma 2.6 If \\(v\\in V\\), then \\(-(-v)=v\\).\n\n\nProof. Let \\(v\\in V\\), then \\[\nv+(-1)v=1 v+(-1) v=(1+(-1)) v=0 v= \\mathbf{0}\n\\] which shows that \\((-1)v\\) is the unique additive inverse of \\(v\\) namely, \\((-1)v=-v\\).\n\n\nLemma 2.7 If \\(v\\in V\\), then \\((-1)\\, v=-v\\).\n\n\nProof. Since \\(-v\\) is the unique additive inverse of \\(v\\), \\(v+(-v)=\\mathbf{0}\\). Then \\((-v)+v=\\mathbf{0}\\) shows that \\(v\\) is the unique additive inverse of \\(-v\\), namely, \\(v=-(-v)\\) as desired.\n\n\nLemma 2.8 If \\(a\\,v=0\\), then \\(a=0\\) or \\(v=0\\).\n\n\nProof. Suppose \\(a\\neq 0\\). If \\(a v =\\mathbf{0}\\) then \\(v=1 v=(a^{-1} a) v=a^{-1} (a v)=a^{-1} \\mathbf{0}=\\mathbf{0}\\). Otherwise \\(a=0\\) as desired.\n\n\nExample 2.17 Let \\(V\\) be a linear space with \\(u\\in V\\) and let \\(a\\) and \\(b\\) be scalars. Prove that if \\(a u=bu\\) and \\(u\\neq 0\\), then \\(a=b\\).\n\nLet \\(V\\) be a linear space and \\(U\\) a nonempty subset of \\(V\\). If \\(U\\) is a linear space with respect to the operations on \\(V\\), then \\(U\\) is called a subspace of \\(V\\).\n\nTheorem 2.15 A subset \\(U\\) of \\(V\\) is a linear subspace of \\(V\\) if and only if \\(U\\) has the following properties:\n\n\\(U\\) contains the zero vector of \\(V\\),\n\\(U\\) is closed under the addition defined on \\(V\\), and\n\\(U\\) is closed under the scalar multiplication defined on \\(V\\).\n\n\n\nProof. Koman pg 103.\n\nMore generally, a subset \\(U\\) of \\(V\\) is called a subspace of \\(V\\) if \\(U\\) is also a vector space using the same addition and scalar multiplication as on \\(V\\). Any vector space is a subspace of itself. The set containing just the \\(0\\) vector is also a subspace of any vector space. Given any vector space with a nonzero vector \\(v\\), the scalar multiples of \\(v\\) is a vector subspace of \\(V\\) and is denoted by \\(\\langle v \\rangle\\). Because any linear space \\(V\\) has \\(V\\) and \\(0\\) as subspaces, these subspaces are called the trivial subspaces of \\(V\\). All other subspaces are called proper subspaces of \\(V\\).\n\nExample 2.18 Give an example of a real linear space \\(V\\) and a nonempty set \\(S\\) of \\(V\\) such that, whenever \\(u\\) and \\(v\\) are in \\(S\\), \\(u+v\\) is in \\(S\\) but \\(S\\) is not a subspace of \\(V\\).\n\n\nExample 2.19 Give an example of a real linear space \\(V\\) and a nonempty set \\(S\\) of \\(V\\) such that, whenever \\(u\\) and \\(v\\) are in \\(S\\), \\(c u\\) is in \\(S\\) for every scalar \\(c\\) but \\(S\\) is not a subspace of \\(V\\).\n\n\nExample 2.20 Show that \\(P_n[0,1]\\) is a proper subspace of \\(C[0,1]\\).\n\n\nExample 2.21 Show that \\(C'[0,1]\\) (continuous first derivative) is a proper subspace of \\(C[0,1]\\).\n\n\nExample 2.22 Show that \\(R[0,1]\\) (Riemann integrable) is a proper subspace of \\(C[0,1]\\).\n\n\nExample 2.23 Show that \\(D[0,1]\\) (Differenable functions) is a proper subspace of \\(C[0,1]\\).\n\n\nDefinition 2.8 A linear combination of a list of vectors \\((\\vlist{v}{m})\\) in \\(V\\) is a vector of the form \\(\\lincomb{a}{v}{m}\\) where \\(\\vlist{a}{m} \\in k\\).\n\n\nLemma 2.9 Let \\(U\\) be a nonempty subset of a vector space \\(V\\). Then \\(U\\) is a subspace of \\(V\\) if and only if every linear combination of vectors in \\(U\\) is also in \\(U\\).\n\n\nProof. If \\(U\\) is a subspace of \\(V\\), then \\(U\\) is a vector space and so is closed under linear combinations by definition of vector space. Conversely, suppose every linear combination of vectors in \\(U\\) is also in \\(U\\). Thus for any \\(a, b \\in k\\), \\(a u+b v \\in U\\) for every \\(u, v\\in U\\). In particular, when \\(a=b=1\\) then \\(u+v \\in U\\) and so \\(U\\) is closed with respect to addition. Notice when \\(b=0\\) and \\(a=-1\\), then \\(-u\\in U\\) for every \\(u\\in U\\) and so \\(U\\) is closed under inverses. Notice when \\(u=v\\), \\(a=1\\), and \\(b=-1\\) then \\(u+(-u)=0\\in U\\) so \\(U\\) contains the identity element. The rest of the axioms in the definition of a vector space hold by containment.\n\n\nDefinition 2.9 The intersection and union of subspaces is just the intersection and union of the subspaces as sets. The sum of subspaces \\(\\vlist{U}{m}\\) of a vector space \\(U\\) is defined by\n\\[\nU_1+ U_2+\\cdots +U_m\n= \\{ u_1 + u_2+\\cdots + u_m \\mid u_i \\in U_i \\text{ for } 1\\leq i \\leq m \\}.\n\\]\n\n\nLemma 2.10 Let \\(V\\) be a linear space over a field \\(k\\). The intersection of any collection of subspaces of \\(V\\) is a subspace of \\(V\\).\n\n\nProof. Let \\(\\{U_i\\, |\\, i \\in I\\}\\) be a collection of subspaces where \\(I\\) is some indexed set. Let \\(a,b\\in k\\) and \\(u,v\\in \\cap_{i\\in I} U_i\\). Since each \\(U_i\\) is a subspace of \\(V\\), \\(a u +b v\\in U_i\\) for every \\(i\\in I\\). Thus \\(a u+b v\\in \\cap_{i\\in I} U_i\\) and therefore \\(\\cap_{i\\in I} U_i\\) is a subspace of \\(V\\).\n\n\nExample 2.24 Show that the \\(x\\)-axis and the \\(y\\)-axis are subspaces on \\(\\mathbb{R}^2\\), yet the union of these axis is not.\n\n\nLemma 2.11 Let \\(V\\) be a linear space over a field \\(k\\). The union of two subspaces of \\(V\\) is a subspace of \\(V\\) if and only if one of the subspaces is contained in the other.\n\n\nProof. Suppose \\(U\\) and \\(W\\) are subspaces of \\(V\\) with \\(U\\subseteq W\\). Then \\(U\\cup W=W\\) and so \\(U\\cup W\\) is also a subspace of \\(V\\). Conversely, suppose \\(U\\), \\(W\\), \\(U\\cup W\\) are subspaces of \\(V\\) and suppose \\(u\\in U\\). If \\(u\\in W\\) then \\(U\\) is contained in \\(W\\) as desired. Thus we assume, \\(u\\not\\in W\\). If \\(w\\in W\\), then \\(u+w \\in U\\cup W\\) and either \\(u+w\\in U\\) or \\(u+w\\in W\\). Notice \\(u+w\\in W\\) and \\(w\\in W\\) together yield \\(u\\in W\\) which is a contradiction. Thus \\(u+w\\in U\\) and so \\(w\\in U\\) which yields \\(W\\subseteq U\\) as desired.\n\n\nLemma 2.12 Let \\(V\\) be a linear space over a field \\(k\\). The sum \\(U_{1}+ U_2+\\cdots +U_{m}\\) is the smallest subspace containing each of the subspaces \\(\\vlist{U}{m}\\).\n\n\nProof. The sum of two subspaces is a subspace since the sum of two subspaces is closed under linear combinations. Thus \\(\\vlist{U}{m}\\) is a subspace containing \\(U_i\\) for each \\(1\\leq i \\leq m\\). Let \\(U\\) be another subspace containing \\(U_{i}\\) for each \\(1\\leq i \\leq m\\). If \\(u\\in U_{1}+ \\cdots +U_{m}\\), then \\(u\\) has the form \\(u=u_1+\\cdots + u_m\\) where each \\(u_i\\in U_i\\subseteq U\\). Since \\(U\\) is a subspace \\(u\\in U\\) and so \\(U_{1}+U_{2}+ \\cdots +U_{m}\\) is the smallest such subspace.\n\n\nDefinition 2.10 If \\(\\{\\vlist{v}{m}\\}\\) is a subset of a linear space \\(V\\), then the subspace of all linear combinations of these vectors is called the subspace generated ( spanned) by \\(\\vlist{v}{m}.\\) The spanning set of the list of vectors \\((\\vlist{v}{m})\\) in \\(V\\) is denoted by \\[\n\\text{span}(\\vlist{v}{m})=\n\\{\\lincomb{a}{v}{m} \\mid \\vlist{a}{m}\\in \\mathbb{F} \\}.\n\\]\n\n\nLemma 2.13 The span of a list of vectors in \\(V\\) is the smallest subspace of \\(V\\) containing all the vectors in the list.\n\n\nProof. Let \\((\\vlist{v}{n})\\) be a list of vectors in \\(V\\) and let \\(S\\) denote \\(\\text{span}(\\vlist{v}{n})\\). Clearly, \\(S\\) contains \\(v_i\\) for each \\(1\\leq i \\leq n\\). Let \\(u,v \\in S\\) and \\(a,b\\in k\\). Then there exists \\(\\vlist{a}{n}\\) in \\(k\\) and \\(\\vlist{b}{n}\\) in \\(k\\) such that \\(u=a_1 v_1+\\cdots a_n v_n\\) and \\(v=b_1 v_1+ \\cdots b_n v_n\\).\nThen \\[\na u+b v\n=(a a_1 +b b_1) v_1+ \\cdots +(a a_n+b b_n) v_n\n\\] which shows \\(a u+b v\\in S\\) since \\(a a_i+b b_i \\in k\\) for each \\(1\\leq i \\leq n\\). Thus \\(S\\) is a subspace containing each of the \\(v_i\\). Let \\(T\\) be a subspace containing \\(v_i\\) for \\(1 \\leq i \\leq n\\). If \\(s\\in S\\), then there exists \\(\\vlist{c}{n} \\in k\\) such that \\(s=c_1 v_1+\\cdots + c_n v_n\\). Since \\(v_i\\in T\\) for each \\(i\\) and \\(T\\) is closed under linear combinations (since \\(T\\) is a subspace), \\(s\\in T\\). Meaning \\(S \\subseteq T\\), so indeed \\(S\\) is the smallest subspace of \\(V\\) containing all the vectors \\(v_i\\).\n\n\nDefinition 2.11 Let \\(\\emptyset\\) denote the empty set. Then \\(\\text{span}(\\emptyset)=\\{0\\}\\).\n\n\nExample 2.25 Let \\(A=\\begin{matrix}1 & 1 \\ 0 & 0 \\end{matrix}\\). Show that \\(S=\\{X\\in M_{2\\times 2} \\mid AX=XA\\}\\) is a subspace of \\(M_{2\\times 2}\\) under the standard operations.\n\n\nExample 2.26 Let \\(f_1=x^2+1, f_2=3x-1, f_3=2\\). Determine the subspace generated by \\(f_1, f_2, f_3\\) in \\(P_4\\).\n\n\nExample 2.27 Let \\(A_1=\\begin{bmatrix} 1 & 0 & 0 & 3 \\\\ 0 & 0 & 2 & 0\\end{bmatrix}\\) and \\(A_2=\\begin{bmatrix} 0 & 2 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}.\\) Determine the subspace generated by \\(A_1\\) and \\(A_2\\) in \\(R_{2\\times 4}\\).\n\n\nExample 2.28 Describe \\(\\text{span}(0)\\).\n\n\nExample 2.29 Consider the subset \\(S=\\{x^3-2x^2+x-3, 2x^3-3x^2+2x+5, 4x^3-7x^2+4x-1, 4x^2+x-3\\}\\) of \\(P\\). Show that \\(3x^3-8x^2+2x+16\\) is in \\(\\text{span} (S)\\) by expressing it as a linear combination of the elements of \\(S\\).\n\n\nExample 2.30 Determine if the matrices \\[\n\\begin{bmatrix} 2 & -1 \\\\ 0 & 2 \\end{bmatrix},\n\\begin{bmatrix} -4 & 2 \\\\ 3 & 0 \\end{bmatrix},  \n\\begin{bmatrix} -1 & 0 \\\\ 2 & 1 \\end{bmatrix},  \n\\begin{bmatrix} 0 & 0 \\\\ 0 & 3 \\end{bmatrix}\n\\] span \\(M_{2\\times 2}\\)."
  },
  {
    "objectID": "vector-spaces.html#linear-independence-and-bases",
    "href": "vector-spaces.html#linear-independence-and-bases",
    "title": "2  Vector Spaces",
    "section": "2.4 Linear Independence and Bases",
    "text": "2.4 Linear Independence and Bases\nLet \\(V\\) be a linear save over a field \\(\\mathbb{F}\\). A subset \\(S\\) of \\(V\\) is said to be linearly dependent if there exist distinct \\(\\vlist{v}{n}\\) in \\(S\\) and scalars \\(\\vlist{a}{n}\\) in \\(\\mathbb{F}\\), not all zero, such that \\(\\lincomb{a}{v}=0.\\) If a set \\(S\\) cntains only finitely many \\(v_i\\) we sometimes say that \\(\\vlist{v}{n}\\) are dependent.\n\nDefinition 2.12 A list of vectors \\((\\vlist{v}{m})\\) in \\(V\\) is called linearly independent if the only choice of \\(\\vlist{a}{m}\\in k\\) is \\(a_1=\\cdots=a_m=0\\). A list of vectors that are not linearly independent is called linearly dependent.\n\nNotice from the definition we can conclude that any set which contains a linearly dependent set is linearly dependent. Any subset of a linearly independent set is linearly independent. Any set which contains the zero vector is linearly dependent. A set \\(S\\) of vectors is linearly independent if and only if each finite subset of \\(S\\) is linearly independent. (Show!)\n::: {#lem- } [Linear Dependence] If \\((\\vlist{v}{m})\\) is linearly dependent in \\(V\\) and \\(v_1\\neq 0\\) then there exists \\(j\\in {2,\\ldots,m}\\) such that the following hold: \\(v_j\\in\\text{span}(v_1,\\ldots,v_{j-1})\\) and if the \\(j^{th}\\) term is removed from \\((\\vlist{v}{m})\\), the span of the remaining list equals \\(\\text{span}(\\vlist{v}{m})\\). :::\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:Linear Dependence Lemma}\\).\n\n\nExample 2.31 Show that the following subset of \\(M_{2\\times 2}\\) is a linear dependent set. \\[\\begin{equation}\n\\label{lindeexample}\n\\left\\{\n\\begin{bmatrix}\n2 & 3 \\\\\n-1 & 4\n\\end{bmatrix},\n\\begin{bmatrix}\n-11 & 3 \\\\\n-2 & 2\n\\end{bmatrix},\n\\begin{bmatrix}\n6 & -1 \\\\\n3 & 4\n\\end{bmatrix},\n\\begin{bmatrix}\n-1 & 0\\\\2 & 2\n\\end{bmatrix}\n\\right\\}\n\\end{equation}\\]\n\n\nExample 2.32 Suppose that \\(S\\) is the subset \\[\nS=\\{2x^3-x+3, 3x^3+2x-2, x^3-4x+8, 4x^3+5x-7\\}\n\\] of \\(P_3\\).\n\nShow that \\(S\\) is linear dependent.\nShow that every three element subset of \\(S\\) is linear dependent.\nShow that every two element subset of \\(S\\) is linear independent.\n\n\n\nExample 2.33 Show that no linear independent set can contain zero.\n\n::: {#lem- } [Linear Independence Lemma] Let \\(S\\) be a linearly independent subset of a vector space \\(V\\). Suppose \\(v\\) is a vector in \\(V\\) which is not in the subspace spanned by \\(S\\). Then the set obtained by adjoining \\(v\\) to \\(S\\) is linearly independent. :::\n\nProof. Suppose \\(u_1,\\ldots,u_n\\) are distinct vectors in \\(S\\) and that \\(a_1 u_1+\\cdots + a_n u_n+a v =0.\\) Then \\(a=0\\); for otherwise, \\[\nv=\\left (-\\frac{a_1}{a}\\right ) u_1+\\cdots + \\left (-\\frac{a_n}{a} \\right )u_n\n\\] and \\(v\\) is in the subspace spanned by \\(S\\). Thus \\(a_1 u_1+\\cdots + a_n u_n=0\\), and since \\(S\\) is a linearly independent set each \\(a_i=0\\).\n\n\nTheorem 2.16 If \\(S\\) is a set of vectors in a vector space \\(V\\) over a field \\(F\\) then the following are equivalent.\n\n\\(S\\) is linearly independent and spans \\(V\\)\nFor every vector \\(v\\in V\\), there is a unique set of vectors \\(v_1,\\ldots,v_n\\) in \\(S\\), along with a unique set of scalars in \\(\\mathbb{F}\\) for which \\(v=a_1 v_1+\\cdots+a_n v_n\\)\n\\(S\\) is a minimal spanning set in the sense that \\(S\\) spans \\(V\\), and any proper subset of \\(S\\) does not span \\(V\\)\n\\(S\\) is a maximal linearly independent set in the sense that \\(S\\) is linearly independent, but any proper superset of \\(S\\) is not linearly independent.\n\n\n\nProof. \\((i) \\Leftrightarrow (ii)\\): Then \\(S\\) is a spanning set. If some proper subset \\(S'\\) of \\(S\\) also spanned \\(V\\), than any vector in \\(S-S;\\) would be a linear combination of the vectors in \\(S'\\). contradicting the fact that the vectors in \\(S\\) are linearly independent. Conversely, if \\(S\\) is a minimal spanning set, then it must be linearly independent. For if not, some vector \\(s\\in S\\) would be a linear of the other vectors in \\(S\\), and so \\(S-S'\\) would be a proper spanning subset of \\(S\\), which is not possible.\n\\((i) \\Leftrightarrow (iv)\\): Then \\(S\\) is linearly independent. If \\(S\\) were not maximal, there would be a vector \\(v\\in V-S\\) for which the set \\(S\\cup \\{v\\}\\) is linear independent. But then \\(V\\) is not in the span of \\(S\\), contradicting the fact that \\(S\\) is a spanning set. Hence, \\(S\\) is a maximal linearly independent set, and so (i) implies (iv). Conversely, if \\(S\\) is a maximal independent set,. then it must span \\(V\\), for if not, we could find a vector \\(v\\in S-S'\\) that is not a linear combination of the vectors in \\(S\\). Hence, \\(S\\cup \\{v\\}\\) would be a contradiction.\n\n\nDefinition 2.13 A basis of \\(V\\) is a list of vectors in \\(V\\) that is linearly independent and spans \\(V\\).\n\n\nExample 2.34 Find a basis for the space of all \\(2\\times 2\\) matrices \\(S\\) such that \\[\n\\begin{bmatrix} 1 & 1 \\\\ 1 & 1\\end{bmatrix}S=S.\n\\] Let \\(S=\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\). Then \\[\n\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n\\] meaning \\[\n\\begin{bmatrix} a+c & b+d \\\\ a+c & b+d \\end{bmatrix}=\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}.\n\\] So \\(a+c=a, b+d=b, a+c=c,\\) and \\(b+d=d\\). These imply, respectively that \\(a=b=c=d=0\\).\n\n\nCorollary 2.4 A list \\((v_1,\\ldots,v_m)\\) of vectors in \\(V\\) is a basis of \\(V\\) if and only if every \\(v\\in V\\) can be written uniquely in the form \\(v=a_1 v_1+\\ldots+a_m v_m\\) where \\(\\vlist{a}{m}\\in k\\).\n\n\nProof. \n\n\nCorollary 2.5 Any \\(n\\) linearly independent vectors in a linear space \\(V\\) of dimension \\(n\\) constitute a basis for \\(V\\).\n\n\nProof. \n\n\nExample 2.35 Show that the space of all \\(m\\times n\\) matrices over a field \\(\\mathbb{F}\\) has dimension \\(mn\\).\n\n\nCorollary 2.6 Any finitely generated linear space, generated by a asset of nonzero vectors, has a basis.\n\n\nProof. \n\nThe zero linear space has no basis, because any subset contains the zero vector and must be linearly dependent.\n\nExample 2.36 Show that \\[\nB=\\{[2,3,0,-1],[-1,1,1,-1],[3,2,-1,0],[2,3,0,-1]\\}\n\\] is a maximal linearly independent subset of \\[\nS=\\{[1,4,1,-2],[-1,1,1,-1],[3,2,-1,0],[2,3,0,-1]\\}.\n\\] Determine \\(\\dim\\text{span}(S)\\) and determine whether or not \\(\\text{span}(S)=P_3\\)."
  },
  {
    "objectID": "vector-spaces.html#finite-dimensional-linear-spaces",
    "href": "vector-spaces.html#finite-dimensional-linear-spaces",
    "title": "2  Vector Spaces",
    "section": "2.5 Finite-dimensional Linear Spaces",
    "text": "2.5 Finite-dimensional Linear Spaces\n\nDefinition 2.14 The vector space \\(V\\) is a direct sum of subspaces \\(\\vlist{U}{m}\\) of \\(V\\), written \\(V=U_{1} \\oplus \\cdots \\oplus U_{m}\\), if each element of \\(V\\) can be written uniquely as a sum \\(u_1 + u_2+\\cdots + u_m\\) where each \\(u_j\\in U_j\\).\n\n\nLemma 2.14 If \\(\\vlist{U}{m}\\) are subspaces of \\(V\\), then \\(V=U_{1} \\oplus \\cdots \\oplus U_{m}\\) if and only if both conditions hold:\n\n\\(V=U_{1} + \\cdots + U_{m}\\).\nthe only way to write \\(0\\) as a sum \\(u_{1} + u_{2}+\\cdots+ u_{m}\\) where each \\(u_{j} \\in U_{j}\\), is by taking all the \\(u_{j}\\)’s equal to \\(0\\).\n\n\n\nProof. If \\(V=U_{1} \\oplus U_2 \\oplus \\cdots \\oplus U_{m}\\), then every element in \\(V\\) can be written uniquely in the form \\(u_1+ u_2+\\cdots +u_m\\) where each \\(u_j\\in U_j\\) for \\(1\\leq i \\leq m\\). Thus both conditions listed above are satisfied. Conversely, suppose both conditions hold and assume \\(u=u_1 + \\cdots +u_m\\) where \\(u_i \\in U_i\\) and \\(u=v_1 + \\cdots + v_m\\) where \\(v_i\\in U_i\\). Since \\[\nu-u=(u_1 + \\cdots +u_m)-(v_1 + \\cdots +v_m)=(u_1-v_1)+\\cdots +(u_m-v_m)=0\n\\] it follows \\(u_i=v_i\\) for \\(1\\leq i \\leq m\\); and so uniqueness is established.\n\n\nExample 2.37 Let \\(V\\) be the linear space of all functions from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\) and let \\(V_e\\) and \\(V_o\\) be the set of all even functions (\\(f(-x)=f(x)\\)) and the set of odd functions (\\(f(-x)=-f(x)\\)), respectively.\n\nProve that \\(V_e\\) and \\(V_o\\) are subspaces of \\(V\\).\nProve that \\(V_e+V_o=V\\)\nProve that \\(V_e\\cap V_o=\\{0\\}\\).\n\n\n\nLemma 2.15 If \\(U\\) and \\(W\\) are subspaces of \\(V\\), then \\(V=U \\oplus W\\) if and only if \\(V=U+W\\) and \\(U \\cap W= \\{0\\}\\).\n\n\nProof. If \\(V=U \\oplus W\\) then \\(V=U+W\\) and \\(\\{0\\} \\subseteq U\\cap W\\) are immediate. Suppose \\(v\\in U \\cap W\\). Since \\(v\\in U \\oplus W\\), there exists unique \\(u\\in U\\) and \\(w\\in W\\) such that \\(v=u+w\\). Assume for a contradiction, \\(u\\not = 0\\). Then \\(u=u+0\\) where \\(0\\in W\\) and \\(u \\in U\\) shows that \\(u\\not \\in W\\) since \\(V=U\\oplus W\\). Also, \\(v=u+w\\in U\\cap W\\) so \\(u+w\\in W\\) and \\(w\\in W\\) implies \\(u\\in W\\). This contradiction leads to \\(u=0\\). Similarly, \\(w=0\\) and so \\(v=u+w=0\\) which yields \\(U \\cap W= \\{0\\}\\). Conversely, suppose \\(V=U+W\\) and \\(U \\cap W= \\{0\\}\\). If \\(0=u+w\\) then \\(u=-w\\in W\\) and together with \\(u\\in U\\) yields \\(u=0\\). Thus, \\(w=0\\) also. So the only way to write 0 as a sum \\(u+w\\) is to have \\(u=w=0\\) therefore \\(V=U \\oplus W\\).\n\n\nExample 2.38 Prove or give a counterexample: if \\(U_1, U_2\\), and \\(W\\) are subspaces of \\(V\\) such that \\(U_1 +W=U_2+W\\), then \\(U_1=U_2\\). False, here is a counterexample. Let \\(V=\\mathbb{R}^2=W\\) with \\(U_1=\\{(x,0) \\mid x \\in \\mathbb{R}\\}\\) and \\(U_2=\\{(0,y) \\mid y\\in \\mathbb{R}\\}\\). Then \\(U_1+W=U_2+W=\\mathbb{R}^2\\). However, \\(U_1\\neq U_2\\).\n\n\nExample 2.39 Prove or give a counterexample: if \\(U_1, U_2, W\\) are subspaces of \\(V\\) such that \\(V=U_1 \\oplus W\\) and \\(V=U_2 \\oplus W\\), then \\(U_1=U_2\\). False, here is a counterexample. Let \\(V=\\mathbb{R}^2\\) with \\(W=\\{(x,x) \\mid x\\in \\mathbb{R}\\}\\), \\(U_1=\\{(x,0) \\mid x \\in \\mathbb{R}\\}\\), and \\(U_2=\\{(0,y) \\mid y\\in \\mathbb{R}\\}\\). All of these sets are subspaces. Let \\(u\\in U_1\\cap W\\), then \\(u=(x,0)\\) and \\(u=(z,z)\\) so \\(z=0\\) and \\(x=0\\) which implies \\(u=(0,0)\\). In fact \\(U_1\\cap W=\\{0\\}\\). Thus, \\(\\mathbb{R}^2=U_1\\oplus W\\). Also \\(\\mathbb{R}^2=U_2\\oplus W\\). However, \\((1,0)\\in U_1\\) and \\((1,0)\\not \\in U_2\\) showing \\(U_1\\neq U_2\\).\n\n\nExample 2.40 Suppose \\(m\\) is a positive integer. Is the set consisting of 0 and all polynomials with coefficients in \\(F\\) and with degree equal to \\(m\\) a subspace of \\(\\mathcal{P}(\\mathbb{F})\\)?\n\n\nDefinition 2.15 We call a linear space \\(V\\) a finite-dimensional linear space if there is a finite list of vectors \\((v_1, \\ldots, v_m)\\) with \\(\\text{span}(v_1, \\ldots, v_m)=V.\\) If a linear space is not a finite-dimensional vector space it is called an infinite-dimensional vector space.\n\n\nLemma 2.16  Let \\(V\\) be a finite-dimensional vector space. Then the length of every linearly independent list of vectors in \\(V\\) is less than or equal to the length of every spanning list of vectors in \\(V\\).\n\n\nProof. \n\n\nLemma 2.17 Let \\(V\\) be a finite-dimensional vector space. Then \\(V\\) only has finite-dimensional subspaces.\n\n\nProof. \n\n\nTheorem 2.17  Let \\(V\\) be a finite-dimensional vector space. Then every spanning list \\((v_1,\\ldots,v_m)\\) of a vector space can be reduced to a basis of the vector space,\n\n\nProof. \n\n\nTheorem 2.18 Let \\(V\\) be a finite-dimensional vector space. Then if \\(V\\) is finite-dimensional then \\(V\\) has a basis.\n\n\nProof. By definition, every finite-dimensional vector space has a finite spanning set. By \\(\\ref{spanning reduce lemma}\\), every spanning set can be reduced to a basis, and so every finite-dimensional vector space does indeed have a basis.\n\n\nTheorem 2.19 Let \\(V\\) be a finite-dimensional vector space. Then if \\(W\\) is a subspace of \\(V\\), every linearly independent subset of \\(W\\) is finite and is part of a finite basis for \\(W\\).\n\n\nProof. \n\n\nTheorem 2.20 Let \\(V\\) be a finite-dimensional vector space. Then every linearly independent list of vectors in \\(V\\) can be extended to a basis of \\(V\\),\n\n\nProof. \n\n\nTheorem 2.21 Let \\(V\\) be a finite-dimensional vector space. Then if \\(U\\) is a subspace of \\(V\\), then there is a subspace \\(W\\) of \\(V\\) such that \\(V=U \\bigoplus W\\), and\n\n\nProof. \n\n\nTheorem 2.22 Let \\(V\\) be a finite-dimensional vector space. Then any two bases of \\(V\\) have the same length.\n\n\nProof. \n\n\nDefinition 2.16 The dimension of a finite-dimensional vector space is defined to be the length of any basis of the vector space.\n\n\nTheorem 2.23 Let \\(V\\) be a finite-dimensional vector space. Then if \\(U\\) is a subspace of \\(V\\) then \\(\\text{dim} U \\leq \\text{dim} V\\).\n\n\nProof. \n\n\nTheorem 2.24 Let \\(V\\) be a finite-dimensional vector space. Then every spanning list of vectors in \\(V\\) with length dim \\(V\\) is a basis of \\(V\\).\n\n\nProof. \n\n\nTheorem 2.25 Let \\(V\\) be a finite-dimensional vector space. Then every linearly independent list of vectors in \\(V\\) with length dim \\(V\\) is a basis of \\(V\\).\n\n\nProof. \n\n\nTheorem 2.26 Prove that if \\(U_1\\) and \\(U_2\\) are subspaces of a finite-dimensional vector space, then \\[\n\\text{dim}  (U_1+U_2)= \\text{dim}  U_1 + \\text{dim}  U_2 -\\text{dim}  (U_1 \\cap U_2).\n\\]\n\n\nProof. \n\n\nTheorem 2.27 If \\(V\\) is finite-dimensional, and \\(U_1, \\ldots, U_m\\) are subspaces of \\(V\\) such that\n\n\\(V=U_1+\\cdots+U_m\\) and\n\\(\\text{dim} V=\\text{dim} U_1+\\cdots+\\text{dim} U_m\\),\n\nthen \\(V=U_1 \\oplus \\cdots \\oplus U_m.\\)\n\n\nProof. Choose a basis for each \\(U_j\\). Put these bases together in one list, forming a list that spans \\(V\\) and has length \\(\\text{dim} (V)\\). Thus this list is a basis of \\(V\\) and in particular it is linearly independent. Now suppose that \\(u_j\\in U_j\\) for each \\(j\\) are such that \\(0=u_1+\\cdots + u_m\\). We can write each \\(u_j\\) as a linear combination of the basis vectors of \\(U_j\\). Substituting these expressions above, we have written \\(0\\) as a linear combination of the basis vectors of \\(V\\). Thus all scalars used in the linear combinations must be zero. Thus each \\(u_j=0\\) which proves \\(V=U_1 \\oplus \\cdots \\oplus U_m.\\)"
  },
  {
    "objectID": "vector-spaces.html#infinite-dimensional-linear-spaces",
    "href": "vector-spaces.html#infinite-dimensional-linear-spaces",
    "title": "2  Vector Spaces",
    "section": "2.6 Infinite-dimensional Linear Spaces",
    "text": "2.6 Infinite-dimensional Linear Spaces\n\nExample 2.41 Prove that \\(k^\\infty\\) is infinite-dimensional. Suppose \\(\\mathbb{F}^\\infty\\) is finite-dimensional with dimension \\(n\\). Let \\(v_i\\) be the vector in \\(\\mathbb{F}^\\infty\\) consisting of all 0’s except with 1 in the \\(i\\)-th position for \\(i=1,...,n\\). The vectors \\((v_1,...,v_n)\\) are linearly independent in \\(\\mathbb{F}^\\infty\\) and so they must form a basis; however they do not span since the vector \\(v_{n+1}\\) consisting of all 0’s except with \\(1\\) in the \\((n+1)\\)-th position. Thus \\(\\mathbb{F}^\\infty\\) can not be finite-dimensional.\n\n\nExample 2.42 Prove that the real vector space consisting of all continuous real-valued functions on the interval \\([0,1]\\) is infinite-dimensional. Notice that \\(f(x)=x^n\\) is continuous on \\([0,1]\\) for every positive integer \\(n\\). If this vector space is finite-dimensional of say dimension \\(n\\) then the list of vectors \\((1,x,x^2,...,x^{n-1})\\) must form a basis since they are linearly independent and there are \\(n\\) of them. However, \\(x^n\\) is not in the span of this list and so this list can not be a basis. This contradiction shows that this vector space must be infinite-dimensional.\n\n\nExample 2.43 Prove that \\(V\\) is infinite-dimensional if and only if there is a sequence \\(v_1, v_2,...\\) of vectors in \\(V\\) such that \\((v_1, ..., v_n)\\) is linearly independent for every positive integer \\(n\\). Suppose \\(V\\) is infinite-dimensional. Then \\(V\\) has no finite spanning list. Pick \\(v_1\\neq 0\\). For each positive integer \\(n\\) choose \\(v_{n+1}\\not \\in \\text{ span}(v_1,...,v_n)\\), by the linear independence lemma, and for each positive integer \\(n\\), the list \\((v_1,...,v_n)\\) is linearly independent. Conversely, suppose there is a sequence \\(v_1,v_2,...,\\) of vectors in \\(V\\) such that \\((v_1,...,v_n)\\) is linearly independent for every positive integer \\(n\\). If \\(V\\) is finite-dimensional, then it has a spanning list with \\(M\\) elements. By the previous theorem, every linearly independent list has no more than \\(M\\) elements. Therefore, \\(V\\) is infinite-dimensional.\n\n\nTheorem 2.28 Every vector space has a basis. Moreover, any two bases have the same carnality.\n\n\nProof. Let \\(V\\) be a nonzero vector space and consider the collection \\(A\\) of all linearly independent subsets of \\(V\\). This collection is nonempty, since any single nonzero vector forms a linearly independent set. Now, if \\(I_1\\subset I_2 \\subset \\cdots\\) is a chain of linearly independent subsets of \\(V\\), then the union \\(U\\) is also a linearly independent set. Hence, every chain in \\(A\\) has an upper bound in \\(A\\), and according to Zorn’s lemma, \\(A\\) must contain a maximal element, that is, \\(V\\) has a maximal linearly independent set, which is a basis for \\(V\\).\nWe may assume that all bases for \\(V\\) are infinite sets, for if any basis is finite, then \\(V\\) has a finite spanning set and so is a finite-dimensional vector space. Let \\(B\\) be a basis for \\(V\\). We may write \\(B=\\{b_i \\mid i\\in I\\}\\) where \\(I\\) is some indexed set, used to index the vectors in \\(B\\). Note that \\(|I|=|B|\\). Now let \\(C\\) be another basis for \\(V\\). Then any vector \\(c\\in C\\) can be written as a finite linear combination of the vectors in \\(B\\), where all the coefficients are nonzero, say \\(c=\\sum_{i\\in U_c} r_i b_i\\). Here \\(U_c\\) is a finite subset of the index set \\(I\\). Now, because \\(C\\) is a basis for \\(V\\), the union of all of the \\(U_c\\)’s as \\(C\\) varies over \\(C\\) must be in \\(I\\), in symbols, \\(\\bigcup_{c\\in C} U_c=I\\). For if all vectors in the basis \\(C\\) can be expressed as a finite linear combination of the vectors \\(B-\\{ b_k\\}\\) spans \\(V\\), which is not the case. Therefore, \\(|B|=|I|\\leq |C| \\alpha_0=|C|.\\) But we may also reverse the roles of \\(B\\) and \\(C\\) we obtain the reverse inequality. Therefore, \\(|B|=|C|\\) as desired."
  },
  {
    "objectID": "linear-transformations.html#introduction-to-linear-transformations",
    "href": "linear-transformations.html#introduction-to-linear-transformations",
    "title": "3  Linear Transformations",
    "section": "3.1 Introduction to Linear Transformations",
    "text": "3.1 Introduction to Linear Transformations\nA linear transformation is a function of the form \\(\\vec y =A \\vec x\\) where \\(A\\) is an \\(n\\times m\\) matrix. More specifically, a linear transformation is a function that assigns to each \\(\\vec x\\in \\mathbb{R}^m\\), a unique \\(\\vec y\\in \\mathbb{R}^n\\) – and this assignment is defined by a matrix \\(A\\). When \\(A\\) is the identity matrix and \\(T(\\vec x)=A \\vec x\\) we call \\(T\\) the identity transformation .\n\nDefinition 3.1  A function \\(T\\) from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\) is called a linear transformation if there exists an \\(n\\times m\\) matrix \\(A\\) such that \\(T(\\vec x)=A \\vec x\\), for all \\(\\vec x\\) in the vector space \\(\\vec R^m\\).\n\n\nLemma 3.1 Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\), then the matrix of \\(T\\) is \\[\\begin{equation}\n\\label{trancol}\nA=\\begin{bmatrix} | &  & | \\\\ T(\\vec e_1) & \\cdots & T(\\vec e_m) \\\\ | &  & | \\end{bmatrix}\n\\end{equation}\\] where \\(\\vec e_i\\) (for \\(0\\leq i \\leq m\\)) are the standard vectors.\n\n\nProof. Suppose \\(T\\) is a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\), then there exists an \\(n\\times m\\) matrix \\(A\\) such that \\(T(\\vec x)=A\\vec x\\) for all \\(\\vec x\\in \\mathbb{R}^m\\). Let \\(\\vec e_1, ..., \\vec e_m\\) be the standard vectors of \\(\\mathbb{R}^m\\) and let \\(A=[a_{ij}]\\), then \\[\nT(\\vec e_1)=A \\vec e_1=\n\\begin{bmatrix}\na_{11} & \\cdots & a_{1m} \\\\\n\\vdots & \\cdots & \\vdots \\\\\na_{n1} & \\cdots & a_{nm}\n\\end{bmatrix}\n\\vectorthree{1}{\\vdots}{0}\n=\\vectorthree{a_{11}}{\\vdots}{a_{n1}}\n\\] \\[\n\\vdots\n\\] \\[\nT(\\vec e_m)=A \\vec e_m=\n\\begin{bmatrix}\na_{11} & \\cdots & a_{1m} \\\\\n\\vdots & \\cdots & \\vdots \\\\\na_{n1} & \\cdots & a_{nm}\n\\end{bmatrix}\n\\vectorthree{0}{\\vdots}{1}\n=\\vectorthree{a_{1m}}{\\vdots}{a_{nm}}\n\\] which are the columns of the matrix \\(A\\).\n\n\nExample 3.1 Determine the linear transformation \\(T\\) given by the system of linear equations: \\[\n\\begin{array}{l}\ny_1=  7x_1+3x_2-9x_3+8x_4 \\\\\ny_2 =  6x_1+2x_2-8x_3+7x_4 \\\\\ny_3 =  8x_1+4x_2+7x_4\n\\end{array}\n\\] The matrix of the linear transformation is \\(A=\\begin{bmatrix} 7 & 3 & -9 & 8 \\\\ 6 & 2 & -8 & 7 \\\\ 8 & 4 & 0 & 7 \\end{bmatrix}\\) since \\[\nT(\\vec e_1)=\\vectorthree{7}{6}{8},  \n\\qquad\nT(\\vec e_2)=\\vectorthree{3}{2}{4},\n\\qquad\nT(\\vec e_3)=\\vectorthree{-9}{-8}{0},\n\\qquad\nT(\\vec e_4)=\\vectorthree{8}{7}{7}.\n\\] Notice \\(T\\) is a linear transformation from \\(\\mathbb{R}^4\\) to \\(\\mathbb{R}^3\\) and \\(A\\) is a \\(3\\times 4\\) matrix.\n\n\nExample 3.2 Is the transformation \\(T(\\vec{x})=\\vec{v}\\cdot \\vec{x}\\) from \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}\\) a linear transformation? If so, find the matrix of \\(T\\). Let \\(\\vec{v}=\\vectorthree{v_1}{v_2}{v_3}\\). Then \\[\nT(\\vec{x})=\\vec{v}\\cdot \\vec{x}=\\vectorthree{v_1}{v_2}{v_3}\\cdot \\vectorthree{x_1}{x_2}{x_3}=v_1 x_1+v_2 x_2+v_3 x_3 =\n\\begin{bmatrix}v_1 & v_2 & v_3 \\end{bmatrix}\\vec{x}.\\] Therefore, by \\(\\ref{lintrdef}\\), \\(T\\) is a linear transformation with matrix \\[\n\\begin{bmatrix}v_1 & v_2 & v_3 \\end{bmatrix}.\n\\]\n\n\nExample 3.3 Is the transformation \\(T(\\vec{x})=\\vec{v}\\times \\vec{x}\\) from \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}\\) a linear transformation? If so, find the matrix of \\(T\\). Let \\(\\vec{v}=\\vectorthree{v_1}{v_2}{v_3}\\). Then \\[\nT(\\vec{x})=\\vec{v}\\times \\vec{x}=\\vectorthree{v_1}{v_2}{v_3}\\times \\vectorthree{x_1}{x_2}{x_3}\n=\\begin{bmatrix}v_2x_3 -v_3x_2\\\\ v_3x_1-v_1x_3 \\\\ v_1 x_2-v_2x_1 \\end{bmatrix}\n=\\begin{bmatrix}0 & -v_3 & v_2\\\\ v_3 & 0 & -v_1 \\\\ -v_2 & v_1 & 0 \\end{bmatrix}\\vec{x}\n\\] Therefore, by \\(\\ref{lintrdef}\\), \\(T\\) is a linear transformation with matrix \\[\n\\begin{bmatrix}0 & -v_3 & v_2\\\\ v_3 & 0 & -v_1 \\\\ -v_2 & v_1 & 0 \\end{bmatrix}.\n\\]\n\n\nTheorem 3.1  A function \\(T\\) from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\) is a linear transformation if and only if both of the following hold:\n\n\\(T(\\vec v+ \\vec w)=T(\\vec v)+T(\\vec w)\\) for all vectors \\(\\vec v\\) and \\(\\vec w\\) in \\(\\mathbb{R}^m\\), and\n\\(T(k \\vec v)=k T(\\vec v)\\) for all vectors \\(\\vec v\\) in \\(\\mathbb{R}^m\\) and all scalars \\(k\\).\n\n\n\nProof. Suppose \\(T\\) is a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\), then there exists an \\(n\\times m\\) matrix \\(A\\) such that \\(T(\\vec x)=A\\vec x\\) for all \\(\\vec x\\in \\mathbb{R}^m\\). The proof of each part follows.\n\nLet \\(\\vec u, \\vec v\\in \\mathbb{R}^m\\), then \\(T(\\vec v+\\vec w)=A (\\vec v+\\vec w)=A \\vec v+A \\vec w=T(\\vec v)+T(\\vec w)\\).\nLet \\(\\vec v\\in \\mathbb{R}^m\\) and \\(k\\in \\mathbb{R}\\). Then \\(T(k \\vec v)=A(k \\vec v)=(A k) \\vec v=k(A \\vec v)=k T(\\vec v)\\).\n\nNow suppose both (i) and (ii) hold. We need to find a matrix \\(A\\) such that \\(Tx =A \\vec x\\) for all \\(\\vec x\\in \\mathbb{R}^m\\). We can use the standard vectors in \\(\\mathbb{R}^m\\). Then by \\(\\ref{trancol}\\), \\[\\begin{align*}\nT(\\vec x)  & = T(x \\vec e_1+\\cdots + x_m \\vec e_m)\n=T(x_1\\vec e_1)+\\cdots +T(x_m\\vec e_m) \\\\\n& = x_1 T(\\vec e_1)+\\cdots +x_m T(\\vec e_m)\n=\n\\begin{bmatrix} | &  & | \\\\\nT(\\vec e_1) & \\cdots & T(\\vec e_m) \\\\\n| &  & | \\end{bmatrix}\n\\vectorthree{x_1}{\\vdots}{x_m}=A\\vec x\n\\end{align*}\\] as desired.\n\n\nExample 3.4 Write \\(\\vectorthree{-1}{1}{0}\\) as a linear combination of \\(\\vectorthree{3}{-1}{2}\\) and \\(\\vectorthree{1}{0}{1}\\). Let \\(T:\\mathbb{R}^3\\to\\mathbb{R}\\) be a linear transformation with \\(T\\vectorthree{3}{-1}{2}=5\\) and \\(T\\vectorthree{1}{0}{1}=2\\). Find \\(T\\vectorthree{-1}{1}{0}\\). Notice \\[\n\\vectorthree{-1}{1}{0}=\n(-1)\\vectorthree{3}{-1}{2}+2\\vectorthree{1}{0}{1}.\n\\] Therefore, \\[\nT\\vectorthree{-1}{1}{0}\n=T\\left((-1)\\vectorthree{3}{-1}{2}+2\\vectorthree{1}{0}{1}\\right)\n=(-1)\\, T\\vectorthree{3}{-1}{2}+2 \\, T\\vectorthree{1}{0}{1}\n=-1.\n\\]\n\n\nExample 3.5 Let \\(T:\\mathbb{R}^2\\to\\mathbb{R}^2\\) be defined by \\(T\\vectortwo{x_1}{x_2}=\\vectortwo{2x_1}{x_2^2}\\). Is \\(T\\) a linear transformation? Let \\(\\alpha=\\vectortwo{x_1}{x_2}\\) and \\(\\beta=\\vectortwo{y_1}{y_2}\\). Then \\[\\begin{align*}\nT(\\alpha+\\beta) &\n=T\\left(\\vectortwo{x_1}{x_2}+\\vectortwo{y_1}{y_2}\\right)\n=T\\vectortwo{x_1+y_1}{x_2+y_2}\n=\\vectortwo{2(x_1+y_1)}{(x_2+y_2)^2}\n\\end{align*}\\] On the other hand \\[\\begin{align*}\nT(\\alpha)+T(\\beta) &\n=T\\vectortwo{x_1}{x_2}+T\\vectortwo{y_1}{y_2}\n=\\vectortwo{2x_1}{x_2^2} + \\vectortwo{2y_1}{y_2^2}\n= \\vectortwo{2(x_1+y_1)}{x_2^2+y_2^2}\n\\end{align*}\\] Since \\(T(\\alpha+\\beta)\\neq T(\\alpha)+T(\\beta)\\), we use \\(\\ref{thm:linthm}\\) to conclude that \\(T\\) is not linear transformation.\n\n\nTheorem 3.2 Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\).\n\nIf \\(\\vec{0}_m\\) is the zero vector in \\(\\mathbb{R}^m\\), then \\(T(\\vec{0}_m)\\) is the zero vector in \\(\\mathbb{R}^n\\).\nFor all \\(\\vec{v}\\) in \\(\\mathbb{R}^m\\), \\(T(-\\vec{v})=-T(\\vec{v})\\).\nFor all \\(\\vec{u}, \\vec{v}\\) in \\(\\mathbb{R}^m\\), \\(T(\\vec{u}-\\vec{v})=T(\\vec{u})-T(\\vec{v})\\).\nFor all \\(a_1,...,a_n\\in \\mathbb{R}\\) and for all \\(\\vec{v}_1, ...., \\vec{v}_n\\in \\mathbb{R}^m\\), \\[\nT(a_1\\vec{v}_1+a_2\\vec{v}_2+\\cdots + a_n\\vec{v}_n)\n=a_1T(\\vec{v}_1)+a_2T(\\vec{v}_2)+\\cdots+a_nT(\\vec{v}_n).\n\\]\n\n\n\nProof. There proof is for the reader."
  },
  {
    "objectID": "linear-transformations.html#linear-transformations-in-geometry",
    "href": "linear-transformations.html#linear-transformations-in-geometry",
    "title": "3  Linear Transformations",
    "section": "3.2 Linear Transformations in Geometry",
    "text": "3.2 Linear Transformations in Geometry\nNext we give several examples of linear transformations from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\) that are commonly used in plane geometry.\n\nTheorem 3.3 Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). If the matrix of \\(T\\) is of the form \\[\n\\begin{bmatrix} k & 0 \\\\ 0 & k  \\end{bmatrix}\n\\] then \\(T\\) is a scaling transformation. If \\(k>1\\) then the scaling is called a dilation, and is called a contraction when \\(k<1\\).\n\n\nProof. The proof is left for the reader.\n\n\nExample 3.6 Is the linear transformation given by the system of linear equations \\[\n\\left\\{\n\\begin{array}{l}\ny_1=  7x_1 \\\\\ny_2 =  7x_2 \\\\\n\\end{array}\n\\right.\n\\] from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\) a scaling? The answer is yes since the matrix of the linear transformation is \\(\\begin{bmatrix} 7 & 0 \\\\ 0 & 7 \\end{bmatrix}\\), which by definition is a scaling. For example, we can write \\[\nT(\\vec x)=\\begin{bmatrix} 7 & 0 \\\\ 0 & 7 \\end{bmatrix} \\vec x.\n\\] We can use \\(T\\) to dilate the vector \\(\\vectortwo{1}{2}\\) by \\(7\\) to obtain \\[\nT\\vectortwo{1}{2}\n=\\begin{bmatrix} 7 & 0 \\\\ 0 & 7 \\end{bmatrix}\\vectortwo{1}{2}\n=\\vectortwo{7}{14}.\n\\]\n\n\nTheorem 3.4  Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). If the matrix of \\(T\\) is of the form \\[\n\\frac{1}{w_1^2+w_2^2} \\begin{bmatrix} w_1^2 & w_1 w_2 \\\\ w_1 w_2 & w_2^2  \\end{bmatrix}\n\\] then \\(T\\) is an orthogonal projection transformation onto the line \\(L\\) spanned by any nonzero vector \\(\\vec w = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\\) parallel to \\(L\\).\n\n\nProof. Suppose line \\(L\\) is spanned by \\(\\vec w\\). We can decompose any vector \\(\\vec x\\) as \\(\\vec x^{||}+\\vec x^\\perp\\) as diagrammed: Notice \\(\\vec x^\\perp\\) is the perpendicular component so \\[\\begin{equation}\\label{perpeq}\n\\vec w \\cdot \\vec x^\\perp =0\n\\qquad \\text{or equivalently} \\qquad\n\\vec w \\cdot (\\vec x -\\vec x^{||}) =0.\n\\end{equation}\\] To project \\(\\vec x\\) onto the line \\(L\\) we notice \\[\\begin{equation}\\label{projfac}\n\\vec x^{||}=k \\vec w\n\\end{equation}\\] for some scalar \\(k\\). By substitution, of \\(\\ref{projfac}\\) into \\(\\ref{perpeq}\\) and solving for \\(k\\) we obtain \\[\\begin{equation}\\label{facdef}\nk=\\frac{\\vec w \\cdot \\vec x}{\\vec w \\cdot \\vec w}.\n\\end{equation}\\] Using \\(\\ref{projfac}\\) and \\(\\ref{facdef}\\) we define the orthogonal projection of a vector \\(\\vec x\\) onto a given line \\(L\\) as \\[\\begin{equation}\\label{projdef}\n\\text{proj}_L(\\vec x)\n=\\frac{\\vec w \\cdot \\vec x}{\\norm{\\vec w}^2}\\vec w.\n\\end{equation}\\] We would like to have \\(\\ref{projdef}\\) in the form of a matrix. To do so let \\(\\vec w=\\vectortwo{w_1}{w_2}\\) and \\(\\vec x=\\vectortwo{x_1}{x_2}\\). Then from \\(\\ref{facdef}\\) and \\(\\ref{projdef}\\) we find \\[\\begin{align*}\n\\text{proj}_L(\\vec x)\n& = \\frac{1}{\\norm{\\vec w}^2} \\left((x_1 w_1+x_2 w_2)\\vectortwo{w_1}{w_2} \\right) \\\\\n& = \\frac{1}{\\norm{\\vec w}^2} \\left((x_1 w_1\\vectortwo{w_1}{w_2}+x_2 w_2 \\vectortwo{w_1}{w_2} \\right) \\\\\n& = \\frac{1}{\\norm{\\vec w}^2} \\left(\\vectortwo{x_1 w_1^2}{x_1w_1 w_2}+\\vectortwo{x_2w_1w_2}{x_2w_2^2} \\right) \\\\\n& = \\frac{1}{\\norm{\\vec w}^2} \\vectortwo{x_1 w_1^2+x_2w_1w_2}{x_1w_1 w_2+x_2w_2^2}\n= \\frac{1}{\\norm{\\vec w}^2}\n\\begin{bmatrix} w_1^2 & w_1 w_2 \\\\ w_1 w_2 & w_2^2  \\end{bmatrix}\n\\vectortwo{x_1}{x_2}\n.\n\\tag*{ }\n\\end{align*}\\]\n\n\nExample 3.7 Find the matrix \\(A\\) of the orthogonal projection onto the line \\(L\\) spanned by \\(\\vec w = \\begin{bmatrix} 4 \\\\3 \\end{bmatrix}\\) and project the vector \\(\\vec u=\\vectortwo{1}{5}\\) onto the line \\(L\\) spanned by \\(\\vec w\\). By \\(\\ref{orthproj}\\), the matrix is \\[\nA=\\frac{1}{w_1^2+w_2^2}\n\\begin{bmatrix}\nw_1^2 & w_1 w_2 \\\\\nw_1 w_2 & w_2^2\n\\end{bmatrix}\n=\\frac{1}{25}\n\\begin{bmatrix}\n16 & 12 \\\\\n12 & 9\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n16/25 & 12/25 \\\\\n12/25 & 9/25\n\\end{bmatrix}\n.\n\\] For example, we can project the vector \\(\\vec u\\) onto the line \\(L\\) spanned by \\(\\vec w\\). The matrix \\(A\\) defines this linear transformation \\(T\\) and so to project onto the line \\(L\\) is just matrix multiplication: \\[\nT\\vectortwo{1}{5}=\\frac{1}{25}\n\\begin{bmatrix}\n16 & 12 \\\\\n12 & 9\n\\end{bmatrix}\n\\vectortwo{1}{5}\n=\\vectortwo{76/25}{57/25}.\n%\\approx \\vectortwo{3.04}{2.28}.\n\\tag*{ }\n\\]\n\n\nTheorem 3.5  Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). If the matrix of \\(T\\) is of the form \\[\n\\begin{bmatrix} 2 u_1^2-1 & 2 u_1 u_2 \\\\ 2 u_1 u_2 & 2 u_2^2 -1   \\end{bmatrix}\n\\] then \\(T\\) defines a reflection transformation about the line \\(L\\), where \\(\\vec u = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}\\) is any unit vector lying on \\(L\\).\n\n\nProof. Suppose we want to reflect \\(\\vec x\\) through the line \\(L\\) as diagrammed: Then \\[\\begin{equation}\\label{ref1}\n\\text{ref}_L(\\vec x)=\\vec x^{||}-\\vec x^\\perp  \n\\end{equation}\\] and \\[\\begin{equation}\\label{ref2}\n\\text{proj}_L(\\vec x)=\\vec x^{||}.\n\\end{equation}\\] By subtracting \\(\\ref{ref2}\\) from \\(\\ref{ref1}\\), we obtain \\[\\begin{equation}\\label{ref3}\n\\text{ref}_L(\\vec x)=2 \\text{proj}_L(\\vec x)-\\vec x.\n\\end{equation}\\] For simplicity assume \\(L\\) is a any line that passes through the origin and let \\(\\vec u\\) be a unit vector \\(\\vec u = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}\\) lying on \\(L\\). Notice \\(\\ref{projdef}\\) in the special case of a unit vector \\(\\vec u\\) becomes \\(\\text{proj}_L(\\vec x)=(\\vec u\\cdot \\vec x)\\vec u.\\) Then \\[\\begin{align*}\n\\text{ref}_L(\\vec x)& =2 \\text{proj}_L(\\vec x)-\\vec x\n=2(\\vec u\\cdot \\vec x)\\vec u-\\vec x \\\\\n& = 2(u_1x_1+u_2x_2)\\begin{bmatrix} u_1 \\\\ u_2  \\end{bmatrix}-\\begin{bmatrix} x_1 \\\\ x_2  \\end{bmatrix}\n=2u_1 x_1 \\begin{bmatrix} u_1 \\\\ u_2  \\end{bmatrix}+2u_2x_2 \\begin{bmatrix} u_1 \\\\ u_2  \\end{bmatrix}-\\begin{bmatrix} x_1 \\\\ x_2  \\end{bmatrix} \\\\\n& = \\begin{bmatrix}  \n2u_1^2x_1+2u_1 u_2x_2-x_1 \\\\ 2u_1u_2x_1+2u_2^2 u_2x_2-x_2\n\\end{bmatrix}\n= \\begin{bmatrix}  \n2u_1^2-1 & 2u_1 u_2\\\\\n2u_1 u_2 & 2u_2^2-1\n\\end{bmatrix}\n\\vectortwo{x_1}{x_2}.\n\\tag*{ }\n\\end{align*}\\]\n\n\nExample 3.8 Find the matrix \\(A\\) of a reflection through the line through the origin spanned by \\(\\vec w = \\begin{bmatrix} 4 \\\\3 \\end{bmatrix}\\) and use it to reflect \\(\\begin{bmatrix} 1 \\\\5 \\end{bmatrix}\\) about the line \\(L\\). Let \\(\\vec u=\\vectortwo{4/5}{3/5}\\). We notice \\(\\vec u\\) is a unit vector, since \\(\\norm{u}=1\\). Then, by \\(\\ref{reflmatrix}\\), the matrix we seek is \\[\nA=\\begin{bmatrix}\n7/25 & 24/25 \\\\ 24/25 & -7/25\n\\end{bmatrix}.\n\\] We can reflect the vector \\(\\begin{bmatrix} 1 \\\\5 \\end{bmatrix}\\) about the line \\(L\\) using matrix multiplication \\[\nT \\begin{bmatrix} 1 \\\\5  \\end{bmatrix}\n=\n\\frac{1}{25}\n\\begin{bmatrix}\n7 & 24 \\\\ 24 & -7\n\\end{bmatrix}\n\\begin{bmatrix} 1 \\\\5  \\end{bmatrix}\n=\\vectortwo{127/25}{-11/25}.\n%\\approx \\vectortwo{5.08}{-0.44}.\n\\tag*{ }\n\\]\n\n\nTheorem 3.6  Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). If the matrix of \\(T\\) is of the form \\[\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta  \\\\\n\\sin \\theta & \\cos \\theta  \\end{bmatrix}\n\\] then \\(T\\) is a (counterclockwise) rotation transformation through an angle \\(\\theta\\).\n\n\nProof. If a vector \\(\\vec x=\\vectortwo{x_1}{x_2}\\) is rotated through an angle of \\(\\pi/2\\), then a vector \\(\\vec y=\\vectortwo{-x_2}{x_1}\\) is obtained, via \\(\\vec x\\cdot \\vec y =\\vec 0\\). More generally, if we rotate (counterclockwise) a given \\(\\vec x\\) through an angle \\(\\theta\\) we determine \\[\\begin{align*}\nT(\\vec x)&=(\\cos \\theta) \\vec x+(\\sin\\theta) \\vec y\n=(\\cos \\theta)\\vectortwo{x_1}{x_2}+(\\sin \\theta)\\vectortwo{-x_2}{x_1} \\\\\n&=\\vectortwo{(\\cos \\theta)x_1-(\\sin\\theta))x_2}{(\\sin \\theta)x_1-(\\cos\\theta))x_2}\n=\\begin{bmatrix} \\cos \\theta & -\\sin \\theta  \\\\ \\sin \\theta & \\cos \\theta  \\end{bmatrix} \\vectortwo{x_1}{x_2}\n=\\begin{bmatrix} \\cos \\theta & -\\sin \\theta  \\\\ \\sin \\theta & \\cos \\theta  \\end{bmatrix} \\vec x\n\\end{align*}\\] as seen from the diagram.\n\n\nExample 3.9 Find the matrix of the linear transformation that rotates the vector \\(\\begin{bmatrix} 4 \\\\ 2 \\end{bmatrix}\\) by 30 degrees counterclockwise. By \\(\\ref{rotmatrix}\\), the matrix of this transformation is \\(A=\\begin{bmatrix} \\sqrt{3}/2 & -1/2 \\\\ 1/2 & \\sqrt{3}/2 \\end{bmatrix}.\\) We will use matrix multiplication to perform the transformation \\[\nT\\vectortwo{4}{2}\n= \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & \\frac{-1}{2} \\\\\n\\frac{1}{2} & \\frac{\\sqrt{3}}{2}\n\\end{bmatrix}\n\\vectortwo{4}{2}\n=\\cos 30^\\circ \\vectortwo{4}{2}+\\sin 30^\\circ \\vectortwo{4}{2}\n=\\vectortwo{2\\left(\\sqrt{3}+1\\right)}{\\sqrt{3}+1}.\n\\approx \\vectortwo{2.46}{3.73}\n\\]\n\n\nTheorem 3.7  Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). If the matrix of \\(T\\) is of the form \\[\n\\begin{bmatrix} 1 & 0  \\\\ k & 1  \\end{bmatrix}\n\\qquad \\text{or} \\qquad\n\\begin{bmatrix} 1 & k  \\\\ 0 & 1  \\end{bmatrix},\n\\] where \\(k\\) is any constant, then \\(T\\) defines a vertical shear or horizontal shear transformation, respectively.\n\n\nProof. The proof is left for the reader.\n\n\nExample 3.10 Given the vector \\(\\begin{bmatrix} 2 \\\\3 \\end{bmatrix}\\) in \\(\\mathbb{R}^2\\) show geometrically a vertical shear of 2 and a horizontal shear of \\(\\frac{1}{2}\\). By \\(\\ref{shearmatrix}\\), we can apply these linear transformations using matrix multiplication by using the matrices \\(\\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\) and \\(\\begin{bmatrix} 1 & 1/2 \\\\ 0 & 1 \\end{bmatrix}\\). \\[\n\\label{verticalshear}\n\\text{Vertical Shear: \\quad }  T\\begin{bmatrix} 2 \\\\3  \\end{bmatrix}\n= \\begin{bmatrix} 1 & 0  \\\\ 2 & 1  \\end{bmatrix} \\begin{bmatrix} 2 \\\\3  \\end{bmatrix} = \\begin{bmatrix} 2 \\\\7  \\end{bmatrix}\n\\] \\[\n\\label{horizontalshear}\n\\text{Horizontal Shear: \\quad } T\\begin{bmatrix} 2 \\\\3  \\end{bmatrix}\n= \\begin{bmatrix} 1 & 1/2  \\\\ 0 & 1  \\end{bmatrix} \\begin{bmatrix} 2 \\\\3  \\end{bmatrix} =\\begin{bmatrix} 7/2 \\\\3  \\end{bmatrix}\n\\]\n\n\nExample 3.11 Interpret the linear transformation \\(T(\\vec x)=\\begin{bmatrix} 0 & -1 \\\\ -1 & 0 \\end{bmatrix}\\vec x\\) geometrically. The transformation has a matrix of the form \\[\n\\begin{bmatrix} 2u_1^2-1 & 2 u_1 u_2 \\\\ 2u_1 u_2 & 2u_2^2-1 \\end{bmatrix}\n\\] where \\(u_1=\\sqrt{2}/2\\) and \\(u_2=-\\sqrt{2}/2\\) since \\(2u_1^2-1=0\\), \\(2u_2^2-1=0\\), and \\(2u_1 u_2=-1\\). Since \\(|| \\vec u ||=1\\) and \\(\\vec u\\) lies on the line \\(y=x\\), then matrix \\(\\begin{bmatrix} 0 & -1 \\\\ -1 & 0 \\end{bmatrix}\\) represents the linear transformation which is a reflection through the line \\(y=x\\).\n\n\\[\\begin{align*}\n\\text{scaling} \\quad &\n\\begin{bmatrix} k & 0 \\\\ 0 & k  \\end{bmatrix} &\n\\text{shears} \\quad &  \n\\begin{bmatrix} 1 & 0  \\\\ k & 1  \\end{bmatrix}\n\\text{ or }\n\\begin{bmatrix} 1 & k  \\\\ 0 & 1  \\end{bmatrix} \\\\\n\\text{reflection} \\quad &\n\\begin{bmatrix} 2 u_1^2-1 & 2 u_1 u_2 \\\\ 2 u_1 u_2 & 2 u_2^2 -1   \\end{bmatrix} &\n\\text{rotation} \\quad &\n\\begin{bmatrix} \\cos \\theta & -\\sin \\theta  \\\\ \\sin \\theta & \\cos \\theta  \\end{bmatrix} &  \\\\\n\\text{orthogonal projection} \\quad &\n\\frac{1}{w_1^2+w_2^2} \\begin{bmatrix} w_1^2 & w_1 w_2 \\\\ w_1 w_2 & w_2^2  \\end{bmatrix} &\n\\end{align*}\\]\n\nExample 3.12 Interpret the linear transformation \\(T(\\vec x)=\\begin{bmatrix} 1 & 1 \\\\ -1 & 1 \\end{bmatrix} \\vec x\\), geometrically. Explain. This is a rotation combined with a scaling. The transformation rotates 45 degrees counterclockwise and has a scaling factor of \\(\\sqrt{2}\\)."
  },
  {
    "objectID": "linear-transformations.html#introduction-to-linear-maps",
    "href": "linear-transformations.html#introduction-to-linear-maps",
    "title": "3  Linear Transformations",
    "section": "3.3 Introduction to Linear Maps",
    "text": "3.3 Introduction to Linear Maps\n\nDefinition 3.2 Let \\(V\\) and \\(W\\) be linear spaces. A function \\(T\\) from \\(V\\) to \\(W\\) is called a linear map if \\[\nT(f+g)=T(f)+T(g)\n\\qquad \\text{and}\\qquad\nT(k f)=k T(f)\n\\] for all elements \\(f\\) and \\(g\\) of \\(V\\) and for all scalars \\(k\\).\n\nThe collection of all linear maps from \\(V\\) to \\(W\\) is denoted by \\(\\mathcal{L}(V,W)\\) and if \\(T\\) is a linear map from \\(V\\) to \\(W\\) we denote this by \\(T\\in \\mathcal{L}(V,W)\\).\n\nDefinition 3.3 Let \\(T\\in \\mathcal{L}(V,W)\\).\n\nThen the kernel of \\(T\\) is the subset of \\(V\\) consisting of the vectors that \\(T\\) maps to 0; that is\n\\(\\ker(T)=\\{v\\in V \\, | \\, T v=0\\}.\\)\nThen the image of \\(T\\) is the subset of \\(W\\) consisting of the vectors of the form \\(Tv\\) for some \\(v\\in V\\); that is \\(\\text{im}(T)=\\{w\\in W \\, | \\, \\text{ there exists } v \\in V \\text{ such that } w=T v\\}.\\)\n\n\n\nExample 3.13 The function from \\(C^{\\infty}\\) to \\(C^{\\infty}\\) defined by \\(T(f)=f''\\) is a linear map. Find the kernel and image of \\(T\\).\n\n\nExample 3.14 The function from \\(C^{\\infty}\\) to \\(C^{\\infty}\\) defined by $T(f)=_0^1 f(x) , dx $ is a linear map. Find the kernel and image of \\(T\\).\n\n\nTheorem 3.8 If \\(T \\in \\mathcal{L}(V,W),\\) then null \\(T\\) is a subspace of \\(V\\).\n\n\nProof. By definition, \\(\\text{ker} T=\\{v\\in V \\mid T v=0\\}\\) and so \\(\\text{ker} T \\subseteq W\\). Let \\(u, v\\in \\text{ker} T\\). Then, \\(T(u+v)=T u+ Tv=0+0=0\\) which shows, \\(u+v\\in \\text{ker} T\\), for all \\(u , v \\in \\text{ker} T\\). Let \\(k\\) be a scalar and \\(u\\in \\text{ker} T\\). Then \\(T(k u)=k T u= k(0)=0\\), which shows, \\(k u\\in \\text{ker} T\\) for all scalars \\(k\\) and all \\(u\\in \\text{ker} T\\). Since \\(T(0)=T(0+0)=T(0)+T(0)\\), \\(T(0)=0\\) in \\(W\\) which shows, \\(0\\in \\text{ker} T\\). Therefore, \\(\\text{ker} T\\) is a subspace of \\(W\\).\n\n\nDefinition 3.4 A linear map \\(T: V\\rightarrow W\\) is called injective whenever \\(u, v\\in V\\) and \\(T u=T v\\), we have \\(u=v\\).\n\n\nTheorem 3.9 Let \\(T\\in \\mathcal{L}(V,W)\\), then \\(T\\) is injective if and only if null \\(T=\\{0\\}\\).\n\n\nProof. Suppose \\(T\\) is injective. Since \\(T(0)=0\\), \\(0\\in \\text{ker} T\\) and so \\(\\{0\\}\\subseteq \\text{ker} T\\). Let \\(v\\in \\text{ker} T\\). Then \\(T(0)=0=T(v)\\) yields \\(v=0\\) because \\(T\\) is injective. Thus, \\(\\text{ker} T\\subseteq \\{0\\}\\). Therefore, \\(\\text{ker} T=\\{0\\}\\). Conversely, assume \\(\\text{ker} T=\\{0\\}\\). Let \\(u, v\\in V\\). If \\(Tu=Tv\\), then \\(T(u-v)=Tu - T v=0\\) which shows \\(u-v \\in \\text{ker} T\\). Thus \\(u=v\\) and therefore, \\(T\\) is injective.\n\n\nDefinition 3.5 For \\(T\\in \\mathcal{L}(V,W)\\), the image of \\(T\\), denoted by \\(\\text{im}(T)\\), is the subset of \\(W\\) consisting of those vectors that are of the form \\(T v\\) for some \\(v\\in V\\).\n\n\nDefinition 3.6 A linear map \\(T: V\\rightarrow W\\) is called surjective if its range equals \\(W\\).\n\n\nTheorem 3.10 If \\(T\\in \\mathcal{L}(V,W)\\), then \\(\\text{im} T\\) is a subspace of \\(W\\).\n\n\nProof. By definition, \\(\\text{im} T=\\{T v \\mid v\\in V\\}\\subseteq W\\). Let \\(w_1,w_2\\in W \\in \\text{im} T\\). Then there exists \\(v_1,v_2\\in V\\) such that \\(w_1=Tv_1\\) and \\(w_2=T v_2\\). By linearity of \\(T\\), \\(w_1+w_2=Tv_1+Tv_2=T(v_1+v_2)\\) which shows \\(w_1+w_2\\in \\text{im} T\\) for all \\(w_1,w_2 \\in \\text{im} T\\). Let \\(w\\in \\text{im} T\\) and let \\(k\\) be a scalar. Then there exists \\(v\\in V\\) such that \\(w=T v\\). By linearity of \\(T\\), $ kw =k Tv =T(k v)$ which shows \\(kw\\in \\text{im} T\\) for all \\(w\\in \\text{im} T\\) and for all scalars \\(k\\). Therefore, \\(\\text{im} T\\) is a subspace of \\(W\\).\n\n::: {#thm- } [Rank-Nullity Theorem] If \\(V\\) is finite-dimensional and \\(T\\in \\mathcal{L}(V,W)\\), then range \\(T\\) is finite-dimensional and \\(\\text{dim} V= \\text{dim} (\\text{im} T) + \\text{dim} (\\text{ker} T)\\). :::\n\nProof. Since \\(V\\) is a finite-dimensional and \\(\\text{ker} T\\) is a subspace of \\(V\\), \\(\\text{ker} T\\) is finite-dimensional and so let \\((u_1,\\ldots,u_n)\\) be a basis of \\(\\text{ker} T\\). Since \\((u_1,\\ldots,u_n)\\) is linearly independent in \\(V\\), it can be extended to a basis of \\(V\\), say \\(\\mathcal{B}=(u_1,\\ldots,u_n,v_1,\\ldots,v_m)\\). It suffices to show \\((T v_1,\\ldots, T v_m)\\) is a basis of \\(\\text{im} T\\), for then \\(\\text{im} T\\) is finite-dimensional is proven and \\(\\text{dim} V=n+m=\\text{dim} \\text{ker} T+\\text{dim} \\text{im} T\\) holds as well.\nLet \\(w\\in \\text{im} T\\). Then there exists \\(v\\in V\\) and scalars \\(a_1,\\ldots,a_n, b_1,\\ldots,b_m\\) such that \\[w=Tv=T(a_1 u_1+\\cdots + a_n u_n+b_1 v_1+\\cdots + b_m v_m)=b_1 T v_1+\\cdots + b_m T v_m.\\] Thus \\((T v_1,\\ldots,T v_m)\\) spans \\(\\text{im} T\\). Suppose \\(c_1,\\ldots,c_m\\) are scalars such that \\(c T v_1+\\cdots + c_m T v_m=0\\). Then there exist scalars \\(d_1,\\ldots,d_n\\) such that \\[c_1 v_1+\\cdots + c_m v_m=d_1 u_1+\\cdots +d_n u_n.\\] Since \\(\\mathcal{B}\\) is a basis of \\(V\\) and \\(c_1 v_1+\\cdots c_m v_m+(-d_1) u_1 + \\cdots +(-d_n)u_n=0\\) it follows \\(c_1=\\cdots = c_m = d_1 = \\cdots = \\cdots =d_n=0\\). In particular, \\(c_1=\\cdots = c_m=0\\) shows \\(T v_1,\\ldots, T v_m\\) are linearly independent. Therefore, \\((T v_1,\\ldots, T v_m)\\) is a basis fo \\(\\text{im} T\\).\n\n\nCorollary 3.1 If \\(V\\) and \\(W\\) are finite-dimensional vector spaces with \\(T\\in \\mathcal{L}(V,W)\\), then\n\n\\(\\text{dim} V > \\text{dim} W \\implies T\\) is not injective, and\n\\(\\text{dim} V < \\text{dim} W \\implies T\\) is not surjective.\n\n\n\nProof. The proof of each part follows.\n\nBy the Rank-Nullity Theorem, \\(\\text{dim} V=\\text{dim} \\text{ker} T+ \\text{dim} \\text{im} T \\leq \\text{dim} \\text{ker} T +\\text{dim} W.\\) Since $ V > W $, it follows \\(0<\\text{dim} V-\\text{dim} W\\leq \\text{dim} \\text{ker} T.\\) Thus, \\(\\text{ker} T \\neq \\{0\\}\\) and so \\(T\\) is not injective.\nAgain by the Rank-Nullity Theorem, \\(\\text{dim} V=\\text{dim} \\text{ker} T+ \\text{dim} \\text{im} T\\) and so \\(\\text{dim} \\text{im} T\\leq \\text{dim} V\\). Since $ V < W $, \\(0<\\text{dim} W-\\text{dim} V\\leq \\text{dim} W-\\text{dim} \\text{im} T\\) and so there exists a nonzero element \\(w\\in W\\) with \\(w\\in \\text{im} T\\). Therefore, \\(T\\) is not surjective.\n\n\n\nExample 3.15 Suppose that \\(T\\) is a linear map from \\(V\\) to \\(\\mathbb{F}\\). Prove that if \\(u \\in V\\) is not in \\(\\text{ker} T\\), then \\(V=\\text{ker} T \\, \\oplus \\{a u:a\\in F\\}\\). Let \\(U=\\{a u \\mid a\\in \\mathbb{F}\\}\\). The following arguments show \\(V=\\text{ker} T +U\\) and \\(\\text{ker} T \\cap U=\\{0\\}\\), respectively.\n\nLet \\(v\\in V\\) with \\(Tv=b\\). Since \\(Tu\\neq 0\\) there is a \\(u_1 \\in U\\) such that \\(T u_1=b\\). Then we can write \\(v=u_1+(v-u_1)\\), and \\(v-u_1\\in \\text{ker} T\\). This gives \\(V=\\text{ker} T+ U\\).\nLet \\(v\\in \\text{ker} T \\cap U\\), then there exists \\(a \\in \\mathbb{F}\\) such that \\(v=a u\\) and so \\(T(v)=a T u=0\\) since \\(T v\\in \\text{ker} T\\). Thus \\(a=0\\) and so \\(v=0\\) meaning \\(\\text{ker} T \\cap U\\subseteq \\{0\\}\\). Since \\(0 \\in \\text{ker} T \\cap U\\), it follows \\(\\text{ker} T \\cap U= \\{0\\}\\).\n\n\n\nExample 3.16 Suppose that \\(T\\in \\mathcal{L}(U,W)\\) is injective and \\((v_1,\\ldots,v_n)\\) is linearly independent in \\(V\\). Prove that \\((T v_1,\\ldots,T v_n)\\) is linearly independent in \\(W\\). Suppose \\(a_1 T v_1+ \\cdots a_n T v_n=0\\) in \\(W\\) where \\(a_1,\\ldots,a_n \\in \\mathbb{F}\\). Then by linearity of \\(T\\), \\(T(a_1 v_1 + \\cdots + a_n v_n)=0\\). Since \\(T(0)\\) and \\(T\\) is injective, \\(a_1 v_1+\\cdots + a_n v_n=0\\). Since \\((v_1,\\ldots,v_n)\\) is linearly independent \\(a_1=\\cdots = a_n =0\\). Therefore, \\((T v_1,\\ldots, T v_n)\\) is linearly independent.\n\n\nExample 3.17 Show that every linear map from a one-dimensional vector space to itself is multiplication by some scalar. More precisely, prove that if \\(\\text{dim} V=1\\) and \\(T\\in \\mathcal{L}(V,V)\\), then there exists \\(a\\in \\mathbb{F}\\) such that \\(T v=a v\\) for all \\(v\\in V\\). Let \\(\\{w\\}\\) be a basis of \\(V\\), and let \\(v\\in V\\). Then there exists \\(c\\in \\mathbb{F}\\) such that \\(v=c w\\). Applying \\(T\\) yields, \\[\nT v=T(c w)=cTw=c(a w)=(ca)w=a(cw)=a v\n\\] where \\(Tw=aw\\) since \\(Tw\\in V\\) and \\(\\{w\\}\\) is a basis of \\(V\\).\n\n\nExample 3.18  linear extension Suppose that \\(V\\) is finite-dimensional. Prove that any linear map on a subspace of \\(V\\) can be extended to a linear map on \\(V\\). In other words, show that if \\(U\\) is a subspace of \\(V\\) and \\(S\\in \\mathcal{L}(U,W)\\), then there exists \\(T\\in \\mathcal{L}(V,W)\\) such that \\(Tu=Su\\) for all \\(u\\in U\\). Let \\((u_1,\\ldots,u_n)\\) be a basis of \\(U\\) and extend this basis of \\(U\\) to a basis of \\(V\\), say \\((u_1,\\ldots,u_n, v_1,\\ldots,v_m)\\). Define \\(T\\) as the linear extension of \\(S\\), as follows \\[\nT(u_i)=S(u_i) \\text{ for } 1\\leq i \\leq n \\hspace{.5cm} \\text{ and } \\hspace{.5cm}  T(v_j)=v_j \\text{ for } 1\\leq j \\leq m.\n\\] Then for all \\(u\\in U\\), \\[\\begin{align*}\nT(u) & =T(a_1 u_1+\\cdots + a_n u_n)\n=a_1 T u_1+\\cdots a_n T u_n \\\\\n& =a_1 S u_1+\\cdots +a_n S u_n\n=S(a_1u_1+\\cdots a_n u_n)\n=S(u)\n\\end{align*}\\] where \\(a_1,\\ldots,a_n\\in \\mathbb{F}\\). By definition of \\(T\\), \\(T\\in \\mathcal{L}(V,W)\\).\n\n\nExample 3.19 Prove that if \\(S_1,\\ldots,S_n\\) are injective linear maps such that \\(S_1 \\cdots S_n\\) makes sense, then \\(S_1 \\cdots S_n\\) is injective. Suppose \\(v\\) and \\(w\\) are any vectors and \\((S_1\\cdots S_n)v=(S_1\\cdots S_n)w\\). Then by definition of composition, \\(S_1(S_2\\cdots S_n)v=S_1(S_2\\cdots S_n)w\\), and since \\(S_1\\) is injective \\(S_2(S_3\\cdots S_n)v=S_2(S_3\\cdots S_n)w\\). Since \\(S_2,\\ldots,S_n\\) are all injective \\(v=w\\) as desired showing \\(S_1\\cdots S_n\\) is injective.\n\n\nExample 3.20 Prove that if \\((v_1,\\ldots,v_n)\\) spans \\(V\\) and \\(T\\in \\mathcal{L}(V,W)\\) is surjective, then \\((T v_1,\\ldots,T v_n)\\) spans \\(W\\). Let \\(w\\in W\\). Then, since \\(T\\) is surjective, there exists \\(v\\in V\\) such that \\(T(v)=w\\). Since \\((v_1,\\ldots,v_n)\\) spans \\(V\\) there exists scalars \\(a_1,\\ldots,a_n\\) such that \\(v=a_1 v_1+ \\dots a_n v_n\\) and by linearity of \\(T\\), \\(T(v)=a_1 T v_1+ \\cdots a_n T v_n=w.\\) Therefore, \\((T v_1,\\ldots,T v_n)\\) spans \\(W\\) because every element \\(w\\) in \\(W\\) is a linear combination of the \\(T v\\)’s.\n\n\nExample 3.21 Suppose that \\(V\\) is finite-dimensional and that \\(T\\in \\mathcal{L}(V,W)\\). Prove that there exists a subspace \\(U\\) of \\(V\\) such that \\(U\\cap \\text{ker} T=\\{0\\}\\) and range \\(T = \\{T u : u\\in U\\}\\). Since \\(V\\) is finite-dimensional so is \\(\\text{ker} T\\). Let \\((v_1,\\ldots,v_n)\\) be a basis of \\(\\text{ker} T\\) and extend this basis to a basis of \\(V\\), namely let \\(\\mathcal{B}=(v_1,\\ldots,v_n,u_1,\\ldots,u_m)\\) be a basis of \\(V\\). Let \\(U=\\text{span} (u_1,\\ldots,u_m)\\), we will show \\(U\\cap \\text{ker} T=\\{0\\}\\) and range \\(T = \\{T u : u\\in U\\}\\). Clearly, \\(\\{0\\}\\subseteq U\\cap \\text{ker} T\\) since both \\(U\\) and \\(\\text{ker} T\\) are subspaces. Let \\(v\\in U\\cap \\text{ker} T\\). Then \\(v\\in U\\) implies there exists \\(a_1,\\ldots,a_m\\) such that \\(v=a_1 u_1+\\cdots +a_m u_m\\). Then \\(v\\in \\text{ker} T\\) implies there exists \\(b_1,\\ldots,b_n\\) such that \\(v=b_1 v_1+\\cdots +b_n v_n\\). Then \\(a_1 u_1+\\cdots +a_m u_m+(-b_1)v_1+\\cdots +(-b_n)v_n=0\\) and since \\(\\mathcal{B}\\) is a basis of \\(V\\) the \\(u\\)’s and \\(v\\)’s are linearly independent and so \\(a_1=\\cdots =a_m=b_1=\\cdots =b_n=0\\) meaning \\(v=0\\); and so \\(\\text{ker} T = \\{0\\}\\). Let \\(v\\in V\\). Then \\(T(v)=a_1T v_1+\\cdots +a_n T v_n+b_1 T u_1+\\cdots + b_m T u_m=b_1 Tu_1+\\cdots +b_m T u_m\\) showing \\(\\{T v : v\\in T\\}\\subseteq \\{T v : v\\in U\\}\\) conversely is obvious, since \\(U\\subseteq V\\).\n\n\nExample 3.22 Prove that if there exists a linear map on \\(V\\) whose null space and range are both finite-dimensional, then \\(V\\) is finite-dimensional.\n\n\nExample 3.23 Suppose that \\(V\\) and \\(W\\) are finite-dimensional and \\(T\\in \\mathcal{L}(V,W)\\). Prove that there exists a surjective linear map from \\(V\\) onto \\(W\\) if and only if \\(\\text{dim} W \\leq \\text{dim} V\\). Suppose \\(T\\) is a linear map of \\(V\\) onto \\(W\\), then \\(\\text{dim} V=\\text{dim} \\text{ker} T+\\text{dim} \\text{im} T\\). Since \\(\\text{dim} \\text{ker} T \\geq 0\\), \\(\\text{dim} V\\geq \\text{dim} \\text{im} T=\\text{dim} W\\), since \\(T\\) is onto. Conversely, assume \\(m=\\text{dim} W\\leq \\text{dim} V=n\\) with bases of \\(W\\) say \\((w_1,\\ldots,w_m)\\) and of \\(V\\) say \\((v_1,\\ldots,v_n)\\). Define \\(T\\) to be the linear extension of: \\[\n\\begin{cases}\nT(v_i)=w_i & \\text{ if } 1\\leq i\\leq m \\\\\nT(v_i)=0 & \\text{ if } i>m\n\\end{cases}\n\\] Then \\(T\\) is surjective since: if \\(w\\in W\\) then there exists \\(a_i\\in \\mathbb{F}\\) such that \\(w=a_1 w_1+\\cdots + a_m w_m=a_1 T(v_1)+\\cdots + a_m T(v_m)=T(a_1 v_1+\\cdots + a_m v_m)\\) showing every element in \\(W\\) is in \\(\\text{im} T\\).\n\n\nExample 3.24 Suppose that \\(W\\) is finite-dimensional and \\(T\\in \\mathcal{L}(V,W)\\). Prove that \\(T\\) is injective if and only if there exists \\(S\\in \\mathcal{L}(W,V)\\) such that \\(S T\\) is the identity map on \\(V\\). Suppose \\(S\\in\\mathcal{L}(W,V)\\) and \\(ST\\) is the identity on \\(V\\). Then \\(\\text{ker} T\\subseteq \\text{ker} ST=\\{0\\}\\) and so \\(\\text{ker} T=\\{0\\}\\). Therefore, \\(T\\) is injective. Conversely, suppose \\(T\\) is injective. So we can define \\(S_1\\in\\mathcal{L}(W,V)\\) by the following: \\(S_1(w)=v\\) where \\(v=T^{-1}(w)\\) (since \\(T\\) is injective). So \\(S_1:\\text{im} T \\rightarrow V\\) and since \\(W\\) is finite-dimensional, by \\(\\ref{Linear Extension}\\), \\(S_1\\) can be extended to \\(S: W\\rightarrow V\\) and by definition of \\(S_1\\), if \\(v\\in V\\), then \\(ST(v)=S(Tv)=T^{-1}(Tv)=v\\).\n\n\nExample 3.25 Suppose that \\(V\\) is finite-dimensional and \\(T\\in \\mathcal{L}(V,W)\\). Prove that \\(T\\) is surjective if and only if there exists \\(S\\in \\mathcal{L}(W,V)\\) such that \\(T S\\) is the identity map on \\(W\\). We will present two proofs.\n\nSuppose \\(S\\in \\mathcal{L}(W,V)\\) and \\(TS=I_W\\). Let \\(w\\in W\\). Then \\(v=S(w)\\in V\\) is such that \\(T(v)=TS(w)=w\\) and therefore \\(T\\) is surjective. Conversely, suppose \\(T\\) is surjective. Since \\(V\\) is a finite-dimensional vector space and \\(T\\) is a linear map, \\(W\\) is finite-dimensional. Let \\((v_1,\\ldots,v_n)\\) be a basis for \\(V\\). Since \\(T\\) is surjective, \\((T v_1,\\ldots,T v_n)\\) spans \\(W\\). Also since \\(T\\) is surjective \\(\\text{dim} W\\leq \\text{dim} V=n\\) Any spanning set reduced to a basis say \\((T v_1,\\ldots,T v_m)\\) is a basis of \\(W\\) where \\(m\\leq n\\). Define \\(S\\) as the linear extension of \\(S(T v_i)=v_i\\) for each \\(1\\leq i \\leq m\\). Then, for all \\(w\\in W\\), \\(S(w)=S(a_1 T v_1+\\cdots +a_m T v_m)=a_1 ST v_1+\\cdots + a_m ST v_m=a_1 v_1+\\cdots + a_m v_m=w\\) for scalars \\(a_1,\\ldots,a_m\\in \\mathbb{F}\\), and so \\(TS=I_W\\).\nSuppose \\(T\\) is surjective. Using \\(\\ref{Null Space Decomposition}\\), there exists a subspace \\(U\\) of \\(V\\) such that \\(U\\cap \\text{ker} T=\\{0\\}\\) and \\(\\text{im} T=\\{T u : u\\in U\\}\\). Define \\(T_1: U\\rightarrow W\\) by \\(T_1 u=T u\\) for \\(u\\in U\\). Notice \\(T_1\\) is injective and surjective and so \\(T_1\\) has an inverse. Define \\(S=T_1^{-1}\\) we have \\(TS w= T_1 T_1^{-1}w=w\\) for all \\(w\\in W\\).\n\n\nA linear map \\(T\\in \\mathcal{L}(V,W)\\) is called invertible if there exists a linear map \\(S\\in \\mathcal{L}(W,V)\\) such that \\(S T\\) equals the identity map on \\(V\\) and \\(TS\\) equal the identity map on \\(W\\). Given \\(T\\in\\mathcal{L}(V,W)\\). A linear map \\(S\\in\\mathcal{L}(W,V)\\) satisfying \\(S T=I\\) and \\(T S=I\\) is called an inverse of \\(T\\). Two vector spaces are called isomorphic if there is an invertible linear map from one vector space onto the other one.\n\nTheorem 3.11 If \\(\\mathcal{B}=(f_1,\\ldots,f_n)\\) is a basis of a linear space \\(V\\), then the coordinate transformation \\[\nL_{\\mathcal{B}}(f) = [f]_{\\mathcal{B}}\n\\] from \\(V\\) to \\(\\mathbb{R}^n\\) is an isomorphism. Thus any linear space \\(V\\) is isomorphic to \\(\\mathbb{R}^n\\).\n\n\nProof. The proof if left for the reader.\n\nAn invertible linear transformation \\(T\\) is called an isomorphism. Recall the rank-nullity theorem states that if \\(V\\) and \\(W\\) are finite-dimensional vector spaces and \\(T\\) is a linear transformation from \\(V\\) to \\(W\\) then \\(\\text{dim} V=\\text{dim} \\ker T+\\text{dim} \\text{im} T\\) where \\(\\text{dim} \\ker T\\) is called the nullity and \\(\\text{dim} \\text{im} T\\) is called the rank.\n\nTheorem 3.12 If \\(V\\) and \\(W\\) are finite-dimensional linear spaces, then\n\n\\(T\\) is an isomorphism from \\(V\\) to \\(W\\) if and only if \\(\\ker(T)= \\{0\\}\\) and \\(\\text{im} (T)=W\\),\nif \\(V\\) is isomorphic to \\(W\\), then \\(\\text{dim} (V)=\\text{dim} (W)\\),\nif \\(\\text{dim} (V)=\\text{dim} (W)\\) and \\(\\ker(T)=\\{0\\}\\), then \\(T\\) is an isomorphism, and\nif \\(\\text{dim} (V)=\\text{dim} (W)\\) and \\(\\text{im}(T)=W\\), then \\(T\\) is an isomorphism.\n\n\n\nProof. The proof of each part follows.\n\nSuppose \\(T\\) is an isomorphism from \\(V\\) to \\(W\\). This means there exists an invertible linear transformation \\(T^{-1}\\) from \\(W\\) to \\(V\\) such that \\(T(T^{-1})=I_W\\) and \\(T^{-1}(T)=I_V\\), where \\(I_W\\) and \\(I_V\\) are the identity transformations on \\(W\\) and \\(V\\) respectively. We will use \\(T^{-1}\\) to show \\(\\ker T=\\{0\\}\\) and \\(\\text{im} T= W\\). Since \\(\\ker T\\) is a subspace, \\(\\{0\\}\\subset \\ker T\\). Conversely, suppose \\(f\\in \\ker T\\). Then \\(T(f)=0\\) and so \\(T^{-1}(T(f))=f=0\\) so that \\(\\ker T\\subseteq \\{0\\}\\). Thus \\(\\ker T = \\{0\\}\\). Since \\(\\text{im} T\\) is a subspace of \\(W\\), \\(\\text{im} T\\subseteq W\\). To show conversely, let \\(w\\in W\\). Then \\(T^{-1}(w)\\in V\\) and so \\(T(T^{-1}w)=w\\) shows \\(w\\in \\text{im} T\\). Thus \\(\\text{im} T=W\\).\n\nConversely, suppose \\(\\ker T=\\{0\\}\\) and \\(\\text{im} T=W\\). We will show \\(T\\) is an isomorphism, which means we must show \\(T(f)=g\\) has a unique solution \\(f\\) for each \\(g\\in W\\). Now since \\(\\text{im} T=W\\), \\(T(f)=g\\) has at least one solution, say \\(f\\). Suppose \\(T(f)=g\\) has two solutions, say \\(T(f_1)=g=T(f_2)\\). Then \\(0=T(f_1)-T(f_2)=T(f_1-f_2)\\) shows \\(f_1-f_2\\in \\ker T\\). Since \\(\\ker T=\\{0\\}\\), \\(f_1=f_2\\) must follow, which means \\(T(f)=g\\) must have only one solution, so that \\(T^{-1}\\) exists, and therefore, \\(T\\) is an isomorphism.\n- If \\(V\\) is isomorphic to \\(W\\), then there exists a linear transformation \\(T\\) such that \\(\\ker T=\\{0\\}\\) and \\(\\text{im} T=W\\). By the rank-nullity theorem, \\(\\text{dim} V=\\text{dim} \\ker T + \\text{dim} \\text{im} T=0+\\text{dim} W\\). - By the previous part, \\(T\\) is an isomorphism when \\(\\ker =\\{0\\}\\) and \\(\\text{im} T=W\\). We are assuming \\(\\ker T=\\{0\\}\\) so it remains to show \\(\\text{im} T=W\\). By the rank-nullity theorem \\(\\text{dim} V=\\text{dim} \\ker T+\\text{dim} \\text{im} T=\\text{dim} \\ker T+\\text{dim} W=\\text{dim} W\\). Thus, \\(\\ker T=0\\) and so \\(\\ker T=\\{0\\}\\). Therefore, \\(T\\) is an isomorphism. - By the previous part, \\(T\\) is an isomorphism when \\(\\ker =\\{0\\}\\) and \\(\\text{im} T=W\\). We are assuming \\(\\text{im} T=W\\) so it remains to show \\(\\ker T=\\{o\\}\\). By the rank-nullity theorem \\(\\text{dim} V=\\text{dim} \\ker T+\\text{dim} \\text{im} T=\\text{dim} \\text{im} T=\\text{dim} W\\). Therefore, \\(\\text{im} T=W\\) and so \\(T\\) is an isomorphism.\n\n\nExample 3.26 Show that the transformation \\(T(A)=S^{-1}AS\\), from \\(\\mathbb{R}^{2\\times 2}\\) to \\(\\mathbb{R}^{2\\times 2}\\) where \\(S=\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) is an isomorphism. Notice that \\(\\text{dim} \\mathbb{R}^{2\\times 2}=\\text{dim} \\mathbb{R}^{2\\times 2}\\). Can we determine an invertible linear transformation \\(T^{-1}\\)? Checking the linearity conditions,\\[\nT(A_1+A_2)=S^{-1}(A_1+A_2)S=S^{-1}A_1S+S^{-1}A_2S=T(A_1)+T(A_2)\n\\] and\\[\nT(k A)=S^{-1}(k A)S=kS^{-1}A S=k T(A).\n\\] Now we know \\(T\\) is a linear transformation. Is it invertible? We need to solve the equation, \\(B=S^{-1} A S\\) for input \\(A\\). We can do this because \\(S\\) is an invertible matrix, so \\(S B=S(S^{-1}AS)=AS\\) and multiplying by \\(S^{-1}\\) on the right \\(S BS^{-1}=A\\) so that \\(T\\) is invertible, and the linear transformation is \\(T^{-1}(B)=S B S^{-1}\\).\n\n\nExample 3.27 Is the transformation \\(L( f)=\\vectorthree{f(1)}{f(2)}{f(3)}\\) from \\(\\mathcal{P}_3\\) to \\(\\mathbb{R}^3\\) an isomorphism? Since \\(\\text{dim} \\mathcal{P}_3=4\\) and \\(\\text{dim} \\mathbb{R}^3=3\\), the spaces \\(\\mathcal{P}_3\\) and \\(\\mathbb{R}^3\\) fail to be isomorphic, so that \\(L\\) is not an isomorphism.\n\n\nExample 3.28 Is the transformation \\(L( f)=\\vectorthree{f(1)}{f(2)}{f(3)}\\) from \\(\\mathcal{P}_2\\) to \\(\\mathbb{R}^3\\) an isomorphism? Notice \\(\\text{dim} \\mathcal{P}_2=3\\) and \\(\\text{dim} \\mathbb{R}^3=3\\) so the dimensions of the domain and the target space have the same dimension. Checking the linearity conditions, \\[\nL(f_1+f_2)\n=\\vectorthree{(f_1+f_2)(1)}{(f_1+f_2)(2)}{(f_1+f_2)(3)}\n=\\vectorthree{f_1(1)}{f_1(2)}{f_1(3)}+\\vectorthree{f_2(1)}{f_2(2)}{(f_2(3)}=L(f_1)+L(f_2)\n\\] and \\[\nT(k f)=\\vectorthree{(k f)(1)}{(k f)(2)}{(kf)(3)}\n=k\\vectorthree{f(1)}{f(2)}{f(3)}\n=k T(f).\n\\] The kernel of \\(T\\) consists of all polynomials \\(f(x)\\) in \\(\\mathcal{P}_2\\) such that \\[\nT(f(x))=\\vectorthree{f(1)}{f(2)}{f(3)}=\\vectorthree{0}{0}{0}.\n\\] Since a nonzero polynomial in \\(\\mathcal{P}_2\\) has at most two zeros, the zero polynomial is the only solution, so that \\(\\ker T=\\{0\\}\\). Therefore, \\(T\\) is an isomorphism.\n\n\nExample 3.29 Determine whether the transformation \\[T(M)=M\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}M\\] from \\(\\mathbb{R}^{2\\times 2}\\) to \\(\\mathbb{R}^{2\\times 2}\\) is an isomorphism. Notice that \\(\\text{dim} \\mathbb{R}^{2\\times 2}=\\text{dim} \\mathbb{R}^{2\\times 2}\\). Can we determine an invertible linear transformation \\(T^{-1}\\)? Checking the linearity conditions,\\[\n\\begin{array}{rl}\nT(M+N)\n&=(M+N)\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}(M+N) \\\\\n&=M\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}M\n+ N\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}N  =T(M)+T(N)\n\\end{array}\n\\] and\\[\n\\begin{array}{rl}\nT(k M)\n& =(kM)\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}(Mk) \\\\\n& =k \\left ( M\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}M \\right ) k\n= k T(M)\n\\end{array}\n\\] The kernel of \\(T\\) consists of \\(M=\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) such that\\[\n\\begin{bmatrix} a & b \\\\ c & d  \\end{bmatrix}\n\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}\n\\begin{bmatrix} a & b \\\\ c & d  \\end{bmatrix}\n=\n\\begin{bmatrix} -a & 0 \\\\ -2c & -d  \\end{bmatrix}\n\\begin{bmatrix} 0 & 0 \\\\ 0 & 0  \\end{bmatrix}.\n\\] Thus \\(a=c=d=0\\). However, when \\(M=\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}\\) then \\(T(M)=0\\) so that \\(\\ker T \\neq \\{0\\}\\), and so \\(T\\) is not an isomorphism.\n\n\nExample 3.30 Determine whether the transformation \\(T(f(t))=\\vectorthree{f(1)}{f'(2)}{f(3)}\\) from \\(\\mathcal{P}_2\\) to \\(\\mathbb{R}^3\\) is a linear transformation. Then determine whether \\(T\\) is an isomorphism and if not then find its image and kernel. This transformation is linear since\\[\nT(f(t)+g(t))=\\vectorthree{f(1)+g(1)}{f'(2)+g'(2)}{f(3)+g(3)}=\n\\vectorthree{f(1)}{f'(2)}{f(3)}+ \\vectorthree{g(1)}{g'(2)}{g(3)}=\nT(f(t))+T(g(t))\n\\] and \\[\nT(k f(t))=\\vectorthree{kf(1)}{k f'(2)}{k f(3)}=\nk \\vectorthree{f(1)}{f'(2)}{f(3)}=\nk T(f(t)).\n\\] Since \\(T(t-1)(t-3))=T(t^2-4t+3)=\\vec 0\\) shows \\(\\ker T \\neq \\{0\\}\\), T is not an isomorphism.\n\n\nTheorem 3.13 A linear map is invertible if and only if it is injective and surjective.\n\n\nProof. Suppose \\(T\\in \\mathcal{L}(V,W)\\) is invertible with inverse \\(T\\). Let \\(u,v\\in V\\). If \\(Tu=Tv\\) then \\[u=(T^{-1})u)=T^{-1}(Tu)=T^{-1}(Tv)=(T^{-1}T)(v)=v\\] and so \\(T\\) is injective. If \\(w\\in W\\), then \\(v=T^{-1} w\\in V\\) with \\(T v=T(T^{-1}w)=w\\) shows \\(T\\) is surjective. Assume \\(T\\) is injective and surjective. For each \\(w\\in W\\) assign \\(T(v)=w\\). Such \\(S(w)=v\\) exists because \\(T\\) is surjective and is unique since \\(T\\) is injective. Then \\(T(v)=w\\) shows \\(ST(v)=S(w)=v\\) so that \\(ST\\) is the identity on \\(V\\). Also, \\(TS(w)=T(Sw)=Tv=w\\) shows \\(TS\\) is the identity on \\(W\\). Thus \\(S\\) and \\(T\\) are inverses.\nNow \\(S\\) is linear since:\n\nif \\(w_1,w_2\\in W\\), then there exists a unique \\(v_1\\) and \\(v_2\\) such that \\(Tv_1=w_1\\) and \\(Tv_2=w_2\\), \\(S(w_1)=v_1\\), and \\(S(w_2)=v_2\\). By linearity of \\(T\\), \\(S(w_1+w_2)=S(T v_1+Tv_2)=(ST)(v_1+v_2)=v_1+v_2=S(w_1)+S(w_2)\\).\nIf \\(w\\in W\\) and \\(k\\in \\mathbb{F}\\) then there exists a unique \\(v\\in V\\) such that \\(Tv=w\\) and \\(S(w)=v\\). By linearity of \\(T\\), \\(S(kw)=S(k Tv)=S(T k v)=k v=kS(w)\\).\n\nTherefore \\(S\\) is linear and is the inverse of \\(T\\).\n\nConsider the transformation \\[\nL \\left( \\, \\begin{bmatrix} a & b \\\\ c & d  \\end{bmatrix} \\, \\right)=\\vectorfour{a}{b}{c}{d}\n\\] from \\(\\mathbb{R}^{2\\times 2}\\) to \\(\\mathbb{R}^4\\). Note that \\(L\\) is the coordinate transformation \\(L_{\\mathcal{B}}\\) with respect to the basis \\[\n\\mathcal{B}=\n\\left(\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 1 \\\\ 0 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 \\\\ 1 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 \\\\ 0 & 1  \\end{bmatrix}\n\\right)\n\\] Being a coordinate transformation, \\(L\\) is both linear and invertible. Therefore \\(L\\) is an isomorphism.\n\nTheorem 3.14 Let \\(T\\) be the linear transformation from \\(V\\) to \\(V\\) and let \\(B\\) be the matrix of \\(T\\) with respect to a basis \\(\\mathcal{B}=(f_1,\\ldots,f_n)\\) of \\(V\\). Then \\[\nB=\n\\begin{bmatrix}[T(f_1)]_{\\mathcal{B}} & \\cdots & [T(f_n)]_{\\mathcal{B}}]\n\\end{bmatrix}\n\\] In other words, the columns of \\(B\\) are the \\(\\mathcal{B}\\)-coordinate vectors of the transformation of the basis elements \\(f_1,\\ldots,f_n\\) of \\(V\\).\n\n\nProof. The proof if left for the reader.\n\n\nTheorem 3.15 Two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension.\n\n\nProof. Suppose \\(V\\) and \\(W\\) are finite-dimensional vector spaces that are isomorphic. If \\(\\text{dim} V > \\text{dim} W\\), then no linear map between \\(T\\) and \\(W\\) can be injective. Thus \\(V\\) and \\(W\\) are not isomorphic contrary to hypothesis. If \\(\\text{dim} V < \\text{dim} W\\), then no linear map between \\(V\\) and \\(W\\) can be surjective. Thus \\(V\\) and \\(W\\) are not isomorphic contrary to hypothesis. Therefore, \\(\\text{dim} V =\\text{dim} W\\).\nSuppose \\(V\\) and \\(W\\) are vector spaces with \\(\\text{dim} V=\\text{dim} W=n\\). Let \\((v_1,\\ldots,v_n)\\) ba a basis of \\(V\\) and let \\((w_1,\\ldots,w_n)\\) be a basis of \\(W\\). Define a linear map \\(T\\) by \\(T(a_1 v_1+\\cdots +a_n v_n)=a_1 w_1+\\cdots +a_n w_n\\). It’s an easy exercise to show \\(T\\) is linear, injective, and surjective. Thus \\(T\\) is an isomorphism as required.\n\n\nTheorem 3.16  If \\(A\\) and \\(B\\) are matrices of a linear transformation \\(T\\) with respect to different bases, then \\(A\\) and \\(B\\) are similar matrices.\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 3.17 Suppose \\(V\\) and \\(W\\) are finite-dimensional vector spaces, then \\(\\mathcal{M}\\) is an invertible linear map between \\(\\mathcal{L}(V,W)\\) and \\(Mat(m,n,\\mathbb{F})\\)\n\n\nProof. Suppose \\((v_1,\\ldots,v_n)\\) and \\(w_1,\\ldots,w_m)\\) are bases of \\(V\\) and \\(W\\) respectively. For each \\(T\\in \\mathcal{L}(V,W)\\) we assign,\\[\n\\mathcal{M}(T)=\\begin{bmatrix}a_{11} & & a_{1n}  \\\\ \\vdots & \\cdots & \\vdots \\\\ a_{m1} & & a_{mn} \\end{bmatrix}.\n\\] where \\(T v_k=a_{1k} w_1+\\cdots + a_{mk} w_m\\). Since \\((w_1,\\ldots,w_m)\\) is a basis of \\(W\\), This assigment is well-defined. The following arguments show \\(\\mathcal{M}\\) is linear, injective, and surjective, respectively.\n\nIf \\(S,T\\in \\mathcal{L}(V,W)\\), then \\(\\mathcal{M}(S+T)=\\mathcal{M}(S)+\\mathcal{M}(T)\\) follows by the definition of matrix addition. If \\(S\\in \\mathcal{L}(V,W)\\) and \\(k\\mathbb{F}\\), then \\(\\mathcal{M}(kS)=k\\mathcal{M}(S)\\) follows by the definition of scalar multiplication of matrices.\nIf \\(T\\in \\mathcal{L}(V,W)\\) with \\(\\mathcal{M}(T)=0\\), then for entries in \\([a_{ij}]\\) are zero showing \\(T v_k=0\\) for each \\(k\\). Since \\((v_1,\\ldots,v_n)\\) is a basis of \\(V\\), \\(T\\) must be the zero map, and so \\(\\text{ker} \\mathcal{M}=\\{0\\}\\). Therefore, \\(\\mathcal{M}\\) is injective.\nIf \\(A \\in Mat(m,n,\\mathbb{F})\\) then \\(A\\) is an \\(m\\times n\\) matrix with entries \\([a_{ij}]\\). We define \\(Tv_k=a_{1k} w_1+\\cdots a_{mk} w_m\\) for each \\(k\\). Then, by the definition of the matrix of a linear map, \\(\\mathcal{M}(T)=A\\) and so \\(\\mathcal{M}\\) is surjective.\n\n\n\nTheorem 3.18 If \\(V\\) and \\(W\\) are finite-dimensional, then \\(\\mathcal{L}(V,W)\\) is finite-dimensional and \\(\\text{dim} \\mathcal{L}(V,W) =(\\text{dim} V) (\\text{dim} W)\\).\n\n\nProof. If \\(V\\) and \\(W\\) are finite-dimensional vector spaces then \\(\\mathcal{L}(V,W)\\) is isomorphic to \\(Mat(m,n,\\mathbb{F})\\) where \\(n=\\text{dim} V\\) and \\(m=\\text{dim} W\\). If follows \\[\n\\text{dim}  \\mathcal{L}(V,W) =\\text{dim}  Mat(mn,\\mathbb{F})= m n=(\\text{dim}  W)(\\text{dim}  V).\n\\] since \\(\\text{dim} Mat(m,n,\\mathbb{F})= m n\\).\n\n\nTheorem 3.19 If \\(V\\) is finite-dimensional and \\(T\\in \\mathcal{L}(V)\\), the the following are equivalent:\n\n\\(T\\) is invertible,\n\\(T\\) is injective, and\n\\(T\\) is surjective.\n\n\n\nProof. The proof of each part follows.\n\nSuppose \\(T\\) is invertible, then by definition \\(T\\) is injective.\nSuppose \\(T\\) is injective. Thus \\(\\text{ker} T=\\{0\\}\\). Since \\(V\\) is finite-dimensional, we can apply the Rank-Nullity Theorem so \\(\\text{dim} V=\\text{dim} \\text{im} T\\). Since \\(\\text{im} T\\) is a subspace of \\(V\\) with the same dimension as \\(V\\), \\(\\text{im} T=V\\). Therefore, \\(T\\) is surjective.\nSuppose \\(T\\) is surjective. Then \\(T(V)=V\\) and so \\(\\text{dim} V=\\text{dim} \\text{im} T\\). Since \\(V\\) is finite-dimensional, \\(\\text{dim} V=\\text{dim} \\text{ker} T+\\text{dim} \\text{im}\\) T by the Rank-Nullity Theorem. Thus, \\(\\text{dim} \\text{ker} T =0\\) and so \\(\\text{ker} T=\\{0\\}\\). Therefore, \\(T\\) is injective, and since \\(T\\) is injective and surjective, \\(T\\) is invertible."
  },
  {
    "objectID": "linear-transformations.html#kernel-and-image-of-a-linear-transformation",
    "href": "linear-transformations.html#kernel-and-image-of-a-linear-transformation",
    "title": "3  Linear Transformations",
    "section": "3.4 Kernel and Image of a Linear Transformation",
    "text": "3.4 Kernel and Image of a Linear Transformation\nIn this section we introduce the kernel and image of a linear transformation. We show how they can be realized as geometric objects and demonstrate how to find spanning sets for them.\n\nDefinition 3.7 Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\) with \\(n\\times m\\) matrix \\(A\\).\n\nThe kernel of \\(T\\), denoted by \\(\\ker(T)\\), is the set of all vectors \\(\\vec x\\) in \\(\\mathbb{R}^n\\) such that \\(T(\\vec x)=A \\vec x = \\vec 0\\).\nThe image of \\(T\\), denoted by \\(\\text{im}(T)\\), is the set of all vectors in \\(\\mathbb{R}^n\\) of the form \\(T (\\vec x)=A\\vec x\\).\n\nThe kernel and image of a matrix \\(A\\) of \\(T\\) is defined as the kernel and image of \\(T\\).\n\nNow to adequately describe the kernel and image of a linear transformation we need the concept of the span of a collection of vectors. We will see that one of the best ways to describe the kernel and image of a linear transformation is to describe them in terms of collections of linear combinations of vectors.\nThe next three lemmas describe basic properties of kernel and image.\n\nLemma 3.2 The kernel of any linear transformation \\(T\\) has the following properties:\n\n\\(\\vec 0 \\in \\ker(T)\\),\nif \\(\\vec v, \\vec w\\in \\ker(T)\\), then \\(\\vec v+\\vec w\\in \\ker(T)\\), and\nif \\(\\vec v\\in \\ker(T)\\) and \\(k\\in \\mathbb{R}\\), then \\(k\\vec v\\in \\ker(T)\\).\n\n\n\nProof. Let \\(A\\) be the matrix for \\(T\\). Then \\(A\\vec 0=\\vec 0\\) which shows \\(\\vec 0 \\in \\ker(T)\\). If \\(\\vec v, \\vec w\\in \\ker(T)\\), then \\(A\\vec v=\\vec 0\\) and \\(A\\vec w=\\vec 0\\). Thus, \\(A\\vec v+A\\vec w=A(\\vec v+\\vec w)=\\vec 0\\) implying \\(\\vec v+\\vec w\\in \\ker(T)\\). If \\(\\vec v\\in \\ker(T)\\) and \\(k\\in \\mathbb{R}\\), then \\(\\vec 0=A\\vec v\\) implying \\(A(k \\vec v)=\\vec 0\\). Thus, \\(k\\vec v\\in \\ker(T)\\).\n\n\nLemma 3.3 Let \\(T\\) be a linear transformation with matrix \\(A\\).\n\nIf \\(A\\) is an \\(n\\times n\\) matrix, then \\(\\ker(A)=\\{\\vec 0\\}\\) if and only if \\(\\text{rank} (A)=n\\).\nIf \\(A\\) is an \\(n\\times m\\) matrix, then\n\\(\\ker(A)=\\{\\vec 0\\} \\implies m\\geq n\\)\n\\(m>n \\implies \\ker(A)\\) contains non-zero vectors\nIf \\(A\\) is a square matrix, then \\(\\ker(A)=\\{\\vec 0\\}\\) if and only if \\(A\\) is invertible.\n\n\n\nProof. If \\(T\\) is a linear transformation \\(T(\\vec x )=A\\vec x\\) from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\) where \\(m>n\\), then there will be free variables for the equation \\(T(\\vec x)=A\\vec x=\\vec 0\\); that is the system will have infinitely many solutions. Therefore, the kernel of \\(T\\) will consists of infinitely many vectors. If \\(m=n\\) and for an invertible \\(n\\times n\\) matrix \\(A\\), how do we find \\(\\ker(A)\\)? Since \\(A\\) is invertible, \\(A\\vec x=\\vec 0\\) can be solved by \\(A^{-1}(A \\vec x)=A^{-1}\\vec 0\\) showing \\(\\vec x=\\vec 0\\); that is the only solution to the system \\(A \\vec x=\\vec 0\\) is \\(\\vec 0\\) so that \\(\\ker(A)=\\{\\vec 0\\}\\) whenever \\(A\\) is invertible.\n\n\nLemma 3.4 The image of any linear transformation \\(T\\) has the following properties:\n\n\\(\\vec 0 \\in \\text{im}(T)\\),\nif \\(\\vec v, \\vec w\\in \\text{im}(T)\\), then \\(\\vec v+\\vec w\\in \\text{im}(T)\\), and\nif \\(\\vec v\\in \\text{im}(T)\\) and \\(k\\in \\mathbb{R}\\), then \\(k\\vec v\\in \\text{im}(T)\\).\n\n\n\nProof. Let \\(A\\) be the matrix for \\(T\\). Then \\(A\\vec 0=\\vec 0\\) which shows \\(\\vec 0 \\in \\text{im}(T)\\). If \\(\\vec v, \\vec w\\in \\text{im}(T)\\), then there exists \\(\\vec x\\) and \\(\\vec y\\) such that \\(A\\vec x=\\vec v\\) and \\(A\\vec y=\\vec w\\). Thus, \\(\\vec v+\\vec w=A\\vec x+A\\vec y=A(\\vec x+\\vec y)\\) implying \\(\\vec v+\\vec w\\in \\text{im}(T)\\). If \\(\\vec v\\in \\text{im}(T)\\) and \\(k\\in \\mathbb{R}\\), then there exists \\(\\vec u\\) such that \\(\\vec v=A\\vec u\\) implying \\(A(k \\vec u)=k \\vec v\\). Thus, \\(k\\vec v\\in \\text{im}(T)\\).\n\n\nTheorem 3.20  Let \\(T:\\mathbb{R}^m\\to \\mathbb{R}^n\\) be a linear transformation with matrix \\(A\\). Then \\(\\text{im}(T)=\\text{span}(\\vec v_1, ....,\\vec v_m)\\) where \\(\\vec v_1, ..., \\vec v_m\\) are the column vectors of \\(A\\).\n\n\nProof. Since \\(\\vec v_1,...,\\vec v_m\\) are the column vectors of \\(A\\) \\[\\begin{equation}\n\\label{lineq}\nT(\\vec x)=\nA \\vec x =\n\\begin{bmatrix}\n\\vec v_1 & \\cdots & \\vec v_m\n\\end{bmatrix}\n\\vectorthree{x_1}{\\vdots}{x_m}=x_1 v_1+\\cdots + x_m v_m.\n\\end{equation}\\] If \\(\\vec u\\in \\text{span}(\\vec v_1, ....,\\vec v_m)\\) then there exists \\(x_1, ..., x_m\\) such that \\(u=x_1 v_1+\\cdots +x_m v_m\\). By \\(\\ref{lineq}\\), \\(u=T(\\vec x)\\) for \\(\\vec x\\in \\mathbb{R}^m\\). Thus, \\(\\vec u\\in \\text{im}(T)\\). Conversely, assume \\(\\vec u\\in\\text{im}(T)\\). Then there exists \\(\\vec x\\) such that \\(\\vec u=T(\\vec x)\\). By \\(\\ref{lineq}\\), there exists \\(x_1, ...,x_m\\) such that \\(u=x_1 v_1+\\cdots +x_m v_m\\). Therefore, \\(\\vec u \\in \\text{span}(\\vec v_1, ....,\\vec v_m)\\) and so \\(\\text{im}(T)=\\text{span}(\\vec v_1, ....\\vec v_m)\\) follows.:::\n\nExample 3.31 Find vectors that span \\(\\ker(A)\\) and \\(\\text{im}(A)\\) given \\[\nA=\n\\begin{bmatrix}\n2 & 1 & 3 \\\\\n3 & 4 & 2 \\\\\n6 & 5 & 7\n\\end{bmatrix}.\n\\] Describe \\(\\text{im}(A)\\) geometrically. To first find a spanning set of \\(\\ker(A)\\) we solve the system \\(A\\vec x=\\vec 0\\). We use the augmented matrix and elementary row operations and find reduced row-echelon form \\[\n\\text{rref}(A)=\n\\begin{bmatrix}\n1 & 0 & 2 & 0 \\\\\n0 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}.\n\\] Thus the solution set is \\[\n\\vectorthree{x_1}{x_2}{x_2}=t\\vectorthree{-2}{1}{1}:=t\\vec{w}\n\\] where \\(t\\in \\mathbb{R}\\). Therefore \\(\\ker(A)=\\text{span}(\\vec{w})\\). Next we find \\(\\text{im}(A).\\) To do so let \\(\\vec v_1, \\vec v_2, \\vec v_3\\) be the column vectors of the matrix \\(A\\). Since \\(\\text{im}(A)\\) is spanned by the columns of \\(A\\) and \\(\\vec v_3=2\\vec v_1+(-1)\\vec v_2\\), we find \\(\\text{im}(A)=\\text{span}\\left(\\vec v_1, \\vec v_2\\right)\\). Therefore the image of \\(A\\) is a plane in \\(\\mathbb{R}^3\\) that passes through the origin.\n\n\nExample 3.32 Give an example of a matrix \\(A\\) such that \\(\\text{im}(A)\\) is the plane with normal vector \\(\\vec w=\\vectorthree{1}{3}{2}\\) in \\(\\mathbb{R}^3\\). Since \\(\\vec w\\) is a normal vector we let \\(A=\\begin{bmatrix} 1& 3 & 2\\end{bmatrix}\\). First we find \\(\\ker(A)\\) because \\[\n\\begin{bmatrix} 1& 3 & 2\\end{bmatrix} \\vectorthree{x}{y}{z}=\\vectorthree{0}{0}{0}\n\\] is the plane \\(x+3y+2z=0\\) where \\(\\vec w\\) is a normal vector. Let \\(z=t\\) and \\(y=s\\), then \\(x=-3s-2t\\) so the solutions to the system \\(A\\vec x=\\vec 0\\) are \\[\n\\vectorthree{x}{y}{z}=\\vectorthree{-3s-2t}{s}{t}=s\\vectorthree{0}{1}{-3}+t\\vectorthree{1}{0}{-2}\n:=s\\vec u+t \\vec v\n\\] where \\(s\\) and \\(t\\) are real numbers; and thus \\(\\ker(A)=\\text{span}(\\vec{u}, \\vec{v})\\). These vectors yield the image of \\(A\\) since \\(\\text{im}(A)\\) is the plane with normal vector \\(\\vec w\\) so \\(A\\) is one such matrix; and \\(\\text{im}(A)=\\text{span}(\\vec{u}, \\vec{v})\\) is the plane in \\(\\mathbb{R}^3\\) with normal vector \\(\\vec{w}\\).\n\nThe kernel, being the most important subspace, has a special name for its dimension; namely, the dimension of \\(\\ker A\\) is called the nullity of \\(A\\).\n\nExample 3.33 Find the reduced row-echelon form of the matrix \\[\nA=\n\\begin{bmatrix}\n1 & 2 & 3 & 2 & 1 \\\\\n3 &  6 & 9 & 6 & 3 \\\\\n1 & 2 & 4 & 1 & 2 \\\\\n2 & 4 & 9 & 1 & 2\n\\end{bmatrix}.\n\\] Find a basis and state the dimension for the image and kernel of \\(A\\).\n\n\nExample 3.34 Find vectors that span \\(\\ker(A)\\) and \\(\\text{im}(A)\\) given \\[\nA=\n\\begin{bmatrix}\n1 & -1 & -1 & 1 & 1 \\\\\n-1 & 1 & 0 & -2 & 2 \\\\\n1 & -1 & -2 & 0 & 3 \\\\\n2 & -2 & -1 & 3 & 4\n\\end{bmatrix}.\n\\] We will solve the system \\(A\\vec x=\\vec 0\\) to find \\(\\ker(A)\\). Using the augmented matrix and elementary row operations, we find \\[\n\\text{rref}(A)=\n\\begin{bmatrix}\n1 & -1 & 0 & 2 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}.\n\\] So the solutions to the system \\(A\\vec x=\\vec 0\\) are \\[\n\\vectorfive{x_1}{x_2}{x_3}{x_4}{x_5}=\\vectorfive{s-2t}{s}{-t}{t}{0}=s\\vectorfive{1}{1}{0}{0}{0}+t\\vectorfive{-2}{0}{-1}{1}{0}:=s\\vec{u}+t\\vec{v}\n\\] where \\(s,t\\in \\mathbb{R}\\). Therefore \\(\\ker(A)=\\text{span}(\\vec{u},\\vec{v}).\\) Since \\(\\text{im}(A)\\) is the span of the column vectors of \\(A\\), we let \\(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3,\\vec{v}_4,\\vec{v}_5\\) be the column vectors of \\(A\\). By \\(\\ref{imspan}\\), \\(\\text{im}(A)=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3,\\vec{v}_4,\\vec{v}_5).\\) Using \\(\\text{rref}(A)\\) as a guide, we notice \\(\\vec{v}_2=(-1)\\vec{v}_1\\) so we eliminate \\(\\vec{v}_2\\). Also, \\(\\vec{v}_4=(2)\\vec{v}_1+\\vec{v}_3\\) so we also eliminate \\(\\vec{v}_4\\). Therefore, \\(\\text{im}(A)=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_5)\\).\n\n\nExample 3.35 Give an example of a linear transformation whose kernel is the line spanned by \\(\\vec{w}=\\begin{bmatrix}-1 \\\\ 1 \\\\2\\end{bmatrix}\\). Considering the intersection of the planes \\(x+y=0\\) and \\(2x+z=0\\), we try to use the linear transformation, \\[\nT(\\vec{x})\n=T\\vectorthree{x}{y}{z}=\\vectortwo{x+y}{2x+z}\n=\\begin{bmatrix}\n1 & 1 & 0\\\\\n2 & 0 & 1\n\\end{bmatrix}\\vec{x}:=A\\vec{x}. \\] To find the kernel of \\(T\\) we solve \\(A \\vec x= \\vec 0\\). Since \\(\\text{rref}(A)=\\begin{bmatrix}  1 & 0 & 1/2 \\\\  0 & 1 & -1/2 \\end{bmatrix}\\) the solutions are of the form \\(t \\vec{w}\\) where \\(t\\) is a real number. Therefore it suffices to let \\(T\\) be the requested linear transformation.\n\n\nExample 3.36 Express the line \\(L\\) in \\(\\mathbb{R}^3\\) spanned by the vector \\(\\vec{w}=\\begin{bmatrix} 1 \\\\1 \\\\1 \\end{bmatrix}\\) as the image of a matrix \\(A\\) and as the kernel of a matrix \\(B\\). Let \\(A=\\vec{w}\\), then \\(L=\\text{im}(A)=\\text{span}(\\vec{w})\\). Therefore it suffices to let \\(A\\) be the requested matrix. Considering the intersection of the planes \\(x=y\\) and \\(y=z\\), we try to find \\(B\\) using the linear transformation \\[\nT(\\vec{x})\n=T\\vectorthree{x}{y}{z}=\\vectortwo{x-y}{y-z}\n=\\begin{bmatrix}\n1 & -1 & 0\\\\\n0 & 1 & -1\n\\end{bmatrix}\n:= B \\vec{x}\n\\] To find the kernel of \\(T\\) we solve \\(B \\vec x= \\vec 0\\). Since \\(\\text{rref}(B)=\\begin{bmatrix}  1 & 0 & -1 \\\\  0 & 1 & -1 \\end{bmatrix}\\) the solutions are of the form \\(t \\vec{w}\\) where \\(t\\) is a real number. Therefore it suffices to let \\(B\\) be the other requested matrix.\n\n\nExample 3.37 Find a basis for the kernel and the image of the linear transformations defined by \\[\nT_1 = \\left \\{\n\\begin{array}{rl}\ny_1 = & x_1+x_2+3x_3 \\\\\ny_2 = & 2x_1+x_2+4x_3\n\\end{array}\n\\right .\n\\qquad \\text{with} \\qquad\n\\text{rref}(T_1)  = \\begin{bmatrix}\n1 & 0 &1 \\\\\n0 & 1 & 2\n\\end{bmatrix}.\n\\] All solutions to the system \\(A\\vec x=\\vec 0\\) are \\(\\vectorthree{x_1}{x_2}{x_3}=t\\vectorthree{-1}{-2}{1}\\) where \\(t\\in\\mathbb{R}\\). Since \\(\\vectorthree{-1}{-2}{1}\\) is linearly independent and spans the kernel, the vector \\(\\vectorthree{-1}{-2}{1}\\) forms a basis of \\(\\ker(A)\\). Since the columns of \\(A\\) spans the image of \\(A\\) and \\(\\vectortwo{3}{4}=(1)\\vectortwo{1}{2}+(2)\\vectortwo{1}{1}\\) we find the vector \\(\\vectortwo{3}{4}\\) redundant. Since the remaining vectors are linearly independent and span \\(\\text{im}(A)\\), they form a basis for \\(\\text{im}(A)\\).\n\n\nExample 3.38 Find a basis for the kernel and the image of the linear transformations defined by \\[\nT_2 =\n\\left \\{\n\\begin{array}{rlll}\ny_1= & x_1 +3x_2 +9x_3  \\\\\ny_2= & 4x_1+5 x_2 +8x_3  \\\\\ny_3= & 7x_1+6 x_2 +3x_3  \\\\\n\\end{array}\n\\right .\n\\qquad \\text{with} \\qquad\n\\text{rref}(T_2)  = \\begin{bmatrix}\n1 & 0 & -3 \\\\\n0 & 1 & 4 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\] To find a basis of the kernel we solve \\(A\\vec x = \\vec 0\\) where \\(A\\) is the matrix of the given transformation. Since \\[\n\\text{rref(A)}=\n\\begin{bmatrix} 1 & 0 &-3 \\\\ 0 & 1 & 4 \\\\ 0 & 0 & 0 \\end{bmatrix}\n\\] all solutions have the form \\(\\vectorthree{3t}{-4t}{t}\\). Therefore a basis of the kernel of \\(A\\) is \\(\\vectorthree{3}{-4}{1}\\). In the original matrix \\(A\\) the third column is redundant since \\(\\vectorthree{9}{8}{3}=(-3)\\vectorthree{1}{4}{7}+(4)\\vectorthree{3}{5}{6}\\), and since the vectors \\(\\vectorthree{1}{4}{7}\\) and \\(\\vectorthree{3}{5}{6}\\) are linearly independent and span, they form a basis of \\(\\text{im}(A)\\).\n\n\nExample 3.39 Find a basis for the kernel and the image of the linear transformations defined by \\[\nT_3 =\n\\left \\{\n\\begin{array}{rlllll}\ny_1= & 4x_1+8 x_2 +x_3 +x_4+6x_5 \\\\\ny_2= & 3x_1+6 x_2 +x_3 +2x_4+5x_5 \\\\\ny_3= & 2x_1+4 x_2 +x_3 +9x_4+10x_5 \\\\\ny_4= & x_1+ 2x_2 + 3x_3 +2x_4 \\\\\n\\end{array}\n\\right .\n\\quad \\text{with} \\quad\n\\text{rref}(T_3)  = \\begin{bmatrix}\n1 & 2 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 &1\n\\end{bmatrix}\n\\] The solutions to the system \\(A\\vec x=\\vec 0\\) are \\[\n\\vec x=\\vectorfive{x_1}{x_2}{x_3}{x_4}{x_5}=t \\vectorfive{-2}{1}{0}{0}{0}=t \\vec v \\qquad \\text{where $t\\in\\mathbb{R}$.}\n\\] The vector \\(\\vec v\\) is linearly independent and spans \\(\\ker(A)\\); thus it forms a basis for \\(\\ker(A)\\). Since \\(\\vectorfour{8}{6}{4}{2}=2\\vectorfour{4}{3}{2}{1}\\) the vector \\(\\vectorfour{8}{6}{4}{2}\\) is redundant and since the column vectors of \\(A\\) are linearly independent and span \\(\\text{im}(A)\\), we have that the vectors \\(\\vectorfour{4}{3}{2}{1}\\), \\(\\vectorfour{1}{1}{1}{3}\\), \\(\\vectorfour{1}{2}{9}{2}\\), and \\(\\vectorfour{6}{5}{10}{0}\\) form a basis of \\(\\text{im}(A)\\).\n\n\nExample 3.40 Show \\(\\ker(A)\\neq \\ker(B)\\) where \\[\nA=\n\\begin{bmatrix}\n1 & 0 & 2 & 0 & 4 & 0 \\\\\n0 & 1 & 3 & 0 & 5 & 0 \\\\\n0 & 0 & 0 & 1 & 6 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\qquad \\text{and} \\qquad\n%$ and $\nB=\n\\begin{bmatrix}\n1 & 0 & 2 & 0 & 0 & 4 \\\\\n0 & 1 & 3 & 0 & 0 & 5 \\\\\n0 & 0 & 0 & 1 & 0 & 6 \\\\\n0 & 0 & 0 & 0 & 1 & 7\n\\end{bmatrix}.\n\\] We solve the system \\(A\\vec x=\\vec 0\\), written out we find: \\[\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6\n\\end{bmatrix}\n= s\n\\begin{bmatrix}\n-4 \\\\ -5 \\\\ 0 \\\\ 6 \\\\ 1 \\\\ 0\n\\end{bmatrix}\n+t\n\\begin{bmatrix}\n-2 \\\\ -3 \\\\ 1\\\\ 0 \\\\ 0 0\n\\end{bmatrix}\n=s\\vec v_1+t\\vec v_2\n\\quad \\text{ where $s,t\\in \\mathbb{R}$}.\n\\] Since \\(v_1\\) and \\(v_2\\) are linearly independent and span \\(\\ker(A)\\), they form a basis of the kernel of \\(A\\). By noticing \\(B \\vec v_1\\neq \\vec 0\\) we conclude \\(\\ker(A)\\neq \\ker(B)\\).\n\n\nTheorem 3.21  For any matrix \\(A\\), \\(\\dim(\\text{im} A )=\\text{rank}(A)\\).\n\n\nProof. \n\n::: {#thm- } [Rank-Nullity] Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\) with \\(n\\times m\\) matrix \\(A\\). Then \\[\n\\text{rank}(A)+\\text{nullity}(A)\n=\\dim(\\text{im} A)+\\dim(\\ker A)=m.\n\\]\n\n\nProof. Let \\(\\vec{y}=A\\vec{x}\\) be the corresponding system of linear equations. Recall,\n\\[\\begin{equation}\n\\label{varequ} \\small\n\\begin{tabular}{ccccccc}\n$\\left(\\parbox[c]{1.55cm}{\\begin{center}\nnumber of free variables\n\\end{center}}\\right)$ & $=$  &\n$\\left(\\parbox[c]{1.55cm}{\\begin{center}\ntotal number of variables\n\\end{center}}\\right)$ & $-$  &\n$\\left(\\parbox[c]{1.55cm}{\\begin{center}\nnumber of leading variables\n\\end{center}}\\right)$ & $=$  \n& $\\parbox[c]{1.75cm}{\\begin{center}\n\\normalsize\n$m-\\text{rank}(A)$\n\\end{center}}$\n\\end{tabular}\n\\end{equation}\\] Apparently, the number of free variables is the dimension of the kernel of \\(A\\). Thus \\(\\ref{varequ}\\) becomes \\(\\text{nullity}(A)=m-\\text{rank}(A)\\). By \\(\\ref{dimenimgrank}\\) we arrive at the conclusion \\(\\text{rank}(A)+\\text{nullity}(A)=m\\).\n\n\nExample 3.41 If possible, find a \\(3\\times 3\\) matrix such that \\(\\text{im} A=\\ker A\\).\n\n\nExample 3.42 If possible, find a \\(4\\times 4\\) matrix such that \\(\\text{im} A=\\ker A\\).\n\n\nExample 3.43 Give an example of a \\(4\\times 5\\) matrix \\(A\\) with \\(\\dim(\\ker A)=3\\).\n\n\nTheorem 3.22 The vectors \\(\\vec{v}_1,\\ldots,\\vec{v}_n\\) in \\(\\mathbb{R}^n\\) form a basis of \\(\\mathbb{R}^n\\) if and only if the matrix whose columns consists of \\(\\vec{v}_1,...,\\vec{v}_n\\) is invertible.\n\n\nProof. \n\nFor example, consider the matrix \\[\nA=\n\\begin{bmatrix}\n1 & 2 & 1 & 2 \\\\\n1 & 2 & 2 & 3 \\\\\n1 & 2 & 3 & 4\n\\end{bmatrix}\n\\] What is the smallest number of vectors needed to span the image of \\(A\\)? Of course we know, \\[\n\\text{im}(A)=\\text{span}\\left(\\vectorthree{1}{1}{1},\\vectorthree{2}{2}{2},\\vectorthree{1}{2}{3},\\vectorthree{2}{3}{4}\\right).\n\\] However, it is easy to show that \\(\\vectorthree{2}{3}{4}\\) and \\(\\vectorthree{2}{2}{2}\\) are redundant; and that the remaining vectors are linearly independent. Thus, \\[\n\\text{im}(A)=\\text{span}\\left(\\vectorthree{1}{1}{1},\\vectorthree{1}{2}{3}\\right).\n\\] Clearly, the image of the linear transformation defined by \\(A\\) is more easily understood by having a spanning set of linearly independent vectors."
  },
  {
    "objectID": "linear-transformations.html#invertible-linear-transformations",
    "href": "linear-transformations.html#invertible-linear-transformations",
    "title": "3  Linear Transformations",
    "section": "3.5 Invertible Linear Transformations",
    "text": "3.5 Invertible Linear Transformations\nAn \\(n\\times n\\) matrix \\(A\\) is called invertible if and only if there exists a matrix \\(B\\) such that \\(A B=I_n\\) and \\(BA=I_n\\). Using the inverse of a matrix we also define the inverse of a linear transformation.\nLet \\(T(\\vec x)=A\\vec x\\) be a linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\). If the matrix \\(A\\) has inverse \\(A^{-1}\\), then the linear transformation defined by \\(A^{-1} \\vec x\\) is called the inverse transformation of \\(T\\) and is denoted by \\(T^{-1}(\\vec x)=A^{-1}.\\)\nA function \\(T\\) from \\(X\\) to \\(Y\\) is called invertible if the equation \\(T(x)=y\\) has a unique solution \\(x\\in X\\) for each \\(y\\in Y\\). A square matrix \\(A\\) is called invertible if the linear transformation \\(\\vec y=T(\\vec x)=A\\vec x\\) is invertible. In this case, then matrix of \\(T^{-1}\\) is denoted by \\(A^{-1}\\). If the linear transformation is invertible, then its inverse is \\(\\vec x = T^{-1} (\\vec y)=A^{-1}.\\)\n\nExample 3.44 Find the inverse transformation of the following linear transformation: \\[\n\\begin{array}{rl}\ny_1 = & x_1+3x_2+3x_3 \\\\\ny_2 = & x_1+4x_2+8x_3 \\\\\ny_3 = & 2x_1+7x_2+12x_3\n\\end{array}.\n\\] To find the inverse transformation we solve for \\(x_1, x_2, x_3\\) in terms of \\(y_1,.y_2,y_3\\). To do this we find the inverse matrix of \\[\nA=\n\\begin{bmatrix}\n1 & 3 & 3 \\\\\n1 & 4 & 8 \\\\\n2 & 7 & 12\n\\end{bmatrix}.\n\\] Applying elementary-row operations, \\[\n\\begin{bmatrix}\n1 & 3 & 3 & 1 & 0 & 0 \\\\\n1 & 4 & 8 & 0 & 1 & 0 \\\\\n2 & 7 & 12 & 0 & 0 & 1\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{R_2-R_1} \\\\\n\\stackrel{\\longrightarrow}{-2R_1+R_3}\n\\end{array}\n\\begin{bmatrix}1 & 3 & 3 & 1 & 0 & 0 \\\\\n0 & 1 & 5 & 1 & 1 & 0 \\\\\n0 & 1 & 6 & -2 & 0 & 1\n\\end{bmatrix}\n\\] \\[\n\\stackrel{\\longrightarrow}{-R_2+R_3}\n\\begin{bmatrix}\n1 & 3 & 3 & 1 & 0 & 0 \\\\\n0 & 1 & 5 & -1 & 1 & 0 \\\\\n0 & 0 & 1 & -1 & -1 & 1\n\\end{bmatrix}\\stackrel{\\longrightarrow}{-5R_3+R_2}\n\\begin{bmatrix}1 & 3 & 3 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 4 & 6 & -5 \\\\\n0 & 0 & 1 & -1 & -1 & 1\n\\end{bmatrix}\n\\] \\[\n\\stackrel{\\longrightarrow}{-3R_3+R_1}\n\\begin{bmatrix}\n1 & 3 & 0 & 4 & -3 & 3 \\\\\n0 & 1 & 0 & 4 & 6 &-5 \\\\\n0 & 0 & 1 & -1 & -1 & 1\n\\end{bmatrix}\\stackrel{\\longrightarrow}{-3R_2+R_1}\n\\begin{bmatrix}\n1 & 0 & 0 & -8 & -15 & 12 \\\\\n0 & 1 & 0 & 4 & 6 & -5 \\\\\n0 & 0 & 1 & -1 & -1 & 1\n\\end{bmatrix}\\] we find \\[\nA^{-1}=\n\\begin{bmatrix}\n-8 & -15 & 12 \\\\\n4 & 6 & -5 \\\\\n1 & -1 & 1\n\\end{bmatrix}.\n\\] Therefore the requested linear transformation is \\[\n\\begin{array}{rl}\nx_1 = & -8y_1-15y_2+12y_3 \\\\\nx_2 = & 4y_1+6y_2-5y_3 \\\\\nx_3 = & -y_1-y_2+y_3.\n\\end{array}\n\\]\n\nOf course inverse transformations makes sense in terms of inverse functions; that is, if \\(T^{-1}\\) is the inverse transformation of \\(T\\) then \\((T\\circ T^{-1})(\\vec x)=\\vec x\\) and \\((T^{-1 }\\circ T)(\\vec x)=\\vec x\\). For example, for \\(T\\) given in \\(\\ref{invtranseq}\\) we illustrate \\[\n(T^{-1}\\circ T)\\vectorthree{1}{2}{3}=T^{-1}\\vectorthree{2}{4}{-5}=\\vectorthree{1}{2}{3}\n\\] as one can verify.\n\nExample 3.45 Find the inverse of the linear transformation \\[\\begin{align*}\n& y_1 = 3x_1 +5x_2 \\\\\n& y_2 =3x_1+4x_2.\n\\end{align*}\\] Reducing the system \\[\n\\begin{bmatrix}\n3x_1+5x_2& =y_1 \\\\\n3x_1+4x_2 & =y_2\n\\end{bmatrix}\n\\] we obtain \\[\n\\begin{bmatrix}\nx_1 & =-\\frac{4}{3}y_1+\\frac{5}{3}y_2 \\\\\nx_2 & = y_1-y_2\n\\end{bmatrix}.\n\\]\n\n\nTheorem 3.23 Let \\(A\\) be an \\(n\\times n\\) matrix. Then\n\n\\(A\\) is invertible if and only if rref(\\(A\\))\\(=I_n\\),\n\\(A\\) is invertible if and only if \\(\\text{rank}(A)=n\\), and\n\\(A\\) is invertible if and only if \\(A^{-1} A= I_n\\) and \\(A A^{-1}=I_n\\).\n\n\n\nProof. The proof is left for the reader.\n\nTo find the inverse of an \\(n \\times n\\) matrix \\(A\\), form the augmented matrix \\([ \\, A \\, | \\, I_n \\, ]\\) and compute $(, [ , A , | , I_n , ] , ) $. If $(, [ , A , | , I_n , ] , ) $ is of the form $(, [ , I_n , | , B , ] , ) $, then \\(A\\) is invertible and \\(A^{-1}=B\\). Otherwise \\(A\\) is not invertible. For example \\[\\begin{equation}\n\\label{invtranseq}\n=\\text{rref}\\left(\n\\begin{bmatrix}\n1 & -1 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 1 & 0 \\\\\n-1 & -2 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\right)\n=\n\\begin{bmatrix}\n1 & 0 & 0 & 2 & -2 & -1 \\\\\n0 & 1 & 0 & -1 & 1 & 0 \\\\\n0 & 0 & 1 & -2 & 3 & 1\n\\end{bmatrix}\n= [ \\,  I_3 \\,  |  \\,  B \\, ]\n\\end{equation}\\] shows \\(B=A^{-1}\\) where \\[\nB=\n\\begin{bmatrix}\n2 & -2 & -1 \\\\\n-1 & 1 & 0 \\\\\n-2 & 3 & 1\n\\end{bmatrix}\n\\] and \\[A=\n\\begin{bmatrix}\n1 & -1 & 1  \\\\\n1 & 0 & 1 \\\\\n-1 & -2 & 0\n\\end{bmatrix}\n\\] as one can verify, by showing \\(AB=I_3\\) and \\(BA=I_3\\).\n\nTheorem 3.24 Let \\(A\\) and \\(B\\) be \\(n \\times n\\) matrices. Then\n\nif \\(A\\) and \\(B\\) are invertible, then \\(B A\\) is invertible as well and \\[\n(B A)^{-1}= A^{-1}B^{-1}\n\\]\nif \\(B A= I_n\\), then \\(A\\) and \\(B\\) are both invertible, \\[\nA^{-1}=B, \\qquad B^{-1}=A, \\qquad \\text{ and } \\qquad AB = I_n.\n\\]\n\n\n\nProof. The proof is left for the reader.\n\n\nExample 3.46 Find the inverse matrices of \\[\nA=\n\\begin{bmatrix} 2 & 3 \\\\ 6 & 9 \\end{bmatrix}\n\\] and \\[\nB=\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 9 \\end{bmatrix}\n\\] Since \\(\\text{rref}(A)=\\begin{bmatrix} 1/2 & 3/2 \\\\ 0 & 0 \\end{bmatrix}\\neq I_2\\), \\(A^{-1}\\) does not exist. The inverse of \\(B\\) does exist and \\(B^{-1}=\\begin{bmatrix} 3 & -2/3 \\\\ -1 & 1/3 \\end{bmatrix}\\) since \\(B^{-1}B=I_2\\) and \\(B B^{-1}=I_2\\).\n\n\nExample 3.47  Show that \\(A=\\begin{bmatrix} a & b \\\\ c& d \\end{bmatrix}\\) is invertible if and only if \\(a d- b c \\neq 0\\) and when possible \\[\\begin{equation}\n\\label{twodet}\nA^{-1}=\\frac{1}{a d - b c}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}.\n\\end{equation}\\] We proceed to find the inverse: \\[\n\\begin{bmatrix}\na & b  & 1 & 0 \\\\\nc & d & 0 & 1\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\frac{1}{a} R_1}\n\\\\\n\\stackrel{\\longrightarrow}{\\frac{1}{c} R_2}\n\\end{array}\n\\begin{bmatrix}\n1 & \\frac{b}{a} & \\frac{1}{a} & 0 \\\\\n1 & \\frac{d}{c} & 0 & \\frac{1}{c}\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{-R_1+R_2}\n\\end{array}\n\\begin{bmatrix}\n1 & \\frac{b}{a} & \\frac{1}{a} & 0 \\\\\n0 & \\frac{ad-bc}{ac} & \\frac{-1}{a} & \\frac{1}{c}\n\\end{bmatrix}\n\\] \\[\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\frac{ac}{ad-bc} R_2}\n\\end{array}\n\\begin{bmatrix}\n1 & \\frac{b}{a} & \\frac{1}{a} & 0 \\\\\n0 & 1 & \\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\frac{-b}{a}R_2+R_1}\n\\end{array}\n\\begin{bmatrix}\n1 & 0 & \\frac{d}{ad-bc} & \\frac{-b}{ad-bc} \\\\\n0 & 1 & \\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\] Therefore, \\(A\\) is invertible if and only if \\(a d- b c \\neq 0\\) and \\(\\ref{twodet}\\) holds.\n\n\nExample 3.48 For which values of constants \\(a, b, c,\\) is the matrix \\[\nA=\n\\begin{bmatrix}\n0 & a & b \\\\\n-a & 0 & c \\\\\n-b & -c & 0\n\\end{bmatrix}\n\\] invertible? Suppose \\(a\\neq 0\\). Applying row-operations \\[\n\\begin{bmatrix}\n0 & a & b & 1 & 0 & 0\\\\\n-a & 0 & c & 0 & 1 & 0 \\\\\n-b & -c & 0 & 0 & 0 & 1\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{R_2\\leftrightarrow R_1}\n\\end{array}\n\\begin{bmatrix}\n-a & 0 & c & 0 & 1 & 0 \\\\\n0 & a & b & 1 & 0 & 0\\\\\n-b & -c & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\] \\[\n\\stackrel{\\longrightarrow}{-\\frac{1}{a}R_1}\n\\begin{bmatrix}\n1 & 0 & -\\frac{c}{a} & 0 & -\\frac{1}{a} & 0 \\\\\n0 & a & b & 1 & 0 & 0\\\\\n-b & -c & 0 & 0 & 0 & 1\n\\end{bmatrix}\\stackrel{\\longrightarrow}{bR_1+R_3}\n\\begin{bmatrix}\n1 & 0 & -\\frac{c}{a} & 0 & -\\frac{1}{a} & 0 \\\\\n0 & a & b & 1 & 0 & 0\\\\\n0 & -c & \\frac{-bc}{a} & 0 & \\frac{-b}{a} & 1\n\\end{bmatrix}\n\\] \\[\n\\stackrel{\\longrightarrow}{\\frac{1}{a}R_2}\n\\begin{bmatrix}\n1 & 0 & -\\frac{c}{a} & 0 & -\\frac{1}{a} & 0 \\\\\n0 & 1 & \\frac{b}{a} & \\frac{1}{a} & 0 & 0\\\\\n0 & -c & \\frac{-bc}{a} & 0 & \\frac{-b}{a} & 1\n\\end{bmatrix}\\stackrel{\\longrightarrow}{cR_2+R_3}\n\\begin{bmatrix}\n1 & 0 & -\\frac{c}{a} & 0 & -\\frac{1}{a} & 0 \\\\\n0 & 1 & \\frac{b}{a} & \\frac{1}{a} & 0 & 0\\\\\n0 & 0 & 0 & \\frac{c}{a} & \\frac{-b}{a} & 1\n\\end{bmatrix}\n\\] Thus, if \\(a\\neq 0\\) then \\(A\\) is not invertible, since \\(\\text{rref}{(A)}\\neq I_3\\). If \\(a=0\\), then clearly, \\(\\text{rref}{(A)}\\neq I_3\\), and so \\(A\\) is not invertible in either case. Therefore, there are no constants \\(a, b, c\\) for which \\(A\\) is invertible.\n\n\nCorollary 3.2 Let \\(A\\) be an \\(n \\times n\\) matrix.\n\nConsider a vector \\(\\vec b\\) in \\(\\mathbb{R}^n\\). If \\(A\\) is invertible, then the system \\(A \\vec x = \\vec b\\) has the unique solution \\(\\vec x = A^{-1} b\\). If \\(A\\) is non-invertible, then the system \\(A \\vec x = \\vec b\\) has infinitely many solutions or none.\nThe system \\(A \\vec x = \\vec 0\\) has \\(\\vec x = \\vec 0\\) as a solution. If \\(A\\) is invertible, then this is the only solution. If \\(A\\) is non-invertible, then the system \\(A \\vec x= \\vec 0\\) has infinitely many solutions.\n\n\n\nProof. The proof is left for the reader.\n\n\nExample 3.49 Find all invertible matrices \\(A\\) such that \\(A^2=A\\). Since \\(A\\) is invertible we multiply by \\(A^{-1}\\) to obtain: \\[\nA=IA=(A^{-1}A)A=A^{-1}(A^2)=A^{-1}A=I_n\n\\] and therefore \\(A\\) must be the identity matrix.\n\n\nExample 3.50 For which values of constants \\(b\\) and \\(c\\) is the matrix \\[\nB=\n\\begin{bmatrix}0 & 1 & b \\\\\n-1 & 0 & c \\\\\n-b & -c & 0\n\\end{bmatrix}\n\\] invertible? The matrix \\(B\\) is not invertible for any \\(b\\) and \\(c\\) since\\[\n\\text{rref}(B)=\n\\begin{bmatrix}1 & 0 & -c \\\\\n0 & 1 & b \\\\\n0 & 0 & 0\n\\end{bmatrix}\\neq I_3\n\\] for all \\(b\\) and \\(c\\).\n\n\nExample 3.51 Find the matrix \\(A\\) satisfying the equation \\[\n\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\nA\n\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\n=\n\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n.\\] Let \\(B=\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\\) and \\(C=\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\\). Then \\[\nB^{-1}=\\begin{bmatrix} 1& 0 \\\\ 0 &-1\\end{bmatrix}\n\\qquad \\text{and}\\qquad\nC^{-1}=\\begin{bmatrix} 1/2 & 0 \\\\ 0 & -1/2 \\end{bmatrix}.\n\\] Multiplying on the right by \\(B^{-1}\\) and on the left by \\(C^{-1}\\) we find \\[\nA=B^{-1}\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}C^{-1}\n=\\begin{bmatrix} 1/2 & -1/2 \\\\ -1/2 & 1/2\\end{bmatrix}.\n\\]\n\n\nExample 3.52 Suppose that \\(A\\), \\(B\\), and \\(C\\) are \\(n\\times n\\) matrices and that both \\(A\\) and \\(B\\) commute with \\(C\\). Show that \\(AB\\) commutes with \\(C\\).\nTo show that \\(AB\\) commutes with \\(C\\) we need to show \\((AB)C=C(AB)\\). This is easy since \\[\n(AB)C=A(BC)=A(CB)=(AC)B=(CA)B=C(AB).\n\\] Can you justify each step?\n\n\nExample 3.53 Show that \\(AB=BA\\) if and only if \\((A-B)(A+B)=A^2-B^2\\). Suppose \\(AB=BA\\) we will show \\((A-B)(A+B)=A^2-B^2\\). Starting with the left-hand side we obtain \\[\\begin{align*}\n(A-B)(A+B)\n& =(A-B)A+(A-B)B\n=A^2-BA+AB-B^2 \\\\\n& =A^2-BA+BA-B^2\n=A^2-B^2\n\\end{align*}\\] Now suppose \\((A-B)(A+B)=A^2-B^2\\), we will show \\(AB=BA\\). This is easy since \\[\n(A-B)(A+B)\n%=(A-B)A+(A-B)B\n=A^2-BA+AB-B^2\n=A^2-B^2\n\\] implies \\(-BA+AB=0\\) as desired."
  },
  {
    "objectID": "linear-transformations.html#coordinates",
    "href": "linear-transformations.html#coordinates",
    "title": "3  Linear Transformations",
    "section": "3.6 Coordinates",
    "text": "3.6 Coordinates\n\nDefinition 3.8 Let \\(\\mathcal{B}=(v_1,...,v_n)\\) be a basis of a subspace \\(V\\) of \\(\\mathbb{R}^n\\). For any \\(\\vec x \\in V\\) we can write \\(\\vec v= c_1 \\vec v_1 + \\cdots +c_m \\vec v_m\\). The scalars \\(c_1,...,c_m\\) are called the \\(\\mathcal{B}\\)-coordinates of \\(\\vec x\\) and the vector \\[\n\\left [ \\vec x \\right ]_{\\mathcal{B}}:= \\vectorthree{c_1}{\\hdots}{c_m}\n\\] is called the \\(\\mathcal{B}\\)-coordinate vector of \\(\\vec x\\)\n\nFor example the coordinates of \\(\\vectortwo{-2}{4}\\) with respect the standard basis \\(\\mathcal{B}=(\\vec{e}_1,\\vec{e}_2)\\) is \\(\\vectortwo{-2}{4}\\) since \\(\\vectortwo{-2}{4}=-2\\vec{e}_1+4\\vec{e}_2\\) which written as \\(\\vectortwo{-2}{4}_{\\mathcal{B}}=\\vectortwo{-2}{4}.\\) Notice the coordinates of \\(\\vectortwo{-2}{4}\\) with respect the basis \\(\\mathcal{B}'=\\left(\\vectortwo{2}{0},\\vectortwo{0}{2}\\right)\\) is \\(\\vectortwo{-1}{2}\\) since \\(\\vectortwo{-2}{4}=(-1)\\vectortwo{2}{0}+2\\vectortwo{0}{2}\\) which written as \\(\\vectortwo{-2}{4}_{\\mathcal{B}'}=\\vectortwo{-1}{2}.\\)\n\nExample 3.54 Consider the plane \\(2x_1-3x_2+4x_3=0\\) with basis \\[\n\\mathcal{B}=\\left(\\vectorthree{8}{4}{-1}, \\vectorthree{5}{2}{-1}\\right).\n\\] Let \\([\\vec x]_{\\mathcal{B}} = \\vectortwo{2}{-1}\\). Find \\(\\vec x\\). By definition of coordinates \\[\n\\vec x= 2\\vectorthree{8}{4}{-1} +(-1)\\vectorthree{5}{2}{-1}=\\vectorthree{11}{6}{-1}.\n\\]\n\n\nLemma 3.5  If \\(\\mathcal{B}=(\\vec{v}_1,...,\\vec{v}_n)\\) is a basis of a subspace \\(V\\) of \\(\\mathbb{R}^n\\), then\n\n\\([\\vec x]_{\\mathcal{B}}+[\\vec y]_{\\mathcal{B}}=[\\vec x+\\vec y]_{\\mathcal{B}}\\), for all vectors \\(\\vec x, \\vec y\\)\n\\([k\\vec x]_{\\mathcal{B}}=k[\\vec x]_{\\mathcal{B}}\\), for all vectors \\(\\vec x\\), and all scalars \\(k\\).\n\n\n\nProof. Let \\([\\vec x]_{\\mathcal{B}}=c_1 \\vec{v}_1+\\cdots + c_n \\vec{v}_n\\)\nand \\([\\vec y]_{\\mathcal{B}}=d_1 \\vec{v}_1+\\cdots + d_n \\vec{v}_n\\) be the representation of \\(\\vec{x}\\) and \\(\\vec{y}\\) with respect to \\(\\mathcal{B}\\). Then \\[\n[\\vec x]_{\\mathcal{B}}+[\\vec y]_{\\mathcal{B}}\n=\\vectorthree{c_1}{\\vdots}{c_3}+\\vectorthree{d_1}{\\vdots}{d_3}\n=\\vectorthree{c_1+d_1}{\\vdots}{c_n+d_n}\n=[\\vec x+y]_{\\mathcal{B}}\n\\] where the last equality holds since \\[\n\\vec{x}+\\vec{y}=(c_1+d_1)\\vec{v}_1+\\cdots (c_n+d_n)\\vec{v}_n.\\] We leave the proof of the second part for the reader.\n\n\nExample 3.55 Determine whether the vector \\(\\vec x = \\vectorthree{1}{-2}{-2}\\) is in \\(\\text{span} V\\) of the vectors \\(\\vec{v}_1=\\vectorthree{8}{4}{-1}\\) and \\(\\vec{v}_2=\\vectorthree{5}{2}{-1}\\) and if so write the coordinates of \\(\\vec x\\) with respect to this basis of \\(V\\). We need to find scalars \\(c_1\\) and \\(c_2\\) such that \\(\\vec x=c_1 \\vec v_1+c_2 \\vec v_2\\). Solving this system we find \\(c_1=-3\\) and \\(c_2=5\\). Therefore we find, \\([\\vec x]_{\\mathcal{B}}=\\vectortwo{-3}{5}\\) where \\(\\mathcal{B}=(\\vec v_1, \\vec v_2)\\).\n\n\nExample 3.56 Consider the plane \\(x+2y+z=0\\). Find a basis of this plane. Find another basis \\(\\mathcal{B}\\) of this plane such that \\([\\vec x]_{\\mathcal{B}}=\\vectortwo{2}{-1}\\) for \\(\\vec x = \\vectorthree{1}{-1}{1}\\). We find a basis by letting \\(z=t\\) and \\(y=s\\) be free variables. Then \\(x=-t-2s\\). All solutions to the equation \\(x+2y+z=0\\) are \\[\n\\vectorthree{x}{y}{z}\n=\\vectorthree{-t-2s}{s}{t}\n=t\\vectorthree{-1}{0}{1}+s\\vectorthree{-2}{1}{0}.\n\\] So a basis for the plane is \\(\\mathcal{B}=\\left(\\vectorthree{-1}{0}{1},\\vectorthree{-2}{1}{0}\\right).\\) Notice \\(\\vectorthree{1}{-1}{1}=(1)\\vectorthree{-1}{0}{1}+(-1)\\vectorthree{-2}{1}{0}\\) and thus \\(\\vectorthree{1}{-1}{1}_{\\mathcal{B}}=\\vectortwo{1}{-1}\\). Thus this is not the basis we seek. However, notice \\[\n\\vectorthree{1}{-1}{1}\n=2\\vectorthree{x_1}{0}{-x_1}+(-1)\\vectorthree{-2y_2}{y_2}{0}\n\\] holds when \\(y_2=1\\) and \\(x_1=-1/2\\). Also notice the vectors \\(\\vectorthree{-1/2}{0}{1/2}\\) and \\(\\vectorthree{-2}{1}{0}\\) span the plane and are linearly independent. Therefore we have the basis we seek, namely \\[\n\\mathcal{B}\n=\\left(\\vectorthree{-1/2}{0}{1/2},\\vectorthree{-2}{1}{0}\\right).\n\\]\n\n\nLemma 3.6  Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) and \\(\\mathcal{B}=(\\vec{v}_1,...,\\vec{v}_n)\\) a basis of \\(\\mathbb{R}^n\\). The \\(n\\times n\\) matrix \\(B\\) that transforms \\([ \\vec{x}]_{\\mathcal{B}}\\) into \\([T(\\vec x)]_{\\mathcal{B}}\\) is called the \\(\\mathcal{B}\\)-matrix of \\(T\\), written as \\([T(\\vec x)]_{\\mathcal{B}}=B [ \\vec x ]_{\\mathcal{B}}\\) for all \\(\\vec x\\) in \\(\\mathbb{R}^n\\) and \\[\nB=\n\\begin{bmatrix}\n[T(\\vec v_1)]_{\\mathcal{B}} &  \\cdots & [T(\\vec v_n)]_{\\mathcal{B}}\n\\end{bmatrix}.\n\\]\n\n\nProof. Since \\(\\mathcal{B}\\) is a basis of \\(\\mathbb{R}^n\\), there exists scalars \\(c_1, ..., c_n\\) such that \\(\\vec{x}=c_1 \\vec{v}_1+c_2\\vec{v}_2+\\cdots + c_n \\vec{v}_n\\). Using the linearity of \\(T\\) we find \\[\nT(\\vec{x})=c_1 T(\\vec{v}_1)+c_2 T(\\vec{v}_2)+\\cdots +c_n T(\\vec{v}_n).\n\\] By \\(\\ref{lincoord}\\) we find \\[\\begin{align*}\n[T(\\vec{x})]_{\\mathcal{B}}\n& = c_1 [T(\\vec{v}_1)]_{\\mathcal{B}} + c_2 [T(\\vec{v}_2)]_{\\mathcal{B}} + \\cdots +c_n [T(\\vec{v}_n)]_{\\mathcal{B}} \\\\\n& =\n\\begin{bmatrix}\n[T(\\vec v_1)]_{\\mathcal{B}} &  \\cdots & [T(\\vec v_n)]_{\\mathcal{B}}\n\\end{bmatrix}\n[\\vec{x}]_{\\mathcal{B}}\n\\end{align*}\\] as desired.\n\n\nExample 3.57 Find the matrix \\(\\mathcal{B}\\) of the linear transformation \\(T(\\vec x)=A \\vec x\\) where \\[\nA=\\begin{bmatrix}\n5 & -4 & -2 \\\\\n-4 & 5 & -2 \\\\\n-2 & -2 & 8\n\\end{bmatrix}\n\\] with respect to the basis \\(\\mathcal{B}=\\left ( \\vectorthree{2}{2}{1}, \\vectorthree{1}{-1}{0}, \\vectorthree{0}{1}{-2} \\right )\\). By \\(\\ref{lem:bmatrix}\\), \\(T\\) with respect to \\(\\mathcal{B}\\) has the following matrix \\(B\\). \\[\n\\begin{bmatrix}\n\\left[T\\vectorthree{2}{2}{1}\\right]_{\\mathcal{B}} &\n\\left[T\\vectorthree{1}{-1}{0}\\right]_{\\mathcal{B}} &\n\\left[T\\vectorthree{0}{1}{-2}\\right]_{\\mathcal{B}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\vectorthree{0}{0}{0}_{\\mathcal{B}} &\n\\vectorthree{9}{-9}{0}_{\\mathcal{B}} &\n\\vectorthree{0}{9}{-18}_{\\mathcal{B}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 9 & 0 \\\\\n0 & 0 & 9\n\\end{bmatrix}.\n\\]\n\n\nTheorem 3.25 Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) with standard matrix \\(A\\) and let \\(B\\) be the \\(\\mathcal{B}\\)-matrix of \\(T\\) where \\(\\mathcal{B}= (\\vec{v}_1,...,\\vec{v}_n)\\) then \\(A S= S B\\) where \\[\nS= \\begin{bmatrix}\n\\vec{v}_1 & \\cdots &  \\vec{v}_n\n\\end{bmatrix}\n.\\]\n\n\nProof. The proof is left for the reader.\n\n\nDefinition 3.9 Two \\(n\\times n\\) matrices \\(A\\) and \\(B\\) are called similar if there exists an invertible matrix \\(S\\) such that \\(A S= S B\\).\n\n\nExample 3.58 Determine whether the following two matrices are similar. \\[\nA=\\begin{bmatrix} 1 & 2 \\\\ 4 &3 \\end{bmatrix}\n\\qquad \\text{and} \\qquad\nB=\\begin{bmatrix} 5 & 0 \\\\ 0 & -1 \\end{bmatrix}\\] We are looking for a matrix \\[\nS=\n\\begin{bmatrix} x & y \\\\ z & t\\end{bmatrix}\n\\] such that \\(AS=SB\\). Writing out we find \\[\n\\begin{bmatrix} 1 & 2 \\\\ 4 &3 \\end{bmatrix}\n\\begin{bmatrix} x & y \\\\ z & t\\end{bmatrix}\n=\n\\begin{bmatrix} x & y \\\\ z & t\\end{bmatrix}\n\\begin{bmatrix} 5 & 0 \\\\ 0 & -1 \\end{bmatrix}\n\\quad \\implies \\quad\n\\begin{bmatrix} x+2z & y+2t \\\\ 4x+3z & 4y+3t \\end{bmatrix}\n=\n\\begin{bmatrix} 5x & -y  \\\\ 5z & -t \\end{bmatrix}.\n\\] This leads to \\(z=2x\\) and \\(t=-y\\) so that \\(S\\) is any invertible matrix of the form \\[\\begin{equation}\n\\label{simform}\n\\begin{bmatrix}x & y \\\\ 2x & -y\\end{bmatrix}.\n\\end{equation}\\] Since \\(S= \\begin{bmatrix}1 & 1 \\\\ 2 & -1 \\end{bmatrix}\\) is invertible and of the form in \\(\\eqref{simform}\\), we can say, yes \\(A\\) is similar to \\(B\\).\n\n\nTheorem 3.26 If \\(A\\) is similar to \\(B\\) then \\(A^k\\) is similar to \\(B^k\\) for any positive integer \\(k\\).\n\n\nProof. If \\(A\\) is similar to \\(B\\) then there exists an invertible matrix \\(S\\) such that \\(B=S^{-1}A S\\). Then \\[\nB^k=(S^{-1}AS)(S^{-1}AS)\\cdots (S^{-1}AS)=S^{-1}A^k S\n\\] which shows that \\(B^k\\) is similar to \\(A^k\\).\n\n\nTheorem 3.27  Two \\(n\\times n\\) matrices \\(A\\), \\(A'\\) are similar if and only if they are matrices of the same linear transformation \\(T\\) from \\(\\mathbf{R}^n\\) to \\(\\mathbf{R}^n\\) with respect to two bases for \\(\\mathbf{R}^n\\)\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 3.28 Similar matrices have the same rank.\n\n\nProof. Let \\(A\\) and \\(B\\) be similar matrices. By \\(\\ref{simsamelin}\\) \\(A\\) and \\(B\\) represent the same linear transformation. Thus they must have the same rank.\n\n\nExample 3.59 Are the following similiar matrices? \\[\nA=\n\\begin{bmatrix}\n-1 & 2 & 1 \\\\\n2 & 0 & 1 \\\\\n2 & 2 & 2\n\\end{bmatrix}\n\\qquad \\text{and}\\qquad\nB=\n\\begin{bmatrix}\n-1 & 2 & 1 \\\\\n2 & 0 & 1 \\\\\n1 & 2 & 2\n\\end{bmatrix}\n\\] Since \\(\\text{rank}(A)=3\\) and \\(\\text{rank}(B)=2\\), these are not similar matrices.\n\n\nTheorem 3.29  Similarity of matrices is an equivalence relation.\n\n\nProof. To show transitivity, assume \\(A\\) is similar to \\(B\\) and \\(B\\) is similar to \\(C\\), then there exists invertible matrices \\(E\\) and \\(F\\) such that \\(AE=EB\\) and \\(BF=FC\\). Then \\[\nA=EBE^{-1}=E(FCF^{-1})E^{-1}=(EF)C(F^{-1}E^{-1})=(EF)C(EF)^{-1}\n\\] which shows that \\(A\\) is similar to \\(C\\). The proof of the reflexive property and the symmetric property are left for the reader.\n\nBy \\(\\ref{simequiv}\\), a linear transformation represents a whole class of (similar) matrices. That is the collection of \\(n\\times n\\) matrices is partitioned into non overlapping sets of matrices, with all matrices in any such set being similar and representing the same linear transformation from \\(\\mathbf{R}^n\\) to \\(\\mathbf{R}^n\\)."
  },
  {
    "objectID": "linear-transformations.html#coordinates-and-the-matrix-of-a-linear-map",
    "href": "linear-transformations.html#coordinates-and-the-matrix-of-a-linear-map",
    "title": "3  Linear Transformations",
    "section": "3.7 Coordinates and the Matrix of a Linear Map",
    "text": "3.7 Coordinates and the Matrix of a Linear Map\nLet \\(V\\) and \\(W\\) be finite dimensional linear spaces.\n\nDefinition 3.10 (The Matrix of a Linear Map) Let \\(T\\in \\mathcal{L}(V,W)\\) and let \\(b_1=\\{v_1,\\ldots ,v_n\\}\\) be a basis for \\(V\\) and \\(b_2=\\{w_1,\\ldots ,w_m\\}\\) be a base for \\(W\\). Then the matrix of \\(T\\) with respect to the bases \\(b_1\\) and \\(b_2\\) is \\[ \\begin{bmatrix}\na_{1 1} & \\cdots & a_{1 n} \\\\\n\\vdots & \\cdots & \\vdots \\\\\na_{m 1} & \\cdots & a_{m n} \\\\\n\\end{bmatrix}\n\\] where the \\(a_{i j}\\in \\mathbb{F}\\) are determined by \\(T v_k=a_{1 k}w_1+\\cdots +a_{m k}w_m\\) for each \\(k=1,\\ldots ,n\\).\n\n\nExample 3.60 Consider the linear transformation \\(T(f)=f'+f''\\) from \\(\\mathcal{P}_2\\) to \\(\\mathcal{P}_2\\). Since \\(\\mathcal{P}_2\\) is isomorphic to \\(\\mathbb{R}^3\\) with isomorphism given by a \\(3\\times 3\\) matrix \\(B\\), how do we find this matrix \\(B\\)? Let \\(a+b x+c x^2\\), then we write \\(T\\) as \\[\\begin{align*}\nT(a+b x+c x^2)& =(a+b x+c x^2)'+(a+b x+c x^2)'' \\\\\n& =b+2c x+2c=(b+2c)+2cx.\n\\end{align*}\\] Next let’s write the input \\(f(x)=a+b x+c x^2\\) and the output \\(T(f(x))=(b+2c)+2c x\\) in coordinates with respect to the standard bases \\(\\mathcal{B}=(1, x, x^2)\\) of \\(\\mathcal{P}_2\\) Written in \\(\\mathcal{P}_2\\) coordinates, transformation \\(T\\) takes \\([f(x)]_{\\mathcal{B}}\\) to \\[\n[T(f(x))]_{\\mathcal{B}}=\\vectorthree{b+2c}{2c}{0}=\n\\begin{bmatrix}\n0 & 1 & 2 \\\\\n0 & 0 & 2 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\vectorthree{a}{b}{c}\n= \\begin{bmatrix}\n0 & 1 & 2 \\\\\n0 & 0 & 2 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n[f(x)]_{\\mathcal{B}}\n\\] The matrix \\[\nB= \\begin{bmatrix}\n0 & 1 & 2 \\\\\n0 & 0 & 2 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\] is called the \\(\\mathcal{B}\\)-matrix of the transformation \\(T\\).\n\n\nExample 3.61 Find the \\(B\\)-matrix for the linear transformation given by \\[\nT(M)=\\begin{bmatrix} 1 & 2 \\\\ 3 & 4  \\end{bmatrix} M - M \\begin{bmatrix} 5 & 0 \\\\ 0 & -1  \\end{bmatrix}M\n\\] from \\(\\mathbb{R}^{2\\times 2}\\) to \\(\\mathbb{R}^{2\\times 2}\\). Determine whether \\(T\\) is an isomorphism and if not find kernel, image, nullity, and the rank of \\(T\\). We will use the standard basis of \\(\\mathbb{R}^{2 \\times 2}\\): \\[\n\\mathcal{B}=\n\\left (\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 1 \\\\ 0 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 \\\\ 1 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 \\\\ 0 & 1  \\end{bmatrix}\n\\right )\n\\] and we will construct the \\(\\mathcal{B}\\)-matrix column-by-column: \\[\n\\begin{array}{rl}\nB & =\n\\begin{bmatrix} \\left [ T\\begin{bmatrix} 1 & 0 \\\\ 0 & 0  \\end{bmatrix} \\right ]_{\\mathcal{B}} &\n\\left [T[\\begin{bmatrix} 0 & 1 \\\\ 0 & 0  \\end{bmatrix}\\right ]_{\\mathcal{B}} &\n\\left [ T\\begin{bmatrix} 0 & 0 \\\\ 1 & 0  \\end{bmatrix}\\right ]_{\\mathcal{B}} &\n\\left [ T\\begin{bmatrix} 0 & 0 \\\\ 0 & 1  \\end{bmatrix} \\right ]_{\\mathcal{B}}\n\\end{bmatrix}\\\\\n\\\\\n&=\n\\begin{bmatrix} \\begin{bmatrix} -4 & 0 \\\\ 4 & 0  \\end{bmatrix}_{\\mathcal{B}} &\n\\begin{bmatrix} 0 & 2 \\\\ 0 & 4  \\end{bmatrix}_{\\mathcal{B}} &\n\\begin{bmatrix} 2 & 0 \\\\ -2 & 0  \\end{bmatrix}_{\\mathcal{B}} &\n\\begin{bmatrix}  0 & 2 \\\\ 0 & 4  \\end{bmatrix}_{\\mathcal{B}}\n\\end{bmatrix}\n=\n\\begin{bmatrix} -4 & 0 & 2 & 0\\\\\n0 & 2 & 0 & 2 \\\\\n4 & 0 & -2 & 0 \\\\\n0 & 4 & 0 & 4\n\\end{bmatrix}\\end{array}\n\\] with \\[\n\\text{rref}(B)=\n\\begin{bmatrix} 1 & 0 & -\\frac{1}{2} & 0\\\\\n0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}.\n\\] After eliminating redundant columns from \\(B\\) we find a basis of \\(\\text{im} T\\) is \\[\\left(\\, \\vectorfour{-4}{0}{4}{0},\\vectorfour{0}{2}{0}{4} \\, \\right).\\] To find \\(\\ker T\\) we solve \\(B \\vec x=\\vec 0\\) and using the \\(\\text{rref}(B)\\) we find \\[\\left(\\, \\vectorfour{-\\frac{1}{2}}{0}{1}{0},\\vectorfour{0}{-1}{0}{1} \\, \\right)\\] to be a basis for \\(\\ker T.\\) Notice since the rank of \\(T\\) is \\(2=\\text{dim} \\text{im} T\\), \\(T\\) is not an isomorphism. Therefore, since \\(\\ker T\\neq \\{0\\}\\), \\(T\\) is not an isomorphism.\n\n\nExample 3.62 Find the matrix of the linear transformation \\(T(f(t))=f(3)\\) from \\(\\mathcal{P}_2\\) to \\(\\mathcal{P}_2\\) with respect to the basis \\((1,t-3,(t-3)^2)\\). Determine whether the transformation is an isomorphism, it it isn’t an isomorphism then determine the kernel and image of \\(T\\), and also determine the nullity and rank of \\(T\\). The matrix of \\(T\\) is \\[\nB=\\begin{bmatrix} [T(1)]_{\\mathcal{B}} & [T(x-3)]_{\\mathcal{B}} & [T((x-3)^2)]_{\\mathcal{B}} \\end{bmatrix}\n= \\begin{bmatrix}\n[1]_{\\mathcal{B}} & [0]_{\\mathcal{B}} & [0]_{\\mathcal{B}}\n\\end{bmatrix}\n= \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\  0 & 0 & 0  \\end{bmatrix}.\n\\] Notice the vectors \\(\\vectorthree{0}{1}{0}, \\vectorthree{0}{0}{1}\\) form a basis of the kernel of \\(B\\) and \\(\\vectorthree{1}{0}{0}\\) is a basis of the image of \\(B\\). Therefore, the rank is 1 and the nullity is 2; and therefore, \\(T\\) is not an isomorphism.\n\n\nDefinition 3.11 (The Matrix of a Vector) Let \\(b=\\{v_1,\\ldots ,v_n\\}\\) be a basis for \\(V\\) and let \\(v\\in V\\). We define the matrix of \\(v\\), denoted by \\(\\mathcal{M}(v)\\), to be the \\(n\\)-by-1 matrix \\(\\begin{bmatrix} b_{1} & \\cdots& b_{n} \\end{bmatrix}^T\\).\n\n\nTheorem 3.30 If \\(T\\in \\mathcal{L}(V,W)\\), then \\(\\mathcal{M}(Tv)=\\mathcal{M}(T) \\mathcal{M}(v)\\) for all \\(v\\in V\\).\n\n\nProof. Let \\((v_1,\\ldots ,v_n)\\) be a basis of \\(V\\) and \\((w_1,\\ldots ,w_m)\\) be a basis of \\(W\\). If \\(v\\in V\\), then there exists \\(b_1,\\ldots ,b_n\\in \\mathbb{F}\\) such that \\(v=b_1 v_1+\\cdots + b_n v_n\\) so that \\(\\mathcal{M}(v)=\\begin{bmatrix} b_{1} & \\cdots& b_{n} \\end{bmatrix}^T\\). For each \\(k\\), \\(1\\leq k \\leq n\\) we write \\(T v_k=a_{1k}w_1+ \\cdots + a_{m k} w_m\\) and so by definition of the matrix of a linear map \\(T\\): \\[\n\\mathcal{M}=\\begin{bmatrix}a_{11} & & a_{1n}  \\\\ \\vdots & \\cdots & \\vdots \\\\ a_{m1} & & a_{mn} \\end{bmatrix}.\n\\] By linearity of \\(T\\): \\[\n\\begin{array}{rl}\nTv& =b_1 T v_1+\\cdots b_n T v_n \\\\\n& = b_1 \\left(\\sum_{j=1}^m a_{j 1}w_j \\right)+\\cdots +b_n \\left(\\sum_{j=1}^m a_{j n}w_j \\right) \\\\\n& =w_1(a_{11}b_1+\\cdots + a_{1n}b_n)+\\cdots + w_m(a_{m1}b_1+\\cdots + a_{mn}b_n).\n\\end{array}\n\\] Therefore, \\[\n\\mathcal{M}(T v)=\n\\vectorthree{a_{11}b_1+\\cdots + a_{1n}b_n}{\\cdots}{a_{m1}b_1+\\cdots + a_{mn}b_n}\n=\\mathcal{M}(T)\\mathcal{M}(v).\n\\] where the last equality holds by definition of matrix multiplication.\n\nLet \\(\\overline{u}=(u_1,\\ldots ,u_p)\\) be a basis of \\(U\\), let \\(\\overline{v}=(v_1,\\ldots ,v_n)\\) be a basis of \\(V\\), and let \\(\\overline{w}=(w_1,\\ldots ,w_m)\\) be a basis of \\(W\\). If \\(T\\in \\mathcal{L}(U,V)\\) and \\(S\\in \\mathcal{L}(V,W)\\), then \\(ST\\in \\mathcal{L}(U,W)\\) and by the definition of matrix multiplication, \\[\\begin{equation}\\label{matrix multiplication}\n\\mathcal{M}(ST,\\overline{u},\\overline{w})=\\mathcal{M}(S,\\overline{v},\\overline{w}) \\mathcal{M}(T,\\overline{u},\\overline{v}).\n\\end{equation}\\]\n\nTheorem 3.31 If \\(\\overline{u}=(u_1,\\ldots ,u_p)\\) and \\(\\overline{v}=(v_1,\\ldots ,v_n)\\) are bases of \\(V\\), then \\(\\mathcal{M}(I,\\overline{u},\\overline{v})\\) is invertible and \\[\n\\mathcal{M}(I,\\overline{u},\\overline{v})^{-1}=\\mathcal{M}(I,\\overline{v},\\overline{u}).\n\\]\n\n\nProof. In \\(\\ref{matrix multiplication}\\), replace \\(U\\) and \\(W\\) with \\(V\\), replace \\(w_j\\) with \\(u_j\\), and replace \\(S\\) and \\(T\\) with \\(I\\), getting \\[\nI=\\mathcal{M}(I,(\\overline{v},\\overline{u}))\\mathcal{M}(I,\\overline{u},\\overline{v}).\n\\] Now interchange the roles of the \\(u\\)’s and \\(v\\)’s, getting \\[\nI=\\mathcal{M}(I,(\\overline{u},\\overline{v}))\\mathcal{M}(I,\\overline{v},\\overline{u}).\n\\] These equations give the desired result.\n\nFor example, obviously, \\[\n\\mathcal{M}\\left(I,\\left(\\vectortwo{4}{2},\\vectortwo{5}{3}\\right),\\left(\\vectortwo{1}{0},\\vectortwo{0}{1}\\right)\\right)=\\begin{bmatrix}  4 & 5 \\\\ 2 & 3 \\end{bmatrix} .\n\\] The inverse of the matrix above is \\(\\begin{bmatrix} 3/2 & -5/2 \\\\ -1 & 2\\end{bmatrix}\\). Thus, \\[\n\\mathcal{M}\\left(I,\\left(\\vectortwo{1}{0},\\vectortwo{0}{1}\\right),\\left(\\vectortwo{4}{2},\\vectortwo{5}{3}\\right)\\right)\n=\\begin{bmatrix} 3/2 & -5/2 \\\\ -1 & 2\\end{bmatrix}.\n\\]\n\nTheorem 3.32 Suppose \\(T\\in\\mathcal{L}(V)\\). Let \\(\\overline{u}=(u_1,\\ldots ,u_n)\\) and \\((v_1,\\ldots ,v_n)\\) be bases of \\(V\\). Let \\(A=\\mathcal{M}(I,\\overline{u},\\overline{v})\\). Then \\[\\begin{equation}\\label{change of basis}\n\\mathcal{M}(T,\\overline{u})=A^{-1}\\mathcal{M}(T,\\overline{v})A.\n\\end{equation}\\]\n\n\nProof. In \\(\\ref{matrix multiplication}\\), replace \\(U\\) and \\(W\\) with \\(V\\), replace \\(w_j\\) with \\(v_j\\), replace \\(T\\) with \\(I\\), and replace \\(S\\) with \\(T\\), getting \\[\\begin{equation}\\label{rchange}\n\\mathcal{M}(T,\\overline{u},\\overline{v})=\\mathcal{M}(T,\\overline{v})A.\n\\end{equation}\\] In \\(\\ref{matrix multiplication}\\), replace \\(U\\) and \\(W\\) with \\(V\\), replace \\(w_j\\) with \\(u_j\\), replace \\(S\\) with \\(I\\), and replace \\(S\\) with \\(T\\), getting \\[\\begin{equation}\\label{lchange}\n\\mathcal{M}(T,\\overline{u})=A^{-1}\\mathcal{M}(T,\\overline{u},\\overline{v})\n\\end{equation}\\] by \\(\\ref{matrix inversion}\\). Substitution of \\(\\ref{rchange}\\) into \\(\\ref{lchange}\\) yields \\(\\ref{change of basis}\\).\n\n\nExample 3.63 Prove that every linear map from \\(\\text{Mat}(n,1,F)\\) to \\(\\text{Mat}(m,1,F)\\) is given by matrix multiplication. In other words, prove that if \\(T\\) is a linear transformation from \\(\\text{Mat}(n,1,F)\\) to \\(\\text{Mat}(m,1,F))\\), then there exists an \\(m\\)-by-\\(n\\) matrix \\(A\\) such that \\(T B=A B\\) for every \\(B\\in \\text{Mat}(n,1,F).\\) Let \\((e_1,\\ldots ,e_n)\\) be a basis for \\(Mat(n,1,\\mathbb{F})\\) and let \\((v_1,\\ldots ,v_m)\\) be a basis for \\(Mat(m,1,\\mathbb{F})\\). For each \\(k\\), there exists \\(a_{1k},\\ldots ,a_{mk}\\in \\mathbb{F}\\) such that \\(T e_k=a_{1k} v_1+\\cdots a_{mk} v_m\\). Define the \\(m \\times n\\) matrix \\(A\\) as follows: \\[\nA=\\begin{bmatrix} T e_1 & \\cdots & T e_n \\end{bmatrix}.\n\\] If \\(B\\in Mat(n,1,\\mathbb{F})\\) there exists \\(b_1,\\ldots ,b_n\\in \\mathbb{F}\\) such that \\(B=b_1 e_1+\\cdots + b_n e_n\\), and thus \\[\nTB=T(b_1 e_1+\\cdots + b_n e_n)=b_1 T e_1+\\cdots + b_n T e_n= BA\n\\] as desired. Notice the word “the” follows since \\((v_1,\\ldots ,v_m)\\) is a basis. In other words one bases have been chosen, the matrix \\(A\\) is unique.\n\n\nExample 3.64 Suppose that \\(V\\) is finite-dimensional and \\(S,T\\in \\mathcal{L}(V)\\). Prove that \\(S T\\) is invertible if and only if both \\(S\\) and \\(T\\) are invertible. Suppose both \\(S\\) and \\(T\\) are invertible. Then both \\(S\\) and \\(T\\) are injective and surjective. Thus, \\(ST\\) is both injective and surjective showing \\(ST\\) is invertible. Conversely, suppose \\(ST\\) is invertible. Since \\(\\text{ker} T\\subseteq \\text{ker} ST =\\{0\\}\\) because \\(ST\\) is injective, \\(T\\) is also injective. Thus, \\(T\\) is invertible. Since \\(ST\\) is surjective, if \\(w\\in W\\), then there exists \\(v\\in V\\) such that \\((ST)v=w\\). Rewriting \\(S(Tv)=w\\), showing \\(S\\) is surjective. Thus, \\(S\\) is also invertible.\n\n\nExample 3.65 Suppose that \\(V\\) is finite-dimensional and \\(S,T\\in \\mathcal{L}(V)\\). Prove that \\(S T=I\\) if and only if \\(T S=I\\). Without loss of generality, we will show \\(ST=I \\text{im}plies TS=I\\). Suppose \\(ST=I\\). Since \\(I\\) is invertible, the previous exercise implies \\(S\\) and \\(T\\) are both invertible. Then \\(ST=I \\text{im}plies S^{-1}(ST)=S^{-1}I \\text{im}plies T=S^{-1}\\). Therefore, \\(TS=S^{-1}S=I\\).\n\n\nExample 3.66 Suppose that \\(V\\) is finite-dimensional and \\(T\\in \\mathcal{L}(V)\\). Prove that \\(T\\) is a scalar multiple of the identity if and only if \\(S T=T S\\) for every \\(S\\in \\mathcal{L}(V)\\). If \\(T\\) is a scalar multiple of the identity, say \\(T=\\alpha I\\), then for all \\(v\\in V\\), \\[\nS T v=S \\alpha v= \\alpha S v= TS v.\n\\] Conversely, suppose \\(ST=TS\\) for every \\(S\\in \\mathcal{L}(V)\\). Pick a basis \\(v_1,\\ldots ,v_N)\\) for \\(V\\). For \\(m=1,\\ldots ,N\\), define linear maps \\(S+m\\in \\mathcal{L}(V)\\) by\\[\nS_m=v_n=\\left\\{ \\begin{array}{rl} v_m & \\text{ if } m=n \\\\ 0 & \\text{ if } m\\neq n \\end{array} \\right.\n\\] Now if \\(v=\\sum \\alpha_n v_n\\), then\\[\nS_m\\sum \\alpha_n v_n=\\alpha_m v_m\n\\] Thus the only vectors satisfying \\(S_m v=v\\) are \\(v=\\alpha v_m\\) for some \\(\\alpha \\in \\mathbb{F}\\). The condition \\(S_m T=T S_m\\) gives\\[\nS_m T v_m=T S_m v_m=T v_m\n\\] and by the above observation \\(T v_m=\\alpha_m v_m\\). Now consider another collection of linear maps \\(A(m,n)\\) defined by\\[\nA(m,n) v_m=v_n, \\hspace{1cm} A(m,n)v_n=v_m, \\hspace{1cm} A(m,n)v_k=0, \\text{ when } k\\neq m,n.\\] The condition \\(A(m,n)T v_n=T A(m,n) v_n\\) gives \\[\nA(m,n)T v_n=TA(m,n)v_n=T v_m=\\alpha_m v_m\n\\] and \\[\nA(m,n)\\alpha_n v_n=A(m,n)\\alpha_n v_n=\\alpha_nA(m,n)v_n=\\alpha_n v_m.\n\\] Whence \\(\\alpha_m v_m=\\alpha_n v_m\\) that is, \\(\\alpha_m=\\alpha_n\\) for \\(m,n=1,\\ldots ,N\\); and thus \\(T\\) is a scalar multiple of the identity.\n\n\nExample 3.67 Prove that if \\(V\\) is finite-dimensional with \\(\\text{dim} V > 1\\), then the set of non-invertible operators on \\(V\\) is not a subspace of \\(\\mathcal{L}(V)\\). Suppose \\((v_1,\\ldots ,v_n)\\) is a basis of \\(V\\), with \\(n \\geq 2\\). Define the linear maps \\(S\\) and \\(T\\) by \\[S v_1=v_1, \\hspace{1cm} S v_k =0, \\text{ when } k\\geq 2\\] \\[T v_1=0, \\hspace{1cm} T v_k =v_k, \\text{ when } k\\geq 2.\\] Since \\(S\\) and \\(T\\) have nontrivial null spaces, they are not invertible. However, \\((S+T) v_k=v_k\\), for \\(k=1,\\ldots ,n,\\) so \\(S+T=I\\), which is invertible. Thus the set of noninvertible operators on \\(V\\) with \\(n\\geq 2\\) is not closed under addition."
  },
  {
    "objectID": "inner-products-spaces.html#orthonormal-bases-and-orthogonal-projections",
    "href": "inner-products-spaces.html#orthonormal-bases-and-orthogonal-projections",
    "title": "4  Inner Products Spaces",
    "section": "4.1 Orthonormal Bases and Orthogonal Projections",
    "text": "4.1 Orthonormal Bases and Orthogonal Projections\nThe norm of a vector \\(\\vec v\\) in \\(\\mathbb{R}^n\\) is \\(\\norm{\\vec v}=\\sqrt{\\vec v \\cdot \\vec v}\\). A vector \\(\\vec u\\) in \\(\\mathbb{R}^n\\) is called a unit vector if \\(\\norm{\\vec u}=1\\).\n\nExample 4.1 If \\(\\vec v\\in \\mathbb{R}^n\\) and \\(k\\) is a scalar, then \\(\\norm{k \\vec v}=|k| \\norm{\\vec v}\\), and if \\(v\\) is nonzero then \\(\\vec u=\\frac{1}{\\norm{\\vec v}} \\vec v\\) is a unit vector. Since \\(\\norm{k \\vec v}^2=(k \\vec v)\\cdot (k \\vec v)=k^2(\\vec v \\cdot \\vec v)=k^2\\norm{\\vec v}^2\\), taking square roots provides \\(\\norm{k \\vec v}=|k| \\norm{\\vec v}\\). If \\(v\\) is nonzero, then \\(\\frac{1}{\\norm{\\vec v}}\\) is defined and so \\(\\norm{\\vec u}= \\frac{1}{\\norm{\\vec v}} \\norm{\\vec v}=1\\) which follows by the first part.\n\nTwo vectors \\(\\vec v\\) and \\(\\vec w\\) in \\(\\mathbb{R}^n\\) are called perpendicular or orthogonal if \\(\\vec v \\cdot \\vec w=0\\). The vectors \\(\\vec u_1,\\ldots,\\vec u_m\\) in \\(\\mathbb{R}^n\\) are called orthonormal if they are all unit vectors and orthogonal to one another. A basis of \\(\\mathbb{R}^n\\) consisting only of orthonormal vectors is called an orthonormal basis.\n\nTheorem 4.1 (Orthonormal Vectors) Orthonormal vectors are linearly independent, and thus orthonormal vectors \\(\\vec u_1,\\ldots,\\vec u_n\\in \\mathbb{R}^n\\) form a basis for \\(\\mathbb{R}^n\\).\n\n\nProof. Suppose \\((\\vec u_1,\\ldots,\\vec u_m)\\) are orthonormal vectors in \\(\\mathbb{R}^n\\). To show linear independence suppose,\n\\[\nc_1 \\vec u_1+\\cdots + c_m \\vec u_m=\\vec 0\n\\] for some scalars \\(c_1,\\ldots,c_m\\) in \\(\\mathbb{R}\\). Applying the dot product with \\(\\vec u_i\\), \\[\n\\left ( c_1 \\vec u_1 + \\cdots +c_m \\vec u_m\\right ) \\cdot \\vec u_i =\\vec 0 \\cdot \\vec u_i=0.\n\\] Because the dot product is distributive, \\(c_1(\\vec u_1 \\cdot \\vec u_i )+\\cdots + c_m (\\vec u_m \\cdot \\vec u_i)=0\\). We know that \\(\\vec u_i\\cdot \\vec u_i=1\\) and all other dot products are zero. Therefore, \\(c_i=0\\). Since this holds for all \\(i=1,\\ldots,m\\), it follows that \\(c_1=\\cdots =c_m=0\\), and therefore, \\((\\vec u_1,\\ldots,\\vec u_m)\\) are linearly independent. The second part follows since \\(n\\) linearly independent vectors in \\(\\mathbb{R}^n\\) always forms a basis.\n\n\nExample 4.2 Find three examples of an orthonormal basis for a subspace they span. The vectors \\(\\vec e_1,\\ldots, \\vec e_m\\) in \\(\\mathbb{R}^n\\) form an orthonormal basis of the subspace they span. For any scalar \\(\\theta\\), the vectors \\(\\vectortwo{\\cos \\theta}{\\sin \\theta}\\), \\(\\vectortwo{-\\sin\\theta}{\\cos \\theta}\\) form an orthonormal basis of \\(\\mathbb{R}^2\\). The vectors \\[\n\\begin{array}{ccc}\n\\vec u_1=\\vectorfour{1/2}{1/2}{1/2}{1/2}, &\n\\vec u_2=\\vectorfour{1/2}{1/2}{-1/2}{-1/2}, &\n\\vec u_3=\\vectorfour{1/2}{-1/2}{1/2}{-1/2}\n\\end{array}\n\\] in \\(\\mathbb{R}^4\\) form an orthonormal basis of the subspace they span.\n\nLet \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). The orthogonal complement \\(V^\\perp\\) of \\(V\\) is the set of those vectors \\(\\vec x\\) in \\(\\mathbb{R}^n\\) that are orthogonal to all vectors in \\(V\\) namely, \\[\\begin{equation}\nV^{\\perp}=\\{ \\vec x \\in \\mathbb{R}^n \\mid \\vec v \\cdot \\vec x =0, \\text{ for all } \\vec v \\text{ in } V\\}.\n\\end{equation}\\] It is easy to verify that \\(V^\\perp\\) is always a subspace and that \\((\\mathbb{R}^n)^\\perp=\\{0\\}\\) and \\(\\{0\\}^\\perp=\\mathbb{R}^n\\). Also notice that if \\(U_1\\subseteq U_2\\) then \\(U_2^\\perp \\subseteq U_1^\\perp\\). If \\(\\vec x\\in V^{\\perp}\\) then \\(\\vec x\\) is said to be perpendicular to \\(V\\). The vector \\(\\vec x^{\\parallel}\\) in the following theorem is called the orthogonal projection of \\(\\vec x\\) on a subspace \\(V\\) of \\(\\mathbb{R}^n\\) and is denoted by \\(\\text{proj}_V (\\vec x)\\).\n\nTheorem 4.2 (Orthogonal Projection)  \n\nIf \\(V\\) is a subspace of \\(\\mathbb{R}^n\\) and \\(\\vec x\\in \\mathbb{R}^n\\), then \\(\\vec x=\\vec x^\\parallel + \\vec x^\\perp\\) where \\(\\vec x^\\perp\\) is perpendicular to \\(V\\), and this representation is unique.\nIf \\(V\\) is a subspace of \\(\\mathbb{R}^n\\) with an orthonormal basis \\(\\vec u_1 ,\\ldots, \\vec u_m\\), then \\[\\begin{equation}\n\\mathrm{proj}_V (\\vec x):=\\vec x^\\parallel = (\\vec u_1 \\cdot \\vec x) \\vec u_1 + \\cdots + (\\vec u_m \\cdot \\vec x) \\vec u_m\n\\end{equation}\\] for all \\(\\vec x\\) in \\(\\mathbb{R}^n\\).\nLet \\(\\vec u_1 ,\\ldots, \\vec u_n\\) be an orthonormal basis in \\(\\mathbb{R}^n\\), then \\[\\begin{equation}\n\\vec x = (\\vec u_1 \\cdot \\vec x) \\vec u_1 + \\cdots + (\\vec u_n \\cdot \\vec x) \\vec u_n\n\\end{equation}\\] for all \\(\\vec x\\) in \\(\\mathbb{R}^n\\).\n\n\n\nProof. The proof is left for the reader.\n\n\nExample 4.3 Find the orthogonal projection of \\(\\vectorthree{49}{49}{49}\\) onto the subspace of \\(\\mathbb{R}^3\\) spanned by \\(\\vectorthree{2}{3}{6}\\) and \\(\\vectorthree{3}{-6}{2}\\). The two given vectors spanning the subspace are orthogonal since \\(2(3)+3(-6)+6(2)=0\\), but they are not unit vectors since both have length 7. To obtain an orthonormal basis \\(\\vec u_1, \\vec u_2\\) of the subspace, we divide by 7: \\(\\vec u_1=\\frac{1}{7}\\vectorthree{2}{3}{6}\\) and \\(\\vec u_2=\\frac{1}{7}\\vectorthree{3}{-6}{2}\\). Now we can use \\(\\eqref{Orthogonal Projection}\\) with \\(\\vec x=\\vectorthree{49}{49}{49}\\). Then \\[\n\\text{proj}_V(\\vec x)=(\\vec u_1 \\cdot \\vec x)\\vec u_1+(u_2\\cdot \\vec x) \\vec u_2=\n11 \\vectorthree{2}{3}{6}+(-1)\\vectorthree{3}{-6}{2}= \\vectorthree{19}{39}{64}.\n\\]\n\n\nExample 4.4 Find the coordinates of the vector \\(\\vec x=\\vectorfour{4}{5}{6}{7}\\) with respect to the orthonormal basis \\[\n\\begin{array}{cccc}\n\\vec u_1=\\vectorfour{1/2}{1/2}{1/2}{1/2}, &\n\\vec u_2=\\vectorfour{1/2}{1/2}{-1/2}{-1/2}, &\n\\vec u_3=\\vectorfour{1/2}{-1/2}{1/2}{-1/2}, &  \n\\vec u_4=\\vectorfour{1/2}{-1/2}{-1/2}{1/2}.\n\\end{array}\n\\] Normally to find the coordinates of \\(\\vec x\\) we would solve the system \\[\n\\vectorfour{4}{5}{6}{7}=\nc_1 \\vectorfour{1/2}{1/2}{1/2}{1/2}+\nc_2 \\vectorfour{1/2}{1/2}{-1/2}{-1/2}+\nc_3 \\vectorfour{1/2}{-1/2}{1/2}{-1/2} +\nc_4 \\vectorfour{1/2}{-1/2}{-1/2}{1/2}\n\\] for \\(c_1, c_2, c_3, c_4\\). However we can use \\(\\eqref{Orthogonal Projection}\\) instead: \\[\nc_1=u_1\\cdot \\vec x=\\vectorfour{1/2}{1/2}{1/2}{1/2} \\cdot \\vectorfour{4}{5}{6}{7}=11,\n\\] \\[\nc_2=u_2\\cdot \\vec x=\\vectorfour{1/2}{1/2}{-1/2}{-1/2} \\cdot \\vectorfour{4}{5}{6}{7}=-2,\n\\] \\[\nc_3=u_3\\cdot \\vec x=\\vectorfour{1/2}{-1/2}{1/2}{-1/2} \\cdot \\vectorfour{4}{5}{6}{7}=-1,\n\\] \\[\nc_4=u_4\\cdot \\vec x=\\vectorfour{1/2}{-1/2}{-1/2}{1/2} \\cdot \\vectorfour{4}{5}{6}{7}=0.\n\\] Therefore the \\(\\mathcal{B}\\)-coordinate vector of \\(\\vec x\\) is \\(\\vectorfour{11}{-2}{-1}{0}\\).\n\n\nTheorem 4.3 (Properties of the Orthogonal Complement) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\), then\n\n\\(\\mathrm{proj}_V(\\vec x)\\) is a linear transformation \\(\\mathbb{R}^n\\to V\\) with kernel \\(V^\\perp\\),\n\\(V \\cap V^\\perp = \\{\\vec 0\\}\\),\n\\(\\text{dim} V +\\text{dim} V^\\perp=n\\), and\n\\((V^\\perp)^\\perp = V\\).\n\n\n\nProof. The proof of each part follows.\n\nTo prove the linearity of \\(T(x):=\\mathrm{proj}_V(\\vec x)\\) we will use the definition of a projection: \\(T(\\vec x)\\) is in \\(V\\), and \\(\\vec x-T(\\vec x)\\) is in \\(V^\\perp\\). To show \\(T(\\vec x+\\vec y)=T(\\vec x)+T(\\vec y)\\), notice \\(T(\\vec x)+T(\\vec y)\\) is in \\(V\\) (since \\(V\\) is a subspace), and \\(\\vec x+\\vec y-(T(\\vec x)+T(\\vec y))=(\\vec x-T(\\vec x))+(\\vec y-T(\\vec y))\\) is in \\(V^\\perp\\) (since \\(V^\\perp\\) is a subspace). To show that \\(T(k \\vec x)=k T(\\vec x)\\), note that \\(k T(\\vec x)\\) is in \\(V\\) (since \\(V\\) is a subspace) and \\(k \\vec x-k T(\\vec x)=k(\\vec x-T(\\vec x))\\) is in \\(V^\\perp\\) (since \\(V^\\perp\\) is a subspace).\nSince \\(\\{\\vec 0\\} \\subseteq V\\) and \\(\\{\\vec 0\\} \\subseteq V^\\perp\\), \\(\\{\\vec 0\\} \\subseteq V\\cap V^\\perp.\\) If a vector \\(\\vec x\\) is in \\(V\\) as well as in \\(V^\\perp\\), then \\(\\vec x\\) is orthogonal to itself: \\(\\vec x \\cdot \\vec x =\\norm{x}^2=0\\), so that \\(\\vec x\\) must equal \\(\\vec 0\\) which shows \\(V \\subseteq \\{\\vec 0\\}\\). Therefore, \\(V\\cap V^\\perp=\\{\\vec 0\\}\\).\nApply the Rank-Nullity Theorem to the linear transformation \\(T(\\vec x)=\\text{proj}_V(\\vec x)\\) yielding \\(n=\\text{dim} \\mathbb{R}^n =\\text{dim} \\text{image} T+\\text{dim} \\ker T=\\text{dim} V+\\text{dim} V^\\perp\\).\nLet \\(\\vec v\\in V\\). Then \\(\\vec v\\cdot \\vec x=0\\) for all \\(\\vec x\\) in \\(V^\\perp\\). Since \\((V^\\perp)^\\perp\\) contains all vectors \\(\\vec y\\) such that \\(\\vec y \\cdot \\vec x =0\\), \\(\\vec v\\) is in \\((V^\\perp)^\\perp\\). So \\(V\\) is a subspace of \\((V^\\perp)^\\perp\\). Using (iii) with \\(V\\) (\\(n=\\text{dim} V+\\text{dim} V^\\perp\\)) and again with \\(V^\\perp\\) (\\(\\text{dim} V^\\perp+\\text{dim} (V^\\perp)^\\perp\\)) yielding \\(\\text{dim} V=\\text{dim} (V^\\perp)^\\perp\\); and since \\(V\\) is a subspace of \\((V^\\perp)^\\perp\\) it follows that \\(V=(V^\\perp)^\\perp\\).\n\n\n\n\nExample 4.5 Find a basis for \\(W^\\perp\\), where \\(W=\\text{span} \\left( \\vectorfour{1}{2}{3}{4}, \\vectorfour{5}{6}{7}{8} \\right)\\). The orthogonal complement \\(W^\\perp\\) of \\(W\\) consists of the vectors \\(\\vec x\\) in \\(\\mathbb{R}^4\\) such that \\[\n\\vectorfour{x_1}{x_2}{x_3}{x_4} \\cdot \\vectorfour{1}{2}{3}{4}\n=0 \\hspace{1cm}\\text{and}  \n\\vectorfour{x_1}{x_2}{x_3}{x_4} \\cdot \\vectorfour{5}{6}{7}{8}=0.\n\\] Finding these vectors amounts to solving the system \\[\n\\begin{cases}\nx_1+2x_2+3x_3+4x_4&=0\n\\\\ 5x_1+6x_2+7x_3+8x_4 & =0\n\\end{cases}\n\\] The solutions are of the form \\[\n\\vectorfour{x_1}{x_2}{x_3}{x_4}=\\vectorfour{s+2t}{-2s-3t}{s}{t}=s\\vectorfour{1}{-2}{1}{0}+t\\vectorfour{2}{-3}{0}{1}.\n\\] The two vectors on the right form a basis of \\(W^{\\perp}\\).\n\n\nTheorem 4.4 Let \\(\\vec x, \\vec y \\in \\mathbb{R}^n\\). Then\n\n( Pythagorean Theorem) \\[\n||\\vec x + \\vec y || ^2 = || \\vec x|| ^2 + || \\vec y || ^2\n\\] holds if and only if \\(\\vec x \\perp \\vec y\\),\n( Cauchy-Schwarz) \\[\n| \\vec x \\cdot \\vec y | \\leq || \\vec x || \\, || \\vec y ||\n\\] where equality holds if and only if \\(\\vec x\\) and \\(\\vec y\\) are parallel,\n( Law of Cosines) the angle \\(\\theta\\) between \\(\\vec x\\) and \\(\\vec y\\) is defined as \\[\n\\theta = \\arccos \\frac{\\vec x \\cdot \\vec y}{|| \\vec x || \\, || \\vec y||},\n\\]\n( Triangular Inequality) \\[\n\\norm{\\vec x+\\vec y}\\leq \\norm{\\vec x}+\\norm{\\vec y}.\n\\]\n\n\n\nProof. The proof of each part follows.\n\nThe verification is straightforward: \\[\\begin{align*}\n\\norm{\\vec x+\\vec y }^2\n& =(\\vec x + \\vec y)\\cdot (\\vec x + \\vec y)\n=\\vec x \\cdot \\vec x+2(\\vec x \\cdot \\vec y) +\\vec y \\cdot \\vec y \\\\\n& =\\norm{\\vec x}^2+2(\\vec x\\cdot \\vec y)+\\norm{\\vec y}^2\n=\\norm{\\vec x}^2+\\norm{\\vec y}^2\n\\end{align*}\\] where the last equality holds if and only if \\(\\vec x\\cdot \\vec y=0\\).\nLet \\(V\\) be a one-dimensional subspace of \\(\\mathbb{R}^n\\) spanned by a nonzero vector \\(\\vec y\\). Let \\(\\vec u=\\frac{1}{\\norm{\\vec y}} \\vec y\\). Then \\[\n\\norm{\\vec x}\n\\geq \\norm{\\text{proj}_V(\\vec x)}\n=\\norm{(\\vec x\\cdot \\vec u)\\vec u}\n=|\\vec x\\cdot \\vec u|\n=\\left| \\vec x \\cdot \\left( \\frac{1}{\\norm{y}}\\vec y \\right)\\right|\n= \\frac{1}{\\norm{y}}|\\vec x \\cdot \\vec y|\n\\] multiplying by \\(\\norm{\\vec y}\\), yields \\(| \\vec x \\cdot \\vec y | \\leq || \\vec x || \\, || \\vec y ||\\). Notice that \\(\\norm{\\vec x} \\geq \\norm{\\text{proj}_V(\\vec x)}\\) holds by applying the Pythagorean theorem to \\(\\vec x=\\vec x^\\parallel +\\vec x^\\perp\\) with \\(\\vec x^\\perp \\cdot \\vec x^\\parallel =0\\) so that \\(\\norm{\\vec x}^2=\\norm{\\text{proj}_V(\\vec x)}^2+\\norm{\\vec x^\\perp}^2\\) which leads to \\(\\norm{\\text{proj}_V \\vec x} \\leq \\norm{\\vec x}\\).\nWe have to make sure that \\(\\eqref{law-of-cosines}\\) is defined, that is \\(\\theta\\) is between \\(-1\\) and \\(1\\), or equivalently, \\[\n\\left | \\frac{\\vec x \\cdot \\vec y }{\\norm{x} \\, \\norm{y}} \\right | \\leq 1.\n\\] But this follows from the Cauchy-Schwarz inequality.\nUsing the Cauchy-Schwarz inequality, the verification is straightforward, \\[\n\\norm{\\vec x+\\vec y}^2=(\\vec x + \\vec y)\\cdot (\\vec x +  \\vec y)=\\norm{\\vec x}^2+\\norm{\\vec y}^2+2(\\vec x\\cdot \\vec y)\n\\] \\[\n\\leq \\norm{\\vec x}^2+\\norm{\\vec y}^2+2\\norm{\\vec x}\\norm{\\vec y}=(\\norm{\\vec x}+\\norm{\\vec y})^2\n\\] Taking the square root of both sides yields \\(\\norm{\\vec x+\\vec y}\\leq \\norm{\\vec x}+\\norm{\\vec y}\\).\n\n\n\nExample 4.6 Determine whether the angle between the vectors \\(\\vec u=\\vectorthree{2}{3}{4}\\), \\(\\vec v=\\vectorthree{2}{-8}{5}\\) is a right angle using the Pythagorean Theorem. Since \\(\\norm{\\vec u}=\\sqrt{2^2+3^2+4^2}=\\sqrt{29}\\) and \\(\\norm{\\vec v}=\\sqrt{2^2+(-8)^2+5^2}=\\sqrt{93}\\). Then \\[\n\\norm{\\vec u+\\vec v}^2=\\left|\\left| \\, \\, \\vectorthree{4}{-5}{9} \\, \\,\n\\right| \\right|^2 =122 = 29+93= || \\vec u|| ^2 + || \\vec v || ^2\n\\] shows \\(\\vec u \\perp \\vec v\\).\n\n\nExample 4.7 Consider the vectors \\(\\vec u =\\vectorfour{1}{1}{\\vdots}{1}\\) and \\(\\vec v=\\vectorfour{1}{0}{\\vdots}{0}\\) in \\(\\mathbb{R}^n\\). For \\(n=2,3,4\\), find the angle \\(\\theta\\) between \\(\\vec u\\) and \\(\\vec v\\). Then find the limit of \\(\\theta\\) as \\(n\\) approaches infinity. For any possible value of \\(n\\), \\[\n\\theta_n=\\arccos \\frac{\\vec u \\cdot \\vec v}{\\norm{\\vec u}\\norm{\\vec v}}=\\arccos \\frac{1}{\\sqrt{n}}.\n\\] Then \\[\n\\begin{bmatrix}\n\\theta_2=\\arccos \\frac{1}{\\sqrt{2}}=\\frac{\\pi}{4}, &\n\\hspace{.5cm}  \\theta_3=\\arccos \\frac{1}{\\sqrt{3}}=\\frac{\\pi}{4}\\sim 0.955 \\text{ rads}, &\n\\hspace{.5cm} \\theta_4=\\arccos \\frac{1}{\\sqrt{4}}=\\frac{\\pi}{3}.\n\\end{bmatrix}\n\\] Since \\(y=\\arccos(x)\\) is a continuous function, \\[\n\\lim_{x\\mapsto \\infty} \\theta_n\n= \\lim_{x\\mapsto \\infty} \\arccos\\left( \\frac{1}{\\sqrt{n}} \\right)\n= \\arccos \\left( \\lim_{x\\mapsto \\infty} \\frac{1}{\\sqrt{n}} \\right)\n= \\arccos(0)\n=\\frac{\\pi}{2}.\n\\]"
  },
  {
    "objectID": "inner-products-spaces.html#gram-schmidt-process-and-qr-factorization",
    "href": "inner-products-spaces.html#gram-schmidt-process-and-qr-factorization",
    "title": "4  Inner Products Spaces",
    "section": "4.2 Gram-Schmidt Process and QR Factorization",
    "text": "4.2 Gram-Schmidt Process and QR Factorization\nThe Gram-Schmidt process represents a change of basis from a basis \\(\\mathcal{B}=(\\vec v_1, \\vec v_2, ..,,\\vec v_m)\\) of a subspace \\(V\\) of \\(\\mathbb{R}^n\\) to an orthonormal basis \\(\\mathcal{U}=(\\vec u_1, \\vec u_2, \\ldots,\\vec u_m)\\) of \\(V\\); it is most sufficiently described in terms of the change of basis matrix \\(R\\) from \\(\\mathcal{B}\\) to \\(\\mathcal{U}\\) via, \\[\\begin{equation}\nM:=\\begin{bmatrix} \\vec v_1 & \\vec v_2 & \\cdots & \\vec v_m \\end{bmatrix}\n=\\begin{bmatrix} \\vec u_1 &\\vec u_2 & \\cdots & \\vec u_m \\end{bmatrix}R=:QR\n\\end{equation}\\]\nThe \\(QR\\) factorization is an effective way to organize and record the work performed in the Gram-Schmidt process; it is also useful for many computational and theoretical purposes.\n\nTheorem 4.5 (Gram-Schmidt Process) Let \\(\\vec v_1, \\vec v_2, \\ldots, \\vec v_m\\) be a basis of a subspace \\(V\\) of \\(\\mathbb{R}^n\\). Then \\[\\begin{equation}\n\\begin{array}{cccc}\n\\vec u_1 = \\frac{1}{||\\vec v_1 || } \\vec v_1,  &\n\\vec u_2 = \\frac{1}{||\\vec v_2^\\perp || } \\vec v_2^\\perp, & \\ldots, &\n\\vec u_m = \\frac{1}{||\\vec v_m^\\perp || } \\vec v_m^\\perp\n\\end{array}\n\\end{equation}\\] is an orthonormal basis of \\(V\\) where \\[\\begin{equation}\n\\vec v_j ^\\perp\n= \\vec v_j - \\vec v_j^\\parallel\n= \\vec v_j - (\\vec u_1 \\cdot \\vec v_j) \\vec u_1 - (\\vec u_2 \\cdot \\vec v_j) \\vec u_2 - \\cdots - (\\vec u_{j-1} \\cdot \\vec v_j) \\vec u_{j-1}.\n\\end{equation}\\]\n\n\nProof. For each \\(j\\), we resolve the vectors \\(\\vec v_j\\) into its components parallel and perpendicular to the span of the preceding vectors \\(\\vec v_1, \\vec v_2, \\ldots, \\vec v_{j-1}\\): \\[\\vec v_j = \\vec v_j^\\parallel + \\vec v_j^\\perp \\hspace{1cm} \\text{with respect to } \\text{span}(\\vec v_1, \\vec v_2, \\ldots, \\vec v_{j-1}).\\] Then use \\(\\ref{Orthogonal Projection}\\).\n\n\nExample 4.8 Perform the Gram-Schmidt process on the vectors \\[\n\\vec v_1=\\vectorthree{4}{0}{3}, \\qquad\n\\vec v_2=\\vectorthree{25}{0}{-25}, \\qquad\n\\vec v_3=\\vectorthree{0}{-2}{0}.\n\\] By Theorem \\(\\eqref{Gram-Schmidt Process}\\), we determine \\(\\vec u_1, \\vec u_2, \\vec u_3\\) as follows: \\[\n\\vec u_1= \\vectorthree{4/5}{0}{3/5},\n\\hspace{1cm}\n\\vec u_2=\\frac{\\vec v_2^\\perp}{\\norm{\\vec v_2^\\perp}}\n=\\frac{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right) \\vec u_1}{\\norm{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right)\\vec u_1}}\n=\\vectorthree{3/5}{0}{-4/5},\n\\] and \\[\n\\vec u_3=\\frac{v_3^\\perp}{\\norm{v_3^\\perp}}\n=\\frac{\\vec v_3-\\left(\\vec u_1 \\cdot \\vec v_3\\right) \\vec u_1-\\left(\\vec u_2 \\cdot \\vec v_3\\right) \\vec u_2}{\\norm{\\vec v_3-\\left(\\vec u_1 \\cdot \\vec v_3\\right) \\vec u_1-\\left(\\vec u_2 \\cdot \\vec v_3\\right) \\vec u_2}}\n=\\vectorthree{0}{-1}{0}.\n\\] Therefore \\[\n\\left( \\vectorthree{4/5}{0}{3/5},  \\vectorthree{3/5}{0}{-4/5}, \\vectorthree{0}{-1}{0}\\right).\n\\] is an orthonormal basis for \\(\\text{span} (\\vec v_1, \\vec v_2, \\vec v_3)\\).\n\n::: {#thm- } [QR Factorization] Let \\(M\\) be an \\(n \\times m\\) matrix with linearly independent columns \\(\\vec v_1 , \\vec v_2, \\ldots,\\vec v_m\\). Then there exists an \\(n\\times m\\) matrix \\(Q\\) whose columns \\(\\vec u_1, \\vec u_2, \\ldots, \\vec u_m\\) are orthonormal and an upper triangular matrix \\(R\\) with positive diagonal entries such that \\(M=Q R\\); and this representation is unique. Furthermore, the entries \\(r_{ij}\\) of \\(R\\) are given by\n\n\\(r_{11}=\\norm{\\vec v_1}\\),\n\\(r_{jj}=||\\vec v_j^\\perp||\\) (for \\(j=2,\\ldots,m\\)), and\n\\(r_{i j}=\\vec u_i \\cdot \\vec v_j\\) (for \\(i<j\\)). :::\n\n\nProof. The proof is left for the reader.\n\n\nExample 4.9 Find the \\(QR\\) factorization of the matrix and display the commutative diagram. \\[\nM=\\begin{bmatrix} 4 & 25 & 0 \\\\ 0 & 0 & -2 \\\\ 3 & -25 & 0\\end{bmatrix}\n\\] Let \\[\n\\begin{array}{ccc}\n\\vec v_1=\\vectorthree{4}{0}{3}, &\n\\vec v_2=\\vectorthree{25}{0}{-25}, &\n\\vec v_3=\\vectorthree{0}{-2}{0}\n\\end{array}\n\\] then (as determined above) an orthonormal basis for the column vectors of \\(M\\) is\n\\[\\left(\\vec u_1, \\vec u_2, \\vec u_3\\right)=\\left( \\vectorthree{4/5}{0}{3/5},  \\vectorthree{3/5}{0}{-4/5}, \\vectorthree{0}{-1}{0}\\right).\\] Determining the entries of \\(R\\) (also as determined above): \\[\n\\begin{array}{ccc}\nr_{11}=\\norm{\\vec v_1}=5, &\nr_{22}=\\norm{\\vec v_2^\\perp}=35, &\nr_{33}=\\norm{\\vec v_3^\\perp}=2\n\\end{array}\n\\] \\[\n\\begin{array}{ccc}\nr_{12}=\\vec u_1 \\cdot \\vec v_2=5, &\nr_{13}=\\vec u_1 \\cdot \\vec v_3=0, &\nr_{23}=\\vec u_2 \\cdot \\vec v_3=0\n\\end{array}\n\\] and therefore, the \\(QR\\)-factorization of \\(M\\) is \\[\nM= \\begin{bmatrix} 4 & 25 & 0 \\\\ 0 & 0 & -2 \\\\ 3 & -25 & 0\\end{bmatrix}\n= \\begin{bmatrix}4/5 & 3/5 & 0 \\\\ 0 & 0 & -1 \\\\ 3/5 & -4/5 & 0 \\end{bmatrix}\n\\begin{bmatrix}5 & 5 & 0 \\\\ 0 & 35 & 0 \\\\ 0 & 0 & 2\\end{bmatrix}\n=QR.\n\\]\n\n\nExample 4.10  \n\nPerform the Gram-Schmidt process on the vectors \\[\n\\vec v_1=\\vectorfour{1}{7}{1}{7}, \\qquad\n\\vec v_2= \\vectorfour{0}{7}{2}{7}, \\qquad\n\\vec v_3 = \\vectorfour{1}{8}{1}{6}.\n\\]\nFind the \\(QR\\) factorization of the matrix \\(M=\\begin{bmatrix} \\vec v_1 & \\vec v_2 & \\vec v_3\\end{bmatrix}.\\)\n\nBy Theorem \\(\\eqref{Gram-Schmidt Process}\\), we determine \\(\\vec u_1, \\vec u_2, \\vec u_3\\) as follows: \\[\n\\vec u_1= \\vectorfour{1/10}{7/10}{1/10}{7/10},\n\\vec u_2=\\frac{\\vec v_2^\\perp}{\\norm{\\vec v_2^\\perp}}\n=\\frac{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right) \\vec u_1}{\\norm{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right)\\vec u_1}}\n=\\vectorfour{-1/\\sqrt{2}}{0}{1/\\sqrt{2}}{0},\n\\] and \\[\n\\vec u_3=\\frac{\\vec v_3^\\perp}{\\norm{\\vec v_3^\\perp}}\n=\\frac{\\vec v_3-\\left(\\vec u_1 \\cdot \\vec v_3\\right) \\vec u_1-\\left(\\vec u_2 \\cdot \\vec v_3\\right) \\vec u_2}{\\norm{\\vec v_3-\\left(\\vec u_1 \\cdot \\vec v_3\\right) \\vec u_1-\\left(\\vec u_2 \\cdot \\vec v_3\\right) \\vec u_2}}\n=\\vectorfour{0}{1/\\sqrt{2}}{0}{-1/\\sqrt{2}}.\n\\] Therefore an orthonormal basis for \\(\\text{span} (\\vec v_1, \\vec v_2, \\vec v_3)\\) is \\[\n\\left( \\vectorfour{1/10}{7/10}{1/10}{7/10},  \\vectorfour{-1/\\sqrt{2}}{0}{1/\\sqrt{2}}{0}, \\vectorfour{0}{1/\\sqrt{2}}{0}{-1/\\sqrt{2}}\\right).\n\\] Determining the entries of \\(R\\) (also as determined above): \\[\n\\begin{array}{ccc}\nr_{11}=\\norm{\\vec v_1}=10, &\nr_{22}=\\norm{\\vec v_2^\\perp}=\\sqrt{2}, &\nr_{33}=\\norm{\\vec v_3^\\perp}=\\sqrt{2}\n\\end{array}\n\\] \\[\n\\begin{array}{ccc}\nr_{12}=\\vec u_1 \\cdot \\vec v_2=10, &\nr_{13}=\\vec u_1 \\cdot \\vec v_3=10, &\nr_{23}=\\vec u_2 \\cdot \\vec v_3=0\n\\end{array}\n\\] and therefore, the \\(QR\\)-factorization of \\(M\\) is \\[\nM=\\begin{bmatrix} 1 & 0 & 1 \\\\ 7 & 7 & 8 \\\\ 1 & 2 & 1 \\\\ 7 & 7 & 6 \\end{bmatrix}\n= \\begin{bmatrix}1/10 & -1/\\sqrt{2} & 0 \\\\ 7/10 & 0 & 1/\\sqrt{2} \\\\ 1/10 & 1/\\sqrt{2} & 0 \\\\ 7/10 & 0 & -1/\\sqrt{2}\\end{bmatrix}\n\\begin{bmatrix}10 & 10 & 10 \\\\ 0 & \\sqrt{2} & 0 \\\\ 0 & 0 & \\sqrt{2}\\end{bmatrix}\n=QR.\n\\]"
  },
  {
    "objectID": "inner-products-spaces.html#orthogonal-transformations-and-orthogonal-matrices",
    "href": "inner-products-spaces.html#orthogonal-transformations-and-orthogonal-matrices",
    "title": "4  Inner Products Spaces",
    "section": "4.3 Orthogonal Transformations and Orthogonal Matrices",
    "text": "4.3 Orthogonal Transformations and Orthogonal Matrices\nA linear transformation \\(T\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) is called an orthogonal transformation if it preserves the length of vectors: \\(\\norm{T(\\vec x)}=\\norm{x}\\) for all \\(\\vec x\\in \\mathbb{R}^n.\\) If \\(T(\\vec x)=A\\vec x\\) is an orthogonal transformation, we say \\(A\\) is an orthogonal matrix .\n\nLemma 4.1 Let \\(T\\) be an orthogonal transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\). If \\(\\vec v, \\vec w \\in \\mathbb{R}^n\\) are orthogonal, then \\(T(\\vec v), T(\\vec w) \\in \\mathbb{R}^n\\) are orthogonal.\n\n\nProof. We want to show \\(T(\\vec v), T(\\vec w)\\) are orthogonal, and by the Pythagorean theorem, we have to show \\[\n\\norm{T(\\vec v)+T(\\vec w)}^2=\\norm{T(\\vec v)}^2+\\norm{T(\\vec w)}^2.\n\\] This equality follows \\[\\begin{align*}\n\\norm{T(\\vec v)+T(\\vec w)}^2\n& =\\norm{T(\\vec v+\\vec w)}^2\n=\\norm{\\vec v+\\vec w}^2 \\\\\n& =\\norm{\\vec v}^2+\\norm{\\vec w}^2\n=\\norm{T(\\vec v)}^2+\\norm{T(\\vec w)}^2\n\\end{align*}\\] since \\(T\\) is linear, orthogonal and that \\(\\vec v, \\vec w\\) are orthogonal, respectively.\n\n\nTheorem 4.6 A linear transformation \\(T\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) is orthogonal if and only if the vectors \\(T(\\vec e_1)\\), , \\(T(\\vec e_n)\\) form an orthonormal basis.\n\n\nProof. If \\(T\\) is an orthogonal transformation, then by definition, the \\(T(\\vec e_i)\\) are unit vectors, and also, by \\(\\ref{orthogonal transformation}\\) they are orthogonal. Therefore, \\(T(\\vec e_1)\\), , \\(T(\\vec e_n)\\) form an orthonormal basis.\nConversely, suppose \\(T(\\vec e_1)\\), , \\(T(\\vec e_n)\\) form an orthonormal basis. Consider a vector \\(\\vec x=x_1 \\vec e_1+\\cdots +x_n \\vec e_n\\). Then \\[\\begin{align*}\n\\norm{T(\\vec x)}^2\n&=\\norm{T(x_1 \\vec e_1+\\cdots + x_n \\vec e_n)}^2\n=\\norm{x_1 T(\\vec e_1)+\\cdots + x_n T(\\vec e_n)}^2 \\\\\n&=\\norm{x_1T(\\vec e_1)}^2+\\cdots + \\norm{x_nT(\\vec e_n)}^2 = x_1^2+\\cdots + x_n^2\n=\\norm{x}^2.\n\\end{align*}\\] Taking the square root of both sides shows that \\(T\\) preserves lengths and therefore, \\(T\\) is an orthogonal transformation.\n\n\nCorollary 4.1 An \\(n \\times n\\) matrix \\(A\\) is orthogonal if and only if its columns form an orthonormal basis.\n\n\nProof. The proof is left for the reader.\n\nThe transpose \\(A^T\\) of an \\(n\\times n\\) matrix \\(A\\) is the \\(n\\times n\\) matrix whose \\(ij\\)-th entry is the \\(ji\\)-th entry of \\(A\\). We say that a square matrix \\(A\\) is symmetric if \\(A^T=A\\), and \\(A\\) is called skew-symmetric if \\(A^T=-A\\).\n\nTheorem 4.7 (Orthogonal and Transpose Properties)  \n\nThe product of two orthogonal \\(n\\times n\\) matrices is orthogonal.\nThe inverse of an orthogonal matrix is orthogonal.\nIf the products \\((A B)^T\\) and \\(B^T A^T\\) are defined then they are equal.\nIf \\(A\\) is invertible then so is \\(A^T\\), and \\((A^T)^{-1}=(A^{-1})^T\\).\nFor any matrix \\(A\\), \\(\\text{rank}\\,(A) = \\text{rank} \\,(A^T)\\).\nIf \\(\\vec v\\) and \\(\\vec w\\) are two column vectors in \\(\\mathbb{R}^n\\), then \\(\\vec v \\cdot \\vec w = \\vec v^T \\vec w\\).\nThe \\(n \\times n\\) matrix \\(A\\) is orthogonal if and only if \\(A^{-1}=A^T\\).\n\n\n\nProof. The proof of each part follows.\n\nSuppose \\(A\\) and \\(B\\) are orthogonal matrices, then \\(AB\\) is an orthogonal matrix since \\(T(\\vec x)=AB \\vec x\\) preserves length because \\(\\norm{T(\\vec x)}=\\norm{AB \\vec x}=\\norm{A(B \\vec x)}=\\norm{B \\vec x}=\\norm{\\vec x}.\\)\nSuppose \\(A\\) is an orthogonal matrix, then \\(A^{-1}\\) is orthogonal an matrix since \\(T(\\vec x)=A^{-1} \\vec x\\) preserves length because \\(\\norm{A^{-1}\\vec x}=\\norm{A(A^{-1}\\vec x)}=\\norm{\\vec x}\\).\nWe will compare entries in the matrices \\((AB)^T\\) and \\(B^T A^T\\) as follows: \\[\n\\begin{array}{rl}\ni j \\text{-th entry of }(AB)^T &= ji \\text{-th entry of }AB\\\\\n& = (j \\text{-th row of } A) \\cdot (i \\text{-th column of } B)\\\\ \\\\\ni j \\text{-th entry of }B^TA^T &=(i \\text{-th row of } B^T) \\cdot (j \\text{th column of } A^T)\\\\\n& = (i \\text{-th column of } B) \\cdot (j \\text{-th row of } A)\\\\\n& = (j \\text{-th row of } A) \\cdot (i \\text{-th column of } B).\n\\end{array}\n\\] Therefore, the \\(ij\\)-th entry of \\((AB)^T\\) is the same of the \\(ij\\)-th entry of \\(B^T A^T\\).\nSuppose \\(A\\) is invertible, then \\(A A^{-1}=I_n\\). Taking the transpose of both sides along with (iii) it yields, \\((A A^{-1})^T=(A^{-1})^T A^T=I_n\\). Thus \\(A^T\\) is invertible and since inverses are unique, it follows \\((A^T)^{-1}=(A^{-1})^T\\).\nExercise.\nIf \\(\\vec v=\\vectorthree{a_1}{\\vdots}{a_n}\\) and \\(\\vec w=\\vectorthree{b_1}{\\vdots}{b_n}\\), then \\[\n\\vec v \\cdot \\vec w=\\vectorthree{a_1}{\\vdots}{a_n}\\cdot \\vectorthree{b_1}{\\vdots}{b_n}=a_1b_1+\\cdots +a_n b_n\n=\\begin{bmatrix}a_1 & \\cdots & a_n\\end{bmatrix}  \\vectorthree{b_1}{\\vdots}{b_n}\n=\\vectorthree{a_1}{\\vdots}{a_n}^T \\vec w=\\vec v^T \\vec w.\n\\]\nLet’s write \\(A\\) in terms of its columns: \\(A=\\begin{bmatrix} \\vec v_1 & \\cdots & \\vec v_n \\end{bmatrix}\\). Then \\[\\begin{equation}\n\\label{ata}\nA^T A=\n\\begin{bmatrix}  \\vec v_1^T  \\\\  \\vdots \\\\  \\vec v_n^T \\end{bmatrix}\n\\begin{bmatrix} \\vec v_1 & \\cdots & \\vec v_n  \\end{bmatrix}\n=\\begin{bmatrix}\\vec v_1 \\cdot \\vec v_1 & & \\vec v_1 \\cdot \\vec v_n \\\\ \\vdots & \\cdots & \\vdots \\\\ \\vec v_n \\cdot \\vec v_1 & & \\vec v_n \\cdot \\vec v_n\\end{bmatrix}.\n\\end{equation}\\] Now \\(A\\) is orthogonal, by \\(\\ref{oob}\\), if and only if \\(A\\) has orthonormal columns, meaning \\(A\\) is orthogonal if and only if \\(A^TA=I_n\\) by \\(\\ref{ata}\\). Therefore, \\(A\\) is orthogonal if and only if \\(A^{-1}=A^T\\).\n\n\n\nTheorem 4.8 (Orthogonal Projection Matrix)  \n\nLet \\(V\\) be a subspace of \\(\\mathbb{R}^n\\) with orthonormal basis \\(\\vec u_1\\), , \\(\\vec u_m\\). The matrix of the orthogonal projection onto \\(V\\) is \\(Q Q^T\\) where \\(Q= \\begin{bmatrix} \\vec u_1 & \\cdots & \\vec u_m \\end{bmatrix}.\\)\nLet \\(V\\) be a subspace of \\(\\mathbb{R}^n\\) with basis \\(\\vec v_1,\\ldots,\\vec v_m\\) and let \\(A=\\begin{bmatrix}\\vec v_1 & \\cdots \\vec v_m \\end{bmatrix}\\), then the orthogonal projection matrix onto \\(V\\) is \\(A(A^T A)^{-1}A^T\\).\n\n\n\nProof. The proof of each part follows.\n\nSince \\(\\vec u_1\\), , \\(\\vec u_m\\) is an orthonormal basis of \\(V\\) we can, by \\(\\ref{Orthogonal Projection}\\), write, \\[\\begin{align*}\n\\text{proj}_V (\\vec x)\n& =(\\vec u_1 \\cdot \\vec x) \\vec u_1 + \\cdots + (\\vec u_m \\cdot \\vec x) \\vec u_m\n=\\vec u_1 \\vec u_1^T \\vec x + \\cdots +\\vec u_m \\vec u_m^T \\vec x &  \\\\\n&=(\\vec u_1 \\vec u_1^T  + \\cdots +\\vec u_m \\vec u_m^T) \\vec x\n= \\begin{bmatrix} \\vec u_1 & \\cdots & \\vec u_m  \\end{bmatrix}\n\\begin{bmatrix} \\vec u_1^T  \\\\  \\vdots \\\\  \\vec u_m^T  \\end{bmatrix} \\vec x\n=QQ^T\\vec x.\n\\end{align*}\\]\nSince \\(\\vec v_1,\\ldots,\\vec v_m\\) form a basis of \\(V\\), there exists unique scalars \\(c_1,\\ldots,c_m\\) such that \\(\\text{proj}_V(\\vec x)=c_1 \\vec v_1+\\cdots +c_m \\vec v_m\\). Since \\(A=\\begin{bmatrix}\\vec v_1 & \\cdots & \\vec v_m \\end{bmatrix}\\) we can write \\(\\text{proj}_V(\\vec x)=A \\vec c\\). Consider the system \\(A^TA\\vec c =A^T \\vec x\\) where \\(A^TA\\) is the coefficient matrix and \\(\\vec c\\) is the unknown. Since \\(\\vec c\\) is the coordinate vector of \\(\\text{proj}_V(\\vec x)\\) with respect to the basis \\((v_1,\\ldots,v_m)\\), the system has a unique solution. Thus, \\(A^TA\\) must be invertible, and so we can solve for \\(\\vec c\\), namely \\(\\vec c=(A^T A)^{-1}A^T\\vec x\\). Therefore, \\(\\text{proj}_V(\\vec x)=A \\vec c =A (A^T A)^{-1}A^T\\) as desired. Notice it suffices to consider the system \\(A^TA\\vec c =A^T \\vec x\\), or equivalently \\(A^T(\\vec x-A \\vec c)=\\vec 0\\), because \\[\nA^T(\\vec x -A \\vec c)=A^T(\\vec x-c_1 \\vec v_1-\\cdots - c_m \\vec v_m)\n\\] is the vector whose \\(i\\)th component is \\[\n(\\vec v_i)^T(\\vec x-c_1 \\vec v_1-\\cdots -c_m \\vec v_m)=\\vec v_i\\cdot(\\vec x-c_1\\vec v_1-\\cdots -c_m \\vec v_m)\n\\] which we know to be zero since \\(\\vec x-\\text{proj}_V(\\vec x)\\) is orthogonal to \\(V\\).\n\n\n\nExample 4.11 Is there an orthogonal transformation \\(T\\) from \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}^3\\) such that \\[\nT\\vectorthree{2}{3}{0}=\\vectorthree{3}{0}{2} \\hspace{1cm} \\text{and}  \\hspace{1cm} T\\vectorthree{-3}{2}{0}=\\vectorthree{2}{-3}{0}?\n\\] No, since the vectors \\(\\vectorthree{2}{3}{0}\\) and \\(\\vectorthree{-3}{2}{0}\\) are orthogonal, whereas \\(\\vectorthree{3}{0}{2}\\) and \\(\\vectorthree{2}{-3}{0}\\) are not, by \\(\\ref{orthogonal transformation}\\).\n\n\nExample 4.12 Find an orthogonal transformation \\(T\\) from \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}^3\\) such that \\[\nT\\vectorthree{2/3}{2/3}{1/3}=\\vectorthree{0}{0}{1}.\n\\] Let’s think about the inverse of \\(T\\) first. The inverse of \\(T\\), if it exists, must satisfy \\(T^{-1}(\\vec e_3)=\\vectorthree{2/3}{2/3}{1/3}=\\vec v_3\\). Furthermore, the vectors \\(\\vec v_1, \\vec v_2, \\vec v_3\\) must form an orthonormal basis of \\(\\mathbb{R}^3\\) where \\(T^{-1}\\vec x=\\begin{bmatrix}\\vec v_1 & \\vec v_2 & \\vec v_3\\end{bmatrix} \\vec x\\). We require a vector \\(\\vec v_1\\) with \\(\\vec v_1\\cdot \\vec v_3=0\\) and \\(\\norm{\\vec v_1}=1\\). By inspection, we find \\(\\vec v_1=\\vectorthree{-2/3}{1/3}{2/3}\\). Then \\[\n\\vec v_2=\\vec v_1\\times \\vec v_3=\\vectorthree{-2/3}{1/3}{2/3} \\times \\vectorthree{2/3}{2/3}{1/3}=\\vectorthree{1/9-4/9}{-(-2/9-4/9)}{-4/9-2/9}=\\vectorthree{-1/3}{2/3}{-2/3}\n\\] does the job since \\(\\norm{v_1}=\\norm{v_2}=\\norm{v_3}=1\\) and \\(\\vec v_1\\cdot \\vec v_2=\\vec v_1\\cdot \\vec v_3=\\vec v_2\\cdot \\vec v_3=0\\). In summary \\[\nT^{-1}=\\begin{bmatrix}-2/3 & -1/3 & 2/3 \\\\ 1/3 & 2/3 & 2/3 \\\\\\ 2/3 & -2/3 & 1/3\\end{bmatrix}\\vec x.\n\\] By \\(\\ref{Orthogonal and Transpose Properties}\\) the matrix of \\(T^{-1}\\) is orthogonal and the matrix \\(T=(T^{-1})^{-1}\\) is the transpose of the matrix of \\(T^{-1}\\). Therefore, it suffices to use \\[\nT=\\begin{bmatrix}-2/3 & -1/3 & 2/3 \\\\ 1/3 & 2/3 & 2/3 \\\\\\ 2/3 & -2/3 & 1/3\\end{bmatrix}^T\\vec x=\\begin{bmatrix}-2/3 & 1/3 & 2/3 \\\\ -1/3 & 2/3 & -2/3 \\\\\\ 2/3 & 2/3 & 1/3 \\end{bmatrix} \\vec x.\n\\]\n\n\nExample 4.13 Show that a matrix with orthogonal columns need not be an orthogonal matrix. For example \\(A=\\begin{bmatrix}4 & -3 \\\\ 3 & 4 \\end{bmatrix}\\) is not an orthogonal matrix \\(T\\vec x=A\\vec x\\) does not preserve length by comparing the lengths of \\(\\vec x\\) and \\(T\\vec x\\) with \\(\\vectortwo{-3}{4}\\).\n\n\nExample 4.14 Find all orthogonal \\(2\\times 2\\) matrices. Write \\(A=\\begin{bmatrix}\\vec v_1 & \\vec v_2\\end{bmatrix}\\). The unit vector \\(\\vec v_1\\) can be expressed as \\(\\vec v_1=\\vectortwo{\\cos \\theta}{\\sin \\theta}\\), for some \\(\\theta\\). Then \\(v_2\\) will be one of the two unit vectors orthogonal to \\(\\vec v_1\\), namely \\(\\vec v_2=\\vectortwo{-\\sin \\theta}{\\cos \\theta}\\) or \\(\\vec v_2=\\vectortwo{\\sin \\theta}{-\\cos \\theta}\\). Therefore, an orthogonal \\(2\\times 2\\) matrix is either of the form \\[\nA=\\begin{bmatrix}\\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix}\\hspace{1cm} \\text{or} \\hspace{1cm} A=\\begin{bmatrix}\\cos \\theta & \\sin \\theta \\\\ \\sin \\theta & -\\cos \\theta \\end{bmatrix}\n\\] representing a rotation or a reflection, respectively.\n\n\nExample 4.15 Given \\(n\\times n\\) matrices \\(A\\) and \\(B\\) which of the following must be symmetric?\n\n\\(B B^T\\)\n\\(A^T B^TB A\\)\n\\(B(A+A^T)B^T\\)\n\nThe solution to each part follows.\n\n\\(\\ref{Orthogonal and Transpose Properties}\\), \\(B B^T\\) is symmetric because \\[\n(B B^T)^T=(B^T)^TB^T=B B^T.\n\\]\n\\(\\ref{Orthogonal and Transpose Properties}\\), \\(A^T B^TB A\\) is symmetric because \\[\n(A^TB^TBA)^T=A^TB^T(B^T)^T(A^T)^T=A^TB^TBA.\n\\]\n\\(\\ref{Orthogonal and Transpose Properties}\\), \\(B(A+A^T)B^T\\) is symmetric because \\[\n(B(A+A^T)B^T)^T=((A+A^T)B^T)^TB^T=B(A+A^T)^TB^T\n\\] \\[\n=B(A^T+A)^TB^T=B((A^T)^T+A^T)B^T=B(A+A^T)B^T.\n\\]\n\n\n\nExample 4.16 If the \\(n\\times n\\) matrices \\(A\\) and \\(B\\) are symmetric which of the following must be symmetric as well?\n\n\\(2I_n+3A-4 A^2\\),\n\\(A B^2 A\\).\n\nThe solution to each part follows.\n\nFirst note that \\((A^2)^T=(A^T)^2=A^2\\) for a symmetric matrix \\(A\\). Now we can use the linearity of the transpose, \\[\n(2I_n+3A-4 A^2)^T=2I_n^T+3A^T-4 (A^2)^T=2I_n+3A-4 A^2\n\\] showing that the matrix \\(2I_n+3A-4 A^2\\) is symmetric.\nThe matrix \\(A B^2 A\\) is symmetric since, \\[\n(AB^2A)^T=(ABBA)^T=(BA)^T(AB)^T=A^TB^TB^TA^T=AB^2A.\n\\]\n\n\n\nExample 4.17 Use \\(\\ref{Orthogonal Projection Matrix}\\) to find the matrix \\(A\\) of the orthogonal projection onto \\[\nW=\\text{span} \\left(\\vectorfour{1}{1}{1}{1},\\vectorfour{1}{9}{-5}{3}\\right).\n\\] Then find the matrix of the orthogonal projection onto the subspace of \\(\\mathbb{R}^4\\) spanned by the vectors \\(\\vectorfour{1}{1}{1}{1}\\) and \\(\\vectorfour{1}{2}{3}{4}\\). First we apply \\(\\ref{Gram-Schmidt Process}\\), the Gram-Schmidt process, to \\(W=\\text{span}(\\vec v_1, \\vec v_2)\\), to find that the vectors \\[\n\\vec u_1=\\frac{\\vec v_1}{\\norm{\\vec v_1}}\n=\\vectorfour{1/2}{1/2}{1/2}{1/2}, \\hspace{1cm}\\vec u_2\n=\\frac{\\vec v_2^\\perp}{\\norm{\\vec v_2^\\perp}}\n=\\frac{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right) \\vec u_1}{\\norm{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right)\\vec u_1}}\n=\\vectorfour{-1/10}{7/10}{-7/10}{1/10}\n\\] form an orthonormal basis of \\(W\\). By \\(\\ref{Orthogonal Projection Matrix}\\), the matrix of the projection onto \\(W\\) is \\(A=Q Q^T\\) where \\(Q=\\begin{bmatrix}\\vec u_1 & \\vec u_2\\end{bmatrix}\\). Therefore the orthogonal projection onto \\(W\\) is \\[\nA=\n\\begin{bmatrix} 1/2 & -1/10 \\\\ 1/2 & 7/10 \\\\ 1/2 & -7/10 \\\\ 1/2 & 1/10 \\end{bmatrix}\n\\begin{bmatrix} 1/2 & 1/2 & 1/2 & 1/2  \\\\ -1/10 & 7/10 & -7/10 & 1/10 \\end{bmatrix}\n=\\frac{1}{100}\n\\begin{bmatrix}\n26 & 18 & 32 & 24 \\\\\n18 & 74 & -24 & 32 \\\\\n32 & -24 & 74 & 18 \\\\\n24 & 32 & 18 & 26\n\\end{bmatrix}.\n\\] Let \\(A=\\begin{bmatrix}1 & 1 \\\\ 1 & 2 \\\\1 & 3 \\\\1 & 4 \\end{bmatrix}\\) and then the orthogonal projection matrix is \\[\nA(A^TA)^{-1}A^T\n=\\frac{1}{10}\\begin{bmatrix}7 & 4 & 1 & -2 \\\\4 & 3 & 2 & 1 \\\\ 1 & 2 & 3 & 4 \\\\ -2 & 1 & 4 & 7 \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "inner-products-spaces.html#inner-products",
    "href": "inner-products-spaces.html#inner-products",
    "title": "4  Inner Products Spaces",
    "section": "4.4 Inner Products",
    "text": "4.4 Inner Products\nRecall that the norm of \\(x\\in \\mathbb{R}^n\\) defined by \\(\\norm{x}=\\sqrt{x_1^2+x_2^2}\\) is not linear. To injective linearity into the discussion we introduce the dot product: for \\(x,y\\in \\mathbb{R}^n\\) the dot product of \\(x\\) and \\(y\\) is defined as \\(x \\cdot y=x_1 y_1+\\cdots +x_n y_n\\). Obviously \\(x \\cdot x=\\norm{x}^2\\), and with the dot product being so useful, so we generalize the dot product into an inner product on a vector space \\(V\\).\nAn inner product on \\(V\\) is a function that takes each ordered pair \\((u,v)\\) of elements of \\(V\\) to a number \\(\\ip{u,v} \\in\\mathbb{F}\\) and has the following properties:\n\n() positivity \\(\\ip{ v,v }\\geq 0\\) for all \\(v\\in V\\);\n() definiteness \\(\\ip{ v,v }=0\\) if and only if \\(v=0\\);\n( additivity in the first slot) \\(\\ip{ u+v,w } = \\ip{u,w} +\\ip{ v,w }\\) for all \\(u,v,w\\in V\\);\n( homogeneity in the first slot) \\(\\ip{av,w} =a\\ip{v,w}\\) for all \\(a\\in \\mathbb{F}\\) and all \\(v,w,\\in V\\);\n( conjugate symmetry) \\(\\ip{v,w} = \\overline{\\ip{w,v} }\\) for all \\(v,w\\in V\\).\n\nRecall that for \\(z\\in \\mathbb{C}^n\\), we define the norm of \\(z\\) by \\[\n\\norm{z}=\\sqrt{|z_1|^2+\\cdots + |z_n|^2}\n\\] where the absolute values are needed because we want \\(\\norm{z}\\) to be a non-negative number. Then \\[\\begin{equation}\\label{cp}\n\\norm{z}^2=z_1 \\overline{z_1}+\\cdots + z_n \\overline{z_n}\n\\end{equation}\\] because every \\(\\lambda\\in\\mathbb{C}\\) satisfies \\(|\\lambda|^2 =\\lambda \\overline{\\lambda}\\). Since \\(\\norm{z^2}\\) is the inner-product of \\(z\\) with itself, as in \\(\\mathbb{R}^n\\), the Equation \\(\\eqref{cp}\\) suggests that the inner product of \\(w\\in \\mathbb{C}^n\\) with \\(z\\) should equal \\[\nw_1 \\overline{z_1}+\\cdots+w_n \\overline{z_n}.\n\\] We should expect that the inner product of \\(w\\) with \\(z\\) equals the complex conjugate of the inner product of \\(z\\) with \\(w\\), thus motivating the definition of conjugate symmetry.\nAn inner-product space is a vector space \\(V\\) along with an inner product on \\(V\\). The inner product defined on \\(\\mathbb{F}^n\\) by \\[\n\\ip{ (w_1,\\ldots,w_n),(z_1,\\ldots,z_n) } = w_1 \\overline{z_1}+\\cdots + w_n \\overline{z_n}\n\\] is called the Euclidean inner product.\nContinue to let \\(V\\) denote a complex or real vector space. In this section we develop the basic theorems for norms. For \\(v\\in V\\), the norm of \\(v\\) is defined by \\(||v||=\\sqrt{\\ip{v,v} }\\). Two vectors \\(u,v\\in V\\) are orthogonal if \\(\\ip{u,v}= 0\\).\n\nTheorem 4.9 [Pythagorean Theorem] If \\(u\\) and \\(v\\) are orthogonal vectors in \\(V\\), then \\[\n\\norm{u + v}^2 = \\norm{u}^2 + \\norm{y}^2.\n\\]\n\n\nProof. Suppose that \\(u,v\\) are orthogonal vectors in \\(V\\). Then \\[\n\\norm{u+v}^2=\\ip{u+v,u+v}=\\norm{u}^2+\\norm{v}^2+\\ip{u,v}+\\ip{v,u}=\\norm{u}^2+\\norm{v}^2,\n\\] as desired.\n\n::: {#thm- } [Orthogonal Decomposition] If \\(v\\) is a nonzero vector in \\(V\\), then \\(u\\) can be written as a scalar multiple of \\(v\\) plus a vector orthogonal to \\(v\\). :::\n\nProof. Let \\(a\\in \\mathbb{F}\\). Then \\[\nu=a v+(u-av).\n\\] Thus we need to choose \\(a\\) so that \\(v\\) is orthogonal to \\((u-a v)\\). In other words, we want \\[\n0=\\ip{u-av,v}=\\ip{u,v}-a\\ip{v,v}=\\ip{u,v}-a\\norm{v}^2.\n\\] The equation above shows that we should choose \\(a\\) to be \\(\\ip{u,v}/\\norm{v}^2\\) (assume that \\(v\\ne 0\\) to avoid division by 0). Making this choice of \\(a\\), we can write \\[\nu=\\frac{\\ip{u,v}}{\\norm{v}^2} v+\\left(u-\\frac{\\ip{u,v}}{\\norm{v}^2}v\\right).\n\\] Thus, if \\(v\\neq 0\\) then the equation above writes \\(u\\) as a scalar multiple of \\(v\\) plus a vector orthogonal to \\(v\\).\n\n::: {#thm- } [Cauchy-Schwarz] If \\(u, v \\in V\\), then \\[\n| \\ip{u,v} | \\leq \\norm{u} \\, \\norm{v}\n\\] where equality holds if and only if one of \\(u, v\\) is a scalar multiple of the other. :::\n\nProof. Let \\(u,v\\in V\\). If \\(v=0\\), then both sides and the desired inequality holds. Thus we can assume that \\(v\\neq 0\\). Consider the orthogonal decomposition \\[\nu=\\frac{\\ip{u,v}}{\\norm{v}^2}v+w\n\\] where \\(w\\) is orthogonal to \\(v\\). By the Pythagorean theorem, \\[\\begin{align*}\n\\norm{u}^2\n=\\norm{\\frac{\\ip{u,v}}{\\norm{v}^2} v}^2+\\norm{w}^2\\\\\n=\\frac{|\\ip{u,v}|^2}{\\norm{v}^2}+\\norm{w}^2\\\\\n\\geq \\frac{|\\ip{u,v}|^2}{\\norm{v}^2}\n\\end{align*}\\] Multiplying both sides by \\(\\norm{v}^2\\) and then taking square roots gives the Cauchy-Schwarz inequality. Notice that there is equality if and only if \\(w=0\\), that is, if and only if \\(u\\) is a multiple of \\(v\\).\n\n::: {#thm- } [Triangular Inequality] If \\(u, v \\in V\\), then \\[\n\\norm{u+v}\\leq \\norm{u}+\\norm{v}\n\\] where equality holds if and only if one of \\(u, v\\) is a nonnegative multiple of the other. :::\n\nProof. Let \\(u,v\\in V\\). Then \\[\\begin{align}\n\\norm{u+v}^2 \\notag\n&=\\ip{u+v,u+v}  \\notag\\\\\n&=\\ip{u,u}+\\ip{v,v}+\\ip{u,v}+\\ip{v,u}  \\notag \\\\\n&=\\ip{u,u}+\\ip{v,v}+\\ip{u,v}+\\overline{\\ip{u,v}}  \\notag \\\\\n&=\\norm{u,u}^2+\\norm{v,v}^2+2 \\text{remark}\\ip{u,v}  \\notag \\\\\n&\\leq \\norm{u,u}^2+\\norm{v,v}^2+2 | \\ip{u,v} | \\label{ti1} \\\\\n&\\leq \\norm{u,u}^2+\\norm{v,v}^2+2 \\norm{u}\\norm{v} \\label{ti2}\\\\\n&= \\left(\\norm{u}+\\norm{v}\\right)^2  \\notag\n\\end{align}\\] and so by taking square root of both sides yields the triangular inequality. This proof shows that the triangle inequality is an equality if and only if we have equality in \\(\\eqref{ti1}\\) and \\(\\eqref{ti2}\\). Thus we have equality in the triangular inequality if and only if \\[\\begin{equation}\\label{ti3}\n\\ip{u,v}=\\norm{u}\\norm{v}.\n\\end{equation}\\] If one of \\(u,v\\) is a nonnegative multiple of the other, then \\(\\eqref{ti3}\\) holds. Conversely, suppose \\(\\eqref{ti3}\\) holds. The the condition for equality in the Cauchy-Schwarz inequality implies that one of \\(u,v\\) must be a scalar multiple of the other. Clearly, then \\(\\eqref{ti3}\\) forces the scalar in question to be nonnegative, as desired.\n\n::: {#thm- } [Parallelogram Equality] If \\(u, v \\in V\\), then \\[\n\\norm{u+v}^2+\\norm{u-v}^2= 2 \\left( \\norm{u}^2+\\norm{v}^2 \\right).\n\\] :::\n\nProof. If \\(u, v \\in V\\), then\n\\[\\begin{align*}\n\\norm{u+v}^2+\\norm{u-v}^2\n&= \\ip{u+v,u+v}+\\ip{u-v,u-v} \\\\\n& = \\norm{u}^2+\\norm{v}^2+\\ip{u,v}+\\ip{v,u}+\\norm{u}^2+\\norm{v}^2-\\ip{u,v}-\\ip{v,u} \\\\\n& =2 \\left( \\norm{u}^2+\\norm{v}^2 \\right )\n\\end{align*}\\] as desired.\n\nA list of vectors is called orthonormal if the vectors in it are pairwise orthogonal and each vector has norm 1.\n\nTheorem 4.10 If \\((e_1,\\ldots,e_m)\\) is an orthonormal list of vectors in \\(V\\), then \\[\n\\norm{a_1 e_1+\\cdots +a_m e_m}^2=|a_1|^2+\\cdots + |a_n|^2\n\\] for all \\(a_1,\\ldots,a_m\\in\\mathbb{F}\\).\n\n\nProof. Because each \\(e_j\\) has norm 1, this follows easily from repeated by application of the Pythagorean theorem.\n\n::: {#thm- } Every orthonormal list of vectors is linearly independent. :::\n\nProof. Suppose \\((e_1,\\ldots,e_n)\\) is an orthonormal list of vectors in \\(V\\) and \\(a_1,\\ldots,a_n\\in \\mathbb{F}\\) are such that \\(a_1 e_1+\\cdots + a_n e_n=0\\). Then \\(|a_1|^2+\\cdots + |a_n|^2=0\\), which means that all the \\(a_j\\)’s are 0, as desired.\n\nAn orthonormal basis of \\(V\\) is an orthonormal list of vectors in \\(V\\) that is also a basis of \\(V\\).\nThe importance of orthonormal bases stems mainly from the following proposition.\n::: {#thm- } Suppose \\((e_1,\\ldots,e_n)\\) is an orthonormal basis of \\(V\\). Then \\[\\begin{equation} \\label{inim1}\nv=\\ip{ v,e_1 } e_1+\\cdots + \\ip{ v,e_n } e_n\n\\end{equation}\\] and \\[\\begin{equation}\\label{inim2}\n\\norm{v}=|\\ip{ v,e_1 } |^2+\\cdots + |\\ip{ v,e_n } |^2\n\\end{equation}\\] for every \\(v\\in V\\). :::\n\nProof. Let \\(v\\in V\\). Because \\((e_1,\\ldots,e_n)\\) is a basis of \\(V\\), there exist scalars \\(a_1,\\ldots,a_n\\) such that \\(v=a_1 e_1+\\cdots + a_n e_n\\). Take the inner product of both sides of this equation with \\(e_j\\), getting \\(\\ip{v,e_j}=a_j\\). Thus \\(\\eqref{inim1}\\) holds. Clearly \\(\\eqref{inim2}\\) holds by \\(\\eqref{inim1}\\) and \\(\\eqref{innorm}\\).\n\n::: {#thm- } [Gram-Schmidt] If \\((v_1,\\ldots,v_m)\\) is a linearly independent list of vectors in \\(V\\), then there exists an orthonormal list \\((e_1,\\ldots,e_m)\\) of vectors in \\(V\\) such that \\[\\begin{equation} \\label{gmeq}\n\\text{span}(v_1,\\ldots,v_j)=\\text{span}(e_1,\\ldots,e_j)\n\\end{equation}\\] for \\(j=1,\\ldots,m\\) :::\n\nProof. Suppose \\((v_1,\\ldots,v_m)\\) is a linearly independent list of vectors in \\(V\\). To construct the \\(e\\)’s, start by setting \\(e_1=\\frac{v_1}{\\norm{v_1}}\\). This satisfies \\(\\eqref{gmeq}\\) for \\(j=1\\). We will choose \\(e_2,\\ldots e_m\\) inductively, as follows. Suppose \\(j>1\\) and an orthonormal list \\((e_1,\\ldots,e_{j-1})\\) has been chosen so that \\[\\begin{equation}\\label{gspan}\n\\text{span}(v_1,\\ldots,v_{j-1})=\\text{span}(e_1,\\ldots,e_{j-1}).\n\\end{equation}\\] Let \\[\\begin{equation}\\label{gsproj}\ne_j=\\frac{v_j-\\ip{v_j,e_1}e_1-\\cdots - \\ip{v_j,e_{j-1}} e_{j-1} }{\\norm{v_j-\\ip{v_j,e_1}e_1-\\cdots - \\ip{v_j,e_{j-1}} e_{j-1}}}.\n\\end{equation}\\] Note that \\(v_j \\not\\in \\text{span}(v_1,\\ldots,v_{j-1})\\) (because \\((v_1,\\ldots,v_m)\\) is linearly independent) and thus \\(v_j\\not \\in \\text{span}(e_1,\\ldots,e_{j-1})\\). Hence we are not dividing by 0 in the equation above, and so \\(e_j\\) is well-defined. Dividing a vector by its norm produces a new vector with norm 1; thus \\(\\norm{e_j}=1\\).\nLet \\(1\\leq k <j\\). Then \\[\\begin{align*}\n\\ip{e_j,e_k}\n&=\\ip{\\frac{v_j-\\ip{v_j,e_1}e_1-\\cdots - \\ip{v_j,e_{j-1}} e_{j-1} }{\\norm{v_j-\\ip{v_j,e_1}e_1-\\cdots - \\ip{v_j,e_{j-1}} e_{j-1}}},e_k} \\\\\n&= \\frac{\\ip{v_j,e_k}-\\ip{v_j,e_k}\\ip{e_k,e_k}}{\\norm{v_j-\\ip{v_j,e_1}e_1-\\cdots - \\ip{v_j,e_{j-1}} e_{j-1}}} \\\\\n&=0.\n\\end{align*}\\] Thus \\((e_1,\\ldots,e_j)\\) is an orthonormal list.\nFrom \\(\\eqref{gsproj}\\), we see that \\(v_j\\in \\text{span}(e_1,\\ldots,e_j)\\). Combining this information with \\(\\eqref{gspan}\\) shows that \\[\n\\text{span}(v_1,\\ldots,v_{j-1})\\subset \\text{span}(e_1,\\ldots,e_{j}).\n\\] Both lists above are linearly independent (the \\(v\\)’s by hypothesis, the \\(e\\)’s by orthonormality and \\(\\eqref{oind}\\). Thus both subspaces above have dimension \\(j\\), and hence must be equal, completing the proof.\n\n\nTheorem 4.11 Every finite-dimensional inner-product space has an orthonormal basis.\n\n\nProof. Choose a basis of \\(V\\). Apply the Gram-Schmidt procedure to it, producing an orthonormal list. This list is linearly independent and it spans \\(V\\). Thus it is an orthonormal basis.\n\n\nTheorem 4.12 Every orthonormal list of vectors in \\(V\\) can be extended to an orthonormal basis of \\(V\\).\n\n\nProof. Suppose \\((e_1, \\ldots ,e_m)\\) is an orthonormal list of vectors in \\(V\\). Then \\((e_1, \\ldots , e_m)\\) is linearly independent and so can be extended to a basis \\[\n\\mathcal{B}=(e_1, \\ldots, e_m, v_1, \\ldots, v_n)\n\\] of \\(V\\). Now apply the Gram-Schmidt procedure to \\(\\mathcal{B}\\) producing an orthonormal list \\((e_1,\\ldots,e_m,f_1,\\ldots,f_n)\\); here the Gram-Schmidt procedure leaves the first \\(m\\) vectors unchanged because they are already orthonormal. Clearly \\(\\mathcal{B}\\) is an orthonormal basis of \\(V\\) because it is linearly independent and its span equals \\(V\\). Hence we have our extension of \\((e_1,\\ldots,e_m)\\) to an orthonormal basis of \\(V\\).\n\nRecall that if \\(V\\) is a complex vector space, then for each operator on \\(V\\) there is a basis with respect to which the matrix of the operator is upper-triangular. Now for inner-product spaces we would like to know the same question.\n::: {#thm- } Suppose \\(T\\in\\mathcal{L}(V)\\). If \\(T\\) has an upper-triangular matrix with respect to some basis of \\(V\\), then \\(T\\) has an upper-triangular matrix with respect to some orthonormal basis of \\(V\\). :::\n\nProof. Suppose \\(T\\) has upper-triangular matrix with respect to some basis \\((v_1,\\ldots,v_n)\\) of \\(V\\). Thus \\(\\text{span}(v_1,\\ldots,v_j)\\) is invariant under \\(T\\) for each \\(j=1,\\ldots,n\\). Apply the Gram-Schmidt procedure to \\((v_1,\\ldots,v_n)\\), producing an orthonormal basis \\((e_1,\\ldots,e_n)\\) of \\(V\\). Because \\[\n\\text{span}(e_1,\\ldots,e_j)=\\text{span}(v_1,\\ldots,v_j)\n\\] for each \\(j\\), we conclude that \\(\\text{span}(e_1,\\ldots,e_j)\\) is invariant under \\(T\\) for each \\(j=1,\\ldots,n\\). Thus, by \\(\\eqref{utm}\\), \\(T\\) has an upper-triangular matrix with respect to the orthonormal basis \\((e_1,\\ldots,e_n)\\).\n\n\nTheorem 4.13 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Then \\(T\\) has an upper-triangular matrix with respect to some orthonormal basis of \\(V\\).\n\n\nProof. This follows immediately from \\(\\eqref{cutm}\\) and \\(\\eqref{outm}\\).\n\nIf \\(U\\) is a subset of an inner-product space \\(V\\), then the orthogonal complement of \\(U\\) is defined as \\(U^\\bot=\\{v\\in V\\, : \\, \\ip{v,u} =0 \\text{ for all } u\\in U\\}.\\)\n::: {#thm- } [Orthogonal Decomposition] If \\(U\\) is a subspace of an inner-product space \\(V\\), then \\(v=U\\oplus U^\\perp\\). :::\n\nProof. Suppose that \\(U\\) is a subspace of \\(V\\). First we will show that \\[\\begin{equation}\\label{sumfirst}\nV=U + U^\\perp.\n\\end{equation}\\] To do this, suppose \\(v\\in V\\). Let \\((e_1,\\ldots,e_m)\\) be an orthonormal basis of \\(U\\). Obviously, \\[\nv=\\underbrace{ \\ip{v,e_1}e_1+\\cdots +\\ip{v,e_m}e_m}_u+\\underbrace{v-\\ip{v,e_1}e_1-\\cdots -\\ip{v,e_m}e_m}_w.\n\\] Clearly, \\(u\\in U\\). Because \\((e_1,\\ldots,e_m)\\) is an orthonormal list, for each \\(j\\) we have \\[\n\\ip{w,e_j}=\\ip{v,e_j}-\\ip{v,e_j}=0.\n\\] Thus \\(w\\) is orthogonal to every vector in \\(\\text{span}(e_1,\\ldots,e_m)\\). In other words, \\(w\\in U^\\perp\\), completing the proof of \\(\\eqref{sumfirst}\\).\nIf \\(v\\in U\\cap U^\\perp\\), then \\(v\\) (which is in \\(U\\)) is orthogonal to every vector in \\(U\\) (including \\(v\\) itself), which implies that \\(\\ip{v,v}=0\\), which implies that \\(v=0\\). Thus \\[\\begin{equation}\\label{orthogonalss}\nU\\cap U^\\perp=\\{0\\}.\n\\end{equation}\\] Now \\(\\eqref{sumfirst}\\) and \\(\\eqref{orthogonalss}\\) imply that \\(U\\oplus U^\\perp\\).\n\n\nTheorem 4.14 If \\(U\\) is a subspace of an inner-product space \\(V\\), then \\(U=(U^\\perp)^\\perp\\).\n\n\nProof. Suppose that \\(U\\) is a subspace of \\(V\\). First we will show that \\[\\begin{equation}\\label{subsetorth}\nU\\subseteq (U^\\perp)^\\perp.\n\\end{equation}\\] To do this, suppose that \\(u\\in U\\). Then \\(\\ip{u,v}=0\\) for every \\(v\\in U^\\perp\\) (by definition of \\(U^\\perp\\)). Because \\(u\\) is orthogonal to every vector in \\(U^\\perp\\), we have \\(u\\in (U^\\perp)^\\perp\\), completing the proof of \\(\\eqref{subsetorth}\\).\nTo prove the inclusion in the other direction, suppose \\(v\\in (U^\\perp)^\\perp\\). By \\(\\eqref{ip-orthogonal-decomposition}\\), we can write \\(v=u+w\\), where \\(u\\in U\\) and \\(w\\in U^\\perp\\). We have \\(v-u=w\\in U^\\perp\\). Because \\(v\\in (U^\\perp)^\\perp\\) and \\(u\\in(U^\\perp)^\\perp\\) (from \\(\\eqref{subsetorth}\\)), we have \\(v-u\\in (U^\\perp)^\\perp\\). Thus \\(v-u\\in U^\\perp\\cap (U^\\perp)^\\perp\\), which implies that \\(v=u\\), which implies that \\(v\\in U\\). Thus \\((U^\\perp)^\\perp \\subseteq U\\), which along with \\(\\eqref{subsetorth}\\) completes the proof.\n\nLet \\(V=U \\oplus U ^\\bot\\) and for \\(v\\in V\\) let \\(v=u+w\\) where \\(w\\in U ^\\bot\\). Then \\(u\\) is called the orthogonal projection of \\(V\\) onto \\(U\\) and is denoted by \\(P_U v\\).\n\nTheorem 4.15 If \\(U\\) is a subspace of an inner-product space \\(V\\) and \\(v\\in V\\). Then \\(\\norm{v-P_U v}\\leq \\norm{v-u}\\) for every \\(u\\in U\\). Furthermore, if \\(u\\in U\\) and the inequality above is an equality; then \\(u=P_U v\\).\n\n\nProof. Suppose \\(u\\in U\\). Then \\[\\begin{align}\n\\norm{v-P_U v}^2\n& \\leq \\norm{v-P_Uv}+ \\norm{P_U v-u}^2 \\label{mp1} \\\\\n& = \\norm{v-P_U v+P_Uv-u}^2 = \\norm{v-u}^2   \\label{mp2}\n\\end{align}\\] where \\(\\eqref{mp2}\\) comes from the Pythagorean theorem, which applies because \\(v-P_U v\\in U^\\perp\\) and \\(P_U v-u\\in U\\). Taking the square root gives the desired inequality. The inequality is an equality if and only if \\(\\eqref{mp1}\\) is an equality, which happens if and only if \\(\\norm{P_U v-u}=0\\), which happens if and only if \\(u=P_u v\\).\n\n\nExample 4.18 Show that if \\(c_1,\\ldots,c_n\\) are positive numbers, then \\[\n\\ip{ (w_1,\\ldots,w_n),(z_1,\\ldots,z_n) } = c_1 w_1 \\overline{z_1}+\\cdots + c_n w_n \\overline{z_n}\n\\] defines an inner product on \\(\\mathbb{F}^n\\).\n\n\nExample 4.19 Show that if \\(p,q\\in \\mathcal{P}_m(\\mathbb{F})\\), then \\[\n\\ip{ p ,q } = \\int_0^1 p(x)\\overline{q(x)}dx\n\\] is an inner product on the vector space \\(\\mathcal{P}_m(\\mathbb{F})\\).\n\n\nExample 4.20 Show that every inner product is a linear map in the first slot, as well as a linear map in the second slot.\n\n\nExample 4.21 If \\(v\\in V\\) and \\(a\\in \\mathbb{F}\\), then \\(\\norm{av}=|a| \\norm{v}\\), and if \\(v\\) is nonzero then \\(u=\\frac{1}{\\norm{v}} v\\) is a unit vector. Since \\(\\norm{a v}^2=\\ip{av,av} =a \\overline{a} \\ip{v,v} =|a|^2 \\norm{v}^2\\), taking square roots provides \\(\\norm{a v}=|a| \\norm{v}\\).\n\n\nExample 4.22 Prove that if \\(x, y\\) are nonzero vectors in \\(\\mathbb{R}^2\\), then \\[\n\\ip{x,y} =\\norm{x}\\norm{y} \\cos \\theta,\n\\] where \\(\\theta\\) is the angle between \\(x\\) and \\(y\\). The law of cosines gives \\[\n\\norm{x-y}^2=\\norm{x}^2+\\norm{y}^2-2\\norm{x}\\norm{y} \\cos \\theta.\n\\] The left hand side of this equation is \\[\n\\norm{x-y}^2=(x-y)\\cdot (x-y)=\\norm{x}^2-2 (x \\cdot y) +\\norm{y}^2\n\\] so \\[\nx\\cdot y= \\norm{x}\\norm{y}\\cos \\theta.\n\\]\n\n\nExample 4.23 Suppose \\(u,v \\in V\\). Prove that \\(\\ip{ u,v } =0\\) if and only if \\(||u||\\leq ||u+a v||\\) for all \\(a\\in F\\). If \\(\\ip{u,v}=0\\), then by the Pythagorean theorem \\[\n\\norm{u+\\alpha v}^2=\\norm{u}^2+\\norm{\\alpha v}^2\\geq \\norm{u}.\n\\] Conversely, we will prove the contrapositive, that is we will prove: if \\(\\ip{u,v}\\neq0\\) then there exists \\(a \\in \\mathbb{F}\\) such that \\(\\norm{u}>\\norm{u+av}\\). Suppose \\(\\ip{u,v}\\neq 0\\) then \\(u\\) and \\(v\\) are both nonzero vectors. By the orthogonal decomposition, we can write \\[\\begin{equation} \\label{ex3}\nu=\\alpha v+w\n\\end{equation}\\] for some \\(\\alpha \\in \\mathbb{F}\\) and where \\(\\ip{w,v}=0\\). Notice \\(\\alpha \\neq 0\\) since \\(\\ip{u,v}\\neq 0\\). Since \\(v\\) and \\(w\\) are orthogonal \\[\n\\norm{u}^2=|\\alpha|^2\\norm{v}^2+\\norm{w}^2\n\\] Let \\(a=-\\alpha\\). Then by equation \\(\\eqref{ex3}\\) \\[\n\\norm{u+a v}^2=\\norm{w}^2\n\\] and so \\[\n\\norm{u}^2=|a|^2\\norm{v}^2 +\\norm{u+a v}^2 >  \\norm{u+a v}^2\n\\] which implies \\[\n\\norm{u}>\\norm{u+a v}\n\\] as desired.\n\n\nExample 4.24 Prove that \\[\n\\left (\\sum_{k=1}^{n} a_k b_k \\right )^2\n\\leq \\left ( \\sum_{k=1}^n k a_k^2 \\right ) \\left ( \\sum_{k=1}^n \\frac{b_k^2}{k}\\right )\n\\] for all real numbers \\(a_1,\\ldots,a_n\\) and \\(b_1,\\ldots,b_n\\). This is a simple trick. \\[\n\\left (\\sum_{k=1}^{n} a_k b_k \\right )^2\n= \\left ( \\sum_{k=1}^n \\sqrt{k} a_k \\frac{b_k}{\\sqrt{k}} \\right )^2\n\\leq \\left ( \\sum_{k=1}^n k a_k^2 \\right ) \\left ( \\sum_{k=1}^n \\frac{b_k^2}{k}\\right )\n\\] where the last inequality is from the Cauchy-Schwarz inequality.\n\n\nExample 4.25 Suppose \\(u, v\\in V\\) are such that \\(||u||=3\\), \\(||u+v||=4\\), and \\(||u-v||=6\\). What number must \\(||v||\\) equal? Using the parallelogram equality \\[\n\\norm{u+v}^2+\\norm{u-v}^2=2 \\left(\\norm{u}^2+\\norm{v}^2\\right)\n\\] to get \\[\n16+36=2(9+\\norm{v}^2), \\qquad \\norm{v}=\\sqrt{17}.\n\\]\n\n\nExample 4.26 Prove or disprove: there is an inner product on \\(\\mathbb{R}^2\\) such that the associated norm is given by \\(||(x_1,x_2)||=|x_1|+|x_2|\\) for all \\((x_1,x_2)\\in \\mathbb{R}^2\\). There is no such inner product. Take for instance, \\[\nu=(1/4,0), \\qquad v=(0,3/4), \\qquad u+v=(1/4,3/4).\n\\] Then we have equality in the triangular inequality \\[\n1=\\norm{u+v}\\leq \\norm{u}+\\norm{v}=1/4+3/4.\n\\] By the triangular inequality, we must have \\(u=a v\\) or \\(v=a v\\), with \\(a\\geq 0\\). But clearly no such \\(a\\in \\mathbb{F}\\) exists.\n\n\nExample 4.27 Prove that if \\(V\\) is a real inner-product space, then \\[\n\\ip{ u,v } =\\frac{||u+v||^2-||u-v||^2}{4}\n\\] for all \\(u,v\\in V\\). Expressing the norms as inner products \\[\\begin{align*}\n%\\ip{ u,v }\n\\frac{\\norm{u+v}^2- \\norm{u-v}^2}{4}\n&=\\frac{\\ip{u+v,u+v}-\\ip{u-v,u-v} }{4} \\\\\n&=\\frac{\\ip{u,u}+\\ip{v,v}+\\ip{u,v}+\\ip{v,u} - \\ip{u,u}-\\ip{v,v}+\\ip{u,v}+\\ip{v,u}}{4} \\\\\n&=\\frac{2\\ip{u,v}+2\\ip{v,u} }{4} \\\\\n&=\\frac{2\\ip{u,v}+2\\ip{u,v} }{4} \\quad \\text{(because $V$ is a real inner product space)} \\\\\n&=\\ip{u,v}\n\\end{align*}\\] as desired.\n\n\nExample 4.28 Prove that if \\(V\\) is a complex inner-product space, then \\[\n\\ip{ u,v }\n\\] is \\[\n\\frac{||u+v||^2-||u-v||^2 + ||u+i v||^2 i - ||u-i v||^2 i}{4}\n\\] for all \\(u,v\\in V\\).\n\n\nExample 4.29 A norm on a vector space \\(U\\) is a function \\(||\\text{  }|| : U\\rightarrow [0,\\infty)\\) such that \\(||u||=0\\) if and only if \\(u=0\\), \\(||\\alpha u ||= |\\alpha | ||u||\\) for all \\(\\alpha \\in F\\) and all \\(u \\in U\\), and \\(||u+v|| \\leq ||u||+||v||\\) for all \\(u,v \\in U\\). Prove that a norm satisfying the parallelogram equality comes from an inner product (in other words, show that if \\(|| \\text{  }||\\) is a norm on \\(U\\) satisfying the parallelogram equality, then there is an inner product \\(\\ip{ \\text{ } , \\text{ } }\\) on \\(U\\) such that \\(||u||=\\ip{ u, u } ^{1/2}\\) for all \\(u \\in U\\)).\n\n\nExample 4.30 Suppose \\(n\\) is a positive integer. Prove that \\[\n\\left (\n\\frac{1}{\\sqrt{2\\pi}},\\frac{\\sin x}{\\sqrt{\\pi}},\\frac{\\sin 2x}{\\sqrt{\\pi}},\\ldots,\\frac{\\sin n x}{\\sqrt{\\pi}}, \\frac{\\cos x}{\\sqrt{\\pi}},\\frac{\\cos 2x}{\\sqrt{\\pi}},\\ldots,\\frac{\\cos n x}{\\sqrt{\\pi}}\n\\right )\n\\] is an orthonormal list of vectors in \\(\\mathcal{C}[-\\pi,\\pi]\\), the vector space of continuous real-valued functions on \\([-\\pi,\\pi]\\) with inner product \\[\n\\ip{ f,g } = \\int_{-\\pi}^{\\pi} f(x)g(x) \\, d x .\n\\] Computation of these integrals is based on the product-to-sum formulas from trigonometry: \\[\\begin{align*}\n\\sin(A)\\sin(B)&=\\frac{1}{2}\\cos(A-B)-\\frac{1}{2}\\cos(A+B) \\\\\n\\cos(A)\\cos(B)&=\\frac{1}{2}\\cos(A-B)+\\frac{1}{2}\\cos(A+B) \\\\\n\\sin(A)\\cos(B)&=\\frac{1}{2}\\sin(A-B)+\\frac{1}{2}\\sin(A+B).\n\\end{align*}\\] Here is a sample computation, valid for \\(m,n=1,2,3,,\\ldots\\) when \\(m\\neq n\\). \\[\\begin{align*}\n\\ip{\\sin(mx),\\cos(nx)}\n&= \\int_{-\\pi}^{\\pi}\\sin(mx)\\cos(nx) \\, dx \\\\\n&= \\int_{-\\pi}^{\\pi}\\left(\\frac{1}{2}\\sin((m-n)x)+\\frac{1}{2}\\sin((m+n)x)\\right)\\, dx \\\\\n&= \\left. \\frac{-1}{2(m-n)}\\cos\\left((m-n)x\\right)+\\frac{-1}{2(m+n)}\\cos\\left((m+n)x\\right) \\right|^{\\pi}_{-\\pi}, \\\\\n&= \\frac{-1}{2(m-n)}[-1-(-1)]+\\frac{-1}{2(m-n)}[-1-(-1)] \\\\\n&=0.\n\\end{align*}\\]\n\n\nExample 4.31 On \\(\\mathcal{P}_2(\\mathbb{R})\\), consider the inner product given by \\[\n\\ip{ p,q } = \\int_o^1 p(x) q(x) \\, d x.\n\\] Apply the Gram-Schmidt procedure to the basis \\((1,x,x^2)\\) to produce an orthonormal basis of \\(\\mathcal{P}_2(\\mathbb{R})\\). Computing \\(e_1\\): A calculation gives \\[\n\\ip{1,1}=\\int_0^1 1 \\, dx=1,\n\\] so \\(e_1=1\\). Computing \\(e_2\\): A calculation gives \\[\n\\ip{x,1}=\\int_0^1 x \\, dx =\\frac{1}{2}.\n\\] Let \\[\nf_2=x-1/2.\n\\] Then \\[\n\\norm{f_2}^2=\\ip{x-1/2,x-1/2}=\\int_0^1\\left(x^2-x+\\frac{1}{4}\\right)\\, dx =\\frac{1}{12},\n\\] so \\[\ne_2=\\frac{f_2}{\\norm{f_2}}=2\\sqrt{3}\\left(x-\\frac{1}{2}\\right).\n\\] Computing \\(e_3\\): A calculation gives \\[\n\\ip{x^2,1}=\\int_0^1 x^2 \\, dx =\\frac{1}{3},\n\\] and \\[\n\\ip{x^2,2\\sqrt{3}\\left(x-\\frac{1}{2}\\right)}=2\\sqrt{3}\\int_0^1\\left(x^3-\\frac{x^2}{2}\\right) \\, dx=\\frac{1}{2\\sqrt{3}}.\n\\] Let \\[\nf_3=x^2-\\frac{1}{2\\sqrt{3}}\\left(x-\\frac{1}{2}\\right)-\\frac{1}{3}=x^2-x+\\frac{1}{6}.\n\\] Then \\[\n\\norm{f_3}^2=\\int_0^1\\left(x^2-x+\\frac{1}{6}\\right)^2 \\, dx\n\\] and \\[\ne_3=\\frac{f_3}{\\norm{f_3}}.\n\\]\n\n\nExample 4.32 What happens if the Gram-Schmidt procedure is applied to a list of vectors that is not linearly independent? By examining the proof, notice that the numerator in the Gram-Schmidt formula is the difference between \\(v_j\\) and the orthogonal projection \\(P_u\\) of \\(v_j\\) onto the subspace \\[\nU=\\text{span}(v_1,\\ldots,v_{j-1})=\\text{span}(e_1,\\ldots,e_{j-1}).\n\\] If \\(v_j\\in U\\), then \\(v_j-P_U v_j=0\\), so the numerator has norm 0 and division by the denominator is not defined. The algorithm can be adapted to handle this case by testing for 0 in the denominator. If 0 is found, throw \\(v_j\\) out of the list and continue. The result will be an orthonormal basis for \\(\\text{span}(v_1,\\ldots,v_N)\\).\n\n\nExample 4.33 Suppose \\(V\\) is a real inner-product space and \\((v_1,\\ldots,v_m)\\) is a linearly independent list of vectors in \\(V\\). Prove that there exist exactly \\(2^m\\) orthonormal lists \\((e_1,\\ldots,e_m)\\) of vectors in \\(V\\) such that span \\((v_1,\\ldots,v_j)=\\) span \\((e_1,\\ldots,e_j)\\) for all \\(j\\in \\{1,\\ldots,m\\}\\).\n\n\nExample 4.34 Suppose \\((e_1,\\ldots,e_M)\\) is an orthonormal list of vectors in \\(V\\). Let \\(v \\in V\\). Prove that \\[\n||v||^2=\\sum_{n=1}^M \\left|\\ip{v,e_n}\\right|^2\n\\] if and only if $v $ span \\((e_1,\\ldots,e_M)\\). Extend \\((e_1,\\ldots,e_M)\\) to an orthonormal basis for \\(V\\). Then \\[\n\\sum_{n=1}^N\\ip{v,e_n} e_n\n\\] and \\[\n\\norm{v}^2=\\sum_{n=1}^N \\left|\\ip{v,e_n}\\right|^2.\n\\] If \\(v\\in \\text{span}(e_1,\\ldots,e_M)\\) then \\(\\ip{v,e_n}=0\\) for \\(n>M\\), and \\[\n\\norm{v}^2=\\sum_{m=1}^M \\left|\\ip{v,e_n}\\right|^2.\n\\] If \\(v \\not\\in\\text{span}(e_1,\\ldots,e_M)\\) then for some \\(n>M\\) we have \\(\\ip{v,e_n}\\neq 0\\). This gives \\[\n\\norm{v}^2=\\sum_{n=1}^N\\left|\\ip{v,e_n}\\right|^2>\\sum_{m=1}^M\\left|v,e_n\\right|^2.\n\\]\n\n\nExample 4.35 Find an orthonormal basis of \\(\\mathcal{P}_2(\\mathbb{R})\\), such that the differentiation operator on \\(\\mathcal{P}_2(\\mathbb{R})\\) has an upper-triangular matrix with respect to this basis.\n\n\nExample 4.36 Suppose \\(U\\) is a subspace of \\(V\\). Prove that \\(\\text{dim} U^{\\bot} = \\text{dim} V - \\text{dim} U\\).\n\n\nExample 4.37 Suppose \\(U\\) is a subspace of \\(V\\). Prove that \\(U^{\\bot} = \\{0\\}\\) if and only if \\(U=V\\). If \\(U=V\\) and \\(v\\in U^\\perp\\) then \\(v\\in U\\cap U^\\perp\\), and \\(\\ip{v,v}=0\\), so \\(v=0\\). Therefore, \\(V=U\\oplus U^\\perp\\). If \\(U^\\perp=\\{0\\}\\), then \\(U=V\\).\n\n\nExample 4.38 Prove that if \\(P\\in \\mathcal{L}(V)\\) is such that \\(P^2=P\\) and every vector in null \\(P\\) is orthogonal to every vector in range \\(P\\), then \\(P\\) is an orthogonal projection.\n\n\nExample 4.39 Prove that if \\(P\\in \\mathcal{L}(V)\\) is such that \\(P^2=P\\) and \\(|| P v ||\\leq ||v||\\) for every \\(v\\in V\\), then \\(P\\) is an orthogonal projection.\n\n\nExample 4.40 Suppose \\(T\\in \\mathcal{L}(V)\\) and \\(U\\) is a subspace of \\(V\\). Prove that \\(U\\) is invariant under \\(T\\) if and only if \\(P_U T= T P_U\\).\n\n\nExample 4.41 Suppose \\(T\\in \\mathcal{L}(V)\\) and \\(U\\) is a subspace of \\(V\\). Prove that \\(U\\) and \\(U^\\bot\\) are both invariant under \\(T\\) if and only if \\(P_U T= T P_U\\).\n\n\nExample 4.42 In \\(\\mathbb{R}^4\\), let \\(U=\\text{span} \\left ( (1,1,0,0),(1,1,1,2) \\right )\\). Find \\(u\\in U\\) such that \\(||u-(1,2,3,4)||\\) is as small as possible. in \\(\\mathbb{R}^4\\) let \\(U=\\text{span}((1,1,0,0),(1,1,1,2)).\\) Find \\(u\\in U\\) such that \\(\\norm{u-(1,2,3,4)}\\) is as small as possible. We want the orthogonal projection \\(P_U(1,2,3,4)\\). Notice that \\(U=\\text{span}((1,1,0,0),(0,0,1,2)).\\) An orthonormal basis for \\(U\\) is \\[\n\\left( \\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}} ,0,0\\right ),\\left(0,0,\\frac{1}{\\sqrt{5}},\\frac{2}{\\sqrt{5}} \\right)\n\\] Thus the desired vector is \\[\nP_U(1,2,3,4)=\\left(\\frac{3}{2},\\frac{3}{2},0,0\\right)+\\left(0,0,\\frac{11}{5},\\frac{2}{5}\\right).\n\\]\n\n\nExample 4.43 Find a polynomial \\(p\\in \\mathcal{P}_3(\\mathbb{R})\\) such that \\(p(0)=0\\), \\(p'(0)=0\\), and \\[\n\\int_0^1 |2+3x-p(x) |^2 \\, dx\n\\] is as small as possible.\n\n\nExample 4.44 Find a polynomial \\(p\\in \\mathcal{P}_5(\\mathbb{R})\\) that makes \\[\n\\int_{-\\pi}^{\\pi} |\\sin x - p(x) |^2 \\, dx\n\\] is as small as possible.\n\n\nExample 4.45 Find a polynomial \\(p\\in \\mathcal{P}_2(\\mathbb{R})\\) such that \\[\n\\phi(p)=p \\left( \\frac{1}{2} \\right) = \\int_{0}^{1} p(x) \\, q(x) \\, dx\n\\] for every \\(p\\in \\mathcal{P}_2(\\mathbb{R})\\). Here is the direct approach. Every \\(q\\in\\mathcal{P}_2(\\mathbb{R})\\) can be expressed as \\[\n\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2.\n\\] The desired polynomial \\(q\\) must satisfy \\[\np(x)=1: \\qquad \\phi(1)=p(1/2)=1=\\int_0^1\\left( \\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2 \\right) \\, dx = \\alpha+\\gamma \\frac{1}{12}.\n\\] Moving to \\(p(x)=x-1/2\\) we find \\[\\begin{align*}\np(x)&=x-1/2: \\qquad \\phi(x-1/2)=p(1/2)=0 \\\\\n&=\\int_0^1(x-1/2)[\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2] \\, dx =\\beta \\frac{1}{12},\n\\end{align*}\\] so \\(\\beta=0\\). Finally \\[\np(x)=(x-1/2)^2: \\qquad \\phi((x-1/2)^2)=p(1/2)=0\n\\] \\[\n=\\int_0^1(x-1/2)^2[\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2] \\, dx =\\alpha \\frac{1}{12}+\\gamma\\frac{1}{80}.\n\\] Solving gives \\[\n\\alpha=\\frac{27}{12}, \\qquad  \\beta=0, \\qquad \\gamma=-15.\n\\] Thus \\[\nq(x)=\\frac{27}{12}-15(x-1/2)^2.\n\\]\n\n\nExample 4.46 Find a polynomial \\(q\\in \\mathcal{P}_2(\\mathbb{R})\\) such that \\[\n\\phi(p)=\\int_{0}^{1} p(x) \\, (\\cos \\pi x)  \\, dx= \\int_{0}^{1} p(x) \\, q(x) \\, dx\n\\] for every \\(p\\in \\mathcal{P}_2(\\mathbb{R})\\). Taking the same approach as in the previous example. We compute \\[\np(x)=1: \\qquad \\phi(1)=\\int_0^1\\cos(\\pi x) \\, dx=0\n\\] \\[\n=\\int_0^1[\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2]\\,dx=\\alpha+\\gamma\\frac{1}{12}.\n\\] Moving to \\(p(x)=x-1/2\\) we find \\[\np(x)=x-1/2: \\qquad \\phi(x-1/2)=\\int_0^1(x-1/2)\\cos(\\pi x)\\, dx=\\int_0^1 x \\cos (\\pi x) \\, dx\n\\] \\[\n=-\\frac{2}{\\pi^2}=\\int_0^1(x-1/2)[\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2]\\, dx=\\beta\\frac{1}{12},\n\\] so \\(\\beta=\\frac{-24}{\\pi^2}\\). Finally, since \\(\\cos(\\pi x)\\) is odd about \\(x=1/2\\), \\[\np(x)=(x-1/2)^2: \\qquad \\phi((x-1/2)^2)=\\int_0^1(x-1/2)^2\\cos(\\pi x)\\, dx =0\n\\] \\[\n=\\int_0^1(x-1/2)^2[\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2]=\\alpha \\frac{1}{12} +\\gamma\\frac{1}{80}.\n\\] Solving gives \\[\n\\alpha=\\gamma=0, \\qquad \\beta=\\frac{-24}{\\pi^2}.\n\\] Thus \\[\nq(x)=\\frac{-24}{\\pi^2}(x-1/2).\n\\]\n\n\nExample 4.47 Give an example of a real vector space \\(V\\) and \\(T\\in \\mathcal{L}(V)\\) such that trace\\((T^2) < 0\\).\n\n\nExample 4.48 Suppose \\(V\\) is a real vector space, \\(T\\in \\mathcal{L}(V)\\), and \\(V\\) has a basis consisting of eignvectors of \\(T\\). Prove that trace\\((T^2)\\geq 0\\).\n\n\nExample 4.49 Suppose \\(V\\) is an inner-product space and \\(v, w\\in V\\). Define \\(T\\in \\mathcal{L}(V)\\) by \\(T u=\\langle u,v \\rangle w\\). Find a formula for trace \\(T\\).\n\n\nExample 4.50 Prove that if \\(P\\in \\mathcal{L}(V)\\) satisfies \\(P^2=P\\), then trace \\(P\\) is a nonnegative integer.\n\n\nExample 4.51 Prove that if \\(V\\) is an inner-product space and \\(T\\in \\mathcal{L}(V)\\), then trace \\(T^*=\\overline{\\text{trace} T}.\\)\n\n\nExample 4.52 Suppose \\(V\\) is an inner-product space and \\(T\\in \\mathcal{L}(V)\\) is a positive operator with trace \\(T=0\\), then \\(T=0\\).\n\n\nExample 4.53 Suppose \\(T\\in \\mathcal{L}(\\mathbb{C}^3)\\) is the operator whose matrix is \\[\n\\begin{bmatrix}\n51 & -12 & -21 \\\\\n60 & -40 & -28 \\\\\n57 & -68 & 1\n\\end{bmatrix} .\n\\] If \\(-48\\) and \\(24\\) are eigenvalues of \\(T\\), find the third eigenvalue of \\(T\\).\n\n\nExample 4.54 Prove or give a counterexample: if \\(T\\in \\mathcal{L}(V)\\) and \\(c\\in F\\), then trace\\((c T ) = c\\) trace \\(T\\).\n\n\nExample 4.55 Prove or give a counterexample,: if \\(S, T\\in \\mathcal{L}(V)\\), then trace \\((S T)=(\\text{trace} S)(\\text{trace} T)\\).\n\n\nExample 4.56 Suppose \\(T\\in \\mathcal{L}(V)\\). Prove that if \\(\\text{trace} (ST)=0\\) for all \\(S\\in \\mathcal{L}(V)\\), then \\(T=0\\).\n\n\nExample 4.57 Suppose \\(V\\) is an inner-product space and \\(T\\in \\mathcal{L}(V)\\). Prove that if \\((e_1,\\ldots.,e_n)\\) is an orthonormal basis of \\(V\\), then \\[\n\\text{trace}(T^* T)=|| T e_1||^2+\\cdots + ||T e_n||^2.\n\\] Conclude that the right side of the equation above is independent of which orthonormal basis \\((e_1,\\ldots,e_n)\\) is chosen for \\(V\\).\n\n\nExample 4.58 Suppose \\(V\\) is a complex inner-product space and \\(T\\in \\mathcal{L}(V)\\). Let \\(\\lambda_1,\\ldots.,\\lambda_n\\) be the eigenvalues of \\(T\\), repeated according to multiplicity.\n\n\nExample 4.59 Suppose \\[\n\\begin{bmatrix}\na_{1,1} & \\cdots & a_{1,n} \\\\\n\\vdots &   & \\vdots \\\\\na_{n,1} & \\cdots & a_{n,n} \\\\\n\\end{bmatrix}\n\\] is the matrix of \\(T\\) with respect to some orthonormal basis of \\(V\\). Prove that \\[\n|\\lambda_1|^2+\\cdots + |\\lambda_n|^2\\leq \\sum^n_{k=1} \\sum_{j=1}^n |a_{j,k}|^2.\n\\]\n\n\nExample 4.60 Suppose \\(V\\) is an inner-product space. Prove that \\(\\langle S, T\\rangle=\\text{trace}(S T^*)\\) defines an inner-product on \\(\\mathcal{L}(V)\\).\n\n\nExample 4.61 Suppose \\(V\\) is an inner-product space and \\(T\\in \\mathcal{L}(V)\\). Prove that if \\(||T^* v ||\\leq ||T v||\\) for every \\(v \\in V\\), then \\(T\\) is normal.\n\n\nExample 4.62 Prove or give a counterexample: if \\(T\\in \\mathcal{L}(V)\\) and \\(c\\in F\\), then \\(\\det(cT)=c^{\\text{dim} V} \\det T\\).\n\n\nExample 4.63 Prove or give a counterexample: if \\(T\\in \\mathcal{L}(V)\\), then \\(\\det(S+T)=\\det S+ \\det T\\).\n\n\nExample 4.64 Suppose \\(A\\) is a block upper-triangular matrix \\[\nA=\n\\begin{bmatrix}\nA_1 & & * \\\\\n& \\ddots & \\\\\n0 & & A_m\n\\end{bmatrix} ,\n\\] where each \\(A_j\\) along the diagonal is a square matrix. Prove that \\[\n\\det A =(\\det A_1) \\cdots (\\det A_m).\n\\]"
  },
  {
    "objectID": "determinants.html#what-are-determinants-and-what-do-they-do",
    "href": "determinants.html#what-are-determinants-and-what-do-they-do",
    "title": "5  Determinants",
    "section": "5.1 What are determinants and what do they do",
    "text": "5.1 What are determinants and what do they do\nDeterminants are mathematical objects that affect the size, shape, or position of something else. In other words, they determine how something looks or behaves. The most common type of determinant is a matrix, which is used to determine the size, shape, and position of a two-dimensional object.\nFor example, a matrix can be used to determine the position of a point on a graph or the angle of a line. Determinants can also be used to solve problems in physics and engineering.\nFor example, they can be used to calculate the forces acting on an object in order to determine its motion. Determinants are also used in economics to predict changes in prices and wages. In short, determinants are powerful tools that can be used to solve a wide variety of problems.\nA determinant is a value that can be computed from the elements of a square matrix. The determinant of a matrix A is denoted by det(A), det A, or \\(\\vert A\\vert\\).\nGeometrically, it can be viewed as the volume of the n-dimensional parallelepiped that has the column vectors of A as its edges. If A is an invertible matrix, then the determinant is nonzero and A can be undone by multiplying it on either side by its inverse matrix. In particular, for a 2×2 matrix, the determinant is simply the product of the diagonal entries (top left times bottom right, minus top right times bottom left)."
  },
  {
    "objectID": "determinants.html#how-to-find-the-determinant-of-a-matrix",
    "href": "determinants.html#how-to-find-the-determinant-of-a-matrix",
    "title": "5  Determinants",
    "section": "5.2 How to find the determinant of a matrix",
    "text": "5.2 How to find the determinant of a matrix\nA determinant is a scalar value that can be computed from the elements of a square matrix. It is a useful tool in linear algebra for solving systems of linear equations and for computing the inverse of a matrix.\nThe determinant of a 2×2 matrix is: \\(|A|=ad-bc\\) where \\(ad-bc\\) is the signed determinant (the “discriminant”) because its value changes sign when two rows or columns are interchanged. \nFor matrices of larger size, the determinant can be computed using any of several methods, such as cofactor expansion or Laplace expansion. When computing the determinant, it is often convenient to change the order of the rows or columns in order to simplify the calculation. \nAnother method is to use Gauss-Jordan elimination. This method involves using row operations to transform the matrix into an upper triangular matrix. The determinant of the matrix can then be found by taking the product of the diagonal elements."
  },
  {
    "objectID": "determinants.html#determinant-properties",
    "href": "determinants.html#determinant-properties",
    "title": "5  Determinants",
    "section": "5.3 Determinant Properties",
    "text": "5.3 Determinant Properties\nThe determinant has several important properties: \n\nIf two rows (or columns) of a matrix are identical, then the determinant is zero. \nIf two rows (or columns) are interchanged, then the sign of the determinant changes. \nIf each element of a row (or column) is multiplied by a nonzero constant, then the determinant is multiplied by that constant."
  },
  {
    "objectID": "determinants.html#expansion-by-cofactors",
    "href": "determinants.html#expansion-by-cofactors",
    "title": "5  Determinants",
    "section": "5.4 Expansion by Cofactors",
    "text": "5.4 Expansion by Cofactors\nThe determinant of a matrix A is equal to the sum of the products of the elements in any one row or column, with each element being multiplied by a corresponding cofactor. Cofactors are special numbers that can be calculated from the minors of the matrix, which are the determinants of the matrices that result from removing one row and one column from A.\nThe sign of each cofactor is determined by its position in the matrix: odd-numbered rows and columns have positive signs, while even-numbered rows and columns have negative signs. The determinant of a matrix can be used to solve systems of linear equations, and it also provides information about the nature of the matrix itself.\nFor example, a matrix with a zero determinant is known as singular, meaning that it cannot be inverted. This makes it an important tool for solving mathematical problems. Determinants can be calculated by hand for small matrices, but larger matrices require the use of specialized software or an electronic calculator."
  },
  {
    "objectID": "determinants.html#cramers-rule",
    "href": "determinants.html#cramers-rule",
    "title": "5  Determinants",
    "section": "5.5 Cramer’s Rule",
    "text": "5.5 Cramer’s Rule\nCramer’s Rule is a method for solving systems of linear equations. In order to use Cramer’s Rule, the system must be expressed in matrix form. The determinant of the matrix is then calculated. This determinant is used to calculate the value of each variable in the system. Cramer’s Rule can be used to solve systems with any number of variables, but the matrices involved become increasingly complex as the number of variables increases.\nDespite this complexity, Cramer’s Rule provides a convenient way to solve systems of linear equations."
  },
  {
    "objectID": "determinants.html#how-i-teach-in-this-book",
    "href": "determinants.html#how-i-teach-in-this-book",
    "title": "5  Determinants",
    "section": "5.6 How I teach in this book",
    "text": "5.6 How I teach in this book\nHow I teach in this book, is by showing examples that students can learn from and understand. I also offer an in-depth look into linear algebra. My approach in this book is to first provide a gentle introduction to the topic at hand. Next, I guide the reader through the necessary calculations required to fully grasp the concepts being presented. Finally, I show how these ideas can be applied in practical scenarios.\nThroughout the book, I include worked examples and exercises for readers to test their understanding as they progress. With this book as your companion, you will soon be confident in manipulating determinants and matrices.\nDeterminants are a key concept in Linear Algebra, and understanding them is crucial for being able to do many matrix operations. This book will take you from the very basics of what determinants are, all the way up to using them in advanced topics like Cramer’s Rule. By the end, you will be a pro at using determinants in all sorts of situations.\nThe book will focus on the fundamentals of eigenvalues and eigenvectors, and how they can be applied in various settings. It is perfect for readers who want to learn more about these topics, or for those who need to brush up on their skills.\nThe book begins with a review of linear algebra, before moving on to discuss eigenvalues and eigenvectors. This is followed by a discussion of applications of these concepts, including in physics and engineering. The book concludes with a set of exercises for readers to test their understanding.\nEigenvalues and eigenvectors are fundamental concepts in linear algebra, with many applications in physics and engineering. In this section, we will discuss how to find eigenvalues and eigenvectors for a given matrix.\nSuppose we have a square matrix \\(A\\). To find the eigenvalues of \\(A\\), we need to solve the equation \\(Ax = \\lambda x\\) for \\(\\lambda\\) and \\(x\\). Here, \\(\\lambda\\) is the eigenvalue and \\(x\\) is the corresponding eigenvector.\nTo solve this equation, we can use the characteristic polynomial of \\(A\\). This is a polynomial equation whose roots are the eigenvalues of \\(A\\). To find the characteristic polynomial, we need to take the determinant of \\(A - \\lambda I\\), where the matrix \\(I\\) is the identity matrix.\nIn other words, an eigenvector is a vector that does not change direction when multiplied by a matrix. Eigenvectors are often used to diagonalize matrices, which means that they can be used to simplify matrices that do not have a simple structure.\nTo find the eigenvalues and eigenvectors of a matrix, one can use the characteristic polynomial of the matrix. The characteristic polynomial is a polynomial equation whose roots are the eigenvalues of the matrix. To find the eigenvectors of the matrix, one can use the inverse of the matrix.\nThe inverse of the matrix is a matrix that satisfies the equation: \\(A*A-1 = I\\). The inverse of a matrix exists if and only if the determinant of the matrix is not equal to zero. If the determinant of the matrix is equal to zero, then the matrix is singular and does not have an inverse. Once you have found the inverse of the matrix, you can multiply it by the original matrix to find the eigenvectors of the original matrix.\nLinear algebra is the study of mathematical objects that can be described by linear equations. These objects include vectors, matrices, and linear transformations. Linear algebra is a powerful tool that can be used to solve many problems in physics and engineering.\nOne of the most important techniques in linear algebra is diagonalization. This technique can be used to simplify complicated systems of linear equations. It is also a powerful tool for solving eigenvalue problems. In general, diagonalization is an essential tool for anyone who needs to work with linear equations.\nEigenvectors and eigenvalues are often introduced in the context of the diagonalization of matrices. In this context, an \\(n\\)-by-\\(n\\) matrix \\(A\\) is diagonalizable if and only if there exists \\(n\\) linearly independent eigenvectors \\(v_i\\) of \\(A\\), \\(i=1,...,n\\) such that \\(Av_i= \\lambda_i v_i\\) for some scalars \\(\\lambda_i\\) which are called eigenvalues. If these conditions hold, then we say that \\(A\\) has \\(n\\) distinct eigenvectors and \\(n\\) distinct eigenvalues.\nDiagonalization is the process of diagonalizing a matrix, which means finding a matrix that is equivalent to the original matrix but with the eigenvalues on the diagonal. This can be done by solving for the eigenvectors and then putting them into a matrix.\nThe eigenvectors are the vectors that stay the same when multiplied by the matrix. They are also called the characteristic vectors or natural modes. The eigenvalues are the scalars that you get when you multiply the eigenvector by the matrix. They tell you how much the vector changes.\nIn order to find them, you need to set up an equation and solve for them. Once you have them, you can put them into a diagonal matrix and then diagonalize it. This is a very important tool in mathematics and physics.\nDiagonalization is not always possible. For example, if the null space of a matrix is not spanned by eigenvectors, then the matrix cannot be diagonalized. However, when diagonalization is possible, it is often very useful. Diagonalization can be used to find solutions to systems of linear equations, simplify calculations, and understand the structure of matrices.\nEigenvalues and eigenvectors are important concepts in linear algebra. They can be used to diagonalize matrices, which is a powerful tool for solving problems. Eigenvalues and eigenvectors can also be used to understand the structure of matrices. In general, they are useful tools for anyone who needs to work with linear equations.\nThis book includes eigenvectors and eigenvalues and diagonalization of matrices. The teaching in this book is clear and concise, with plenty of opportunity for practice. The finding eigenvectors and eigenvalues chapter is particularly well explained, and the examples are helpful.\nThe diagonalization of matrices is also explained well, with plenty of opportunity for practice. The new exercises have been added to help reinforce the concepts learned in the book. Overall, this is an excellent resource for those who want to learn more about linear algebra.\nWrite a content brief for a book review on canonical forms from linear algebra. In mathematical terms, canonical forms are a way of representing a matrix in a simplified way. This book provides an in-depth introduction to the topic, starting with the basics and progressing to more complex concepts.\nIt is aimed at readers who have a basic understanding of linear algebra, and it covers topics such as invariant subspaces, the Jordan form, and the singular value decomposition. The book also includes worked examples and exercises to help readers develop their understanding of the material.\nInvariant subspaces are key to understanding canonical forms. In short, they are mathematical objects that do not change under certain transformations. This book starts with an introduction to invariant subspaces. It explains what they are and how they can be used to simplify calculations. Invariant subspaces are a powerful tool for understanding and manipulating linear transformations. With this book, you will learn how to use them to your advantage.\nIn mathematics, an invariant subspace of a linear mapping \\(T: V \\to V\\), i.e. from some vector space \\(V\\) to itself, is a subspace \\(W\\) of \\(V\\) that is preserved by \\(T\\); that is, \\(T(W) \\subseteq W\\). In other words, an invariant subspace is a subspace that is in some sense ``left unchanged” by the transformation. Intuitively, this means that if we take any vector in the subspace and apply the transformation to it, the result will still be in the subspace.\nInvariant subspaces arise naturally in many areas of mathematics, and they can be used to great effect in studying linear transformations. For instance, the study of invariant subspaces can help us to understand the structure of a linear transformation and to decompose it into simpler parts.\nIn addition, the existence of an invariant subspace can often be used to simplify computations involving the transformation. Consequently, the study of invariant subspaces is a fundamental tool in many areas of mathematics.\nThe dimension of an invariant subspace is called the multiplicity of the eigenvalue associated with that subspace. If the multiplicity of an eigenvalue is equal to its algebraic multiplicity (i.e. if it has the same number of eigenvectors associated with it as there are linearly independent solutions to the equation), then the eigenvalue is said to be geometrically simple.\nIn contrast, if the multiplicity of an eigenvalue is less than its algebraic multiplicity, then the eigenvalue is said to be geometrically multiple. Geometrically multiple eigenvalues may or may not have distinct eigenspaces; if they do, then each such eigenspace is called an invariant subspace of T.\nGeometrically simple eigenvalues always have distinct eigenspaces; in this case, each such eigenspace is also an invariant subspace of T. The study of invariant subspaces plays an important role in many areas of mathematics, including functional analysis, differential equations, and numerical linear algebra.\nJordan normal form is a specific type of upper triangular matrix that is used to represent linear operators on a finite-dimensional vector space. This matrix is made up of Jordan blocks, which are submatrices with identical eigenvalues that have each non-zero off-diagonal entry equal to 1.\nJordan normal form is useful because it allows for the efficient computation of matrix powers and roots. In addition, this form can be used to transform one matrix into another that is similar, meaning that they have the same eigenvalues. Jordan normal form is an important tool in linear algebra and has many applications in physics and engineering.\nJordan Canonical Form, also called Jordan normal form, is a block diagonal matrix consisting of Jordan blocks. This is not entirely unique as the order of the Jordan blocks are not fixed. The Jordan blocks are grouped together by their eigenvalues, but there is no ordering imposed among the eigenvalues or Jordan blocks. Jordan normal form can be applied to any square matrix as long as the field of coefficients contains all the eigenvalues of the matrix. This normal form is named after mathematician Camille Jordan.\nToday, Jordan blocks are used in many different areas of mathematics and physics.\nIn this book, I provide a comprehensive introduction to invariant subspaces and Jordan canonical forms. This will include a review of the necessary linear algebra, as well as numerous examples and applications. In addition, I will discuss how these concepts can be used to simplify computations involving linear transformations.\nMy approach is to first introduce the necessary theory, followed by worked examples that illustrate how the theory can be applied in practice. I believe that this approach will provide readers with a strong understanding of the material covered in this book.\nMy approach is based on two fundamental principles: first, that students should be given a firm foundation in the core material; and second, that they should be given ample opportunity to work with the material in order to develop their understanding.\nThe study of invariant subspaces and Jordan canonical forms is a fundamental tool in many areas of mathematics. These concepts are important in understanding the structure of linear transformations and in simplifying computations. In addition, the existence of an invariant subspace can often be used to simplify computations involving the transformation. Consequently, the study of invariant subspaces and Jordan canonical forms is a fundamental tool in many areas of mathematics.\nThe determinant function can be defined by essentially two different methods. The advantage of the first definition, one which uses permutations, is that it provides an actual formula for \\(\\det(A)\\), a fact of theoretical importance. The disadvantage is that, quite frankly, computing a determinant by this method can be cumbersome.\n\nA pattern in an \\(n\\times n\\) matrix is a way to choose \\(n\\) entries of the matrix so that there is one chosen entry in each row and in each column of \\(A\\).\nWith a pattern \\(P\\) we associate the product of all its entries, denoted by prod \\(P\\).\nTwo entries in a pattern are said to be inverted if one of them is located to the right and above the other in the matrix.\nThe signature of a pattern \\(P\\) is defined as \\(\\text{sgn} P=(-1)^{(\\text{number of inversions in } P)}.\\)\n\n\nDefinition 5.1 The determinant of an \\(n\\times n\\) matrix \\(A\\) is defined as \\[\n\\det (A)=\\sum (\\text{sgn } P) (\\text{prod } P)\n\\] where the sum is taken over all \\(n!\\) patterns \\(P\\).\n\n\nExample 5.1 Use the definition of an \\(n\\times n\\) determinant to prove the formulas for determinants for \\(2\\times 2\\) matrices and \\(3\\times 3\\) matrices. We find \\[\n\\begin{vmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{vmatrix}\n=a_{11}a_{22}-a_{12}a_{21}\n\\] \\[\n\\begin{vmatrix}\na_{11} & a_{12} & a_{13} \\\\\na_{21} & a_{22} & a_{23} \\\\\na_{31} & a_{32} & a_{33}\n\\end{vmatrix}\n=a_{11} a_{22} a_{33} + a_{12} a_{23} a_{31}\n+ a_{13} a_{21} a_{32} - a_{11} a_{23} a_{32}\n- a_{12} a_{21} a_{33} -a_{13} a_{22} a_{31}\n\\]\n\n\nExample 5.2 Find the determinant of the following matrix using the definition of determinant \\[\nM=\\begin{bmatrix}  \n0 & 0 & 1 & 0 & 2 \\\\\n5 & 4 & 3 & 2 & 1 \\\\\n1 & 3 & 5 & 0 & 7 \\\\\n2 & 0 & 4 & 0 & 6 \\\\\n0 & 0 & 3 & 0 & 4\n\\end{bmatrix} .\n\\] There are only two patterns with a nonzero product. Therefore \\[\n\\det M= (-1)^8(2\\cdot 3\\cdot 3\\cdot2\\cdot2)+(-1)^{5}(2\\cdot 3\\cdot 1\\cdot 2 \\cdot 4)=24.\n\\]\n\n\nLemma 5.1 The determinant of an (upper or lower) triangular matrix \\(A\\) is the product of the diagonal entries of the matrix.\n\n\nProof. Suppose \\[\nA=\n\\begin{bmatrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\n0 & a_{22} & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & a_{n-1,n} \\\\\n0 & \\cdots & 0 & a_{nn} &\n\\end{bmatrix}\n\\qquad \\text{or} \\qquad\nA=\n\\begin{bmatrix}\na_{11} & 0 & \\cdots & 0 \\\\\na_{21} & a_{22} & \\ddots & \\vdots \\\\\n\\vdots & \\ddots & \\ddots & 0 \\\\\na_{n,1} & \\cdots & a_{n,n-1} & a_{nn} &\n\\end{bmatrix}.\n\\] To have a nonzero product a pattern must contain the first component of the first column, then the second component of the second column, and so on. Thus, only the diagonal pattern \\(P\\) makes a nonzero contribution. We conclude that \\[\n\\det A = (\\text{sgn } P)(\\text{prod }P)=(-1)^0 a_{11} \\cdots a_{nn}= a_{11} \\cdots a_{nn}.\n\\]\n\n\nTheorem 5.1  Prove that if \\(A\\) and \\(C\\) are square matrices (not necessarily of the same size), then \\[\n\\det \\begin{bmatrix}  A & B \\\\ 0 & C \\end{bmatrix}\n= \\det \\begin{bmatrix}  A & 0 \\\\ B & C \\end{bmatrix}\n= (\\det A)(\\det C).\n\\]\n\n\nProof. The proof is left for the reader.\n\n\nExample 5.3 Find the determinant of the matrix \\[\nM=\n\\begin{bmatrix}\na_{11} & a_{12} & b_{11} & b_{12} \\\\\na_{21} & a_{22} & b_{21} & b_{22} \\\\\n0 & 0 & c_{11} & c_{12} \\\\\n0 & 0 & c_{21} & c_{22}\n\\end{bmatrix}.\n\\] Let \\[\nA=\n\\begin{bmatrix}\na_{11} & a_{12} \\\\\na_{21} & a_{22}\n\\end{bmatrix}\n\\] and \\[\nC=\n\\begin{bmatrix}\nc_{11} & c_{12} \\\\\nc_{21} & c_{22}\n\\end{bmatrix},\n\\] then \\[\n\\det M=(\\det A)(\\det B)=(a_{11}a_{22}-a_{21}a_{12})(c_{11}c_{22}-c_{21}c_{12}).\n\\]\n\n\nExample 5.4 Find \\(2\\times 2\\) matrices \\(A, B, C, D\\) such that \\[\n\\det \\begin{bmatrix} A & B \\\\ C & D \\end{bmatrix} \\neq (\\det A)(\\det D)-(\\det B)(\\det C).\n\\] The standard basis for \\(\\mathbb{R}^{2 \\times 2}\\) works.\n\nThe trace of an \\(n\\times n\\) matrix \\(A\\) is defined as the sum of the diagonal entries of \\(A\\) and is denoted by \\(\\text{trace}(A)\\).\n\nTheorem 5.2 If \\(A\\) is a square matrix then \\(\\det (A^T)=\\det (A)\\) and \\(\\text{trace}(A^T)=\\text{trace}(A)\\).\n\n\nProof. For each pattern \\(P\\) in \\(A\\) we can consider the corresponding (transposed) pattern \\(P^T\\) in \\(A^T\\). The two patterns \\(P\\) and \\(P^T\\) involve the same numbers, and they contain the same number of inversions, but the role of the two numbers in each inversion is reversed. Therefore, the two patterns make the same contribution to the respective determinant and so, \\((\\text{sgn} P)(\\text{prod} P)=(\\text{sgn} P^T)(\\text{prod} P^T)\\). Therefore, we conclude that \\(\\det(A)=\\det(A^T)\\).\n\n\nExample 5.5 Find the determinant of the matrices \\(M\\), \\(N\\), \\(M^T\\), and \\(N^T\\) where \\[\nM=\\begin{bmatrix} 4 & 3 & 2 & 1 \\\\ 0 & 5 & 6 & 7 \\\\ 0 & 0 & 3 & 2 \\\\ 0 & 0 & 0 & 4 \\end{bmatrix}\n\\qquad \\text{and} \\qquad\nN=\\begin{bmatrix}  0 & 0 & 0 & 8 \\\\ 0 & 0 & 2 & 3 \\\\ 0 & 7 & 6 & 5 \\\\ 1 & 2 & 3 & 4  \\end{bmatrix} .\n\\] Since \\(M\\) and \\(N\\) are upper triangular and lower triangular, respectively, \\[\n\\det M= 4\\cdot 5\\cdot 3\\cdot 4=240\n\\qquad \\text{and} \\qquad\n\\det N= 1\\cdot 7\\cdot 2\\cdot 8=112.\n\\] Since \\[\nM^T=\\begin{bmatrix}  4 & 0 & 0 & 0 \\\\3 & 5 & 0 & 0 \\\\ 2 & 6 & 3 & 0 \\\\1 & 7 & 2 & 4\\end{bmatrix}\n\\qquad \\text{and} \\qquad\nN^T=\\begin{bmatrix}   0 & 0 & 0 & 1 \\\\ 0 & 0 & 7 & 2 \\\\ 0 & 2 & 6 & 3\\\\ 8 & 3 & 5 & 4 \\end{bmatrix}\\] and these matrices are lower triangular and upper triangular, respectively, \\[\n\\det M=\\det M^T\n\\qquad \\text{and} \\qquad\n\\det N= \\det N^T.\n\\]\n\n\nLemma 5.2  Suppose \\(A\\) and \\(B\\) are \\(n\\times n\\) matrices.\n\n If \\(B\\) is obtained from \\(A\\) by dividing a row of \\(A\\) by a scalar \\(k\\), then \\(\\det B=(1/k)\\det (A)\\).\n If \\(B\\) is obtained from \\(A\\) by a row swap, then \\(\\det(B)=-\\det(A)\\).\n If \\(B\\) is obtained from \\(A\\) by adding a multiple of a row of \\(A\\) to another row, then \\(\\det(B)=\\det(A)\\).\n Suppose you swap rows \\(s\\) times as you transform \\(A\\) into \\(B\\), and you divide various rows by the scalars \\(k_1,\\ldots ,k_r\\), then \\[\n\\det(A)=(-1)^sk_1\\cdots k_r\\det(B).\n\\]\n If \\(s\\) is the number of row swaps and \\(k_1,\\ldots ,k_r\\) are the scalars used to divide rows to obtain pivots in producing \\(\\text{rref}(A)\\) when performing Gauss-Jordan elimination, then \\[\\begin{equation}\n\\label{detgjr}\n\\det(A)=(-1)^sk_1\\cdots k_r\\det(\\text{rref} (A)).\n\\end{equation}\\]\n\n\n\nProof. The proof is left for the reader.\n\n\nExample 5.6 Use Gauss-Jordan elimination to find the determinant of the matrix \\[\nM=\n\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n1 & 2 & 3 & 4 & 5 \\\\\n1 & 3 & 6 & 10 & 15 \\\\\n1 & 4 & 10 & 20 & 35  \\\\\n1 & 5 & 15 & 35 & 70\n\\end{bmatrix} .\n\\] Using elementary row operations \\(R_2\\to R_2-R_1\\), \\(R_3\\to R_3-R_1\\), \\(R_4\\to R_4-R_1\\), and \\(R_5\\to R_5-R_1\\), \\[\nM \\rightarrow\nM_1=\n\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n0 & 1 & 2 & 3 & 4 \\\\\n0 & 2 & 5 & 9 & 14 \\\\\n0 & 3 & 9 & 19 & 34  \\\\\n0 & 4 & 14 & 34 & 69\n\\end{bmatrix}\n\\qquad \\text{with} \\qquad\n\\det(M)=\\det(M_1).\n\\] Using elementary row operations \\(R_3\\to -2R2+R_3\\), \\(R_4\\to -3R2+R_4\\), and \\(R_5\\to -4R2+R_5\\), \\[\nM_1 \\rightarrow\nM_2=\n\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n0 & 1 & 2 & 3 & 4 \\\\\n0 & 0 & 1 & 3 & 6 \\\\\n0 & 0 & 3 & 10 & 22  \\\\\n0 & 0 & 6 & 22 & 53\n\\end{bmatrix}\n\\qquad \\text{with} \\qquad\n\\det(M_1)=\\det(M_2).\n\\] Using elementary row operations \\(R_4\\to -3R3+R_4\\), \\(R_5\\to -6R3+R_5\\), \\[\nM_2 \\rightarrow\nM_3=\n\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n0 & 1 & 2 & 3 & 4 \\\\\n0 & 0 & 1 & 3 & 6 \\\\\n0 & 0 & 0 & 1 & 4  \\\\\n0 & 0 & 0 & 4 & 17\n\\end{bmatrix}\n\\qquad \\text{with} \\qquad\n\\det(M_2)=\\det(M_3).\n\\] Using the elementary row operation \\(R_5\\to -4R4+R_5\\), \\[\nM_3 \\rightarrow\nM_4=\n\\begin{bmatrix}\n1 & 1 & 1 & 1 & 1 \\\\\n0 & 1 & 2 & 3 & 4 \\\\\n0 & 0 & 1 & 3 & 6 \\\\\n0 & 0 & 0 & 1 & 4  \\\\\n0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\qquad \\text{with} \\qquad\n\\det(M_3)=\\det(M_4).\n\\] Therefore \\(\\det(M)=\\det(M_4)=1\\).\n\n\nTheorem 5.3 A square matrix \\(A\\) is invertible if and only if \\(\\det A\\neq 0\\).\n\n\nProof. If \\(A\\) is invertible, then \\(\\text{rref}(A)=I_n\\), so that by \\(\\ref{detgjr}\\) \\(\\det(A)\\neq 0\\). Conversely, by contrapositive, if \\(A\\) is noninvertible, then the last row of \\(\\text{rref}(A)\\) contains all zeros, so that \\(\\det(\\text{rref}(A))=0\\). Therefore, \\(\\ref{detgjr}\\) shows \\(\\det(A)=0\\).\n\n\nExample 5.7 Consider a \\(4\\times 4\\) matrix \\(A\\) with rows \\(\\vec v_1, \\vec v_2, \\vec v_3, \\vec v_4\\). If \\(\\det (A)=8\\) find the determinant of the matrix \\[\n\\vectorfour{\\vec v_4}{\\vec v_2+9\\vec v_4}{-\\vec v_3}{\\vec v_1}.\n\\] Switching rows 1 and 4, then multiplying row 3 by \\(-1\\), then adding \\(-9 R_4\\) to \\(R_2\\) yields, \\[\n\\det \\vectorfour{\\vec v_4}{\\vec v_2+9\\vec v_4}{-\\vec v_3}{\\vec v_1}\n=- \\det \\vectorfour{\\vec v_1}{\\vec v_2+9\\vec v_4}{-\\vec v_3}{\\vec v_4}\n= \\det \\vectorfour{\\vec v_1}{\\vec v_2+9\\vec v_4}{\\vec v_3}{\\vec v_4}\n= \\det \\vectorfour{\\vec v_1}{\\vec v_2}{\\vec v_3}{\\vec v_4}\n=8.\n\\]\n\n\nExample 5.8 Determine whether the linear transformation given by the following linear equations is an isomorphism. If so, find the inverse transformation. \\[\n\\left\\{ \\begin{array}{rl}\ny_1&=x_1+2x_2+3x_3 \\\\\ny_2& =3x_2+4x_3 \\\\\ny_3 &= x_1+6x_2 +5x_3.\n\\end{array}\n\\right.\n\\] The coefficient matrix is \\[\nA=\\begin{bmatrix}  \n1 & 2 & 3 \\\\\n0 & 3 & 4 \\\\\n1 & 6 & 5\n\\end{bmatrix} .\n\\] We will form the matrix \\(\\left[ A | I_3 \\right]\\) and apply row operations, thus completing two tasks at once. This process will convert the matrix \\(A\\) into an upper-triangular matrix and thus be able to determine the determinant of \\(A\\) (of thus of the linear transformation) before we actually find the inverse matrix. If at any time we see the determinant will be 0 we will stop and decide that the linear transformation is not an isomorphism. Otherwise we continue applying row operations until we determine \\(A^{-1}\\). After completing these steps we find \\[\nA^{-1}=\n\\begin{bmatrix}  \n9/10 & -4/5 & 1/10 \\\\\n-2/5 & -1/5 & 2/5 \\\\\n3/10 & 2/5 & -3/10\n\\end{bmatrix} .\n\\] Therefore the linear transformation is an isomorphism and the inverse transformation is \\[\n\\left\\{ \\begin{array}{rl}\nx_1&= \\frac{9}{10}y_1 -\\frac{4}{5}y_2+\\frac{1}{10}y_3 \\\\\nx_2& = -\\frac{2}{5}y_1 -\\frac{1}{5}y_2+\\frac{2}{5}y_3 \\\\\nx_3 &=  \\frac{3}{10} y_1+ \\frac{2}{5}y_2 -\\frac{3}{10}y_3.\n\\end{array}\n\\right.\n\\]\n\n\nExample 5.9  The matrix defined by \\[\\begin{equation}\n\\label{vandermondematrix}\nV_n=\n\\begin{bmatrix}\n1 & x_1 & x_1^2 & \\cdots & x_1^{n-1} \\\\\n1 & x_2 & x_2^2 & \\cdots & x_2^{n-1} \\\\\n1 & x_3 & x_3^2 & \\cdots & x_3^{n-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\cdots & x_n^{n-1} \\\\\n\\end{bmatrix}\n\\end{equation}\\] is called the \\(n\\)-th Vandermonde matrix . Prove \\[\\begin{equation}\n\\label{vandermondedet}\ndet(V_n)=\\prod_{1\\leq i<j\\leq n} (x_j-x_i).\n\\end{equation}\\] By \\(\\ref{detgjrlemma}\\)-\\(\\eqref{detgjrlemmathree}\\), we can perform the row operations \\(R_i\\rightarrow R_i-R_1\\) for \\(2\\leq i \\leq n\\) leaving the value of the determinant unchanged, so \\[\\begin{equation}\n\\det(V_n)\n=\n\\begin{vmatrix}\n1 & x_1 & x_1^2 & \\cdots & x_1^{n-1} \\\\\n1 & x_2 & x_2^2 & \\cdots & x_2^{n-1} \\\\\n1 & x_3 & x_3^2 & \\cdots & x_3^{n-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n1 & x_n & x_n^2 & \\cdots & x_n^{n-1} \\\\\n\\end{vmatrix}\n=\n\\begin{vmatrix}\n1 & x_1 & x_1^2 & \\cdots & x_1^{n-1} \\\\\n0 & x_2-x_1 & x_2^2-x_1^2 & \\cdots & x_2^{n-1}-x_1^{n-1} \\\\\n0 & x_3-x_1 & x_3^2-x_1^2 & \\cdots & x_3^{n-1}-x_1^{n-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & x_n-x_1 & x_n^2-x_1^2 & \\cdots & x_n^{n-1}-x_1^{n-1} \\\\\n\\end{vmatrix}\n\\end{equation}\\] By \\(\\ref{detgjrlemma}\\)-\\(\\eqref{detgjrlemmathree}\\) and \\(\\ref{propdettrace}\\)-\\(\\eqref{propdettraceone}\\), we can perform the column operations \\(C_i\\rightarrow C_{i}-x_1C_{i-1}\\) for \\(2< i \\leq n\\) leaving the value of the determinant unchanged, so \\[\\begin{equation}\n=\n\\begin{vmatrix}\n1 & 0 & 0 & \\cdots & 0 \\\\\n0 & x_2-x_1 & (x_2-x_1)x_2 & \\cdots & (x_2-x_1)x_2^{n-2} \\\\\n0 & x_3-x_1 & (x_3-x_1)x_3 & \\cdots & (x_3-x_1)x_3^{n-2} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & x_n-x_1 & (x_n-x_1)x_n & \\cdots & (x_n-x_1)x_n^{n-2} \\\\\n\\end{vmatrix}\n=\n\\prod_{k=2}^n (x_k-x_1) \\det(V_{n-1}).\n\\end{equation}\\] This process ends with \\(\\det(V_2)=x_n-x_{n-1}\\) and so \\(\\ref{vandermondedet}\\) follows.\n\n\nExample 5.10 Show the transformation \\[\nT(\\vec x )=\\det \\begin{bmatrix}  \\vec v_1 & \\cdots & \\vec v_{i-1} & \\vec x & \\vec v_{i+1} & \\cdots & \\vec v_n \\end{bmatrix}\n\\] from \\(\\mathbb{R}^{n\\times 1}\\) to \\(\\mathbb{R}\\) is a linear transformation. Similarly, show the transformation \\[\nT(\\vec x )=\\det \\begin{bmatrix}  \\vec v_1 & \\cdots & \\vec v_{i-1} & \\vec x & \\vec v_{i+1} & \\cdots & \\vec v_n \\end{bmatrix} ^T\n\\] from \\(\\mathbb{R}^{1\\times n}\\) to \\(\\mathbb{R}\\) is a linear transformation.\n\n\nExample 5.11 Find the image and kernel of the linear transformation \\[\nT(\\vec x)=\\det \\begin{bmatrix} \\vec x & \\vec u & \\vec v \\end{bmatrix}\n\\] from \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}\\), where \\(\\vec u =\\vectorthree{2}{0}{0}^T\\) and \\(\\vec v=\\vectorthree{0}{1}{3}^T\\). Notice that \\[\\begin{align*}\nT(\\vec e_1) &=\\det \\begin{bmatrix} 1 & 2 & 0 \\\\ 0 & 0 & 1 \\\\ 0 & 0 & 3 \\end{bmatrix} = 0, \\\\\nT(\\vec e_2) &=\\det \\begin{bmatrix} 0 & 2 & 0 \\\\ 1 & 0 & 1 \\\\ 0 & 0 & 3 \\end{bmatrix}\n=- \\det \\begin{bmatrix} 1 & 0 & 1 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 3 \\end{bmatrix}\n=-6, \\quad \\text{and} \\\\\nT(\\vec e_3) &=\\det \\begin{bmatrix} 0 & 2 & 0 \\\\ 0 & 0 & 1 \\\\ 1 & 0 & 3 \\end{bmatrix}\n= \\det \\begin{bmatrix} 1 & 0 & 3 \\\\ 0 & 2 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n=2.\n\\end{align*}\\] Therefore, the matrix of \\(T\\) with respect to the standard basis \\(\\vec e_1, \\vec e_2, \\vec e_3\\) is \\(B=\\begin{bmatrix} 0 & -6 & 2 \\end{bmatrix}\\), and so \\(T(\\vec x)=B \\vec x\\). To find the kernel we solve the system \\[\n\\begin{bmatrix} 0 & -6 & 2 \\end{bmatrix} \\vectorthree{x_1}{x_2}{x_3}^T=0.\n\\] Thus the kernel has basis \\[\n\\left ( \\vectorthree{1}{0}{0}^T, \\vectorthree{0}{1}{1}^T\\right )\n\\] which means the dimension of the kernel of \\(T\\) is 2, and so by the rank nullity theorem the dimension of the image of \\(T\\) is 1.\n\n\nExample 5.12 Consider the linear transformation \\[\nT(\\vec x)=\\det \\begin{bmatrix} \\vec x & \\vec v_2 & \\vec v_3 & \\cdots & v_n \\end{bmatrix}\n\\] from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}\\), where \\(\\vec v_2, \\vec v_3,\\ldots ,\\vec v_n\\) are linearly independent vectors in \\(\\mathbb{R}^n\\). Describe the image and kernel of this transformation, and determine their dimensions. Since \\(\\vec v_1,\\ldots ,\\vec v_n\\) are linearly independent, \\(T(\\vec x)=0\\) only if \\(\\vec x\\) is a linear combination of the \\(\\vec v_i\\)’s, (otherwise the matrix \\(\\begin{bmatrix} \\vec x & \\vec v_1 & \\cdots & \\vec v_n\\end{bmatrix}\\) is invertible, and \\(T(\\vec x)\\neq 0\\)). Hence, the kernel of \\(T\\) is the span of \\(\\vec v_2,\\ldots , \\vec v_n\\), an \\((n-1)\\)-dimensional subspace of \\(\\mathbb{R}^n\\). The image of \\(T\\) is the real line \\(\\mathbb{R}\\) (since it must be 1-dimensional).\n\n\nExample 5.13 For a fixed positive integer \\(n\\), let \\(D\\) be a function which assigns to any \\(n\\times n\\) matrix \\(A\\) a number \\(D(A)\\) such that\n\n\\(D\\) is linear in the rows,\n\\(D(B)=-D(A)\\) if \\(B\\) is obtained from \\(A\\) by a row swap, and\n\\(D(I_n)=1\\).\n\nShow that \\(D(A)=\\det(A)\\) for all \\(n\\times n\\) matrices \\(A\\). First we comment that if a square matrix \\(A\\) has two equal rows, then \\(D(A)=0\\). Indeed, if we swap two equal rows and call the resulting matrix \\(B\\), then \\(B=A\\), so that \\(D(A)=D(B)=-D(A)\\), by property (ii), and \\(D(A)=0\\) as claimed.\nNext we need to understand how the elementary row operations affect \\(D\\). Properties (i) and (ii) tell us about row multiplication and row swaps, but we still need to think about row additions. We will show that if \\(B\\) is obtained from \\(A\\) by adding \\(k\\) times the \\(i\\)th row to the \\(j\\)th, then \\(D(B)=D(A)\\), Let’s label the row vectors of \\(A\\) by \\(\\vec v_1,.,,,\\vec v_n\\). By linearity of \\(D\\) in the \\(j\\)th row (i) we have \\[\nD(B)=D\\left(\\begin{bmatrix} \\vdots \\\\ \\vec v_i \\\\ \\vdots \\\\ \\vec v_j+k \\vec v_i \\\\ \\vdots  \\end{bmatrix} \\right)\n=D(A)+kD\\left(\\begin{bmatrix} \\vdots \\\\ \\vec v_i \\\\ \\vdots \\\\ \\vec v_i \\\\ \\vdots \\end{bmatrix} \\right)\n=D(A).\n\\] Note that in the last step we have used the remark made at the beginning. Now we can write \\[\nD(A)=(-1)^sk_1\\cdots k_r D(\\text{rref}(A))\n\\] where \\(s\\) is the number of row swaps and \\(k_1,\\ldots ,k_r\\) are scalars used to obtain the pivots in the Gaussian reduction.\nNext we observe that if \\(D(\\text{rref}(A))=\\det(\\text{rref}(A))\\) for all square matrices \\(A\\). Indeed, if \\(A\\) is invertible, then \\(\\text{rref}(A)=I_n\\), and \\(D(I_n)=1=\\det(I_n)\\) by property (iii). If \\(A\\) fails to be invertible then \\[\nD(\\text{rref}(A))=0=\\det(\\text{rref}(A)\n\\] by linearity in the last row.\nIt follows that \\[\nD(A)=(-1)^sk_1\\cdots k_r D(\\text{rref}(A))=(-1)^sk_1\\cdots k_r \\det(\\text{rref}(A))=\\det(A)\n\\] as desired.\n\n::: {#thm- } [Properties of Determinant and Trace] \n\nIf \\(A\\) and \\(B\\) are square matrices, then \\(\\det (AB)=(\\det A)(\\det B)\\).\nIf \\(A\\) and \\(B\\) are square matrices of the same size, then \\(\\text{trace}(AB)=\\text{trace}(BA)\\) and \\(\\text{trace}(A+B)=\\text{trace}(A)+\\text{trace}(B)\\).\nIf \\(A\\) and \\(B\\) are similar matrices, then \\(\\det(A)=\\det(B)\\) and \\(\\text{trace}(A)=\\text{trace}(B)\\).\nIf \\(A\\) is an invertible matrix, then \\(\\det (A^{-1})=(\\det A)^{-1}\\). :::\n\n\nProof. The proof of each part follows.\n\nSuppose \\(A=\\begin{bmatrix} a_{i j} \\end{bmatrix}\\) and \\(B=\\begin{bmatrix} b_{i j} \\end{bmatrix}.\\) The \\(i^\\text{th}\\) term on the diagonal of \\(A+B\\) is \\(a_{i,i}+b_{i,i}\\), thus \\[\n\\text{trace}(A+B)=\\sum_{i=1}^n  (a_{i,i}+b_{i,i})=\\sum_{i=1}^n  a_{i,i}+\\sum_{i=1}^nb_{i,i}=\\text{trace}(A)+\\text{trace}(B).\n\\] The \\(i^\\text{th}\\) term on the diagonal of \\(AB\\) is \\(\\sum_{j=1}^na_{i,j}b_{j,i}\\) and the \\(i^\\text{th}\\) term on the diagonal of \\(BA\\) is \\(\\sum_{i=1}^n b_{j,i}a_{i,j}.\\) Therefore, \\[\n\\text{trace}(AB)\n=\\sum_{i=1}^n \\sum_{j=1}^n a_{i,j}b_{j,i}\n=\\sum_{i=1}^n \\sum_{j=1}^n b_{j,i}a_{i,j}\n=\\text{trace}(BA)\n\\] as desired.\nIf matrices \\(A\\) and \\(B\\) are similar there exists an invertible matrix \\(P\\) such that \\(B=P^{-1}AB\\). Then, \\[\n\\text{trace}(P^{-1}AP)\n=\\text{trace}(P^{-1}(AP))\n=\\text{trace}((AP)P^{-1})\n=\\text{trace}(A).\n\\]\n\n\n\nExample 5.14 Consider an \\(n\\times n\\) matrix \\(A\\) such that both \\(A\\) and \\(A^{-1}\\) have integer entries. What are the possible values of \\(\\det(A)\\)? Applying the determinant to the equation \\(A A^{-1}=I_n\\), it follows \\(\\det(A) \\det (A^{-1})=1\\). The only way the product of the two integers \\(\\det(A)\\) and \\(\\det(A^{-1})\\) can be 1 is when they are both 1 or both \\(-1\\). Therefore, \\(\\det(A)=\\pm 1\\).\n\nConsider the \\((n-1)\\times (n-1)\\) matrix \\(M_{i,j}\\) obtained from an \\(n\\times n\\) matrix \\(A\\) by deleting the \\(i\\)-th row and \\(j\\)-th column. Then the determinant of \\(M_{i,j}\\) is called the \\((i,j)\\)-th minor of \\(A\\).\n\nTheorem 5.4 (Laplace Expansion) The determinant of an \\(n\\times n\\) matrix \\(A=[a_{i,j}]\\) can be evaluated by using either one of the following equations: \\[\\begin{equation}\\label{row expansion}\n\\det(A)=(-1)^{i+1}a_{i,1}\\det(M_{i,1})+\\cdots + (-1)^{i+n} a_{i,n}\\det(M_{i,n})\n\\end{equation}\\] where \\(i\\) is any integer such that \\(1\\leq i \\leq n\\), or \\[\\begin{equation}\\label{column expansion}\n\\det(A)=(-1)^{1+j}a_{1,j}\\det(M_{1,j})+\\cdots + (-1)^{n+j} a_{n,j}\\det(M_{n,j})\n\\end{equation}\\] where \\(j\\) is any integer such that \\(1\\leq j \\leq n\\).\n\n\nProof. The proof is left for the reader.\n\n\nExample 5.15 Evaluate the determinant of the matrix \\[\nA=\n\\begin{bmatrix}\n3 & 2 & 6 & 4 \\\\\n1 & -2 & -3 & 1 \\\\\n0 & 2 & 3 & 8 \\\\\n4 & -1 & 7 & 2\n\\end{bmatrix}\n\\] by using expansion of minors along the second row. Expanding along the second row, by using \\(\\ref{row expansion}\\) with \\(i=2\\) and \\(n=4\\) \\[\\begin{align*}\n\\det(A)\n&= - \\det(M_{2,1})  -2\\det(M_{2,2}) +3\\det(M_{2,3})  + \\det(M_{2,4}) \\\\\n& =-(-104)-2(-6)+3(68)+27=347.\n\\tag*{ }\n\\end{align*}\\]\n\nThe trace and determinant share come common properties, for example, they both are invariants of a linear transformation. To be more explicit, let \\(T\\) be a linear transformation from a linear space \\(V\\) to \\(V\\), where \\(V\\) is a finite-dimensional linear space. If \\(\\mathcal{B}\\) is a basis of \\(V\\) and \\(B\\) is the \\(\\mathcal{B}\\)-matrix of \\(T\\), then we define the determinant and trace of the linear transformation \\(T\\) as \\(\\det(T)=\\det(B)\\) and \\(\\text{trace}(T)=\\text{trace}(B)\\), respectively.\n\nTheorem 5.5 The determinant and trace of a linear transformation are independent of the basis chosen.\n\n\nExample 5.16 Compute the determinant of the linear transformation \\(T(f(t))=f(3t-2)\\) from \\(\\mathcal{P}_2\\) to \\(\\mathcal{P}_2\\) and use it to determine whether the linear transformation is an isomorphism. We choose to use the standard basis of \\(\\mathcal{P}_2\\), namely \\((1, t, t^2)\\). With respect to this basis the matrix of \\(T\\) is \\(A=\\begin{bmatrix} 1 & -2 & 4 \\\\ 0 & 3 & -12 \\\\ 0 & 0 & 9 \\end{bmatrix},\\) so that \\(\\det(T)=\\det(A)=27\\). Therefore, the given transformation is an isomorphism.\n\n\nExample 5.17 Find the determinant of the linear transformation \\[\nT(M)=\\begin{bmatrix} 1 & 2 \\\\ 2 & 3\\end{bmatrix} M+M\\begin{bmatrix} 1 & 2 \\\\ 2 & 3\\end{bmatrix}\n\\] from the space \\(V\\) of symmetric \\(2\\times 2\\) matrices to \\(V\\) and use it to determine whether the linear transformation is an isomorphism. A basis for the space of symmetric \\(2\\times 2\\) matrices is \\[\n\\left(\n\\begin{bmatrix}  1 & 0 \\\\ 0 & 0 \\end{bmatrix},\n\\begin{bmatrix}  0 & 1 \\\\ 1 & 0 \\end{bmatrix},\n\\begin{bmatrix}  0 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\right).\n\\] The matrix of \\(T\\) with respect to this basis is \\[\nA=\\begin{bmatrix} 2 & 4 & 0 \\\\ 2 & 4 & 2 \\\\ 0 & 4 & 6 \\end{bmatrix}\n\\] so that \\(\\det(T)=\\det(A)=-16\\). Therefore, the given transformation is an isomorphism.\n\n::: {#thm- } Cramer\nCramer’s rule can be used to prove the Cayley Hamilton theorem of linear algebra, as well as Nakayama’s lemma, which is fundamental in commutative ring theory."
  },
  {
    "objectID": "determinants.html#cramer",
    "href": "determinants.html#cramer",
    "title": "5  Determinants",
    "section": "5.7 Cramer",
    "text": "5.7 Cramer\nConsider the system of equations \\(A \\vec x=\\vec b\\) where \\(A\\) is an \\(n\\times n\\) invertible matrix. Then the unique solution to the system is given by \\[\nx_1=\\frac{\\det(A_1)}{\\det(A)},\\qquad x_2=\\frac{\\det(A_2)}{\\det(A)},\\cdots, \\qquad x_n=\\frac{\\det(A_n)}{\\det(A)}\n\\] where the matrix \\(A_i\\) is the matrix \\(A\\) with the \\(i\\)-th column replaced by \\(\\vec b\\). :::\n\nProof. The proof is left for the reader.\n\n\nExample 5.18 Solve the system using Cramer’s Rule. \\[\n\\left\\{\n\\begin{matrix}\n2x_1 +3x_2-x_3 & =1 \\\\\n4x_1+x_2+2x_3 & = 5 \\\\\nx_1-x_2+x_3 & =2\n\\end{matrix}\n\\right.\n\\] First we compute the determinant of the coefficient matrix \\(A\\), and find \\(\\det (A)=5\\) Since \\(\\det(A)\\neq 0\\), we can apply Cramer’s rule, so we compute the required determinants and obtain, according to Cramer’s rule, the solution to the system is \\[\nx_1=\\frac{\\det(A_1)}{\\det(A)}=\\frac{7}{5},\\quad x_2=\\frac{\\det(A_2)}{\\det(A)}=-\\frac{3}{5}, \\quad x_3=\\frac{\\det(A_n)}{\\det(A)}=0.\n\\]\n\n\nExample 5.19 Use paper, pencil, and Cramer’s rule to solve the system \\(A\\vec x=\\vec b\\) where \\(A\\) and \\(\\vec b\\) are the following matrices \\[\nA=\n\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n-5 & -6 & -7 & -8 \\\\\n9 & -10 & 11 & -12 \\\\\n-13 & 14 & -15 & 16\n\\end{bmatrix}\n\\qquad \\text{and} \\qquad\n\\vec b=\n\\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{bmatrix} .\n\\] First we find \\(\\det(A)=-256\\) and then we find the solution vector \\[\n\\vec x= \\vectorfour{-51/8}{5}{41/8}{-9/2}.\n\\]"
  },
  {
    "objectID": "eigenvalues-and-eigenvectors.html#diagonalization",
    "href": "eigenvalues-and-eigenvectors.html#diagonalization",
    "title": "6  Eigenvalues and Eigenvectors",
    "section": "6.1 Diagonalization",
    "text": "6.1 Diagonalization\nAn \\(n\\times n\\) matrix \\(A\\) is called diagonalizable if \\(A\\) is similar to some diagonal matrix \\(D\\). If the matrix of a linear transformation \\(T\\) with respect to some basis is diagonal then we call \\(T\\) diagonalizable .\n\nTheorem 6.5  An \\(n\\times n\\) matrix \\(A\\) is diagonalizable if and only if it has \\(n\\) linearly independent eigenvectors. In that case, the diagonal matrix \\(D\\) is similar to \\(A\\) and is given by \\[\\begin{equation}\n\\label{diagmat}\nD=\n\\begin{bmatrix}\n\\lambda_1 & 0 & \\cdots & 0 \\\\\n0 & \\lambda_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots& \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_n\n\\end{bmatrix}\n\\end{equation}\\] where \\(\\lambda_1, ..., \\lambda_n\\) are the eigenvalues of \\(A\\). If \\(C\\) is a matrix whose columns are linearly independent eigenvectors of \\(A\\), then \\(D=C^{-1}A C\\).\n\n\nProof. The proof is left for the reader.\n\n\nCorollary 6.1  Let \\(T\\) be a linear transformation given by \\(T(\\vec x)=A\\vec x\\) where \\(A\\) is a square matrix. If \\(\\mathcal{D}=(\\vec v_1, ....,\\vec v_n)\\) is an eigenbasis for \\(T\\), with \\(A\\vec v_i=\\lambda_i \\vec v_i\\), then the \\(\\mathcal{D}\\)-matrix \\(D\\) of \\(T\\) given in \\(\\ref{diagmat}\\) is \\(D=[\\vec v_1, ...., \\vec v_n]^{-1}A [\\vec v_1, ...., \\vec v_n]\\).\n\n\nProof. The proof follows from \\(\\ref{dialineigvec}\\) and \\(\\ref{eigenveceigenvallemma}\\).\n\n\nCorollary 6.2  A matrix \\(A\\) is diagonalizable if andy only if there exists an eigenbasis for \\(A\\). In particular, if an \\(n\\times n\\) matrix \\(A\\) has \\(n\\) distinct eigenvalues, then \\(A\\) is diagonalizable.\n\n\nProof. The proof follows from \\(\\ref{dialineigvec}\\) and \\(\\ref{eigenveceigenvallemma}\\).\n\n\nExample 6.22 Let \\(T: \\mathcal{P}_2\\to \\mathcal{P}_2\\) be the linear transformation defined by\n\\[\nT(a_0+a_1 x+a_2x^2)=(a_0+a_1+a_2)+(a_1+a_2)x+a_2x^2.\n\\] Show that \\(T\\) is not diagonalizable. The matrix of \\(T\\) with respect to the usual basis \\((, x, x^2)\\) for \\(\\mathcal{P}_2\\) is easily seen to be \\[\nA=\n\\begin{bmatrix}\n1 & 1 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\] The characteristic polynomial is \\(f_A(x)=-(x-1)^3\\) since \\(A\\) is upper triangular. So \\(T\\) has only one (repeated) eigenvalue \\(\\lambda=1\\). A nonzero polynomial \\(g\\) with \\(g(x)=a_0+a_1 x+a_2 x^2\\) is an eigenvector if and only if \\[\n\\label{notdiageq}\n\\begin{bmatrix}\n0 & 1 & 1 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\vectorthree{a_0}{a_1}{a_2}=\\vectorthree{0}{0}{0}.\n\\] Thus \\(a_1=0\\) and \\(a_2=0\\), so there is only one linearly independent eigenvector for \\(\\lambda=1\\). Thus \\(T\\) is not diagonalizable by \\(\\ref{diagonalizablechar}\\).\n\n\nExample 6.23 Let \\(T:\\mathcal{P}_2\\to \\mathcal{P}_2\\) be the linear transformation defined by\n\\[\\begin{equation}\nT(f(x))=x^2f''(x)+(3x-2)f'(x)+5 f(x).\n\\end{equation}\\] Find a basis for \\(\\mathcal{P}_2\\) such that the matrix representation of \\(T\\) with respect to \\(\\mathcal{B}\\) is diagonal. Since \\(T(x^2)=13x^2-4x\\), \\(T(x)=8x-2\\), and \\(T(1)=5\\) the matrix representation of \\(T\\) with respect to the basis \\(\\mathcal{B}=(x^2,x,1)\\) is \\[\nA=\\begin{bmatrix}13 & 0 & 0 \\\\ -4 & 8 & 0 \\\\ 0 & -2 & 5 \\end{bmatrix}.\n\\] Hence \\[\\begin{equation}\nf_T(x)=f_A(x)=\\begin{vmatrix} x-13  & 0 & 0 \\\\ 4 & x-8 & 0 \\\\ 0 & 2 & x-5 \\end{vmatrix}=(x-13)(x-8)(x-5).\n\\end{equation}\\] The eigenvalues of \\(T\\) are \\(\\lambda_1=13\\), \\(\\lambda_2=8\\) and \\(\\lambda_3=5\\). Solving each of the homogenous systems \\((A-13I_3)\\vec x=\\vec 0\\), \\((A-8I_3)\\vec x=\\vec 0\\), and \\((A-5I_3)\\vec x=\\vec 0\\) yields the eigenvectors \\(\\vec v_1=5x^2-4x+1\\), \\(\\vec v_2=3x-2\\), and \\(\\vec v_3=1\\), respectively. Notice \\(\\vec v_1, \\vec v_2, \\vec v_3\\) are 3 linearly independent vectors, so by \\(\\ref{dialineigvec}\\), \\(T\\) is diagonalizable. We let \\(\\mathcal{D}=(\\vec v_1, \\vec v_2, \\vec v_3)\\) and since \\(T(\\vec v_1)=13\\vec v_1\\),\\(T(\\vec v_2)=8\\vec v_2\\), and \\(T(\\vec v_3)=5\\vec v_3\\), the matrix representation of \\(T\\) with respect to \\(\\mathcal{D}\\) is the diagonal matrix \\[\n\\begin{bmatrix}13 & 0 & 0 \\\\ 0 & 8 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}\n\\] according to \\(\\ref{dialineigvec}\\).\n\n\nExample 6.24 Let \\(T:\\mathcal{P}_3\\to \\mathcal{P}_3\\) be the linear transformation defined by\n\\[\\begin{equation}\nT(f(x))=xf'(x)+f(x+1).\n\\end{equation}\\] Find a basis for \\(\\mathcal{P}_3\\) such that the matrix representation of \\(T\\) with respect to \\(\\mathcal{B}\\) is diagonal. Since \\(T(x^3)=4x^3+3x^2+3x+1\\), \\(T(x^2)=3x^2+2x+1\\), \\(T(x)=2x+1\\), and \\(T(1)=1\\) the matrix representation of \\(T\\) with respect to the basis \\(\\mathcal{B}=(x^3,x^2,x,1)\\) is \\[\nA=\\begin{bmatrix} 4 & 0 & 0 & 0 \\\\ 3 & 3 & 0 & 0 \\\\ 3 & 2 & 2 & 0 \\\\ 1 & 1 & 1 & 1  \\end{bmatrix}.\n\\] Since \\(A\\) is lower triangular, \\(f_T(x)=f_A(x)=(x-4)(x-3)(x-2)(x-1)\\); and so the eigenvalues are \\(\\lambda_1=4\\), \\(\\lambda_2=3\\), \\(\\lambda_3=2\\), and \\(\\lambda_4=1\\). Solving for a basis for each eigenspace of \\(A\\) yields \\[\nE_{\\lambda_1}=\\left(\\vectorfour{6}{18}{27}{17}\\right), \\quad\nE_{\\lambda_2}=\\left(\\vectorfour{0}{2}{4}{3}\\right), \\quad\nE_{\\lambda_3}=\\left(\\vectorfour{0}{0}{1}{1}\\right),\\quad\nE_{\\lambda_4}=\\left(\\vectorfour{0}{0}{0}{1}\\right).\n\\] By taking the polynomial corresponding to the basis vectors, we let \\(\\mathcal{D}=(\\vec v_1, \\vec v_2, \\vec v_3, \\vec v_4)\\) where \\(\\vec v_1=6x^3+18x^2+27x+17\\), \\(\\vec v_2=2x^2+4x+3\\), \\(\\vec v_3=x+1\\), and \\(\\vec v_4=1\\). The diagonal matrix \\[\n\\begin{bmatrix}\n4 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 2 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\] is the matrix representation of \\(T\\) in \\(\\mathcal{D}\\)-coordinates and has the eigenvalues of \\(T\\) on its main diagonal. The transition matrix \\(P\\) from \\(\\mathcal{B}\\)-coordinates to \\(\\mathcal{D}\\)-coordinates is \\[\nP=\\begin{bmatrix}\n6 & 0 & 0 & 0 \\\\\n18 & 2 & 0 & 0 \\\\\n27 & 4 & 1 & 0 \\\\\n17 & 3 & 1 & 1\n\\end{bmatrix}\n\\]\nand satisfies the required relation \\(D=P^{-1}AP\\) as can be verified.\n\n\nExample 6.25 If \\(A\\) is similar to \\(B\\), show that \\(A^n\\) is similar to \\(B^n\\), for any positive integer \\(n\\).\n\n\nExample 6.26 Suppose that \\(C^{-1}AC=D\\). Show that for any integer \\(n\\), \\(A^n=CD^nC^{-1}\\).\n\n\nExample 6.27 Let \\(a\\) and \\(b\\) be real numbers. By diagonalizing \\[\nM=\n\\begin{bmatrix}\na & b-a \\\\\n0 & b\n\\end{bmatrix},\n\\] prove that \\[\nM^n=\n\\begin{bmatrix}\na^n & b^n-a^n \\\\\n0 & b^n\n\\end{bmatrix}\n\\] for all positive integers \\(n\\). We need a basis of \\(\\mathbb{R}^2\\) consisting of eigenvectors of \\(M\\). One such basis is \\(\\vec v_1=\\vec e_1\\) and \\(\\vec v_2=\\vec e_1+\\vec e_2\\) where \\(a\\) and \\(b\\) are eigenvalues for corresponding to these eigenvectors, respectively. Let \\(P=\\begin{bmatrix}\\vec v_1 & \\vec v_2\\end{bmatrix},\\) then by \\(\\ref{eigendmatrix}\\), the diagonalization is \\[\\begin{equation*}\nD=\\begin{bmatrix}\\vec v_1 & \\vec v_2\\end{bmatrix}^{-1}M\\begin{bmatrix}\\vec v_1 & \\vec v_2\\end{bmatrix}=\\begin{bmatrix}a & 0 \\\\ 0 & b \\end{bmatrix}.\n\\end{equation*}\\] Therefore \\[\\begin{equation*}\nM^n=(PDP^{-1})^n=\\underbrace{(PDP^{-1})\\cdots (PDP^{-1})}_{n\\text{-times}}=PD^n P^{-1}\n=\\begin{bmatrix}\na^n & b^n-a^n \\\\\n0 & b^n\n\\end{bmatrix}.\n\\end{equation*}\\]"
  },
  {
    "objectID": "canonical-forms.html#invariant-subspaces",
    "href": "canonical-forms.html#invariant-subspaces",
    "title": "7  Canonical Forms",
    "section": "7.1 Invariant Subspaces",
    "text": "7.1 Invariant Subspaces\nIn this section we let \\(V\\) and \\(W\\) denote real or complex vector spaces.\nSuppose \\(T \\in \\mathcal{L}(V)\\) and \\(U\\) a subspace of \\(V\\), then we say \\(U\\) is an invariant subspace of \\(T\\) if \\(u\\in U\\) implies \\(Tu\\in U\\).\n\nLemma 7.1 Suppose \\(T \\in \\mathcal{L}(V)\\) and \\(U\\) a subspace of \\(V\\). Then all of the following hold\n\n\\(U\\) is invariant under \\(T\\) if and only if \\(T|_U\\) is an operator on \\(U\\),\n\\(\\text{ker} T\\) is invariant under \\(T\\), and\n\\(\\text{im} T\\) is invariant under \\(T\\).\n\n\n\nProof. The proof of each part follows.\n\nBy definition, invariant subspace \\(U\\) is invariant under \\(T\\) if and only if \\(u\\in U \\implies Tu\\in U\\), which, by definition of operator, is the same as \\(T|_U\\) being an operator.\nIf $u T $ then \\(Tu=0\\), and hence \\(Tu\\in \\text{ker} T\\).\nIf $u T $, then by the definition of range \\(Tu\\in\\text{im} T\\).\n\n\n\nTheorem 7.1 Suppose \\(T\\in \\mathcal{L}(V)\\). Let \\(\\lambda_1,\\ldots,\\lambda_m\\) denote distinct eigenvalues of \\(T\\). The the following are equivalent:\n\n\\(T\\) has a diagonal matrix with respect to some basis of \\(V\\);\n\\(V\\) has a basis consisting of eigenvectors of \\(T\\);\nthere exist one-dimensional \\(T\\)-invariant subspaces \\(U_1, \\ldots, U_n\\) of \\(V\\) such that \\(V=U_1 \\oplus \\cdots \\oplus U_n\\);\n\\(V=\\null (T-\\lambda_1 I) \\oplus \\cdots \\oplus \\null (T-\\lambda_m I)\\);\n\\(\\text{dim} V = \\text{dim} \\null (T-\\lambda_1 I) + \\cdots + \\text{dim} \\null (T-\\lambda_m I)\\).\n\n\n\n\nProof. \n\\(\\Longleftrightarrow\\) (ii): Exercise.\n\\(\\Longleftrightarrow\\) (iii): Suppose (ii) holds; thus suppose \\(V\\) has a basis consisting of eigenvectors of \\(T\\). For each \\(j\\), let \\(U_j=\\text{span}(v_j)\\). Obviously each \\(U_j\\) is a one-dimensional subspace of \\(V\\) that is invariant under \\(T\\) (because each \\(v_j\\) is an eigenvector of \\(T\\)). Because \\((v_1,\\ldots,v_n)\\) is a basis of \\(V\\), each vector in \\(V\\) can be written uniquely as a linear combination of \\((v_1,\\ldots,v_n)\\). In other words, each vector in \\(V\\) can be written uniquely as a sum \\(u_1+\\cdots +u_n\\), where each \\(u_j\\in U_j\\). Thus \\(V=U_1 \\oplus \\cdots \\oplus U_n\\). Hence (ii) implies (iii). Conversely, suppose now that (iii) holds; thus there are one-dimensional subspaces \\(U_1,\\ldots,U_n\\) of \\(V\\), each invariant under \\(T\\), such that \\(V=U_1 \\oplus \\cdots \\oplus U_n\\). For each \\(j\\), let \\(v_j\\) be a nonzero vector in \\(U_j\\). Then each \\(v_j\\) is an eigenvector of \\(T\\). Because each vector in \\(V\\) can be written uniquely as a sum \\(u_1+\\cdots + u_n\\), where each \\(u_j\\in U_j\\) ( so each \\(u_j\\) is a scalar multiple of \\(v_j\\)), we see that \\((v_1,\\ldots,v_n)\\) is a basis of \\(V\\). Thus (iii) implies (ii).\n\\(\\Longrightarrow\\) (iv): Suppose (ii) holds; thus thus suppose \\(V\\) has a basis consisting of eigenvectors of \\(T\\). Thus every vector in \\(V\\) is a linear combination of eigenvectors of \\(T\\). Hence \\(V=\\null (T-\\lambda_1)I + \\cdots + \\null (T-\\lambda_m)I.\\) To show that the sum above is direct, suppose that \\(0=u_1+\\cdots + u_m\\), where each \\(u_j\\in \\null (T-\\lambda_j I)\\). Because nonzero eigenvectors correspond to distinct eigenvalues are linearly independent, this implies that each \\(u_j\\) equals 0. This implies that the sum is a direct sum, completing the proof that\nimplies (iii).\n\\(\\Longrightarrow\\) (v): Exercise.\n\\(\\Longrightarrow\\) (ii): Suppose (v) holds; thus \\(\\text{dim} V=\\text{dim} \\null (T-\\lambda_1I)+\\cdots + \\text{dim} \\null (T-\\lambda_m I)\\). Choose a basis of each \\(\\null(T-\\lambda_j I)\\); put all these bases together to form a list \\((v_1,\\ldots,v_n)\\) of eigenvectors of \\(T\\), where \\(n=\\text{dim} V\\). To show that this list is linearly independent, suppose \\(a_1 v_1+\\cdots + a_n v_n=0\\), where \\(a_1,\\ldots,a_n\\) are scalars. For each \\(j=1, \\ldots., m\\), let \\(u_j\\) denote the sum of all the terms \\(a_k v_k\\) such that \\(v_k\\in \\null(T-\\lambda_j I)\\). Thus each \\(u_j\\) is an eigenvector of \\(T\\) with eigenvalue \\(\\lambda_j\\), and \\(u_1+\\cdots + u_m=0\\). Because nonzero eigenvectors corresponding to distinct eigenvalues are linearly independent, this implies that each \\(u_j\\) equals to 0. Because each \\(u_j\\) is a sum of terms \\(a_k v_k\\) where the \\(v_k\\)’s where chosen to be a basis of \\(\\null (T-\\lambda_j I)\\), this implies that all the \\(a_k\\)’s equal to 0. Thus \\((v_1,\\ldots,v_n)\\) is linearly independent and hence a basis of \\(V\\). Thus (v) implies (ii).\n\n\nA vector \\(v\\) is called a generalized eigenvector of \\(T\\) corresponding to \\(\\lambda\\), where \\(\\lambda\\) is an eigenvalue of \\(T\\), if \\((T-\\lambda I)^j v=0\\) for some positive integer \\(j\\).\n\nLemma 7.2 If \\(T\\in \\mathcal{L}(V)\\). Then, if \\(m\\) is a nonnegative integer such that \\(\\text{ker} T^m=\\text{ker} T^{m+1}\\), then\n\n\\(\\{0\\}=\\text{ker} T^0 \\subseteq \\text{ker} T^1 \\subseteq \\cdots \\subseteq \\text{ker} T^m = \\text{ker} T^{m+1} = \\text{ker} T^{m+2} = \\cdots\\)\n\\(\\text{ker} T^{\\text{dim} V}=\\text{ker} T^{\\text{dim} V+1} =\\text{ker} T^{\\text{dim} V+2}\\cdots\\)\n\\(V=\\text{im} T^0 \\supseteq \\text{im} T^1 \\supseteq \\cdots \\supseteq \\text{im} T^k \\supseteq \\text{im} T^{k+1} \\supseteq \\cdots\\)\n\\(\\text{im} T^{\\text{dim} V}= \\text{im} T^{\\text{dim} V+1} = \\text{im} T^{\\text{dim} V+2}\\cdots\\)\n\n\n\nLemma 7.3 Suppose \\(T\\in \\mathcal{L}(V)\\) and \\(\\lambda\\) is an eigenvalue of \\(T\\). Then the set of generalized eigenvalues of \\(T\\) corresponding to \\(\\lambda\\) equals \\(\\text{ker}(T-\\lambda I)^{\\text{dim} V}\\).\n\n\nProof. The proof is left for the reader.\n\nAn operator is called nilpotent if some power of it equal 0.\n\nLemma 7.4 Suppose \\(N\\in \\mathcal{L}(V)\\) is nilpotent, then \\(N^{\\text{dim} V}=0\\).\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 7.2 Let \\(T\\in \\mathcal{L}(V)\\) and \\(\\lambda\\in\\mathbb{F}\\). Then for every basis of \\(V\\) with respect to which \\(T\\) has an upper-triangular matrix, \\(\\lambda\\) appears on the diagonal of the matrix of \\(T\\) precisely \\(\\text{dim} (T-\\lambda I)^{\\text{dim} V}\\) times.\n\n\nProof. The proof is left for the reader.\n\nThe multiplicity of an eigenvalue \\(\\lambda\\) of \\(T\\) is defined to be the dimension of the subspace of generalized eigenvectors corresponding to \\(\\lambda\\), that is the multiplicity of \\(\\lambda\\) is equal to \\(\\text{dim} \\text{ker}(T-\\lambda I)^{\\text{dim} V}\\).\n\nTheorem 7.3 If \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\), then the sum of the multiplicities of all the eigenvalues of \\(T\\) equals \\(\\text{dim} V\\).\n\n\nProof. The proof is left for the reader.\n\nLet \\(d_j\\) denote the multiplicity of \\(\\lambda_j\\) as an eigenvalue of \\(T\\), the polynomial \\[\n(z-\\lambda_1)^{d_1} \\cdots (z-\\lambda_m)^{d_m}\n\\] is called the characteristic polynomial of \\(T\\).\n\nTheorem 7.4 Suppose that \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Let \\(q\\) denote the characteristic polynomial of \\(T\\). Then \\(q(T)=0\\).\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 7.5 If \\(T\\in \\mathcal{L}(V)\\) and \\(p\\in \\mathcal{P}(\\mathbb{F})\\), then \\(\\text{ker} p(T)\\) is invariant under \\(T\\).\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 7.6 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Let \\(\\lambda_1,\\ldots,\\lambda_m\\) be the distinct eigenvalues of \\(T\\), and let \\(U_1,\\ldots,U_m\\) be the corresponding subspaces of generalized eigenvectors. Then\n\n\\(V=U_1\\oplus \\cdots \\oplus U_m\\);\neach \\(U_J\\) is invariant under \\(T\\);\neach \\(\\left.(T-\\lambda_j I) \\right|_{U_j}\\) is nilpotent.\n\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 7.7 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Then there is a basis of \\(V\\) consisting of generalized eigenvectors of \\(T\\).\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 7.8 Suppose \\(N\\) is a nilpotent operator on \\(V\\). Then there is a basis of \\(V\\) with respect to which the matrix of \\(N\\) has the form \\[\n\\begin{bmatrix} 0 & & * \\\\ & \\ddots & \\\\ 0 & & 0\\end{bmatrix};\n\\] here all entries on and below the diagonal are 0’s.\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 7.9 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Let \\(\\lambda_1,\\ldots,\\lambda_m\\) be the distinct eigenvalues of \\(T\\). Then there is a basis of \\(V\\) with respect to which \\(T\\) has block diagonal matrix of the form \\[\n\\begin{bmatrix} A_1 & & 0\\\\ & \\ddots & \\\\ 0 & & A_m\\end{bmatrix},\n\\] where each \\(A_j\\) is an upper-triangular matrix of the form \\[\n\\begin{bmatrix} \\lambda_1 & & * \\\\ & \\ddots & \\\\ 0 & & \\lambda_j\\end{bmatrix}.\n\\]\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 7.10 Suppose \\(N\\in\\mathcal{L}(V)\\) is nilpotent. Then \\(I+N\\) has a square root.\n\n\nProof. The proof is left for the reader.\n\nOn real vector spaces there exist invertible operators that have no square roots. For example, the operator of multplication by \\(-1\\) on \\(\\mathbb{R}\\) has no square root because no real number has its square equal to \\(-1\\).\n\nTheorem 7.11 Suppose \\(V\\) is a complex vector space. If \\(T\\in \\mathcal{L}(V)\\) is invertible, then \\(T\\) has a square root.\n\n\nProof. The proof is left for the reader.\n\nThe minimal polynomial of \\(T\\) is the monic polynomial \\(p\\in \\mathcal{P}(\\mathbb{F}\\) of smallest degree such that \\(p(T)=0\\).\n\nTheorem 7.12 Let \\(T\\in \\mathcal{L}(V)\\) and let \\(q\\in \\mathcal{P}(\\mathbb{F})\\). Then \\(q(T)=0\\) if and only if the minimal polynomial of \\(T\\) divided \\(q\\).\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 7.13 Let \\(T\\in \\mathcal{L}(V)\\). Then the roots of the minimal polynomial of \\(T\\) are precisely the eigenvalues of \\(T\\).\n\n\nProof. The proof is left for the reader.\n\nEvery \\(T\\in \\mathcal{L}(V)\\) where \\(V\\) is a complex vector space, there is a basis of \\(V\\) with respect to which \\(T\\) has a nice upper-triangular matrix. We can do even better. There is a basis of \\(V\\) with respect to which the matrix of \\(T\\) contains zeros everywhere except possibly on the diagonal and the line directly above the diagonal.\nSuppose \\(N\\in \\mathcal{L}(V)\\) is nilpotent. For each nonzero vector \\(v\\in V\\), let \\(m(v)\\) denote the largest nonnegative integer such that \\(N^{m(v)}\\neq 0\\).\n\nTheorem 7.14 If \\(N\\in \\mathcal{L}(V)\\) is nilpotent, then there exist vectors \\(v_1,\\ldots,v_k \\in V\\) such that\n\n\\(\\left(v_1,N v_1,\\ldots,N^{m(v_1)},\\ldots,v_k, N v_k, \\ldots, N^{m(v_k)}v_k\\right)\\) is a basis of \\(V\\);\n\\(\\left(N^{m(v_1)}v_1,\\ldots,N^{m(v_k)}v_k\\right)\\) is a basis of \\(\\text{ker} N\\).\n\n\n\nProof. The proof is left for the reader.\n\n\nExample 7.1 Suppose \\(T\\in \\mathcal{L}(V)\\). Prove that if \\(U_1,\\ldots,U_m\\) are subspaces of \\(V\\) invariant under \\(T\\), then \\(U_1 + \\cdots +U_m\\) is invariant under \\(T\\). Suppose \\(v\\in U_1+\\cdots + U_m\\). Then there exists \\(u_1,\\ldots,u_m\\) such that \\(v=u_1+\\cdots +u_m\\) with \\(u_j\\in U_j\\). Then \\(Tv=T u_1+\\cdots + T u_m\\). Since each \\(U_j\\) is invariant under \\(T\\), \\(T u_j \\in U_j\\), so \\(T v \\in U_1+ \\cdots + U_m\\).\n\n\nExample 7.2 Suppose \\(T\\in \\mathcal{L}(V)\\). Prove that the intersection of any collection of subspaces of \\(V\\) invariant under \\(T\\) is invariant under \\(T\\). Suppose we have subspaces \\(\\{U_j\\}\\) with each \\(U_j\\) invariant under \\(T\\). Let \\(v\\in \\cap_j U_j\\). Then \\(Tv\\in U_j\\) for each \\(j\\), and so \\(\\cap_j U_j\\) is invariant under \\(T\\).\n\n\nExample 7.3 Prove or give a counterexample: if \\(U\\) is a subspace of \\(V\\) that is invariant under every operator on \\(V\\), then \\(U=\\{0\\}\\) or \\(U=V\\). We will prove the contrapositive: if \\(U\\) is a subspace of \\(V\\) and \\(U\\neq \\{0\\}\\) and \\(U\\neq V\\), then there exists an operator \\(T\\) on \\(V\\) such that \\(U\\) is not invariant under \\(T\\). Let \\((u_1,\\ldots,u_m)\\) be a basis for \\(U\\), which we extend to a basis \\((u_1,\\ldots,u_m, v_1,\\ldots,v_n)\\) of \\(V\\). The assumption \\(U\\neq \\{0\\}\\) and \\(U\\neq V\\) means that \\(m\\geq 1\\) and \\(n\\geq 1\\). Define a linear map \\(T\\) by \\(Tu_1=v_1\\) and for \\(j>1\\), \\(T u_j=0\\). Since \\(v_1\\not \\in U\\), the subspace \\(U\\) is not invariant under the operator \\(T\\).\n\n\nExample 7.4 Suppose that \\(S,T\\in \\mathcal{L}(V)\\) are such that \\(S T= T S\\). Prove that \\(\\text{null }(T-\\lambda I)\\) is invariant under \\(S\\) for every \\(\\lambda \\in F\\). Suppose \\(v\\in \\text{ker}(T-\\lambda I)\\). Then \\(Tv = \\lambda v\\) and using \\(TS=ST\\), \\(Sv\\) satisfies $ T(S v)=S(T v)=S v)=(S v). $ Thus \\(S v\\in \\text{ker}(T-\\lambda I)\\) and so \\(\\text{ker} (T-\\lambda I)\\) is invariant under \\(S\\).\n\n\nExample 7.5 Define \\(T\\in \\mathcal{L}(F^2)\\) by \\(T(w,z)=(z,w)\\). Find all eigenvalues and eigenvectors of \\(T\\). Suppose \\((w,z)\\neq (0,0)\\) and \\(T(w,z)=(z,w)=\\lambda(w,z)\\). Then \\(z=\\lambda w\\) and \\(w=\\lambda z\\). Of course this leads to \\(w=\\lambda z=\\lambda^2w\\), \\(z=\\lambda w=\\lambda^2 z\\). Since \\(w\\neq 0\\) or \\(z\\neq 0\\), we see that \\(\\lambda^2=1\\) so that \\(\\lambda =\\pm 1\\). A basis of eigenvectors is \\((w_1,z_1)=(1,1)\\), \\((w_2,z_2)=(-1,1)\\) and they have eigenvalues 1 and \\(-1\\) respectively.\n\n\nExample 7.6 Define \\(T\\in \\mathcal{L}(F^3)\\) by \\(T(z_1,z_2,z_3)=(2z_2,0,5z_3)\\). Find all eigenvalues and eigenvectors of \\(T\\). Suppose \\((z_1,z_2,z_3)\\neq (0,0,0)\\) and \\[\nT(z_1,z_2,z_3)=(2z_2,0,5z_3)=\\lambda (z_1,z_2,z_3).\n\\] If \\(\\lambda=0\\) then \\(z_2=z_3=0\\), and one checks that \\(v_1=(0,0,0)\\) is an eigenvector with eigenvalue 0. If \\(\\lambda\\neq 0\\) then \\(z_2=0\\), \\(2z_2=\\lambda z_1=0\\), \\(5z_3=\\lambda z_3\\), so \\(z_1=0\\) and \\(\\lambda =5\\). The eigenvector for \\(\\lambda=5\\) is \\(v_2=(0,0,1)\\). These are the only eigenvalues and each eigenspace is one dimensional.\n\n\nExample 7.7 Suppose \\(n\\) is a positive integer and \\(T\\in \\mathcal{L}(\\mathbb{F}^n)\\) is defined by \\[\nT(x_1,\\ldots,x_n)=(x_1+ \\cdots + x_n,\\ldots,x_1+\\cdots +x_n).\n\\] Find all eigenvalues and eigenvectors of \\(T\\). First, any vector of the form \\(v_1=(\\alpha,\\ldots,\\alpha)\\), for \\(\\alpha\\in \\mathbb{F}\\), is an eigenvector with eigenvalue \\(n\\). If \\(v_2\\) is any vector $v_2=(x_1,,x_n), $ such that \\(x_1+\\cdots + x_n=0\\) then \\(v_2\\) is an eigenvector with eigenvalue 0. Here are the independent eigenvectors: \\(v_1=(1,1,\\ldots,1)\\), and \\(v_n=(1,0,\\ldots,0)-E_n\\), for \\(n\\geq 2\\) where \\(E_n\\) denoted the \\(n\\)-th standard basis vector.\n\n\nExample 7.8 Suppose \\(T\\in \\mathcal{L}(V)\\) is invertible and \\(0\\neq \\lambda \\in F\\). Prove that \\(\\lambda\\) is an eigenvalue of \\(T\\) if and only if \\(\\frac{1}{\\lambda}\\) is an eigenvalue of \\(T^{-1}\\). Suppose \\(v\\neq 0\\) and \\(T v =\\lambda v\\). Then \\(v=T^{-1}T v=\\lambda T^{-1}v\\), or \\(T^{-1}v=\\frac{1}{\\lambda}v\\), and the other direction is similar.\n\n\nExample 7.9 Suppose \\(S,T\\in \\mathcal{L}(V)\\). Prove that \\(S T\\) and \\(T S\\) have the same eigenvalues. Suppose \\(v\\neq 0\\) and \\(STv=\\lambda v\\). Multiply by \\(T\\) to get \\(T S(Tv)=\\lambda T v\\). Thus if \\(t\\neq 0\\) then \\(\\lambda\\) is also an eigenvalue of \\(TS\\), with nonzero=o eigenvector \\(Tv\\). On the other hand, if \\(T v=0\\), then \\(\\lambda =0\\) is an eigenvalue of \\(ST\\). But if \\(T\\) is not invertible, then \\(\\text{im} TS \\subset\\text{im} T\\)is not equal to \\(V\\), so \\(TS\\) has a nontrivial null space, hence 0 is an eigenvalue of \\(TS\\).\n\n\nExample 7.10 Suppose \\(T\\in \\mathcal{L}(V)\\) is such that every vector in \\(V\\) is an eigenvector of \\(T\\). Prove that \\(T\\) is a scalar multiple of the identity operator. Pick a basis \\((v_1,\\ldots,v_N)\\) for \\(V\\). By assumption, \\(T v_n=\\lambda_n v_n\\). Pick any two distance indices, \\(m,n\\). We also have \\(T(v_m+v_n)=\\lambda(v_m+v_n)=\\lambda(v_m+v_n)=\\lambda_m v_m + \\lambda_n v_n\\). Write this as \\(0=(\\lambda-\\lambda_m)v_m+(\\lambda-\\lambda_n)v_n\\). Since \\(v_m\\) and \\(v_n\\) are independent, \\(\\lambda=\\lambda_m=\\lambda_n\\), and all the \\(\\lambda_n\\) are equal.\n\n\nExample 7.11 Suppose \\(S, T\\in \\mathcal{L}(V)\\) and \\(S\\) is invertible. Prove that if \\(p \\in \\mathcal{P}(\\mathbb{F})\\) is a polynomial, then \\(p(S T S^{-1})=S p(T) S^{-1}\\). First let’s show that for positive integers \\(n\\), \\((STS^{-1})^n=S T^n S^{-1}\\). We may do this by induction, with nothing to show if \\(n=1\\). Assume it’s true for \\(n=k\\), and consider \\[\n(STS^{-1})^{k+1}=(STS^{-1})^k(STS^{-1})=ST^k S^{-1}STS^{-1}=S T^{k+1}S^{-1}.\n\\] Now suppose \\(P(z)=a_n z^N+\\cdots + a_1 z+a_0.\\) Then \\[\\begin{align*}\np(STS^{-1})&=\\sum_{n=0}^N a_n (STS^{-1})^n=\\sum_{n=0}^N a_n ST^n S^{-1}\\\\ & =S\\left( \\sum_{n=0}^N a_n T^n \\right) S^{-1}=Sp(T)S^{-1}.\n\\tag*{}\n\\end{align*}\\]\n\n\nExample 7.12 Suppose \\(F=\\mathbb{C}\\), \\(T\\in \\mathcal{L}(V)\\), \\(p\\in \\mathcal{P}(\\mathbb{C})\\), and \\(a\\in \\mathbb{C}\\). Prove that \\(a\\) is an eigenvalue of \\(p(T)\\) if and only if \\(a=p(\\lambda)\\) for some eigenvalue $$ of \\(T\\). Suppose first that \\(v\\neq 0\\) is an eigenvalue of \\(T\\) with eigenvalue \\(\\lambda\\); that is \\(T v = \\lambda v\\). Then for positive integers \\(n\\), \\(T^n v=T^{n-1} \\lambda v = \\cdots \\lambda^n v\\), and so \\(p(T)v=p(\\lambda) v\\). That is \\(\\alpha=p(\\lambda)\\) is an eigenvalue of \\(p(T)\\) if \\(\\lambda\\) is an eigenvalue of \\(T\\). Conversely, suppose now that \\(\\alpha\\) is a eigenvalue of \\(p(T)\\), so there is a \\(v\\neq 0\\) with \\(p(T)v =\\alpha v\\), or \\((p(T)-\\alpha I)v=0\\). Since \\(\\mathbb{F}=\\mathbb{C}\\), we may factor the polynomial $p(T)-I $ into linear factors \\[\n0=(p(T)-\\alpha I)v=\\prod (T-\\lambda_n I)v.\n\\] At least one of the factors is not invertible, so at least one of the \\(\\lambda_n\\), say \\(\\lambda_1\\), is an eigenvalue of \\(T\\). Let \\(w\\neq 0\\) be an eigenvector for \\(T\\) with eigenvalue \\(\\lambda_1\\). Then \\[\n0=(T-\\lambda_N I)\\cdots (T-\\lambda_1 I)w=(p(T)-\\alpha I) w,\n\\] so \\(w\\) is an eigenvalue for \\(p(T)\\) with eigenvalue \\(\\alpha\\). But by the first part of the argument, \\(p(T)w=p(\\lambda_1)w=\\alpha w\\) and \\(\\alpha=p(\\lambda_1)\\).\n\n\nExample 7.13 Show that the previous exercise does not hold with \\(F=\\mathbb{R}\\). Take \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) given by \\(T(x,y)=(-y,x)\\). We’ve seen perviously that \\(T\\) has no real eigenvalues. On the other hand, \\(T^2(x,y)=(-x,-y)=-1(x,y)\\).\n\n\nExample 7.14 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Prove that \\(T\\) has an invariant subspace of dimension \\(j\\) for each \\(j=1,\\ldots, \\text{dim} V\\). Let \\((v_1,\\ldots,v_N)\\) be a basis with respect to which \\(T\\) has an upper triangular matrix. Then by a previous proposition, \\(T: \\text{span}(v_1,\\ldots,v_j) \\rightarrow \\text{span}(v_1,\\ldots,v_j)\\).\n\n\nExample 7.15 Give an example of an operator whose matrix with respect to some basis contains only 0’s on the diagonal, but the operator is invertible. Consider \\(T=\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\\).\n\n\nExample 7.16 Give an example of an operator whose matrix with respect to some basis contains only nonzero numbers on the diagonal, but the operator is not invertible. Taking \\(T=\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\\). If \\(v=[1,-1]\\), then \\(Tv=0\\).\n\n\nExample 7.17 Give an example of an operator on \\(\\mathbb{C}^4\\) whose characteristic and minimal polynomials both equal \\(z (z-1)^2(z-3)\\).\n\n\nExample 7.18 Give an example of an operator on \\(\\mathbb{C}^4\\) whose characteristic polynomial equals \\(z (z-1)^2(z-3)\\) and whose minimal polynomial equals \\(z (z-1)(z-3)\\).\n\n\nExample 7.19 Suppose \\(a_0, \\ldots, a_{n-1}\\in \\mathbb{C}\\). Find the minimal and characteristic polynomials of the operator on \\(\\mathbb{C}^n\\) whose matrix is \\[\n\\begin{bmatrix}\n0 & & & & & -a_0 \\\\\n1 & 0 & & & & -a_1 \\\\\n& 1 & \\ddots & & & -a_2 \\\\\n&  & \\ddots & & & \\vdots \\\\\n& 1 & & & 0 & -a_{n-2} \\\\\n&  & & & 1 & -a_{n-1}\n\\end{bmatrix}\n\\] with respect to the standard bases."
  },
  {
    "objectID": "canonical-forms.html#jordan-canonical-form",
    "href": "canonical-forms.html#jordan-canonical-form",
    "title": "7  Canonical Forms",
    "section": "7.2 Jordan Canonical Form",
    "text": "7.2 Jordan Canonical Form\nA basis of \\(V\\) is called a Jordan basis for \\(T\\) if with respect to this basis \\(T\\) has block diagonal matrix \\[\n\\begin{bmatrix}\nA_1 &   & 0 \\\\\n& \\ddots &  \\\\\n0 &  & A_m \\\\\n\\end{bmatrix}\n\\] where each \\(A_j\\) is an upper triangular matrix of the form \\[\nA_j =\n\\begin{bmatrix}\n\\lambda_j & 1 &  & 0 \\\\\n& \\ddots & \\ddots & \\\\\n&  &  \\ddots & 1 \\\\\n0 &  &  & \\lambda_j \\\\\n\\end{bmatrix}\n\\] where the diagonal is filled with some eigenvalue \\(\\lambda_j\\) of \\(T\\).\nBecause there exist operators on real vector spaces that have no eigenvalues, there exist operators on real vector spaces for which there is no corresponding Jordan basis.\n\nTheorem 7.15 Suppose \\(V\\) is a complex vector space. If \\(T\\in \\mathcal{L}(V)\\), then there is a basis of \\(V\\) that is a Jordan basis for \\(T\\).\n\n\nProof. The proof is left for the reader.\n\nA basis of \\(V\\) is called a Jordan basis for \\(T\\) if with respect to this basis \\(T\\) has block diagonal matrix \\[\n\\begin{bmatrix}\nA_1 &   & 0 \\\\\n& \\ddots &  \\\\\n0 &  & A_m \\\\\n\\end{bmatrix}\n\\] where each \\(A_j\\) is an upper triangular matrix of the form \\[\nA_j =\n\\begin{bmatrix}\n\\lambda_j & 1 &  & 0 \\\\\n& \\ddots & \\ddots & \\\\\n&  &  \\ddots & 1 \\\\\n0 &  &  & \\lambda_j \\\\\n\\end{bmatrix}\n\\] where the diagonal is filled with some eigenvalue \\(\\lambda_j\\) of \\(T\\).\nAn operator \\(T\\) can be put into Jordan canonical form if its characteristic and minimal polynomials factor into linear polynomials. this is always true if the vector space is complex.\n\nTheorem 7.16 Let \\(T\\in\\mathcal{L}(V)\\) whose characteristic and minimal polynomials are, respectively, \\[\nc(t)=(t-\\lambda_1)^{n_1} \\cdots (t-\\lambda_r)^{n_r})\n\\qquad \\text{and} \\qquad\nm(t)=(t-\\lambda_1)^{m_1} \\cdots (t-\\lambda_r)^{m_r})\n\\] where the \\(\\lambda_i\\) are distinct scalars. Then \\(T\\) has block diagonal matrix representation \\(J\\) whose diagonal entries are of the form \\[\nJ_{ij}=\n\\begin{bmatrix}\n\\lambda_i & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & \\lambda_i & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\lambda_i & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & \\lambda_i\n\\end{bmatrix}\n\\] For each \\(\\lambda_i\\) the corresponding blocks have the following properties:\n\nThere is at least one \\(J_{ij}\\) of order \\(m_i\\); all other \\(J_{ij}\\) are of order \\(\\leq m_i\\)\nThe sum of the orders of the \\(J_{ij}\\) is \\(n_i\\).\nThe number of \\(J_{ij}\\) equals the geometric multiplicity of \\(\\lambda_i\\).\nThe number of \\(J_{ij}\\) of each possible order is uniquely determined by \\(T\\).\n\n\n\nProof. The proof is left for the reader.\n\nThe matrix \\(J\\) in the above proposition is called the Jordan canonical form of the operator \\(T\\). A diagonal block \\(J_{ij}\\) is called a Jordan block belongng to the eigenvalue \\(\\lambda_i\\). Observe that \\[\n\\begin{bmatrix}\n\\lambda_i & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & \\lambda_i & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\lambda_i & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & \\lambda_i\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\lambda_i & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & \\lambda_i & 0 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\lambda_i & 0 \\\\\n0 & 0 & 0 & \\cdots & 0 & \\lambda_i\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n0 & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & 0\n\\end{bmatrix}\n\\] That is, \\(J_{ij}=\\lambda_i I+N\\) where \\(N\\) is the nilpotent block.\n\nExample 7.20 Suppose the characteristic and minimum polynomials of an operator \\(T\\) are, respectively, \\[\nc(t)=(t-2)^4(t-3)^3 \\qquad \\text{and} \\qquad m(t)=(t-2)^2(t-3)^2.\n\\] Then the Jordan canonical form of \\(T\\) is one of the following matrices: \\[\n\\begin{bmatrix}\n2 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 2 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 3 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 3\n\\end{bmatrix}\n\\qquad \\text{or} \\qquad\n\\begin{bmatrix}\n2 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 2 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 3 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 3\n\\end{bmatrix}\n\\] The first matrix occurs if \\(T\\) has two independent eigenvectors belonging to the eigenvalue 2; and the second matrix occurs if \\(T\\) has three independent eigenvectors belonging to 2.\n\n\nExample 7.21 Suppose \\(N\\in \\mathcal{L}(V)\\) is nilpotent. Prove that the minimal polynomial of \\(N\\) is \\(z^{m+1}\\), where \\(m\\) is the length of the longest consecutive string of \\(1'\\text{s}\\) that appears on the line directly above the diagonal in the matrix of \\(N\\) with respect to any Jordan basis for \\(N\\).\n\n\nExample 7.22 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Prove that there does not exist a direct sum decomposition of \\(V\\) into two proper subspaces invariant under \\(T\\) if and only if the minimal polynomial of \\(T\\) is of the form \\((z-\\lambda)^{\\text{dim} V}\\) for some \\(\\lambda \\in \\mathbb{C}\\).\n\n\nExample 7.23 Suppose \\(T\\in \\mathcal{L}(V)\\) and \\((v_1,\\ldots,v_n)\\) is a basis of \\(V\\) that is a Jordan basis for \\(T\\). Describe the matrix of \\(T\\) with respect to the basis \\((v_n,\\ldots,v_1)\\) obtained by reversing the order of the \\(v\\)’s.\n\n\nExample 7.24 Consider a 2-by-2 matrix of real numbers \\[\n\\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}.\n\\]\n\n\nExample 7.25 Suppose \\(A\\) is a block diagonal matrix \\[\nA=\\begin{bmatrix}\nA_1 & & 0 \\\\\n& \\ddots & \\\\\n0 & & A_m\n\\end{bmatrix},\n\\] where each \\(A_j\\) is a square matrix. Prove that the set of eigenvalues of \\(A\\) equals the union of the eigenvalues of \\(A_1,\\ldots,A_m\\).\n\n\nExample 7.26 Suppose \\(A\\) is a block upper-triangular matrix \\[\nA=\n\\begin{bmatrix}\nA_1 & & * \\\\\n& \\ddots & \\\\\n0 & & A_m\n\\end{bmatrix},\n\\] where each \\(A_j\\) is a square matrix. Prove that the set of eigenvalues of \\(A\\) equals the union of the eigenvalues of \\(A_1\\),,\\(A_m\\).\n\n\nExample 7.27 Suppose \\(V\\) is a real vector space and \\(T\\in \\mathcal{L}(V)\\). Suppose \\(\\alpha, \\beta \\in \\mathbb{R}\\) are such that \\(T^2+\\alpha T+\\beta I=0\\). Prove that \\(T\\) has an eigenvalue if and only if \\(\\alpha^2 \\geq 4 \\beta\\).\n\n\nExample 7.28 Suppose \\(V\\) is a real inner-product space and \\(T\\in \\mathcal{L}(V)\\). Prove that there is an orthonormal basis of \\(V\\) with respect to which \\(T\\) has a block upper-triangular matrix \\[\n\\begin{bmatrix}\nA_1 & & * \\\\\n& \\ddots & \\\\\n0 & & A_m\n\\end{bmatrix}.\n\\] where each \\(A_j\\) is a 1-by-1 matrix or a 2-by-2 matrix with no eigenvalues.\n\n\nExample 7.29 Prove that if \\(T\\in \\mathcal{L}(V)\\) and \\(j\\) is a positive integer such that \\(j \\leq \\text{dim} V\\), then \\(T\\) has an invariant subspace whose dimension equals \\(j-1\\) or \\(j\\).\n\n\nExample 7.30 Prove that there does not exist an operator \\(T\\in \\mathcal{L}(\\mathbb{R}^7)\\) such that \\(T^2+T+I\\) is nilpotent.\n\n\nExample 7.31 Give an example of an operator \\(T\\in \\mathcal{L}(\\mathbb{C}^7)\\) such that \\(T^2+T+I\\) is nilpotent.\n\n\nExample 7.32 Suppose \\(V\\) is a real vector space and \\(T\\in \\mathcal{L}(V)\\). Suppose \\(\\alpha, \\beta \\in \\mathbb{R}\\) are such that \\(\\alpha^2< 4\\beta\\). Prove that null \\((T^2+\\alpha T + \\beta I)^k\\) has even dimension for every positive integer \\(k\\).\n\n\nExample 7.33 Suppose \\(V\\) is a real vector space and \\(T\\in \\mathcal{L}(V)\\). Suppose \\(\\alpha, \\beta \\in \\mathbb{R}\\) are such that \\(\\alpha^2< 4\\beta\\) and \\(T^2+\\alpha T+\\beta I\\) is nilpotent. Prove that \\(\\text{dim} V\\) is even and \\((T^2+\\alpha T+\\beta I)^{\\text{dim} V/2}=0.\\)\n\n\nExample 7.34 Prove that if \\(T\\in \\mathcal{L}(\\mathbb{R}^3)\\) and 5, 7 are eigenvalues of \\(T\\), then \\(T\\) has no eigenpairs.\n\n\nExample 7.35 Suppose \\(V\\) is a real vector space with $ V =n $ and \\(T\\in \\mathcal{L}(V)\\) is such that null $T^{n-2}$ null \\(T^{n-1}\\). Prove that if \\(T\\) has at most two distinct eigenvalues and that \\(T\\) has no eigenpairs.\n\n\nExample 7.36 Prove that 1 is an eigenvalue of every square matrix with the property that the sum of the entries in each row equals 1.\n\n\nExample 7.37 Suppose \\(V\\) is a real vector space with $ V =2 $. Prove that if \\[\n\\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\\] is the matrix of \\(T\\) with respect to some basis of \\(V\\), then the characteristic polynomial of \\(T\\) equals \\((z-a)(z-d)-b c\\).\n\n\nExample 7.38 Suppose \\(V\\) is a real inner-product space and \\(S\\in \\mathcal{L}(V)\\) is an isometry. Prove that if \\((\\alpha, \\beta)\\) is an eigenpair of \\(S\\), then \\(\\beta=1\\)."
  }
]