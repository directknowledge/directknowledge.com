[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Learning Linear Algebra",
    "section": "",
    "text": "Preface"
  },
  {
    "objectID": "index.html#about-this-book",
    "href": "index.html#about-this-book",
    "title": "Learning Linear Algebra",
    "section": "About this book",
    "text": "About this book"
  },
  {
    "objectID": "index.html#other-resources",
    "href": "index.html#other-resources",
    "title": "Learning Linear Algebra",
    "section": "Other resources",
    "text": "Other resources"
  },
  {
    "objectID": "index.html#update-history",
    "href": "index.html#update-history",
    "title": "Learning Linear Algebra",
    "section": "Update history",
    "text": "Update history\n\nFirst draft published on 1/18/2023."
  },
  {
    "objectID": "index.html#acknowledgements",
    "href": "index.html#acknowledgements",
    "title": "Learning Linear Algebra",
    "section": "Acknowledgements",
    "text": "Acknowledgements"
  },
  {
    "objectID": "systems-of-linear-equations.html#introducing-systems-of-linear-equations",
    "href": "systems-of-linear-equations.html#introducing-systems-of-linear-equations",
    "title": "1  Systems of Linear Equations",
    "section": "1.1 Introducing Systems of Linear Equations",
    "text": "1.1 Introducing Systems of Linear Equations\nA system of linear equations is a set of two or more linear equations that share the same set of unknowns. In other words, a system of linear equations is a collection of linear equations that you must solve together. There are three main types of systems of linear equations:\n\nConsistent and independent: A consistent and independent system of linear equations has a unique solution. This means that there is only one way to solve the system, and the solution will not change if different values are used for the unknowns.\nConsistent and dependent: A consistent and dependent system of linear equations has infinitely many solutions. This means that there is more than one way to solve the system, and the solution will change if different values are used for the unknowns.\nInconsistent: An inconsistent system of linear equations has no solution. This means that there is no way to solve the system."
  },
  {
    "objectID": "systems-of-linear-equations.html#the-graphical-method",
    "href": "systems-of-linear-equations.html#the-graphical-method",
    "title": "1  Systems of Linear Equations",
    "section": "1.2 The Graphical Method",
    "text": "1.2 The Graphical Method\nThe graphing method is a visual way to find the solution to a system of linear equations. To use the graphing method, graph both equations on the same coordinate plane. The point of intersection is the solution to the system.\nTo graph a linear equation, start by plotting the y-intercept. This is the point where the line crosses the y-axis. To find the y-intercept, set x=0 and solve for y. Then, use the slope to find additional points on the line. The slope is the ratio of the change in y to the change in x. To find the slope, pick two points on the line and calculate the rise (the change in y) divided by the run (the change in x).\nOnce you have the y-intercept and the slope, you can plot additional points on the line and connect them with a straight line."
  },
  {
    "objectID": "systems-of-linear-equations.html#the-substitution-method",
    "href": "systems-of-linear-equations.html#the-substitution-method",
    "title": "1  Systems of Linear Equations",
    "section": "1.3 The Substitution Method",
    "text": "1.3 The Substitution Method\nThe substitution method is an algebraic way to find the solution to a system of linear equations. To use the substitution method, solve one of the equations for one of the unknowns. Then, substitute this value into the other equation. This will give you a new equation with one unknown. Solve this equation for the unknown. Now, substitute the value of the unknown back into one of the original equations. This will give you the solution to the system.\nThe substitution method is usually easiest when one of the equations is already solved for one of the unknowns. However, it is sometimes necessary to solve one of the equations for an unknown before you can substitute."
  },
  {
    "objectID": "systems-of-linear-equations.html#the-elimination-method",
    "href": "systems-of-linear-equations.html#the-elimination-method",
    "title": "1  Systems of Linear Equations",
    "section": "1.4 The Elimination Method",
    "text": "1.4 The Elimination Method\nThe elimination method is an algebraic way to find the solution to a system of linear equations. To use the elimination method, add or subtract the equations so that one of the unknowns cancels out. This will give you a new equation with one fewer unknown. Repeat this process until you are left with an equation that has only one unknown. Solve this equation for the unknown. Now, substitute the value of the unknown back into one of the original equations. This will give you the solution to the system.\nThe elimination method is usually easiest when the coefficients (the numbers in front of the unknowns) are already opposites. However, it is sometimes necessary to multiply one or both equations by a constant before you can eliminate an unknown.\nThere are many ways to solve a system of linear equations. The best way to choose a method is to look at the equations and decide which one will be the easiest to work with. In general, the substitution method is best when one equation is already solved for one unknown. The elimination method is best when the coefficients of one unknown are opposites. The graphing method is a good way to check your work, but it can be difficult to find the intersection of two lines."
  },
  {
    "objectID": "systems-of-linear-equations.html#row-operations-and-similar-matrices",
    "href": "systems-of-linear-equations.html#row-operations-and-similar-matrices",
    "title": "1  Systems of Linear Equations",
    "section": "1.5 Row Operations and Similar Matrices",
    "text": "1.5 Row Operations and Similar Matrices\nIn solving systems of linear equations, we will often need to use row operations. Row operations are mathematical operations that can be performed on the rows of a matrix. There are three types of row operations:\n\nInterchange two rows\nMultiply a row by a nonzero constant\nAdd a multiple of one row to another row\n\nRow operations do not change the solution set of a system of linear equations. This means that if we perform row operations on a matrix, the new matrix will have the same solution set as the original matrix. We can use row operations to transform a matrix into an equivalent matrix. Two matrices are equivalent if they have the same solution set.\nWe can use row operations to solve systems of linear equations. In general, we will use row operations to transform the coefficient matrix into an upper triangular matrix. An upper triangular matrix is a matrix where all the elements below the main diagonal are zero. Once we have transformed the coefficient matrix into an upper triangular matrix, we can use back substitution to solve the system.\nWe can use row operations to transform a matrix into any equivalent matrix. However, some matrices are more difficult to work with than others. In general, it is easier to work with matrices that are in reduced row echelon form. A matrix is in reduced row echelon form if it is upper triangular and if the leading entry in each nonzero row is a 1. The leading entry in a row is the first nonzero entry in the row.\nWe can use row operations to transform any matrix into an equivalent matrix that is in reduced row echelon form. Once we have transformed the matrix into reduced row echelon form, we can use back substitution to solve the system of linear equations."
  },
  {
    "objectID": "systems-of-linear-equations.html#gaussian-elimination-method",
    "href": "systems-of-linear-equations.html#gaussian-elimination-method",
    "title": "1  Systems of Linear Equations",
    "section": "1.6 Gaussian Elimination Method",
    "text": "1.6 Gaussian Elimination Method\nThe Gaussian elimination method is another way to solve a system of linear equations. To use the Gaussian elimination method, you must first arrange the equations into row form. This will make it easier to see how to eliminate an unknown. Next, multiply each equation by a constant so that one of the unknowns cancels out. Then, add or subtract these equations so that another unknown cancels out. Finally, solve the equation resulting from this process for the last unknown. Now, substitute this value back into one of the original equations and you have the solution to the system!\nThe Gaussian Elimination Method is a procedure used to solve systems of linear equations. The method is named after German mathematician Carl Friedrich Gauss and Irish mathematician William Rowan Hamilton, who developed it independently in the early 19th century."
  },
  {
    "objectID": "systems-of-linear-equations.html#gauss-jordan-elimination-method",
    "href": "systems-of-linear-equations.html#gauss-jordan-elimination-method",
    "title": "1  Systems of Linear Equations",
    "section": "1.7 Gauss-Jordan Elimination Method",
    "text": "1.7 Gauss-Jordan Elimination Method\nThe Gauss-Jordan Elimination Method is similar to the traditional Gaussian Elimination Method, but it is more efficient and easier to use. The method works by first transforming the system of equations into an equivalent system with a triangular form. Then, the solution is obtained by a backward substitution.\nThis method is called Gauss-Jordan Elimination because it was developed by two mathematicians, Carl Friedrich Gauss and Wilhelm Jordan. Although it is not the quickest or most intuitive method, it is a robust way to solve linear equations. When done correctly, it will always give you the correct answer."
  },
  {
    "objectID": "systems-of-linear-equations.html#conclusion",
    "href": "systems-of-linear-equations.html#conclusion",
    "title": "1  Systems of Linear Equations",
    "section": "1.8 Conclusion",
    "text": "1.8 Conclusion\nThere are many ways to solve a system of linear equations. The best way to choose a method is to look at the equations and decide which one will be the easiest to work with. In general, the substitution method is best when one equation is already solved for one unknown. The elimination method is best when the coefficients of one unknown are opposites. The Gaussian elimination method is best when there are a lot of equations and unknowns. Lastly, the Gauss-Jordan elimination method is the most robust way to solve linear equations. It is not always the quickest or easiest method, but it will always give you the correct answer.\nNo matter which method you choose, make sure you understand the steps involved and why they work. This will help you not only solve the equations correctly but also to understand what is happening behind the scenes. With a little practice, you will be solving systems of linear equations like a pro!\nA linear system of equations in two variables \\(x\\) and \\(y\\) has the form \\[\n\\begin{cases}\na x + b y =c\\\\\nd x+ e y =f\n\\end{cases}\n\\] where \\(a, b, c, d, e, f\\) are given numbers such as real numbers or complex numbers. Sometimes a linear system is also written using indices\n\\[\\begin{equation}\n\\label{2by2sys}\n\\begin{cases}\na_{11} x+a_{12} y=b_1\\\\\na_{21}x+ a_{22} y =b_2\n\\end{cases}\n\\end{equation}\\]\nto help reduce on the number of letters used. For simplicity let’s temporally assume that the coefficients are nonzero real numbers and ask the question: how many solutions can there be to a \\(2\\times 2\\) system? The key idea is to realize that each linear equation in \\(\\ref{2by2sys}\\) represents a line in the Cartesian plane. If we consider the possible ways lines can intersect in the plane we come to the conclusion that there must either be no solutions, one unique solution, or infinitely many points \\((x,y)\\) that solves the system in \\(\\ref{2by2sys}\\).\n\nExample 1.1  Determine whether the linear system \\[\n\\begin{cases}\n2x+3y =0 \\\\\n4x+5y=0.\n\\end{cases}\n\\] has no solutions, exactly one solution, or infinitely many solutions. If we multiple the first equation, namely \\(2x+3y=0\\) by 2 and subtract from the second equation, \\(4x+5y=0\\), we obtain \\(y=0\\). Therefore the solution is unique and is \\((x,y)=(0,0)\\).\n\nA \\(3\\times 3\\) linear system has the form \\[\\begin{equation}\n\\label{3by3sys}\n\\begin{cases}\na_{11} x+a_{12} y+a_{13}z =b_1\\\\\na_{21} x+a_{22} y+a_{23}z =b_2\\\\\na_{31} x+a_{32} y+a_{33}z =b_3\\\\\n\\end{cases}\n\\end{equation}\\] where \\(a_{ij}\\) and \\(b_1, b_2, b_3\\) are numbers and \\(x, y, z\\) are variables. Geometrically, linear equations in three variables are just planes in three dimensions. So what are the different types of solution sets for the system in \\(\\ref{3by3sys}\\)? By considering the possible ways three planes might intersect in three dimensions we come to the conclusion that there must either be no solutions, one unique solution, or infinitely many points \\((x,y,z)\\) that solves the system in \\(\\ref{3by3sys}\\).\n\nExample 1.2  Determine whether the linear system \\[\\begin{equation}\n\\label{linesysex1}\n\\begin{cases}\nx+2y+3z=0 \\\\\n4x+5y+6z=3 \\\\\n7x+8y+9z =0\n\\end{cases}\n\\end{equation}\\] has no solutions, exactly one solution, or infinitely many solutions. Multiply the first equation by \\(-2\\) and add to the second equation obtaining the equation \\(2x+y=3\\). Eliminating \\(z\\) from the first and third equations we obtain \\(4x+2y=0\\) by multiplying the first equation by \\(-3\\) and adding to the third equation. The \\(x\\) and \\(y\\) that satisfies \\(\\ref{linesysex1}\\) must also satisfy the system \\[\\begin{equation}\n\\label{linesysex1b}\n\\begin{cases}\n2x+y=3 \\\\\n4x+2y=0.\n\\end{cases}\n\\end{equation}\\] Multiply the first equation of \\(\\ref{linesysex1b}\\) by \\(2\\) yields \\(4x+2y=6\\). Notice there are no \\(x\\) and \\(y\\) that satisfies both \\(4x+2y=6\\) and \\(4x+2y=0\\). Thus the system in \\(\\ref{linesysex1b}\\) has no solutions; therefore the system in \\(\\ref{linesysex1}\\) also has no solutions.\n\n\nExample 1.3 Given numbers \\(a, b\\), and \\(c\\), show that the linear system \\[\n\\begin{cases}\nx+2y +3z = a\\\\\nx+3y +8z = b\\\\\nx+2y +2z = c\n\\end{cases}\n\\] either has no solutions, exactly one solution, or infinitely many solutions. We choose to eliminate \\(x\\) first and we obtain the system \\[\n\\begin{cases}\n-y-5z  =a-b\\\\\nz  =a-c.\n\\end{cases}\n\\] Next we eliminate \\(z\\) obtaining \\(y=-6a+b+5c\\). Therefore the unique solution is \\((x,y,z)=(10a-2b-7c,-6a+b+5c,a-c)\\).\n\nLet’s consider a set of linear equations that involves \\(n\\) unknown quantities represented by \\(\\vlist{x}{n}\\). Let \\(a_{ij}\\) represent the number that is the coefficient of \\(x_j\\) in the \\(i\\)th equation. Let \\(\\vlist{b}{m}\\) be given numbers. The linear system of equations \\[\\begin{equation}\n\\label{syseq}\n\\begin{cases}\na_{11} x_1+a_{12} x_2+\\cdots +a_{1n} x_n =  b_1 \\\\\na_{21} x_1+a_{22} x_2+\\cdots +a_{2n} x_n =  b_2 \\\\\n\\hfill \\vdots \\hfill \\\\\na_{m1} x_1+a_{m2} x_2+\\cdots +a_{mn} x_n =  b_m \\\\\n\\end{cases}\n\\end{equation}\\] is called a system of simultaneous linear algebraic equations. A solution of this system is an ordered set of \\(n\\) numbers that satisfies each of the \\(m\\) statements in the system. A linear system of equations with no solution is called and a system with at least one solution is called inconsistent . The array\n\\[\n\\left[\n\\begin{array}{l|l}\n\\begin{matrix}\na_{11} & a_{12} & \\cdots & a_{1n} \\\\\na_{21} & a_{22} & \\cdots & a_{2n}\\\\\n& & \\vdots \\\\\na_{m1} & a_{m2} & \\cdots  & a_{mn}\\\\\n\\end{matrix}\n&\n\\begin{matrix}\nb_1 \\\\ b_2 \\\\ \\vdots \\\\ b_m\n\\end{matrix}\n\\end{array}\n\\right]\n\\] is called the corresponding to the linear system in \\(\\ref{syseq}\\) augmented matrix.\nFor example, the linear system in \\(\\ref{example:2by2system}\\) was shown to be consistent and the linear system in \\(\\ref{example:3by3system}\\) was shown to be inconsistent. The augmented matrices for these systems are as follows. \\[\n\\left[\n\\begin{array}{l|l}\n\\begin{matrix}\n2 & 3 \\\\\n4 & 5\n\\end{matrix}\n&\n\\begin{matrix}\n0 \\\\ 0\n\\end{matrix}\n\\end{array}\n\\right]\n\\qquad\n%\\text{and}\n\\qquad\n\\left[\n\\begin{array}{l|l}\n\\begin{matrix}\n1& 2 & 3 \\\\\n4 & 5 & 6 \\\\\n7 & 8 & 9\n\\end{matrix}\n&\n\\begin{matrix}\n0 \\\\ 3 \\\\0\n\\end{matrix}\n\\end{array}\n\\right]\n\\]\n\nExample 1.4  Find all solutions to the following linear system. \\[\n\\begin{cases}\n-150 x+500y=z \\\\\n50x+100y+z =200\n\\end{cases}\n\\] The given system is equivalent to \\[\n\\begin{cases}\n-150 x+500y-z=0 \\\\\n50x+100y+z =200.\n\\end{cases}\n\\] Adding these equations yields \\(-100x+600y=200\\). Since we have one equation with two variables, one of the variables is free. We choose to let \\(y\\) be free. Let \\(y=t\\) for an arbitrary number \\(t\\). Then solving for \\(x\\) we obtain \\(-100x=200y-600y\\), or equivalently \\(x=-2+6t\\). By substituting into the original system, we find \\(z=-150(-2+6t)+500t=300-400t.\\) Therefore, there are an infinitely many solutions, which can be represented by the set \\[\n\\{(x,y,z) \\mid x=-2+6t, y=t, z=300-400t \\text{ where $t\\in \\mathbb{R} $} \\}.\n\\]\n\nThe augmented matrix for the system in \\(\\ref{example:2by3system}\\) is \\[\n\\left[\n\\begin{array}{l|l}\n\\begin{matrix}\n-150 & 500 & -1 \\\\\n50 & 100 & 1\n\\end{matrix}\n&\n\\begin{matrix}\n0 \\\\ 200\n\\end{matrix}\n\\end{array}\n\\right]\n\\] Do you think it is possible to decide, by inspection of an augmented matrix, whether or not the corresponding linear system will be consistent or inconsistent?\nIn our examples so far we have seen linear systems having no solutions, a unique solution, or perhaps an infinite number of solutions. These examples suggest the following definition.\n\nDefinition 1.1 Two linear systems are called equivalent if they have the same solution set.\n\n\nExample 1.5 Find a system of linear equations with three unknowns \\(x,y,z\\) whose solutions are \\(x=6+5t\\), \\(y=4+3t\\), \\(z=2+t\\) where \\(t\\) is arbitrary. We want to eliminate \\(t\\). Solving for \\(t\\) yields \\(t=z-2\\). By substitution, \\(x=6+5(z-2)\\) and \\(y=4+3(z-2)\\). Thus we have a linear system \\[\n\\begin{cases}\nx-5z=-4\\\\\ny-3z =-2\n\\end{cases}\n\\] which has an infinitely many solutions.\n\n\nTheorem 1.1  Linear systems of equations are equivalent if each can be obtained from the other by one or more of the following operations.\n\nInterchange the order of the equations.\nMultiply (or divide) one equation by a nonzero scalar.\nAdd one multiple of one equation to another.\n\n\n\nProof. If we consider the solution set as a geometric object, it should be very easy to understand that they way in which we write the equations that represents the object does not change the object. Thus it should be obvious that interchanging the order of the equations will not change the solutions to a linear system. Neither will multiplying (or dividing) both sides of an equation by a nonzero constant.\nLet system \\(A\\) be an \\(m\\times n\\) system represented by \\[\\begin{equation}\n\\label{syseqA}\n\\begin{cases}\na_{11} x_1+a_{12} x_2+\\cdots +a_{1n} x_n =  b_1 \\\\\n\\hfill \\vdots \\hfill \\\\\na_{i1} x_1+a_{i2} x_2+\\cdots +a_{in} x_n =  b_i  \\\\\n\\hfill \\vdots \\hfill \\\\\na_{m1} x_1+a_{m2} x_2+\\cdots +a_{mn} x_n =  b_m & \\\\\n\\end{cases}\n\\end{equation}\\] Consider system \\(B\\) obtained from system \\(A\\) by adding \\(k\\) times equation \\(i\\) to equation \\(j\\) as follows: \\[\\begin{equation}\n\\label{syseqB}\n\\begin{cases}\na_{11} x_1+a_{12} x_2+\\cdots +a_{1n} x_n =  b_1  \\\\\n\\qquad \\qquad \\vdots \\qquad \\qquad \\qquad  \\vdots \\\\\n(a_{j1}+k a_{i1}) x_1+(a_{j2}+k a_{i2}) x_2+\\cdots +(a_{jn}+k a_{in}) x_n =  b_j +k b_i\\\\ \\qquad \\qquad  \\vdots \\qquad \\qquad \\qquad  \\vdots \\\\\na_{m1} x_1+a_{m2} x_2+\\cdots +a_{mn} x_n =  b_m \\\\\n\\end{cases}\n\\end{equation}\\] Let \\(S_A\\) and \\(S_B\\) be the solutions sets of systems \\(A\\) and \\(B\\), respectively. We will show \\(S_A=S_B\\). Let \\(x_0=(\\vlist{x}{n})\\) be a solution of system \\(A\\). Thus \\(x_0\\) satisfies every linear equation in \\(\\ref{syseqA}\\). So \\(x_0\\) satisfies every equation in system B except possibly the \\(j\\)th equation. Working with the \\(j\\)th equation in system \\(B\\) we find \\[\\begin{align*}\n& (a_{j1}+k a_{i1}) x_1+(a_{j2}+k a_{i2}) x_2+\\cdots +(a_{jn}+k a_{in}) x_n \\\\\n& \\qquad = a_{j1}x_1+\\cdots +a_{jn} x_n+k (a_{i1}x_1+\\cdots a_{jn}x_n)\\\\\n& \\qquad =  b_j +k b_i.\\end{align*}\\] This shows \\(x_0\\) also satisfies the \\(j\\)th equation of system \\(B\\). Since \\(x_0\\) satisfies every equation in \\(\\ref{syseqB}\\), \\(x_0\\) is also a member of \\(S_B\\). So far we have shown \\(S_A\\subseteq S_B\\). Conversely, assume \\(x_0\\) is a solutions to every equation in \\(\\ref{syseqB}\\). Working with the \\(j\\)th equation of system \\(A\\) we find, \\[\\begin{align}\n& a_{j1} x_1+a_{j2} x_2+\\cdots +a_{jn}x_n=b_j \\notag \\\\\n\\Longleftrightarrow \\quad & a_{j1} x_1+ (k a_{i1}-ka_{i1})x_1\n%+ a_{j2} x_2+ (k a_{21}-ka_{21})x_2\n+\\cdots +a_{jn} + (k a_{i1}-ka_{i1})x_n=b_j \\label{eqsys}\\notag \\\\\n\\Longleftrightarrow \\quad & (a_{j1}+k a_{i1}) x_1+(a_{j2}+k a_{i2}) x_2+\\cdots +(a_{jn}+k a_{in}) x_n = b_j+k b_i.\n\\end{align}\\] Notice \\(\\ref{eqsys}\\) holds since \\(x_0\\) satisfies system \\(B\\); and thus \\(x_0\\) satisfies the \\(j\\)th equation of system \\(A\\). Therefore \\(S_A=S_B\\) as desired.\n\nWhile working through the details of the following two examples, notice how the operations in \\(\\ref{proprowoperations}\\) are being used.\n\nExample 1.6 Let \\(a, b\\), and \\(c\\) be constants. Solve the linear system \\[\n\\begin{cases}\ny+z  =a \\\\\nx+z =b \\\\\nx+y = c.\n\\end{cases}\n\\] Eliminating \\(z\\) from the first and second equations we obtain the system \\[\n\\begin{cases}\nx-y = b-a \\\\\nx+y=c\n\\end{cases}\n\\] Solving for \\(x\\) yields \\(x=(c+b-a)/2\\). Using the original system we find \\(y\\) and \\(z\\) to be \\[\ny= c-x=c-(c+b-a)/2=(c-b+a)/2\n\\] \\[\nz=a-y=a-(c-b+a)/2=(a+b-c)/2\n\\] Therefore, the solution to the system is \\[\n(x,y,z)=\\left(\\frac{c+b-a}{2},\\frac{a+c-b}{2},\\frac{a+b-c}{2}\\right).\n\\]\n\n\nExample 1.7 Find the smallest positive integer \\(C\\) such that \\(x, y, z\\) are integers and satisfies the linear system of equations \\[\n\\begin{cases}\n2x+y =C \\\\\n3y+z =C \\\\\nx+4z = C.\n\\end{cases}\n\\] Multiply the third equation by \\(-2\\) and add to the first equation, obtaining \\(y-8z=-C\\). Multiplying the second equation by 8 and adding to \\(y-8z=-C\\), yields \\(25y=7C\\). Solving for \\(y\\) we obtain \\(y=(7/25)C\\). By substitution, \\(x=(9/25)C\\) and \\(z=(4/25)C\\). Therefore, 25 is the smallest integer \\(C\\) such that \\(x,y,z\\) are integers and solves the system.\n\nWe will solve a linear system of equations using elementary row operations on matrices using a procedure known as Gaussian elimination . The solution set will be a collection of vectors. Here is the overall strategy to solving linear systems using matrices. \\[\n\\small\n\\begin{tabular}{c}\n$\\left\\{\\parbox[c]{1.75cm}{\\begin{center}linear system of equations\\end{center}}\\right\\}$  $\\rightarrow$  \n$\\left\\{\\parbox[c]{1.75cm}{\\begin{center}matrix representation of system\\end{center}}\\right\\}$\n$\\rightarrow$\n$\\left\\{\\parbox[c]{1.75cm}{\\begin{center}row echelon form of system\\end{center}}\\right\\}$\n$\\rightarrow$  \n$\\left\\{\\parbox[c]{1.75cm}{\\begin{center}solutions of system as a set of vectors\\end{center}}\\right\\}$\n\\end{tabular}\\]\n\nDefinition 1.2 The following operations are collectively known as elementary row operations .\n\nInterchange two rows.\nMultiply a row by a nonzero scalar.\nAdd a multiple of a row from another row.\n\n\nFrom \\(\\ref{proprowoperations}\\) it is obvious that applying elementary row-operations to a linear system of equations leads to an equivalent system. Reducing a linear system of equations, while preserving the solutions set, is an extremely useful idea that we will develop extensively.\nFor example, in \\(\\ref{gjeunique}\\) (see below) we find that the linear systems \\[\\begin{equation}\n\\label{refex}\n\\begin{cases}\nx+2y+z =3 \\\\\n2x+5y-z =-4 \\\\\n3x-2y-z =5\n\\end{cases}\n\\qquad\n\\qquad\n\\begin{cases}\nx=2 \\\\\ny=-1 \\\\\nz=3\n\\end{cases}\n\\end{equation}\\] are equivalent. In \\(\\ref{gjeunique}\\) we will show how to apply elementary row-operations to obtain the system on the right. While the system on the left might be a given linear system of equations, the system on the right is solved.\n\nDefinition 1.3 A matrix is said to be in row echelon form if it satisfies all the following conditions.\n\nAll rows with at least one nonzero coefficient are above any rows of all zeroes.\nThe first nonzero number from the left, (called the leading coefficient ) of a nonzero row is always strictly to the right of the leading coefficient of the row above it.\nAll entries in a column below a leading coefficient are zeroes.\n\nFurther, a matrix is said to be in reduced row echelon form if it is in row echelon form and the additional condition holds.\n\nEvery leading coefficient is 1 and is the only nonzero entry in its column.\n\n\nFor example, the augmented matrices for the linear systems in \\(\\ref{refex}\\) are \\[\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 1 & 3 \\\\\n2 & 5 & -1 & -4 \\\\\n3 & -2 & -1 & 5\n\\end{array}\n\\end{bmatrix}\\qquad\n\\qquad\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 0 & 0 & 2 \\\\\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1& 3\n\\end{array}\n\\end{bmatrix}.\n\\] By checking conditions (i)-(iv) we can see that the matrix on the right is in reduced row echelon form. For more examples, consider the following matrices. \\[\nA = \\begin{bmatrix} 0 & 5 \\\\ 2 & 3 \\end{bmatrix}\n\\qquad\nB = \\begin{bmatrix} 1 & 0 \\\\ 1 & 0 \\end{bmatrix}\n\\qquad\nC = \\begin{bmatrix} 0 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\] \\[\nD = \\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}\n\\qquad\nE = \\begin{bmatrix} 1 & 0 \\\\ 0 & 1 \\end{bmatrix}\n\\qquad\nF = \\begin{bmatrix} 1 & 2 \\\\ 0 & 1 \\end{bmatrix}\n\\] Notice that matrices \\(D\\) and \\(E\\) are in reduced row echelon form and the others are not.\n\nExample 1.8  Use Gauss-Jordan elimination to solve the system. \\[\n\\begin{cases}\nx+2y+z =3 \\\\\n2x+5y-z =-4 \\\\\n3x-2y-z =5\n\\end{cases}\n\\] Using row operations on the augmented matrix we obtain the reduced row echelon form. \\[\\begin{align*}\n& \\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 1 & 3 \\\\\n2 & 5 & -1 & -4 \\\\\n3 & -2 & -1 & 5\n\\end{array}\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_1+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle -3R_1+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 1 & 3 \\\\\n0 & 1 & -3 & -10 \\\\\n0 & -8 & -4 & -4\n\\end{array}\n\\end{bmatrix}\n\\\\ & \\qquad \\qquad\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle 8 R_2+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 1 & 3 \\\\\n0 & 1 & -3 & -10 \\\\\n0 & 0 & -28 & -84\n\\end{array}\n\\end{bmatrix}\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -\\frac{1}{28}R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 1 & 3 \\\\\n0 & 1 & -3 & -10\\\\\n0 & 0 & 1 & 3\n\\end{array}\n\\end{bmatrix}\n\\\\ &  \\qquad \\qquad\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle 3R_3+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle -R_3+R_1}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 2 & 0 & 0 \\\\\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1 & 3\n\\end{array}\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_2+R_1}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{ccc|c}\n1 & 0 & 0 & 2 \\\\\n0 & 1 & 0 & -1 \\\\\n0 & 0 & 1& 3\n\\end{array}\n\\end{bmatrix}.\n\\end{align*}\\] Therefore the unique solution is \\(x=2\\), \\(y=-1\\), and \\(z=3\\).\n\n\nExample 1.9 Use the Gauss-Jordan elimination method to solve the following linear system. \\[\\begin{equation}\n\\label{gjee1}\n\\begin{cases}\nx+y -2z+4t =5 \\\\\n2x+2y-3z+t =3 \\\\\n3x+3y-4z-2t  = 1\n\\end{cases}\n\\end{equation}\\] Using row operations on the augmented matrix we obtain the reduced row echelon form. \\[\\begin{align*}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & -2 & 4 & 5 \\\\\n2 & 2 & -3 & 1 & 3 \\\\\n3 & 3 & -4 & -2 & 1\n\\end{array}\n\\end{bmatrix}\n& \\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_1+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle -3R_1+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & -2 & 4 & 5 \\\\\n0 & 0 & 1 & -7  & -7 \\\\\n0 & 0 & 2 & -14 & -14\n\\end{array}\n\\end{bmatrix}\n\\\\ &\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_2+R_3}\n\\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle 2R_2+R_1}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & 0 & -10 & -9 \\\\\n0 & 0 & 1 & -7 & -7 \\\\\n0 & 0 & 0 & 0 & 0\n\\end{array}\n\\end{bmatrix}\n\\end{align*}\\] The system in \\(\\ref{gjee1}\\) is equivalent to \\[\n\\begin{cases}\nx+y-10t =-9 \\\\\nz-7t  = -7\n\\end{cases}\n\\quad \\text{ or more simply } \\quad\n\\begin{cases}\nx =-9-y+10t \\\\\nz =-7+7t.\n\\end{cases}\n\\] Therefore \\[\n\\{(x,y,z,w)\\in \\mathbb{R}^4 \\mid x=-9-y+10t, y=s, z=-7+7t, w=t,\n\\text{ for } s,t\\in \\mathbb{R}\\}.\n\\] is the solution set.\n\n\nExample 1.10 Use the Gauss-Jordan elimination method to solve the following linear system. \\[\n\\begin{cases}\nx_1+x_2-2x_3+3x_4 =4 \\\\\n2x_1+3x_2+3x_3-x_4=3 \\\\\n5 x_1+7 x_2+4 x_3+x_4 = 5\n\\end{cases}\n\\] Using row operations on the augmented matrix we obtain the reduced row echelon form. \\[\\begin{align*}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & -2 & 3 & 4 \\\\\n2 & 3 & 3 & -1 & 3 \\\\\n5 & 7 & 4 & 1 & 5\n\\end{array}\n\\end{bmatrix}\n& \\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_1+R_2}\n\\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle -5R_1+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & -2 & 3 & 4 \\\\\n0 & 1 & 7 & -7 & -5 \\\\\n0 & 2 & 14 & -14 & 15\n\\end{array}\n\\end{bmatrix}\n\\\\ & \\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_2+R_3}\n\\end{array}\n\\begin{bmatrix}\n\\begin{array}{cccc|c}\n1 & 1 & -2 & 3 & 4\\\\\n0 & 1 & 7 & -7 & -5 \\\\\n0 & 0 & 0 & 0 & 25\n\\end{array}\n\\end{bmatrix}\n\\end{align*}\\] Notice the last row corresponds to the equation \\(0x_1+0x_2+0x_3+0x_4=25\\). Therefore there are no solutions to this system. Actually using Gauss-Jordan elimination means reaching reduced row echelon form; however, once we encounter the row \\([0 \\, \\, \\, 0 \\, \\, \\, 0 \\, \\, \\, 0 \\, \\, \\, 25 ]\\) we immediately stop and reach the correct conclusion: no solutions to the system.\n\n\nExample 1.11 Find the polynomial of degree 2 whose graph goes through the points \\((1,-1)\\), \\((2,3)\\), and \\((3,13)\\). Let \\(f(t)=a+b t+ c t^2\\) be such a polynomial. Using the given points we setup a system and solve. \\[\\begin{align*}\n&\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & b & c\\\\\na & 2b & 4c \\\\\na & 3b & 9c\n\\end{matrix} &  \n\\begin{matrix}\n-1 \\\\ 3 \\\\ 13\n\\end{matrix}\n\\end{array} \\right]\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -R_1+R_2} \\\\\n\\stackrel{\\longrightarrow}{\\scriptstyle -R_1+R_3}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & b & c\\\\\n0 & b & 3c \\\\\n0 & 2b & 8c\n\\end{matrix} &  \n\\begin{matrix}\n1 \\\\ 4 \\\\ 14\n\\end{matrix}\n\\end{array} \\right]\n\\\\ & \\qquad \\qquad\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -2R_2+R_3}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & b & c\\\\\n0 & b & 3c \\\\\n0 & 0 & 2c\n\\end{matrix} &  \n\\begin{matrix}\n-1 \\\\ 4 \\\\ 6\n\\end{matrix} \\end{array} \\right]\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle \\frac{1}{2}R_3}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & b & c\\\\\n0 & b & 3c \\\\\n0 & 0 & c\n\\end{matrix} &  \n\\begin{matrix}\n-1 \\\\ 4 \\\\ 3\n\\end{matrix}\n\\end{array} \\right]\n\\\\ & \\qquad \\qquad\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -3R_3+R_2}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & b & c\\\\\n0 & b & 0 \\\\\n0 & 0 & c\n\\end{matrix} &  \n\\begin{matrix}\n-1 \\\\ -5 \\\\ 3\n\\end{matrix}\n\\end{array} \\right]\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -R_2+R_1}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na& 0 & c\\\\\n0 & b & 0 \\\\\n0 & 0 & c\n\\end{matrix} &  \n\\begin{matrix}\n4 \\\\ -5 \\\\ 3\n\\end{matrix}\n\\end{array} \\right]\n\\\\ & \\qquad \\qquad\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\scriptstyle -R_3+R_1}\n\\end{array}\n\\left[ \\begin{array}{c|c}\n\\begin{matrix}\na & 0 & 0 \\\\\n0 & b & 0 \\\\\n0 & 0 & c\n\\end{matrix} &  \n\\begin{matrix}\n1 \\\\ -5 \\\\ 3\n\\end{matrix}\n\\end{array} \\right].\n\\end{align*}\\] Therefore the required polynomial is \\(f(t)=3t^2-5t+1\\)."
  },
  {
    "objectID": "systems-of-linear-equations.html#exercises",
    "href": "systems-of-linear-equations.html#exercises",
    "title": "1  Systems of Linear Equations",
    "section": "1.9 Exercises",
    "text": "1.9 Exercises\n\nExercise 1.1 Verify that \\((2,3,-1)\\) is a solution of the linear system.\n\n\\(\\begin{cases}x+2y+z=7 \\\\ x-y=-1 \\\\ 4x+y+z=10 \\end{cases}\\)\n\\(\\begin{cases} x+y=5 \\\\ x-z=3 \\\\ y+z=2 \\end{cases}\\)\n\n\n\nExercise 1.2 Verify that every triple of the form \\((7-2k,8+6k,k)\\) is a solution of the linear system of equations\n\n\\(\\begin{cases} x_1+2x_3=7 \\\\ x_2-6x_3=8 \\end{cases}\\)\n\\(\\begin{cases} 2x_1+4x_3=14 \\\\ x_1+3x_2-16x_3=31 \\end{cases}\\)\n\n\n\nExercise 1.3 Do the following systems have no solutions, exactly one solution, or infinitely many solutions? Justify your answer.\n\n\\(\\begin{cases}x+y=1\\\\ x+2y=1 \\end{cases}\\)\n\\(\\begin{cases} 3x+y=1\\\\ y=-2 \\end{cases}\\)\n\\(\\begin{cases}x+y=1\\\\ 2x+2y=2 \\end{cases}\\)\n\\(\\begin{cases}3x+y=2 \\\\ 6x+2y=4 \\end{cases}\\)\n\n\n\nExercise 1.4 Sketch the graph of each equation of the linear system and decide whether it has no solutions, exactly one solution, or infinitely many solutions.\n\n\\(\\begin{cases} x+y=2\\\\ 2x+3y=0 \\end{cases}\\)\n\\(\\begin{cases} -x+3y=2\\\\ 2x-6y=-4 \\end{cases}\\)\n\n\n\nExercise 1.5 Find the solutions, if any, of the following linear systems of equations without using matrices. If an equation has more than one solution, write the general solution.\n\n\\(\\begin{cases} 3x-2y= 7 \\\\ 2x+y=15 \\end{cases}\\)\n\\(\\begin{cases} x+y=7\\\\ 3x+4y=12 \\end{cases}\\)\n\\(\\begin{cases} 2x_1-3x_2=1\\\\ 4x_1-6x_2=-2\\\\ x_1+x_2=1 \\end{cases}\\)\n\\(\\begin{cases} 2x_1-5x_2=12 \\end{cases}\\)\n\\(\\begin{cases} i x_1-3ix_2= 1\\\\ (2+i)x_1-x_2=-1 \\end{cases}\\)\n\\(\\begin{cases} (1+i)x_1-2ix_2=2\\\\ 2x_1-3i x_2=4-3i \\end{cases}\\)\n\n\n\nExercise 1.6 If possible, find the point(s) of intersection.\n\n\\(x-4y=11\\) and \\(7x-2y=9\\)\n\\(x-4y+3z=11\\), \\(7x-2y-z=-1\\), \\(7x-2y+z=-2\\)\n\n\n\nExercise 1.7 Let \\(u=(1,1,2,-1)\\) and \\(v=(1,1,1,0)\\). For which scalars \\(a\\) and \\(b\\) is it true that \\(a u+b v\\) is a solution to the following system?\n\n\\(\\begin{cases} 4x-2y-z-w=1 \\\\ x+3y-2z-2w=2 \\end{cases}\\)\n\\(\\begin{cases}x-4y-z-2w=4 \\\\ 7x+y-5z-2w=12 \\end{cases}\\)\n\n\n\nExercise 1.8 Find all solutions, if any exist, for the following linear systems of equations.\n\n\\(\\begin{cases} x-y=1\\\\ 2x=4 \\end{cases}\\)\n\\(\\begin{cases} 2x+3y-z=19\\\\ 3x-2y+3z=7\\end{cases}\\)\n\\(\\begin{cases}x-3y=2 \\\\-2x+6y=-4\\end{cases}\\)\n\\(\\begin{cases}x-y+2z-2w=1\\\\2x+y+3w=4 \\\\2x+y+3w=6\\end{cases}\\)\n\n\n\nExercise 1.9 Find all solutions, if any exist, for the following linear systems of equations.\n\n\\(\\begin{cases}2x-z=-1\\\\x+y-z=0\\\\2x-y+2z=3\\end{cases}\\)\n\\(\\begin{cases}2x-3y+2z=1\\\\x-6y+z=2\\\\-x-3y-z=1\\end{cases}\\)\n\n\n\nExercise 1.10 Find all solutions, if any exist, for the following linear systems of equations.\n\n\\(\\begin{cases} x_1+2x_2-x_3=0\\\\ 2x_1+5x_2+5x_3=0\\\\ x_1+4x_2+7x_3=0\\\\ x_1+3x_2+3x_3=0 \\end{cases}\\)\n\\(\\begin{cases} x+y+z+w=1\\\\ 2x-2y+z+2w=3\\\\ 2x+6y+3z+2w=1\\\\ 5x-3y+3z+5w=8 \\end{cases}\\)\n\n\n\nExercise 1.11 For what values of the constant \\(k\\) do the systems have no solution, exactly one solution, or infinity many solutions.\n\n\\(\\begin{cases} x+y=1\\\\ 3x+3y=k \\end{cases}\\)\n\\(\\begin{cases} x+ky=1\\\\ 2x-y=k \\end{cases}\\)\n\n\n\nExercise 1.12 Find \\(a, b, c\\) such that \\[\n\\frac{x^2-x+3}{(x^2+2)(2x-1)}=\\frac{a x+b}{x^2+2}+\\frac{c}{2x-1}.\n\\]\n\n\nExercise 1.13 Let \\(a\\) and \\(b\\) be arbitrary constants. Find all solutions to the linear system.\n\n\\(\\begin{cases} x+2y=a\\\\ 3x+5y=b \\end{cases}\\)\n\\(\\begin{cases} a x+2y=1\\\\ 3x+by=4 \\end{cases}\\)\n\n\n\nExercise 1.14 Let \\(a\\) and \\(b\\) be arbitrary constants. Find all solutions to the linear system.\n\n\\(\\begin{cases}x+2y+3z=a\\\\ x+3y+8z=b\\\\ x+2y+2z=c \\end{cases}\\)\n\\(\\begin{cases} x-2y+4z=a\\\\ x-3y+5z=b\\\\ x-2y+6z=c \\end{cases}\\)\n\n\n\nExercise 1.15 A system of linear equations all of whose constant terms are zero is called a homogenous system.\n\nShow that a homogenous system always has at least one solution.\nGive examples to show that a homogenous system may have more than one solution or exactly one solution.\n\n\n\nExercise 1.16 Write the system corresponding to each augmented matrix.\n\n\\(\\left[\\begin{array}{ccc|c} 1 & 2 & 1 & 1 \\\\ 0 & 4 & 0 & 1 \\\\ 0 & 0 & 3 & 3 \\end{array}\\right]\\)\n\\(\\left[\\begin{array}{cccc|c} 1 & 0 & 0 & 0 & 0 \\end{array}\\right]\\)\n\\(\\left[\\begin{array}{ccc|c} 1 & 0 & 0 & 0 \\\\ 0 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 0 \\end{array}\\right]\\)\n\\(\\left[\\begin{array}{c|c} 2 & 1 \\\\ 0 & 1 \\\\ 1 & 2 \\end{array}\\right]\\)\n\\(\\left[\\begin{array}{c|c} 1 & 1 \\\\ 1 & 0 \\end{array}\\right]\\)\n\\(\\left[\\begin{array}{cccc|c} 1 & 1 & 1 & 1 & 0 \\end{array}\\right]\\)\n\n\n\nExercise 1.17 Which of the following matrices are in reduced row echelon form?\n\n\\(\\begin{bmatrix} 0 & 1 & 2 & 0 & 3 \\\\ 0 & 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 & 0 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 0 & 0 & 2 & 1 \\\\ 0 & 1 & 0 & 0 & 0 \\\\ 0 & 0 & 1 & 0 & 0 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 1 & 1 \\\\ 0 & 0 & 1\\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 0 & 0 & 1\\\\ 0 & 0 & 1\\\\ 0 & 0 & 1\\\\ \\end{bmatrix}\\)\n\n\n\nExercise 1.18 Which of the following matrices are in reduced row echelon form?\n\n\\(\\begin{bmatrix} 0 & 0 & 0 & 1 \\\\ 0 & 0 & 0 & 0\\\\ \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 0 & 0 & 3 & 1 \\\\ 0 & 0 & 0 & 1 & 1 \\\\ 0 &0 & 0 & 0 & 1 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 0 & 2 & 1 \\\\ 0 & 1 & 0 & 1 \\\\ 0 & 0 & 1 & 0 \\\\ 0 & 0 & 0 & 0 \\end{bmatrix}\\)\n\n\n\nExercise 1.19 Find all solutions of the linear system using the Gauss-Jordan elimination method.\n\n\\(\\begin{cases} x_2+2x_4+3x_5=0\\\\ 4x_4+8x_5=0 \\end{cases}\\)\n\\(\\begin{cases} 3x+4y =0\\\\ -2x+7y=0 \\end{cases}\\)\n\n\n\nExercise 1.20 Find all solutions of the linear system using the Gauss-Jordan elimination method.\n\n\\(\\begin{cases} x_4+2x_5-x_6=2 \\\\ x_1+2x_2+x_5-x_6=0\\\\ x_1+2x_2+2x_3-x_5+x_6=2 \\end{cases}\\)\n\\(\\begin{cases} 3x_1+3x_2-4x_3+x_4=2 \\\\ x_1+x_2-x_3-x_4=5 \\end{cases}\\)\n\\(\\begin{cases} 2 x_1+3x_2+4x_3=6-x_4 \\\\ 3x_1-2x_2-x_4=1+4x_3 \\\\ 3x_1+3x_3+x_4=4-x_2 \\\\ x_2+x_3-4x_4=-3-4x_1\\end{cases}\\)\n\\(\\begin{cases} 4x_1+3x_2+2x_3-x_4=4 \\\\ 5x_1+4x_2+3x_3-x_4=4\\\\ -2x_1-2x_2-x_3+2x_4=-3\\\\ 11x_1+6x_2+4x_3+x_4=11 \\end{cases}\\)\n\\(\\begin{cases} 2x_1-3x_2+x_3=5 \\\\ x_1+x_2-x_3=3\\\\ 4x_1-x_2-x_3=1 \\end{cases}\\)\n\\(\\begin{cases} x_1-x_2=4\\\\ 2x_1+x_2=7\\\\ 5x_1-2x_2=19 \\end{cases}\\)\n\n\n\nExercise 1.21 Find all \\(4\\times 1\\) matrices in reduced row echelon form.\n\n\nExercise 1.22 How many types of \\(3\\times 2\\) matrices in reduced row echelon form are there? - How many types of \\(2\\times 3\\) matrices in reduced row echelon form are there?\n\n\nExercise 1.23  \n\nDescribe the possible reduced row echelon forms for a matrix with two rows and two columns.\nDescribe the possible reduced row echelon forms for a matrix with three rows and three columns.\n\n\n\nExercise 1.24 Find the polynomial of degree 3 whose graph passes through the points \\((0,1)\\), \\((1,0)\\), \\((-1,0)\\), and \\((2,-15)\\). Sketch the graph of this cubic.\n\n\nExercise 1.25 Find the polynomial of degree 4 whose graph passes through the points \\((1,1)\\), \\((2,-1)\\), \\((3,-59)\\), and \\((-2, -29)\\). Sketch the graph of this quartic.\n\n\nExercise 1.26 Find the values of \\(k\\), if any, for which the system has\n\nonly one solution,\nno solutions,\nan infinite number of solutions\n\\(\\begin{cases} x_1-x_2=3 \\\\ 3x_1+kx_2=6 \\end{cases}\\)\n\\(\\begin{cases}x_1+2k x_2 =7\\\\ x_1+(k^2+1)x_2=3 \\end{cases}\\)\n\\(\\begin{cases}k x_1-x_2=1 \\\\ x_1+k x_2=i\\end{cases}\\)\n\\(\\begin{cases} k x+|k| y=1 \\\\ |k|x-ky=-1 \\end{cases}\\)\n\n\n\nExercise 1.27 Find values for \\(a\\) such that the system of linear equations \\[\\begin{cases} x_1-3x_2+x_3=1 \\\\ 2x_1+x_2-x_3=-1 \\\\ 5x_1-8x_2+(a^2-2)x_3=a  \\end{cases}\\] will have\n\ninfinitely many solutions,\nno solutions\nexactly one solution.\n\n\n\nExercise 1.28 Use Gauss-Jordan elimination to solve the linear system \\[\n\\left\\{ \\begin{array}{lll}\na x_1+ b x_2 & & =r \\quad (a\\neq 0) \\\\\nc x_1+ d x_2 & & =s \\quad (e\\neq 0)\\\\\n& e x_3+f x_4 & =t \\\\\n& g x_3+h x_4 & =u \\\\\n\\end{array}\\right.\\] State conditions on \\(a, b, c, d, e, f, g\\) and \\(h\\) which guarantee a unique solution.\n\n\nExercise 1.29 Show that the system \\[\\begin{cases}a_1 x+ b_1 y+c_1 z=0 \\\\ a_2 x+b_2 y +c_2 z=0 \\end{cases}\\] always has a solution other than \\(x=0\\) and \\(y=0\\).\n\n\nExercise 1.30 What can you say about the last row of the reduced row echelon form of the following matrix? \\[\n\\begin{bmatrix}\n1 & 2 & 3 & \\cdots & n\\\\\nn+1 & n+2 & n+3 & \\cdots & 2n \\\\\n\\vdots & \\vdots & \\vdots & \\vdots &  \\vdots \\\\\nn^2-n+1 & n^2-n+2 & n^2-n+3 & \\cdots & n^2\n\\end{bmatrix}\n\\]\n\n\nExercise 1.31 Show that the linear system of equations \\[\\begin{cases} 3x_1+2x_2-4x_3=a \\\\ -4x_1+x_2-x_3 =b \\\\ 7x_1+12x_2-22x_3 =c \\end{cases}\\] will have infinitely many solutions if \\(c=5a+2b\\).\n\n\nExercise 1.32 Find an equation of the form \\(x^2+y^2+a x+by+c=0\\) of the circle passing through the following points.\n\n\\((-2,1)\\), \\((5,0)\\), and \\((4,1)\\)\n\\((1,1)\\), \\((5,-3)\\), and \\((-3,-3)\\)"
  },
  {
    "objectID": "matrices-and-vectors.html#exercises",
    "href": "matrices-and-vectors.html#exercises",
    "title": "2  Matrices and Vectors",
    "section": "2.1 Exercises",
    "text": "2.1 Exercises\n\nExercise 2.1 Let \\(A\\) be a matrix of size \\(4\\times 5\\), let \\(B\\) be a matrix of size \\(6\\times 4\\), let \\(C\\) be a matrix of size \\(4\\times 5\\), and let \\(D\\) be a matrix of size \\(4\\times 2\\). Which of the following are defined, and for those that are, what is their size?\n\n\\(BA\\)\n\\(DA\\)\n\\(B+A\\)\n\\(C+A\\)\n\\(C(AB)\\)\n\\((BA)C\\)\n\\(C+DB\\)\n\\(D(C+A)\\)\n\\(A+CB\\)\n\n\n\nExercise 2.2 Let \\(A=\\begin{bmatrix} 2 & 1 & 3 \\\\ 4 & 0 & -1 \\\\ -1 & 1 & 0 \\end{bmatrix}\\) and \\(B=\\begin{bmatrix} 1 & -1 & 1 \\\\ 0 & 2 & 0 \\\\ 4 & 3 & 2 \\end{bmatrix}.\\) Where possible, find the following matrices.\n\n\\(A+B\\)\n\\(B-A\\)\n\\(3B\\)\n\\(-2A\\)\n\\(AB-BA\\)\n\\(-2BA\\)\n\\(2A-4B\\)\n\\(B^2-A^2\\)\n\n\n\nExercise 2.3 Find \\(a, b, c\\), and \\(d\\) that satisfies the equation.\n\n\\(\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} c-3d & -d \\\\ 2a+d & a+b \\end{bmatrix}\\)\n\\(3 \\begin{bmatrix} a \\\\ b \\end{bmatrix} + 2 \\begin{bmatrix} b \\\\ a \\end{bmatrix} = \\begin{bmatrix} 1\\\\ 2 \\end{bmatrix}\\)\n\n\n\nExercise 2.4 Let \\(A=\\begin{bmatrix}2 & 1 \\\\ 0 & -1\\end{bmatrix},\\) \\(B=\\begin{bmatrix}3 & -1 & 2\\\\ 0 & 1 & 4\\end{bmatrix},\\) \\(C=\\begin{bmatrix}3 & -1 \\\\ 2 & 0\\end{bmatrix},\\) and \\(D=\\begin{bmatrix}1 & 3 \\\\ -1 & 0 \\\\1 & 4\\end{bmatrix}.\\) Where possible, find the following\n\n\\(3A-2B\\)\n\\(5C\\)\n\\(B+D\\)\n\\(2B-3D\\)\n\\(A-D\\)\n\\(A-2C\\)\n\n\n\nExercise 2.5 Find \\(A\\) in terms of \\(B\\).\n\n\\(A+B=3A+2B\\)\n\\(2A-B=5(A+2B)\\)\n\n\n\nExercise 2.6 Simpify the following expressions where \\(A\\), \\(B\\), and \\(C\\) are matrices.\n\n\\(2[9(A-B)+7(2B-A)]-2[3(2B+A)-2(A+3B)]\\)\n\\(5[3(A-B+2C)-2(3C-B)-A]+2[3(3A-B+C)+2(B-2A)-2C]\\)\n\n\n\nExercise 2.7 Show that if \\(Q+A=A\\) holds for every \\(m\\times n\\) matrix \\(A\\), then \\(Q=0_{mn}\\).\n\n\nExercise 2.8 Show that if \\(A\\) is an \\(m\\times n\\) matrix with \\(A+A'=0_{mn}\\), then \\(A'=-A\\).\n\n\nExercise 2.9 Show that if \\(A\\) denotes an \\(m\\times n\\) matrix, then \\(A=-A\\) if and only if \\(A=0\\).\n\n\nExercise 2.10 Given the following vectors, determine the values of \\(a\\) and \\(b\\) such that \\(\\vec{u}\\) is a linear combination of \\(\\vec{v_1}\\) and\n\\(\\vec{v_2}\\). \\[\n\\vec{u}=\\vectorfour{5}{7}{a}{b}\n\\qquad\n\\vec{v_1}=\\vectorfour{1}{1}{1}{1}\n\\qquad\n\\vec{v_2}=\\vectorfour{4}{3}{2}{1}\n\\]\n\n\nExercise 2.11 Find all solutions \\(x_1, x_2, x_3\\) of the equation \\(\\vec b = x_1 \\vec v_1 + x_2 \\vec v_2 + x_3 \\vec v_3\\) given the following vectors. \\[\n\\vec b = \\vectorfour{-8}{-1}{2}{15}\n\\qquad\n\\vec v_1 = \\vectorfour{1}{4}{7}{5}\n\\qquad\n\\vec v_2 = \\vectorfour{2}{5}{8}{3}\n\\qquad\n\\vec v_3 = \\vectorfour{4}{6}{9}{1}\n\\]\n\n\nExercise 2.12 Determine the value of \\(a\\) such that \\(\\vec{u}\\) is a linear combination of \\(\\vec{v_1}\\) and \\(\\vec{v_2}\\) given the following vectors. \\[\n\\vec{u}=\\vectorthree{1}{a}{a^2}\n\\qquad\n\\vec{v_1}=\\vectorthree{1}{2}{4}\n\\qquad\n\\vec{v_2}=\\vectorthree{1}{3}{9}\n\\]\n\n\nExercise 2.13 Find the product of the given matrices.\n\n\\(\\begin{bmatrix} 1 & -1 & 2 \\\\ 2 & 0 & 4 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 2 & 3 & 1 \\\\ 1 & 9 & 7\\\\ -1 & 0 & 2 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 3 & -3 \\\\ \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 3 & 0 \\\\ -2 & 1 \\\\ 0 & 6 \\end{bmatrix}\\)\n\n\n\nExercise 2.14 Let \\(A=\\begin{bmatrix}2 & 3-i \\\\ 1 & i \\end{bmatrix},\\) \\(B=\\begin{bmatrix}i & 1-1 \\\\ 0 & i\\end{bmatrix},\\) and \\(C=\\begin{bmatrix} 2+1 & 1 \\\\ 3 & i+1 \\end{bmatrix}.\\) Find each of the following.\n\n\\(A+B+C\\)\n\\(AB+AC\\)\n\\(A+BC\\)\n\\(CB\\)\n\\(A^2 C\\)\n\\(C^2 A\\)\n\n\n\nExercise 2.15 Find all matrices that commute with the given matrix.\n\n\\(\\begin{bmatrix}0 & 1 \\\\ 0 & 0 \\end{bmatrix}\\)\n\\(\\begin{bmatrix}0 & 0 \\\\ 1 & 0\\end{bmatrix}\\)\n\\(\\begin{bmatrix}2 & 3 \\\\ 1 & 0\\end{bmatrix}\\)\n\n\n\nExercise 2.16 For what values of the constants \\(a, b, c\\) and \\(d\\) is\n\\(\\vec b\\) a linear combination of the vectors \\(\\vec v_1\\), \\(\\vec v_2\\), \\(\\vec v_3\\) given \\[\n\\vec b = \\begin{bmatrix} a \\\\ b  \\\\ c \\\\ d \\end{bmatrix}\n\\qquad\n\\vec v_1 =\\vectorfour{0}{0}{3}{0}\n\\qquad\n\\vec v_2 = \\vectorfour{1}{0}{4}{0}\n\\qquad\n\\vec v_3 = \\vectorfour{2}{0}{5}{6}?\n\\]\n\n\nExercise 2.17 If \\(A=\\begin{bmatrix}a & b \\\\c & d\\end{bmatrix}\\) where \\(a\\neq 0\\), show that \\(A\\) factors in the form \\[\nA=\n\\begin{bmatrix}\n1 & 0 \\\\\nx & 1\n\\end{bmatrix}\n\\begin{bmatrix}\ny & z \\\\\n0 & w\n\\end{bmatrix}.\n\\]\n\n\nExercise 2.18 Find all vectors in \\(\\mathbb{R}^4\\) that are perpendicular to the following three vectors. \\[\nu=\\vectorfour{1}{1}{1}{1} \\qquad\nv=\\vectorfour{1}{2}{3}{4} \\qquad  \nw=\\vectorfour{1}{9}{9}{7}\n\\]\n\n\nExercise 2.19 Determine a scalar \\(t\\) such that \\(A X=t X\\) where \\(A=\\begin{bmatrix}2 & 1 \\\\ 1 & 2 \\end{bmatrix}\\) and \\(X=\\begin{bmatrix}1 \\\\ 1\\end{bmatrix}.\\)\n\n\nExercise 2.20 Show that if \\(A\\) and \\(B\\) commute with \\(C\\), then so does \\(A+B\\).\n\n\nExercise 2.21 Show that if \\(A\\) and \\(B\\) are \\(n\\times n\\) matrices, then \\(A B=BA\\) if and only if \\((A+B)^2=A^2+2A B+B^2\\).\n\n\nExercise 2.22 Let \\(A\\) and \\(B\\) be the \\(2\\times 2\\) matrices \\[\nA=\n\\begin{bmatrix}\n\\begin{bmatrix}\n1 & 3 \\\\ 2& 1\n\\end{bmatrix}\n&\n\\begin{bmatrix}\n0 & 3 \\\\ 1 & 3\n\\end{bmatrix}\n\\\\ \\vspace{-10pt} & \\\\\n\\begin{bmatrix}\n3 & 4 \\\\\n1 & 1\n\\end{bmatrix}\n&\n\\begin{bmatrix}\n2 & 0 \\\\ 0 & 1\n\\end{bmatrix}\n\\end{bmatrix}\n\\quad\nB=\n\\begin{bmatrix}\n\\begin{bmatrix}\n3 & 3 \\\\ 1 & 1\n\\end{bmatrix}\n&\n\\begin{bmatrix}\n2 & 6 \\\\\n4 & 3\n\\end{bmatrix}\n\\\\ \\vspace{-10pt} & \\\\\n\\begin{bmatrix}\n1 & 3 \\\\ 1 & 3\n\\end{bmatrix}\n&\n\\begin{bmatrix}\n2 & 1 \\\\ 1 & 1\n\\end{bmatrix}\n\\end{bmatrix}\n\\] whose entries are themselves \\(2\\times 2\\) matrices. Where possible, find each of the following.\n\n\\(A+B\\)\n\\(-2B\\)\n\\(3A\\)\n\\(2A-3B\\)\n\\(B^2\\)\n\\(AB\\)\n\\(BA\\)\n\\(AB-BA\\)\n\\(A^2-B^2\\)\n\n\n\nExercise 2.23 Let \\(A=\\begin{bmatrix}\\cos \\theta & \\sin \\theta \\\\ -\\sin \\theta & \\cos \\theta \\end{bmatrix}\\). Find an expression for\n\n\\(A^2\\),\n\\(A^3\\), and\n\\(A^n\\) where \\(n\\) is a positive integer.\n\n\n\nExercise 2.24 The Pauli spin matrices \\[\nP_1\n=\n\\begin{bmatrix}\n0 & 1 \\\\ 1 & 0\n\\end{bmatrix},\n\\quad\nP_2\n=\n\\begin{bmatrix}\n0 & -i \\\\ i & 0\n\\end{bmatrix},\n\\quad\nP_3\n=\n\\begin{bmatrix}\n1 & 0 \\\\ 0 &-1\n\\end{bmatrix}\n\\] are used when studying electron spin. Find\n\n$P_1 P_2, $\n\n\\(P_2 P_1\\),\n\\(P_2^2\\),\n\\(P_1 P_3\\),\n\n\\(P_3 P_1\\), and\n\\(P_1+i P_2\\).\n\n\n\nExercise 2.25 Let \\(A\\) denote an arbitrary matrix and let \\(k\\) denote an arbitrary scalar. Prove the following properties hold.\n\n\\(A-A=0\\)\n\\(0A=0\\)\n\\(0A=0\\)\n\\(k I A=k A\\)\n\n\n\nExercise 2.26 Find \\(A\\vec e_1\\), \\(A\\vec e_2\\), and \\(A\\vec e_3\\) given the following. \\[\n\\vec e_1=\\vectorthree{1}{0}{0} \\quad\n\\vec e_2=\\vectorthree{0}{1}{0} \\quad\n\\vec e_3=\\vectorthree{0}{0}{1} \\qquad\n\\text{and} \\qquad\nA=\n\\begin{bmatrix}\na_1 & a_2 & a_3 \\\\\nb_1 & b_2 & b_3 \\\\\nc_1 & c_2 & c_3\n\\end{bmatrix}\n\\]\n\n\nExercise 2.27 Find a \\(3\\times 3\\) matrix \\(A\\) that satisfies all of the following. \\[\nA \\vectorthree{1}{0}{0}= \\vectorthree{1}{2}{3} \\qquad\nA \\vectorthree{0}{1}{0}= \\vectorthree{4}{5}{6}  \n\\qquad \\text{and} \\qquad\nA \\vectorthree{0}{0}{1}= \\vectorthree{7}{8}{9}\n\\]\n\n\nExercise 2.28 Find all vectors \\(\\vec x\\) such that \\(A\\vec x=\\vec b\\) given the following. \\[\nA=\n\\begin{bmatrix}\n1 & 2  & 0\\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0 \\end{bmatrix}\n\\quad \\text{ and } \\quad\n\\vec b=\\vectorthree{2}{1}{0}.\n\\]\n\n\nExercise 2.29 Write the system \\[\n\\begin{cases}\n2x_1-3x_2+5x_3 =7 \\\\\n9x_1+4x_2-6x_3 =8\n\\end{cases}\n\\] in matrix form and write \\(\\begin{bmatrix} 7 \\\\ 8 \\end{bmatrix}\\) as a linear combination of the columns vectors of the coefficient matrix.\n\n\nExercise 2.30 Show that the sum of any two diagonal matrices is diagonal.\n\n\nExercise 2.31 Show that the sum of any two upper triangular matrices is upper triangular.\n\n\nExercise 2.32 Let \\(A\\) be an \\(m\\times n\\) matrix and let \\[\nB=\\vectorfour{b_1}{b_2}{\\vdots}{b_n}\n\\qquad \\text{and} \\qquad\nC=\\begin{bmatrix} c_1 & c_2 & \\cdots & c_m\\end{bmatrix}\n\\] be a column vector and a row vector, respectively. Prove that \\(AB=\\sum_{j=1}^n b_j A_j\\) and \\(CA=\\sum_{j=1}^n c_j A_j\\).\n\n\nExercise 2.33 Let \\(A\\) and \\(B\\) denote \\(n\\times n\\) matrices. The Jordan product , \\(A\\star B\\) is defined by \\(A\\star B=\\frac{1}{2}(AB+BA)\\). Determine whether this product is commutative, associative, and/or distributive.\n\n\nExercise 2.34  Complete the proof of \\(\\ref{PropertiesofMatrixAddition}\\).\n\n\nExercise 2.35  Complete the proof of \\(\\ref{Properties of Scalar Multiplication}\\).\n\n\nExercise 2.36  Complete the proof of \\(\\ref{Properties of Matrix Multiplication}\\)."
  },
  {
    "objectID": "on-the-solutions-of-linear-systems.html#exercises",
    "href": "on-the-solutions-of-linear-systems.html#exercises",
    "title": "3  On the Solutions of Linear Systems",
    "section": "3.1 Exercises",
    "text": "3.1 Exercises\n\nExercise 3.1 Find an inconsistent system of two linear equations in three unknowns. Describe the situation geometrically.\n\n\nExercise 3.2 Find the rank of the matrix.\n\n\\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 0 & 3 \\\\ 0 & 0 & 3 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 2 & 3 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 1 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 2 & 2 & 2 \\\\ 2 & 2 & 2 \\\\ 2 & 2 & 2 \\\\ 2 & 2 & 2 \\\\ \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 4 & 7 \\\\ 2 & 5 & 8 \\\\ 3 & 6 & 9 \\end{bmatrix}\\)\n\\(\\begin{bmatrix} 1 & 4 & 7 \\\\ 2 & 5 & 8 \\\\ -3 & -6 & -9\\end{bmatrix}\\)\n\n\n\nExercise 3.3  \n\nIf the rank of a \\(4\\times 4\\) matrix \\(A\\) is 4, what is \\(\\text{rref}(A)\\)?\nIf the rank of a \\(5\\times 3\\) matrix \\(A\\) is 3, what is \\(\\text{rref}(A)\\)?\nIf the rank of a \\(2\\times 5\\) matrix \\(A\\) is 1, what is \\(\\text{rref}(A)\\)?\n\n\n\nExercise 3.4 Find the rank of the following matrices.\n\n\\(\\begin{bmatrix} a & 0 & 0 \\\\ 0 & c & 0\\\\ 0 & 0 & f\\end{bmatrix}\\)\n\\(\\begin{bmatrix}a & b & c \\\\ 0 & c & d \\\\ 0 & 0 & f \\end{bmatrix}\\)\n\\(\\begin{bmatrix} a & 0 & 0 \\\\ b & c & 0\\\\ d & e & f\\end{bmatrix}\\)\n\nwhere \\(a\\), \\(d\\), \\(f\\) are nonzero and \\(b\\), \\(c,\\) and \\(e\\) are arbitrary scalars.\n\n\nExercise 3.5 Find the rank of the system of equations.\n\n\\(\\begin{cases} x+2y+3z=0 \\\\ 2x+3y+4z=0 \\\\ 3x+4y+6z=0 \\end{cases}\\)\n\\(\\begin{cases} x+2y+3z=2\\\\ 2x+3y+4z=-2\\\\ 3x+4y+6z=2 \\end{cases}\\)\n\\(\\begin{cases} x+2y+3z=0 \\\\ 2x+3y+4z=1\\\\ 3x+4y+6z=3 \\end{cases}\\)\n\\(\\begin{cases} x+2y+3z=a\\\\ 2x+3y+4z=b \\\\ 3x+4y+6z=c \\end{cases}\\)\n\n\n\nExercise 3.6 Find all solutions to the homogenous system.\n\n\\(\\begin{cases} 4x_1-x_2=0 \\\\ 7x_1+3x_2=0\\\\ -8x_1+6x_2=0 \\end{cases}\\)\n\\(\\begin{cases}x_1-2x_2+x_3=0\\\\3x_1+2x_3+x_4=0\\\\ 4x_2-x_3-x_4=0\\\\ 5x_1+3x_3-x_4=0\\end{cases}\\)\n\\(\\begin{cases} x_1-3x_2=0\\\\ -2x_1+6x_2=0\\\\ 4x_1-12x_2=0 \\end{cases}\\)\n\\(\\begin{cases}x_1+x_2-x_3=0\\\\ 4x_1-x_2+5x_3=0\\\\ 2x_1-x_2-2x_3=0\\\\ 3x_1+2x_2-x_3=0\\end{cases}\\)\n\n\n\nExercise 3.7 Show that the homogenous system of linear equations \\[\n\\begin{cases}\na x+by=0 \\\\\ncx+dy =0\n\\end{cases}\n\\] has an infinite number of solutions if and only if \\(ad-bc=0\\).\n\n\nExercise 3.8 For any positive integer \\(n\\), find a system of \\(n\\) equations in two variables that has infinitely many solutions.\n\n\nExercise 3.9 If possible, find a condition of the coefficients of the homogenous system of linear equations so that (i) the system only has the zero solution, and (ii) the system has infinitely many solutions.\n\n\\(\\begin{cases} a_{11}x_1+a_{12}x_2+a_{13}x_3=0 \\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3=0 \\end{cases}\\)\n\\(\\begin{cases} a_{11}x_1+a_{12}x_2+a_{13}x_3=0 \\\\ a_{21}x_1+a_{22}x_2+a_{23}x_3=0 \\\\ a_{31}x_1+a_{32}x_2+a_{33}x_3=0\\end{cases}\\)\n\n\n\nExercise 3.10 Show that if a system of linear equations is inconsistent, then its reduced row echelon form has a row of the form\n\\[\n\\left[\n\\begin{array}{cccc|c}\n0 & 0 & \\cdots & 0 & 1\n\\end{array}\n\\right].\n\\]\n\n\nExercise 3.11  Prove \\(\\ref{cor:linsystmecor1}\\).\n\n\nExercise 3.12  Prove \\(\\ref{cor:linsystmecor2}\\).\n\n\nExercise 3.13 Determine the values of \\(k\\) for which the system has nontrivial solutions.\n\n\\(\\begin{cases} 2x_1-3x_2+5x_3=0\\\\ -x_1+7x_2-x_3=0\\\\ 4x_1-11x_2+k x_3=0 \\end{cases}\\)\n\\(\\begin{cases} x_1-2x_2-5x_3=0\\\\ 2x_1+3x_2+x_3=0\\\\ x_1-7x_2-k x_3=0\\end{cases}\\)\n\n\n\nExercise 3.14 Show that if \\(AX=\\vec 0\\) is a homogenous system with an infinite number of solutions and the system \\(AX=B\\) has a solution, then \\(AX=B\\) must have an infinite number of solutions.\n\n\nExercise 3.15 Find an example, where possible, for each of the following.\n\nA linear system of four equations in four unknowns that has a line as a solution set.\nA linear system of four equations in four unknowns that has a plane as a solution set.\nA linear system of four equations in three unknowns that has a line as a solution set.\nA linear system of four equations in two unknowns that has a plane as a solution set.\n\n\n\nExercise 3.16 Determine whether or not the following system is consistent. \\[\n\\begin{bmatrix}\n0 & i & 1-i \\\\\n-i & 0 & i\\\\\n1-i & -i & 0\n\\end{bmatrix}\n\\cdot\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n-1 \\\\ 0 \\\\ 1\n\\end{bmatrix}\n\\]\n\n\nExercise 3.17 Show that if \\(AX=0\\) for all vectors \\(X\\), then \\(A=0\\).\n\n\nExercise 3.18 Under what conditions will \\(k\\) planes \\(a_j x +b_j y+c_j z=d_j\\) for \\(j=1, 2, ..., k\\) intersect in exactly one point?\n\n\nExercise 3.19 Given that \\(AX=B\\) is consistent and of rank \\(r\\), for what sets of \\(r\\) unknowns can one solve?\n\n\nExercise 3.20 If possible, write the matrix \\(A\\) as a linear combination of the matrices \\[\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 1\n\\end{bmatrix}\n\\qquad \\text{and} \\qquad\n\\begin{bmatrix}\n1 & 0 \\\\\n0 & 0\n\\end{bmatrix}.\n\\]\n\n\\(A=\\begin{bmatrix}3 & 0 \\\\ 0 & 2\\end{bmatrix}\\)\n\\(A=\\begin{bmatrix} 4 & 1 \\\\ 0 & -3 \\end{bmatrix}\\)\n\\(A=\\begin{bmatrix}11 & 2 \\\\ 0 & -4\\end{bmatrix}\\)\n\n\n\nExercise 3.21 Let \\(A Z=B\\) be a given system of linear equations where \\(A_{n\\times n}\\) and \\(B_{n\\times 1}\\) are complex matrices. Let \\(Z=X+i Y\\). Show that the original complex \\(n\\times n\\) system is equivalent to the \\(2n\\times 2n\\) real system \\[\n\\begin{cases}\nCX-DY=S \\\\\nCX+DY=T\n\\end{cases}\n\\] where \\(B=S+iT\\).\n\n\nExercise 3.22  Prove \\(\\ref{lem:homosys}\\).\n\n\nExercise 3.23  Prove \\(\\ref{cor:unsol}\\)."
  },
  {
    "objectID": "vector-spaces.html",
    "href": "vector-spaces.html",
    "title": "4  Vector Spaces",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\n\nDefinition 4.1 Let \\(\\mathbb{F}\\) be a field. The collection of vectors, as \\(n\\times 1\\) matrices over \\(\\mathbb{F}\\), is called a vector space over the field of scalars \\(\\mathbb{F}\\). We denote a vector space by \\(V\\) where \\(\\mathbb{F}\\) is a positive integer \\(n\\).\n\nSince vectors are just \\(n\\times 1\\) matrices, the proof of the next two propositions follow immediately from \\(\\ref{PropertiesofMatrixAddition}\\) and \\(\\ref{Properties of Scalar Multiplication}\\).\n\nTheorem 4.1  Let \\(V\\) be a vector space. Then the following hold.\n\nFor all \\(\\vec{u}, \\vec{v}\\in V\\), \\(\\vec{u}+\\vec{v}=\\vec{v}+\\vec{u}\\).\nFor all \\(\\vec{u}, \\vec{v}, \\vec{w} \\in V\\), \\((\\vec{u}+\\vec{v})+\\vec{w}=\\vec{u}+(\\vec{v}+\\vec{w})\\).\nFor all \\(\\vec{v}\\in V\\), there exists \\(\\vec{0} \\in V\\) such that \\(\\vec{v}+\\vec{0}=\\vec{v}\\).\nFor every \\(\\vec{v}\\in V\\), there exists \\(\\vec{w}\\in V\\) such that \\(\\vec{v}+\\vec{w}=\\vec{0}\\).\n\n\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:PropertiesofVectorAddition1}\\).\n\n\nTheorem 4.2  Let \\(V\\) be a vector space. Then the following hold.\n\nFor all \\(\\vec{v}\\in V\\), \\(1\\vec{v}=\\vec{v}\\).\nFor all \\(a, b\\in k\\) and \\(\\vec{u}\\in V\\), \\((a b) \\vec{v}=a (b \\vec{v})\\).\nFor all \\(a \\in k, u, v\\in V\\), \\(a (\\vec{u}+\\vec{v})=a \\vec{u}+ a\\vec{v}\\).\n\nFor all $a, b k $ and \\(\\vec{u}\\in V\\), \\((a+b)\\vec{u}=a \\vec{u}+ b \\vec{u}\\).\n\n\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:PropertiesofVectorAddition2}\\).\n\n\nTheorem 4.3 Let \\(V\\) be a vector space. Then the following hold.\n\nThere exists a unique additive identity (denoted by \\(\\vec{0}\\)).\nEvery \\(\\vec{v}\\in V\\) has a unique additive inverse, (denoted by \\(-\\vec{v}\\)).\n\n\n\n\nProof. \nLet \\(\\vec{u}_1\\) and \\(\\vec{u}_2\\) be additive identities in \\(V\\), then \\(\\vec{v}+\\vec{u}_1=\\vec{v}\\) and \\(\\vec{v}+\\vec{u}_2=\\vec{v}\\) for every \\(\\vec{v}\\in V\\). Then, \\[\n\\vec{u}_1\n=\\vec{u}_1+\\vec{u}_2\n=\\vec{u}_2+\\vec{u}_1\n=\\vec{u}_2\n\\] as desired.\nLet \\(\\vec{v}_1\\) and \\(\\vec{v}_2\\) be additive inverses of \\(\\vec{w}\\) in \\(V\\), then \\(\\vec{w}+\\vec{v}_1=\\vec{0}\\) and \\(\\vec{w}+\\vec{v}_2=\\vec{0}\\). Then, \\[\n\\vec{v}_1\n=\\vec{v}_1+\\vec{0}\n=\\vec{v}_1+(\\vec{w}+\\vec{v}_2)\n=(\\vec{v}_1+\\vec{w})+\\vec{v}_2\n=(\\vec{w}+\\vec{v}_1)+\\vec{v}_2\n=\\vec{0}+\\vec{v}_2=\\vec{v}_2\n\\] as desired.\n\n\n\nTheorem 4.4 Let \\(V\\) be a vector space. Then the following hold.\n\nIf \\(\\vec{v}\\in V\\), then \\(0\\, \\vec{v}=\\vec{0}\\).\nIf \\(a\\in k\\), then \\(a\\, \\vec{0}=\\vec{0}\\).\n\n\n\n\nProof. \nLet \\(\\vec{v}\\in V\\), then \\[\n\\vec{v}=1 \\vec{v}=(1+0) \\vec{v}= 1 \\vec{v}+0 \\vec{v}= \\vec{v}+0\\vec{v}\n\\] which shows that \\(0 \\vec{v}\\) is the additive identity of \\(V\\), namely \\(0 \\vec{v}=\\vec{0}\\).\nLet \\(a\\in k\\), then \\[\na \\vec{0}\n=a(\\vec{0}+\\vec{0})\n=a\\vec{0}+a\\vec{0}\n\\] which shows that \\(a \\vec{0}\\) is the additive identity of \\(V\\), namely \\(a \\vec{0}=\\vec{0}\\).\n\n\n\nTheorem 4.5 Let \\(V\\) be a vector space. Then the following hold.\n\nIf \\(\\vec{v}\\in V\\), then \\(-(-\\vec{v})=\\vec{v}\\).\nIf \\(\\vec{v}\\in V\\), then \\((-1)\\, \\vec{v}=-\\vec{v}\\).\n\n\n\n\nProof. \nLet \\(\\vec{v}\\in V\\), then \\[\n\\vec{v}+(-1)\\vec{v}\n=1 \\vec{v}+(-1) \\vec{v}\n=(1+(-1)) \\vec{v}\n=0 \\vec{v}\n=\\vec{0}\n\\] which shows that \\((-1)\\vec{v}\\) is the unique additive inverse of \\(\\vec{v}\\) namely, \\((-1)\\vec{v}=-\\vec{v}\\).\nSince \\(-\\vec{v}\\) is the unique additive inverse of \\(\\vec{v}\\), \\(\\vec{v}+(-\\vec{v})=\\vec{0}\\). Then \\((-\\vec{v})+\\vec{v}=\\vec{0}\\) shows that \\(\\vec{v}\\) is the unique additive inverse of \\(-\\vec{v}\\), namely, \\(\\vec{v}=-(-\\vec{v})\\) as desired.\n\n\n\nTheorem 4.6 Let \\(V\\) be a vector space with \\(a\\in k\\) and \\(\\vec{v}\\in V\\). If \\(a\\,\\vec{v}=\\vec{0}\\), then \\(a=0\\) or \\(\\vec{v}=\\vec{0}\\).\n\n\nProof. Suppose \\(a\\neq 0\\). If \\(a v =\\vec{0}\\) then \\[\n\\vec{v}=1 \\vec{v}\n=(a^{-1} a) \\vec{v}\n=a^{-1} (a \\vec{v})\n=a^{-1} \\vec{0}\n=\\vec{0}.\n\\] Otherwise \\(a=0\\) as desired.\n\n\nExercise 4.1 Determine whether the following collection of vectors in \\(\\mathbb{R}^3\\) are linearly independent or linearly dependent.\n\n\\((0,1,1), (1,2,1), (0,4,6), (1,0,-1)\\)\n\\((0,1,0), (1,2,1), (0,-4,6), (-1,1,-1)\\)\n\n\n\nExercise 4.2 Determine whether the following collection of vectors in \\(\\mathbb{R}^4\\) are linearly independent or linearly dependent.\n\n\\((0,1,1,1), (1,2,1,1), (0,4,6,2), (1,0,-1, 2)\\)\n\\((0,1,0,1), (1,2,1,3), (0,-4,6,-2), (-1,1,-1, 2)\\)\n\n\n\nExercise 4.3 Show that the given vectors do not form a basis for the vector space \\(V\\).\n\n\\((21,-7), (-6, 1)\\); \\(V=\\mathbb{R}^2\\)\n\\((21,-7,14), (-6, 1,-4), (1,0,0)\\); \\(V=\\mathbb{R}^3\\)\n\\((48,24,108,-72), (-24, -12,-54,36), (1,0,0,0), (1,1,0,0)\\); \\(V=\\mathbb{R}^4\\)\n\n\n\nExercise 4.4 Reduce the vectors to a basis of the vector space \\(V\\).\n\n\\((1,0), (1,2), (2,4)\\), \\(V=\\mathbb{R}^2\\)\n\\((1,2,3), (-1, -10, 15), (1, 2, -3), (2,0,6), (1, -2, 3)\\), \\(V=\\mathbb{R}^3\\)\n\n\n\nExercise 4.5 Which of the following collection of vectors in \\(\\mathbb{R}^3\\) are linearly dependent? For those that are express one vector as a linear combination of the rest.\n\n\\((1,1,0), (0,2,3), (1,2,3)\\)\n\\((1,1,0), (3,4,2), (0,2,3)\\)\n\n\n\nExercise 4.6  Prove \\(\\ref{PropertiesofVectorAddition1}\\).\n\n\nExercise 4.7  Prove \\(\\ref{PropertiesofVectorAddition2}\\).\n\n\nExercise 4.8 Let \\(S=\\{v_1, v_2, ..., v_k\\}\\) be a set of vectors in a a vector space \\(V\\). Prove that \\(S\\) is linearly dependent if and only if one of the vectors in \\(S\\) is a linear combination of all other vectors in \\(S\\).\n\n\nExercise 4.9 Suppose that \\(S=\\{v_1, v_2, v_3\\}\\) is a linearly independent set of vector in a vector space \\(V\\). Prove that \\(T=\\{u_1, u_2, u_3\\}\\) is also linearly independent where \\(u_1=v_1\\), \\(u_2=v_1+v_2\\), and \\(u_3=v_1+v_2+v_3\\).\n\n\nExercise 4.10 Which of the following sets of vectors form a basis for the vector space \\(V\\).   - \\((1,3), (1,-1)\\); \\(V=\\mathbb{R}^2\\) - \\((1,3),(-2,6)\\); \\(V=\\mathbb{R}^2\\) - \\((3,2,2), (-1,2,1), (0,1,0)\\); \\(V=\\mathbb{R}^3\\) - \\((3,2,2), (-1,2,0), (1,1,0)\\); \\(V=\\mathbb{R}^3\\) - \\((2,2,2,2), (3,3,3,2), (1,0,0,0), (0,1,0,0)\\); \\(V=\\mathbb{R}^4\\) - $(1,1,2,0), (2,2,4,0), (1,2,3,1), (2,1,3,-1), (1,2,3,-1) $; \\(V=\\mathbb{R}^4\\)\n\n\nExercise 4.11 Find a basis for the subspace of the vector space \\(V\\).\n\nAll vectors of the form \\((a,b,c)\\) where \\(b=a+c\\) where \\(V=\\mathbb{R}^3\\).\nAll vectors of the form \\((a,b,c)\\) where \\(b=a-c\\) where \\(V=\\mathbb{R}^3\\).\nAll vectors of the form \\(\\vectorfour{b-a}{a+c}{b+c}{c}\\) where \\(V=\\mathbb{R}^4\\).\n\n\n\nExercise 4.12  Let \\(\\vec{v}_1=\\vectorthree{0}{1}{1}\\), \\(\\vec{v}_2=\\vectorthree{1}{0}{0}\\) and \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2)\\).\n\nIs \\(S\\) a subspace of \\(\\mathbb{R}^3\\)?\nFind a vector \\(\\vec{u}\\) in \\(S\\) other than \\(\\vec{v}_1\\), \\(\\vec{v}_2\\).\nFind scalars which verify that \\(3\\vec{u}\\) is in \\(S\\).\nFind scalars which verify that \\(\\vec{0}\\) is in \\(S\\).\n\n\n\nExercise 4.13 Let \\(\\vec{u}_1=\\vectorthree{0}{2}{2}\\), \\(\\vec{u}_2=\\vectorthree{2}{0}{0}\\) and \\(T=\\text{span}(\\vec{u}_1,\\vec{u}_2)\\). Show \\(S=T\\) by showing \\(S\\subseteq T\\) and \\(T\\subseteq S\\) where \\(S\\) is defined in Exercise \\(\\ref{vecex1}\\).\n\n\nExercise 4.14 Prove that the non-empty intersection of two subspaces of \\(\\mathbb{R}^3\\) is a subspace of \\(\\mathbb{R}^3\\).\n\n\nExercise 4.15 Let \\(S\\) and \\(T\\) be subspaces of \\(\\mathbb{R}^3\\) defined by \\[\nS=\\text{span}\\left(\\vectorthree{1}{0}{2},\\vectorthree{0}{2}{1}\\right)\n\\qquad \\text{and} \\qquad\nT=\\text{span}\\left(\\vectorthree{2}{-2}{3},\\vectorthree{3}{-4}{4}\\right).\n\\] Show they are the same subspace of \\(\\mathbb{R}^3\\).\n\n\nExercise 4.16 Let \\({\\vec{v}_1,\\vec{v}_2, \\vec{v}_3}\\) be a linearly independent set of vectors. Show that if \\(\\vec{v}_4\\) is not a linear combination of \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_3\\), then \\({\\vec{v}_1,\\vec{v}_2, \\vec{v}_3},\\vec{v}_4\\) is a linearly independent set of vectors.\n\n\nExercise 4.17 If\n\\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) is a linearly independent set of vectors in \\(V\\), show that\n\\(\\{\\vec{v}_1,\\vec{v}_1+\\vec{v}_2, \\vec{v}_1+\\vec{v}_2+\\vec{v}_3\\}\\) is also a linearly independent set of vectors in \\(V\\).\n\n\nExercise 4.18 If\n\\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) is a linearly independent set of vectors in \\(V\\), show that\n\\(\\{\\vec{v}_1+\\vec{v}_2,\\vec{v}_2+\\vec{v}_3, \\vec{v}_3+\\vec{v}_1\\}\\) is also a linearly independent set of vectors in \\(V\\).\n\n\nExercise 4.19 Let \\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) be a linearly dependent set. Show that at least one of the \\(\\vec{v}_i\\) is a linear combination of the others.\n\n\nExercise 4.20 Prove or provide a counterexample to the following statement. If a set of vectors \\(T\\) spans the vector space \\(V\\), then \\(T\\) is linearly independent.\n\n\nExercise 4.21 Which of the following are not a basis for \\(\\mathbb{R}^3\\)?\n\n\\(\\vec{v_1}=\\vectorthree{1}{0}{0}, \\vec{v_2}=\\vectorthree{0}{1}{1}, \\vec{v_3}=\\vectorthree{1}{-1}{-1}\\)\n\\(\\vec{u_1}=\\vectorthree{0}{0}{1}, \\vec{u_2}=\\vectorthree{1}{0}{1}, \\vec{u_3}=\\vectorthree{2}{3}{4}\\)\n\n\n\nExercise 4.22 Let \\(S\\) be the space spanned by the vectors \\[\n\\vec{v}_1=\\vectorfour{1}{0}{1}{1}\n\\quad\n\\vec{v}_1=\\vectorfour{-1}{-3}{1}{0}\n\\quad\n\\vec{v}_1=\\vectorfour{2}{3}{0}{1}\n\\quad\n\\vec{v}_1=\\vectorfour{2}{0}{2}{2}\n\\] Find the dimension of \\(S\\) and a subset of \\(T\\) which could serve as a basis for \\(S\\).\n\n\nExercise 4.23 Let \\(\\{\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\}\\) be a basis for \\(V\\), and suppose that \\(\\vec{u} =a_1 \\vec{v_1}+a_2 \\vec{v_2}+\\cdots + a_n \\vec{v_n}\\) with \\(a_1\\neq 0\\). Prove that \\(\\{\\vec{u}, \\vec{v}_2, ..., \\vec{v}_n\\}\\) is also a basis for \\(V\\).\n\n\nExercise 4.24 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) and \\(T=\\text{span}(\\vec{u}_1,\\vec{u}_2,\\vec{u}_3)\\) where \\(\\vec{v}_i\\) and \\(\\vec{u}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{2}{0}\n\\quad\n\\vec{v}_2=\\vectorfour{2}{1}{1}{1}\n\\quad\n\\vec{v}_3=\\vectorfour{3}{-1}{2}{-1}\n\\qquad\n\\vec{u}_1=\\vectorfour{3}{0}{3}{1}\n\\quad\n\\vec{u}_2=\\vectorfour{1}{2}{-1}{1}\n\\quad\n\\vec{u}_3=\\vectorfour{4}{-1}{5}{1}\n\\] Is one of these two subspaces strictly contained in the other or are they equal?\n\n\nExercise 4.25 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) where \\(\\vec{v}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{2}{3}{1}\n\\qquad\n\\vec{v}_2=\\vectorfour{2}{-1}{1}{-3}\n\\qquad\n\\vec{v}_3=\\vectorfour{1}{3}{4}{2}\n\\qquad \\text{and}\\qquad\n\\vec{u}=\\vectorfour{1}{2}{3}{1}\n\\] Is the vector \\(\\vec{u}\\) in \\(S\\)?\n\n\nExercise 4.26 If possible, find a value of \\(a\\) so that the vectors \\[\n\\vectorthree{1}{2}{a}\n\\qquad\n\\vectorthree{0}{1}{a-1}\n\\qquad\n\\vectorthree{3}{4}{5}\n\\qquad\n\\] are linearly independent.\n\n\nExercise 4.27 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) where \\(\\vec{v}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{2}{3}\n\\qquad\n\\vec{v}_2=\\vectorfour{1}{0}{1}{0}\n\\qquad\n\\vec{v}_3=\\vectorfour{3}{-2}{5}{7}\n\\qquad \\text{and}\\qquad\n\\vec{u}=\\vectorfour{1}{1}{0}{-1}\n\\] Find a basis of \\(S\\) which includes the vector \\(\\vec{u}\\).\n\n\nExercise 4.28 Find a vector \\(\\vec{u}\\) in \\(\\mathbb{R}^4\\) such that \\(\\vec{u}\\) and the vectors \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{-1}{1}\n\\qquad\n\\vec{v}_2=\\vectorfour{1}{0}{1}{1}\n\\qquad\n\\vec{v}_3=\\vectorfour{1}{2}{1}{1}\n\\] for a basis of \\(\\mathbb{R}^4\\).\n\n\nExercise 4.29 Show that every subspace of \\(V\\) has no more than \\(n\\) linearly independent vectors.\n\n\nExercise 4.30 Find two bases of \\(\\mathbb{R}^4\\) that have only the vectors \\(\\vec{e}_3\\) and \\(\\vec{e}_4\\) in common.\n\n\nExercise 4.31 Prove that if a list of vectors is linearly independent so is any sublist.\n\n\nExercise 4.32 Suppose \\(\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\) and \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_4\\) are two sets of linearly dependent vectors, and suppose that \\(\\vec{v}_1\\) and \\(\\vec{v}_2\\) are linearly independent. Prove that any set of three vectors chosen from \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_3, \\vec{v}_4\\) is linearly dependent.\n\n\nExercise 4.33 If \\(\\vec{u}\\) and \\(\\vec{v}\\) are linearly independent vectors in \\(V\\), prove that the vectors \\(a\\vec{u}+b\\vec{v}\\) and \\(c\\vec{u}+d\\vec{v}\\) are also linearly independent if and only if \\(ad-bc\\neq 0\\).\n\n\nExercise 4.34  Complete the proof of \\(\\ref{prop:spnlinbasis}\\).\n\n\nExercise 4.35 Let \\(U\\) be the collection of vectors that satisfy the equations \\(x+y+z=0\\) and \\(x+2y-z=0\\). Show \\(U\\) is a subspace of \\(\\mathbb{R}^3\\), find a basis for \\(U\\), and find \\(\\dim(U)\\).\n\n\nExercise 4.36 Let \\(U\\) be the collection of vectors that satisfy the equations \\(x+y+z=0\\), \\(x+2y-z=0\\), and \\(y-2z=0\\). Show \\(U\\) is a subspace of \\(\\mathbb{R}^3\\), find a basis for \\(U\\), and find \\(\\dim(U)\\).\n\n\nExercise 4.37 Show that the only subspaces of \\(\\mathbb{R}\\) are \\(\\{\\vec{0}\\}\\) and \\(\\{\\mathbb{R}\\}\\).\n\n\nExercise 4.38 Show that the only subspaces of \\(\\mathbb{R}^2\\) are \\(\\{\\vec{0}\\}\\), \\(\\{\\mathbb{R}^2\\}\\), and any set consisting of all scalar multiples of a nonzero vector. Describe these subspaces geometrically.\n\n\nExercise 4.39 Determine the various types of subspaces of \\(\\mathbb{R}^3\\) and describe them geometrically.\n\n\nExercise 4.40 For \\(\\vec{b}\\neq\\vec{0}\\), show that the set of solutions of the \\(n\\times m\\) linear system \\(A \\vec{x}=\\vec{b}\\), is not a subspace of \\(V\\).\n\n\nExercise 4.41 Suppose that \\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\) are linearly independent in \\(\\mathbb{R}^n\\). Show that if \\(A\\) is an \\(n\\times n\\) matrix with \\(\\text{rref}(A)=I_n\\), then \\(A\\vec{v}_1, A\\vec{v}_2, ..., A\\vec{v}_n\\) are also linearly independent in \\(\\mathbb{R}^n\\).\n\n\nExercise 4.42 Let \\(S=\\{\\vlist{v}{s}\\}\\) and \\(T=\\{\\vlist{u}{t}\\}\\) be two sets of vectors in \\(V\\) where each \\(\\vec{u}_i\\), \\((i=1,2,...,t)\\) is a linear combination of the vectors in \\(S\\). Show that \\(\\vec{w}=\\lincomb{a}{u}{t}\\) is a linear combination of the vectors in \\(S\\).\n\n\nExercise 4.43 Let \\(S=\\{\\vlist{v}{m}\\}\\) be a set of non-zero vectors in a vector space \\(V\\) such that every vector in \\(V\\) can be uniquely as a linear combination of the vectors in \\(S\\). Prove that \\(S\\) is a basis for \\(V\\).\n\n\nExercise 4.44 Find a basis for the solution space of the homogeneous system \\((\\lambda I_n-A)\\vec{x}=\\vec{0}\\) for the given \\(\\lambda\\) and \\(A\\).\n\n\\(\\lambda=1, A=\\begin{bmatrix} 0 & 0 & 1 \\\\ 1 & 0 & -3 \\\\ 0 & 1 & 3 \\end{bmatrix}\\)\n\\(\\lambda=2, A=\\begin{bmatrix} -2 & 0 & 0 \\\\ 0 & -2 & -3 \\\\ 0 & 4 & 5 \\end{bmatrix}\\)\n\n\n\nExercise 4.45  Prove \\(\\ref{prop:roweqtoidn}\\).\n\n\nExercise 4.46  Prove \\(\\ref{cor:explincomb}\\).\n\n\nExercise 4.47  Prove \\(\\ref{sumprop}\\)."
  },
  {
    "objectID": "introduction-to-linear-transformations.html",
    "href": "introduction-to-linear-transformations.html",
    "title": "5  Introduction to Linear Transformations",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nA linear transformation is a function of the form \\(\\vec y =A \\vec x\\) where \\(A\\) is an \\(n\\times m\\) matrix. More specifically, a linear transformation is a function that assigns to each \\(\\vec x\\in \\mathbb{R}^m\\), a unique \\(\\vec y\\in \\mathbb{R}^n\\) – and this assignment is defined by a matrix \\(A\\). When \\(A\\) is the identity matrix and \\(T(\\vec x)=A \\vec x\\) we call \\(T\\) the identity transformation .\n\nDefinition 5.1  A function \\(T\\) from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\) is called a linear transformation if there exists an \\(n\\times m\\) matrix \\(A\\) such that \\(T(\\vec x)=A \\vec x\\), for all \\(\\vec x\\) in the vector space \\(\\vec R^m\\).\n\n\nLemma 5.1 Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\), then the matrix of \\(T\\) is \\[\\begin{equation}\n\\label{trancol}\nA=\\begin{bmatrix} | &  & | \\\\ T(\\vec e_1) & \\cdots & T(\\vec e_m) \\\\ | &  & | \\end{bmatrix}\n\\end{equation}\\] where \\(\\vec e_i\\) (for \\(0\\leq i \\leq m\\)) are the standard vectors.\n\n\nProof. Suppose \\(T\\) is a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\), then there exists an \\(n\\times m\\) matrix \\(A\\) such that \\(T(\\vec x)=A\\vec x\\) for all \\(\\vec x\\in \\mathbb{R}^m\\). Let \\(\\vec e_1, ..., \\vec e_m\\) be the standard vectors of \\(\\mathbb{R}^m\\) and let \\(A=[a_{ij}]\\), then \\[\nT(\\vec e_1)=A \\vec e_1=\n\\begin{bmatrix}\na_{11} & \\cdots & a_{1m} \\\\\n\\vdots & \\cdots & \\vdots \\\\\na_{n1} & \\cdots & a_{nm}\n\\end{bmatrix}\n\\vectorthree{1}{\\vdots}{0}\n=\\vectorthree{a_{11}}{\\vdots}{a_{n1}}\n\\] \\[\n\\vdots\n\\] \\[\nT(\\vec e_m)=A \\vec e_m=\n\\begin{bmatrix}\na_{11} & \\cdots & a_{1m} \\\\\n\\vdots & \\cdots & \\vdots \\\\\na_{n1} & \\cdots & a_{nm}\n\\end{bmatrix}\n\\vectorthree{0}{\\vdots}{1}\n=\\vectorthree{a_{1m}}{\\vdots}{a_{nm}}\n\\] which are the columns of the matrix \\(A\\).\n\n\nExample 5.1 Determine the linear transformation \\(T\\) given by the system of linear equations: \\[\n\\begin{array}{l}\ny_1=  7x_1+3x_2-9x_3+8x_4 \\\\\ny_2 =  6x_1+2x_2-8x_3+7x_4 \\\\\ny_3 =  8x_1+4x_2+7x_4\n\\end{array}\n\\] The matrix of the linear transformation is \\(A=\\begin{bmatrix} 7 & 3 & -9 & 8 \\\\ 6 & 2 & -8 & 7 \\\\ 8 & 4 & 0 & 7 \\end{bmatrix}\\) since \\[\nT(\\vec e_1)=\\vectorthree{7}{6}{8},  \n\\qquad\nT(\\vec e_2)=\\vectorthree{3}{2}{4},\n\\qquad\nT(\\vec e_3)=\\vectorthree{-9}{-8}{0},\n\\qquad\nT(\\vec e_4)=\\vectorthree{8}{7}{7}.\n\\] Notice \\(T\\) is a linear transformation from \\(\\mathbb{R}^4\\) to \\(\\mathbb{R}^3\\) and \\(A\\) is a \\(3\\times 4\\) matrix.\n\n\nExample 5.2 Is the transformation \\(T(\\vec{x})=\\vec{v}\\cdot \\vec{x}\\) from \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}\\) a linear transformation? If so, find the matrix of \\(T\\). Let \\(\\vec{v}=\\vectorthree{v_1}{v_2}{v_3}\\). Then \\[\nT(\\vec{x})=\\vec{v}\\cdot \\vec{x}=\\vectorthree{v_1}{v_2}{v_3}\\cdot \\vectorthree{x_1}{x_2}{x_3}=v_1 x_1+v_2 x_2+v_3 x_3 =\n\\begin{bmatrix}v_1 & v_2 & v_3 \\end{bmatrix}\\vec{x}.\\] Therefore, by \\(\\ref{lintrdef}\\), \\(T\\) is a linear transformation with matrix \\[\n\\begin{bmatrix}v_1 & v_2 & v_3 \\end{bmatrix}.\n\\]\n\n\nExample 5.3 Is the transformation \\(T(\\vec{x})=\\vec{v}\\times \\vec{x}\\) from \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}\\) a linear transformation? If so, find the matrix of \\(T\\). Let \\(\\vec{v}=\\vectorthree{v_1}{v_2}{v_3}\\). Then \\[\nT(\\vec{x})=\\vec{v}\\times \\vec{x}=\\vectorthree{v_1}{v_2}{v_3}\\times \\vectorthree{x_1}{x_2}{x_3}\n=\\begin{bmatrix}v_2x_3 -v_3x_2\\\\ v_3x_1-v_1x_3 \\\\ v_1 x_2-v_2x_1 \\end{bmatrix}\n=\\begin{bmatrix}0 & -v_3 & v_2\\\\ v_3 & 0 & -v_1 \\\\ -v_2 & v_1 & 0 \\end{bmatrix}\\vec{x}\n\\] Therefore, by \\(\\ref{lintrdef}\\), \\(T\\) is a linear transformation with matrix \\[\n\\begin{bmatrix}0 & -v_3 & v_2\\\\ v_3 & 0 & -v_1 \\\\ -v_2 & v_1 & 0 \\end{bmatrix}.\n\\]\n\n\nTheorem 5.1  A function \\(T\\) from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\) is a linear transformation if and only if both of the following hold:\n\n\\(T(\\vec v+ \\vec w)=T(\\vec v)+T(\\vec w)\\) for all vectors \\(\\vec v\\) and \\(\\vec w\\) in \\(\\mathbb{R}^m\\), and\n\\(T(k \\vec v)=k T(\\vec v)\\) for all vectors \\(\\vec v\\) in \\(\\mathbb{R}^m\\) and all scalars \\(k\\).\n\n\n\nProof. Suppose \\(T\\) is a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\), then there exists an \\(n\\times m\\) matrix \\(A\\) such that \\(T(\\vec x)=A\\vec x\\) for all \\(\\vec x\\in \\mathbb{R}^m\\). The proof of each part follows.\n\nLet \\(\\vec u, \\vec v\\in \\mathbb{R}^m\\), then \\(T(\\vec v+\\vec w)=A (\\vec v+\\vec w)=A \\vec v+A \\vec w=T(\\vec v)+T(\\vec w)\\).\nLet \\(\\vec v\\in \\mathbb{R}^m\\) and \\(k\\in \\mathbb{R}\\). Then \\(T(k \\vec v)=A(k \\vec v)=(A k) \\vec v=k(A \\vec v)=k T(\\vec v)\\).\n\nNow suppose both (i) and (ii) hold. We need to find a matrix \\(A\\) such that \\(Tx =A \\vec x\\) for all \\(\\vec x\\in \\mathbb{R}^m\\). We can use the standard vectors in \\(\\mathbb{R}^m\\). Then by \\(\\ref{trancol}\\), \\[\\begin{align*}\nT(\\vec x)  & = T(x \\vec e_1+\\cdots + x_m \\vec e_m)\n=T(x_1\\vec e_1)+\\cdots +T(x_m\\vec e_m) \\\\\n& = x_1 T(\\vec e_1)+\\cdots +x_m T(\\vec e_m)\n=\n\\begin{bmatrix} | &  & | \\\\\nT(\\vec e_1) & \\cdots & T(\\vec e_m) \\\\\n| &  & | \\end{bmatrix}\n\\vectorthree{x_1}{\\vdots}{x_m}=A\\vec x\n\\end{align*}\\] as desired.\n\n\nExample 5.4 Write \\(\\vectorthree{-1}{1}{0}\\) as a linear combination of \\(\\vectorthree{3}{-1}{2}\\) and \\(\\vectorthree{1}{0}{1}\\). Let \\(T:\\mathbb{R}^3\\to\\mathbb{R}\\) be a linear transformation with \\(T\\vectorthree{3}{-1}{2}=5\\) and \\(T\\vectorthree{1}{0}{1}=2\\). Find \\(T\\vectorthree{-1}{1}{0}\\). Notice \\[\n\\vectorthree{-1}{1}{0}=\n(-1)\\vectorthree{3}{-1}{2}+2\\vectorthree{1}{0}{1}.\n\\] Therefore, \\[\nT\\vectorthree{-1}{1}{0}\n=T\\left((-1)\\vectorthree{3}{-1}{2}+2\\vectorthree{1}{0}{1}\\right)\n=(-1)\\, T\\vectorthree{3}{-1}{2}+2 \\, T\\vectorthree{1}{0}{1}\n=-1.\n\\]\n\n\nExample 5.5 Let \\(T:\\mathbb{R}^2\\to\\mathbb{R}^2\\) be defined by \\(T\\vectortwo{x_1}{x_2}=\\vectortwo{2x_1}{x_2^2}\\). Is \\(T\\) a linear transformation? Let \\(\\alpha=\\vectortwo{x_1}{x_2}\\) and \\(\\beta=\\vectortwo{y_1}{y_2}\\). Then \\[\\begin{align*}\nT(\\alpha+\\beta) &\n=T\\left(\\vectortwo{x_1}{x_2}+\\vectortwo{y_1}{y_2}\\right)\n=T\\vectortwo{x_1+y_1}{x_2+y_2}\n=\\vectortwo{2(x_1+y_1)}{(x_2+y_2)^2}\n\\end{align*}\\] On the other hand \\[\\begin{align*}\nT(\\alpha)+T(\\beta) &\n=T\\vectortwo{x_1}{x_2}+T\\vectortwo{y_1}{y_2}\n=\\vectortwo{2x_1}{x_2^2} + \\vectortwo{2y_1}{y_2^2}\n= \\vectortwo{2(x_1+y_1)}{x_2^2+y_2^2}\n\\end{align*}\\] Since \\(T(\\alpha+\\beta)\\neq T(\\alpha)+T(\\beta)\\), we use \\(\\ref{thm:linthm}\\) to conclude that \\(T\\) is not linear transformation.\n\n\nTheorem 5.2 Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\).\n\nIf \\(\\vec{0}_m\\) is the zero vector in \\(\\mathbb{R}^m\\), then \\(T(\\vec{0}_m)\\) is the zero vector in \\(\\mathbb{R}^n\\).\nFor all \\(\\vec{v}\\) in \\(\\mathbb{R}^m\\), \\(T(-\\vec{v})=-T(\\vec{v})\\).\nFor all \\(\\vec{u}, \\vec{v}\\) in \\(\\mathbb{R}^m\\), \\(T(\\vec{u}-\\vec{v})=T(\\vec{u})-T(\\vec{v})\\).\nFor all \\(a_1,...,a_n\\in \\mathbb{R}\\) and for all \\(\\vec{v}_1, ...., \\vec{v}_n\\in \\mathbb{R}^m\\), \\[\nT(a_1\\vec{v}_1+a_2\\vec{v}_2+\\cdots + a_n\\vec{v}_n)\n=a_1T(\\vec{v}_1)+a_2T(\\vec{v}_2)+\\cdots+a_nT(\\vec{v}_n).\n\\]\n\n\n\nProof. There proof is for the reader."
  },
  {
    "objectID": "linear-transformations-in-geometry.html",
    "href": "linear-transformations-in-geometry.html",
    "title": "6  Linear Transformations in Geometry",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nNext we give several examples of linear transformations from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\) that are commonly used in plane geometry.\n\nTheorem 6.1 Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). If the matrix of \\(T\\) is of the form \\[\n\\begin{bmatrix} k & 0 \\\\ 0 & k  \\end{bmatrix}\n\\] then \\(T\\) is a scaling transformation. If \\(k>1\\) then the scaling is called a dilation, and is called a contraction when \\(k<1\\).\n\n\nProof. The proof is left for the reader.\n\n\nExample 6.1 Is the linear transformation given by the system of linear equations \\[\n\\left\\{\n\\begin{array}{l}\ny_1=  7x_1 \\\\\ny_2 =  7x_2 \\\\\n\\end{array}\n\\right.\n\\] from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\) a scaling? The answer is yes since the matrix of the linear transformation is \\(\\begin{bmatrix} 7 & 0 \\\\ 0 & 7 \\end{bmatrix}\\), which by definition is a scaling. For example, we can write \\[\nT(\\vec x)=\\begin{bmatrix} 7 & 0 \\\\ 0 & 7 \\end{bmatrix} \\vec x.\n\\] We can use \\(T\\) to dilate the vector \\(\\vectortwo{1}{2}\\) by \\(7\\) to obtain \\[\nT\\vectortwo{1}{2}\n=\\begin{bmatrix} 7 & 0 \\\\ 0 & 7 \\end{bmatrix}\\vectortwo{1}{2}\n=\\vectortwo{7}{14}.\n\\]\n\n\nTheorem 6.2  Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). If the matrix of \\(T\\) is of the form \\[\n\\frac{1}{w_1^2+w_2^2} \\begin{bmatrix} w_1^2 & w_1 w_2 \\\\ w_1 w_2 & w_2^2  \\end{bmatrix}\n\\] then \\(T\\) is an orthogonal projection transformation onto the line \\(L\\) spanned by any nonzero vector \\(\\vec w = \\begin{bmatrix} w_1 \\\\ w_2 \\end{bmatrix}\\) parallel to \\(L\\).\n\n\nProof. Suppose line \\(L\\) is spanned by \\(\\vec w\\). We can decompose any vector \\(\\vec x\\) as \\(\\vec x^{||}+\\vec x^\\perp\\) as diagrammed: Notice \\(\\vec x^\\perp\\) is the perpendicular component so \\[\\begin{equation}\\label{perpeq}\n\\vec w \\cdot \\vec x^\\perp =0\n\\qquad \\text{or equivalently} \\qquad\n\\vec w \\cdot (\\vec x -\\vec x^{||}) =0.\n\\end{equation}\\] To project \\(\\vec x\\) onto the line \\(L\\) we notice \\[\\begin{equation}\\label{projfac}\n\\vec x^{||}=k \\vec w\n\\end{equation}\\] for some scalar \\(k\\). By substitution, of \\(\\ref{projfac}\\) into \\(\\ref{perpeq}\\) and solving for \\(k\\) we obtain \\[\\begin{equation}\\label{facdef}\nk=\\frac{\\vec w \\cdot \\vec x}{\\vec w \\cdot \\vec w}.\n\\end{equation}\\] Using \\(\\ref{projfac}\\) and \\(\\ref{facdef}\\) we define the orthogonal projection of a vector \\(\\vec x\\) onto a given line \\(L\\) as \\[\\begin{equation}\\label{projdef}\n\\text{proj}_L(\\vec x)\n=\\frac{\\vec w \\cdot \\vec x}{\\norm{\\vec w}^2}\\vec w.\n\\end{equation}\\] We would like to have \\(\\ref{projdef}\\) in the form of a matrix. To do so let \\(\\vec w=\\vectortwo{w_1}{w_2}\\) and \\(\\vec x=\\vectortwo{x_1}{x_2}\\). Then from \\(\\ref{facdef}\\) and \\(\\ref{projdef}\\) we find \\[\\begin{align*}\n\\text{proj}_L(\\vec x)\n& = \\frac{1}{\\norm{\\vec w}^2} \\left((x_1 w_1+x_2 w_2)\\vectortwo{w_1}{w_2} \\right) \\\\\n& = \\frac{1}{\\norm{\\vec w}^2} \\left((x_1 w_1\\vectortwo{w_1}{w_2}+x_2 w_2 \\vectortwo{w_1}{w_2} \\right) \\\\\n& = \\frac{1}{\\norm{\\vec w}^2} \\left(\\vectortwo{x_1 w_1^2}{x_1w_1 w_2}+\\vectortwo{x_2w_1w_2}{x_2w_2^2} \\right) \\\\\n& = \\frac{1}{\\norm{\\vec w}^2} \\vectortwo{x_1 w_1^2+x_2w_1w_2}{x_1w_1 w_2+x_2w_2^2}\n= \\frac{1}{\\norm{\\vec w}^2}\n\\begin{bmatrix} w_1^2 & w_1 w_2 \\\\ w_1 w_2 & w_2^2  \\end{bmatrix}\n\\vectortwo{x_1}{x_2}\n.\n\\tag*{ }\n\\end{align*}\\]\n\n\nExample 6.2 Find the matrix \\(A\\) of the orthogonal projection onto the line \\(L\\) spanned by \\(\\vec w = \\begin{bmatrix} 4 \\\\3 \\end{bmatrix}\\) and project the vector \\(\\vec u=\\vectortwo{1}{5}\\) onto the line \\(L\\) spanned by \\(\\vec w\\). By \\(\\ref{orthproj}\\), the matrix is \\[\nA=\\frac{1}{w_1^2+w_2^2}\n\\begin{bmatrix}\nw_1^2 & w_1 w_2 \\\\\nw_1 w_2 & w_2^2\n\\end{bmatrix}\n=\\frac{1}{25}\n\\begin{bmatrix}\n16 & 12 \\\\\n12 & 9\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n16/25 & 12/25 \\\\\n12/25 & 9/25\n\\end{bmatrix}\n.\n\\] For example, we can project the vector \\(\\vec u\\) onto the line \\(L\\) spanned by \\(\\vec w\\). The matrix \\(A\\) defines this linear transformation \\(T\\) and so to project onto the line \\(L\\) is just matrix multiplication: \\[\nT\\vectortwo{1}{5}=\\frac{1}{25}\n\\begin{bmatrix}\n16 & 12 \\\\\n12 & 9\n\\end{bmatrix}\n\\vectortwo{1}{5}\n=\\vectortwo{76/25}{57/25}.\n%\\approx \\vectortwo{3.04}{2.28}.\n\\tag*{ }\n\\]\n\n\nTheorem 6.3  Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). If the matrix of \\(T\\) is of the form \\[\n\\begin{bmatrix} 2 u_1^2-1 & 2 u_1 u_2 \\\\ 2 u_1 u_2 & 2 u_2^2 -1   \\end{bmatrix}\n\\] then \\(T\\) defines a reflection transformation about the line \\(L\\), where \\(\\vec u = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}\\) is any unit vector lying on \\(L\\).\n\n\nProof. Suppose we want to reflect \\(\\vec x\\) through the line \\(L\\) as diagrammed: Then \\[\\begin{equation}\\label{ref1}\n\\text{ref}_L(\\vec x)=\\vec x^{||}-\\vec x^\\perp  \n\\end{equation}\\] and \\[\\begin{equation}\\label{ref2}\n\\text{proj}_L(\\vec x)=\\vec x^{||}.\n\\end{equation}\\] By subtracting \\(\\ref{ref2}\\) from \\(\\ref{ref1}\\), we obtain \\[\\begin{equation}\\label{ref3}\n\\text{ref}_L(\\vec x)=2 \\text{proj}_L(\\vec x)-\\vec x.\n\\end{equation}\\] For simplicity assume \\(L\\) is a any line that passes through the origin and let \\(\\vec u\\) be a unit vector \\(\\vec u = \\begin{bmatrix} u_1 \\\\ u_2 \\end{bmatrix}\\) lying on \\(L\\). Notice \\(\\ref{projdef}\\) in the special case of a unit vector \\(\\vec u\\) becomes \\(\\text{proj}_L(\\vec x)=(\\vec u\\cdot \\vec x)\\vec u.\\) Then \\[\\begin{align*}\n\\text{ref}_L(\\vec x)& =2 \\text{proj}_L(\\vec x)-\\vec x\n=2(\\vec u\\cdot \\vec x)\\vec u-\\vec x \\\\\n& = 2(u_1x_1+u_2x_2)\\begin{bmatrix} u_1 \\\\ u_2  \\end{bmatrix}-\\begin{bmatrix} x_1 \\\\ x_2  \\end{bmatrix}\n=2u_1 x_1 \\begin{bmatrix} u_1 \\\\ u_2  \\end{bmatrix}+2u_2x_2 \\begin{bmatrix} u_1 \\\\ u_2  \\end{bmatrix}-\\begin{bmatrix} x_1 \\\\ x_2  \\end{bmatrix} \\\\\n& = \\begin{bmatrix}  \n2u_1^2x_1+2u_1 u_2x_2-x_1 \\\\ 2u_1u_2x_1+2u_2^2 u_2x_2-x_2\n\\end{bmatrix}\n= \\begin{bmatrix}  \n2u_1^2-1 & 2u_1 u_2\\\\\n2u_1 u_2 & 2u_2^2-1\n\\end{bmatrix}\n\\vectortwo{x_1}{x_2}.\n\\tag*{ }\n\\end{align*}\\]\n\n\nExample 6.3 Find the matrix \\(A\\) of a reflection through the line through the origin spanned by \\(\\vec w = \\begin{bmatrix} 4 \\\\3 \\end{bmatrix}\\) and use it to reflect \\(\\begin{bmatrix} 1 \\\\5 \\end{bmatrix}\\) about the line \\(L\\). Let \\(\\vec u=\\vectortwo{4/5}{3/5}\\). We notice \\(\\vec u\\) is a unit vector, since \\(\\norm{u}=1\\). Then, by \\(\\ref{reflmatrix}\\), the matrix we seek is \\[\nA=\\begin{bmatrix}\n7/25 & 24/25 \\\\ 24/25 & -7/25\n\\end{bmatrix}.\n\\] We can reflect the vector \\(\\begin{bmatrix} 1 \\\\5 \\end{bmatrix}\\) about the line \\(L\\) using matrix multiplication \\[\nT \\begin{bmatrix} 1 \\\\5  \\end{bmatrix}\n=\n\\frac{1}{25}\n\\begin{bmatrix}\n7 & 24 \\\\ 24 & -7\n\\end{bmatrix}\n\\begin{bmatrix} 1 \\\\5  \\end{bmatrix}\n=\\vectortwo{127/25}{-11/25}.\n%\\approx \\vectortwo{5.08}{-0.44}.\n\\tag*{ }\n\\]\n\n\nTheorem 6.4  Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). If the matrix of \\(T\\) is of the form \\[\n\\begin{bmatrix}\n\\cos \\theta & -\\sin \\theta  \\\\\n\\sin \\theta & \\cos \\theta  \\end{bmatrix}\n\\] then \\(T\\) is a (counterclockwise) rotation transformation through an angle \\(\\theta\\).\n\n\nProof. If a vector \\(\\vec x=\\vectortwo{x_1}{x_2}\\) is rotated through an angle of \\(\\pi/2\\), then a vector \\(\\vec y=\\vectortwo{-x_2}{x_1}\\) is obtained, via \\(\\vec x\\cdot \\vec y =\\vec 0\\). More generally, if we rotate (counterclockwise) a given \\(\\vec x\\) through an angle \\(\\theta\\) we determine \\[\\begin{align*}\nT(\\vec x)&=(\\cos \\theta) \\vec x+(\\sin\\theta) \\vec y\n=(\\cos \\theta)\\vectortwo{x_1}{x_2}+(\\sin \\theta)\\vectortwo{-x_2}{x_1} \\\\\n&=\\vectortwo{(\\cos \\theta)x_1-(\\sin\\theta))x_2}{(\\sin \\theta)x_1-(\\cos\\theta))x_2}\n=\\begin{bmatrix} \\cos \\theta & -\\sin \\theta  \\\\ \\sin \\theta & \\cos \\theta  \\end{bmatrix} \\vectortwo{x_1}{x_2}\n=\\begin{bmatrix} \\cos \\theta & -\\sin \\theta  \\\\ \\sin \\theta & \\cos \\theta  \\end{bmatrix} \\vec x\n\\end{align*}\\] as seen from the diagram.\n\n\nExample 6.4 Find the matrix of the linear transformation that rotates the vector \\(\\begin{bmatrix} 4 \\\\ 2 \\end{bmatrix}\\) by 30 degrees counterclockwise. By \\(\\ref{rotmatrix}\\), the matrix of this transformation is \\(A=\\begin{bmatrix} \\sqrt{3}/2 & -1/2 \\\\ 1/2 & \\sqrt{3}/2 \\end{bmatrix}.\\) We will use matrix multiplication to perform the transformation \\[\nT\\vectortwo{4}{2}\n= \\begin{bmatrix} \\frac{\\sqrt{3}}{2} & \\frac{-1}{2} \\\\\n\\frac{1}{2} & \\frac{\\sqrt{3}}{2}\n\\end{bmatrix}\n\\vectortwo{4}{2}\n=\\cos 30^\\circ \\vectortwo{4}{2}+\\sin 30^\\circ \\vectortwo{4}{2}\n=\\vectortwo{2\\left(\\sqrt{3}+1\\right)}{\\sqrt{3}+1}.\n\\approx \\vectortwo{2.46}{3.73}\n\\]\n\n\nTheorem 6.5  Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^2\\) to \\(\\mathbb{R}^2\\). If the matrix of \\(T\\) is of the form \\[\n\\begin{bmatrix} 1 & 0  \\\\ k & 1  \\end{bmatrix}\n\\qquad \\text{or} \\qquad\n\\begin{bmatrix} 1 & k  \\\\ 0 & 1  \\end{bmatrix},\n\\] where \\(k\\) is any constant, then \\(T\\) defines a vertical shear or horizontal shear transformation, respectively.\n\n\nProof. The proof is left for the reader.\n\n\nExample 6.5 Given the vector \\(\\begin{bmatrix} 2 \\\\3 \\end{bmatrix}\\) in \\(\\mathbb{R}^2\\) show geometrically a vertical shear of 2 and a horizontal shear of \\(\\frac{1}{2}\\). By \\(\\ref{shearmatrix}\\), we can apply these linear transformations using matrix multiplication by using the matrices \\(\\begin{bmatrix} 1 & 0 \\\\ 2 & 1 \\end{bmatrix}\\) and \\(\\begin{bmatrix} 1 & 1/2 \\\\ 0 & 1 \\end{bmatrix}\\). \\[\n\\label{verticalshear}\n\\text{Vertical Shear: \\quad }  T\\begin{bmatrix} 2 \\\\3  \\end{bmatrix}\n= \\begin{bmatrix} 1 & 0  \\\\ 2 & 1  \\end{bmatrix} \\begin{bmatrix} 2 \\\\3  \\end{bmatrix} = \\begin{bmatrix} 2 \\\\7  \\end{bmatrix}\n\\] \\[\n\\label{horizontalshear}\n\\text{Horizontal Shear: \\quad } T\\begin{bmatrix} 2 \\\\3  \\end{bmatrix}\n= \\begin{bmatrix} 1 & 1/2  \\\\ 0 & 1  \\end{bmatrix} \\begin{bmatrix} 2 \\\\3  \\end{bmatrix} =\\begin{bmatrix} 7/2 \\\\3  \\end{bmatrix}\n\\]\n\n\nExample 6.6 Interpret the linear transformation \\(T(\\vec x)=\\begin{bmatrix} 0 & -1 \\\\ -1 & 0 \\end{bmatrix}\\vec x\\) geometrically. The transformation has a matrix of the form \\[\n\\begin{bmatrix} 2u_1^2-1 & 2 u_1 u_2 \\\\ 2u_1 u_2 & 2u_2^2-1 \\end{bmatrix}\n\\] where \\(u_1=\\sqrt{2}/2\\) and \\(u_2=-\\sqrt{2}/2\\) since \\(2u_1^2-1=0\\), \\(2u_2^2-1=0\\), and \\(2u_1 u_2=-1\\). Since \\(|| \\vec u ||=1\\) and \\(\\vec u\\) lies on the line \\(y=x\\), then matrix \\(\\begin{bmatrix} 0 & -1 \\\\ -1 & 0 \\end{bmatrix}\\) represents the linear transformation which is a reflection through the line \\(y=x\\).\n\n\\[\\begin{align*}\n\\text{scaling} \\quad &\n\\begin{bmatrix} k & 0 \\\\ 0 & k  \\end{bmatrix} &\n\\text{shears} \\quad &  \n\\begin{bmatrix} 1 & 0  \\\\ k & 1  \\end{bmatrix}\n\\text{ or }\n\\begin{bmatrix} 1 & k  \\\\ 0 & 1  \\end{bmatrix} \\\\\n\\text{reflection} \\quad &\n\\begin{bmatrix} 2 u_1^2-1 & 2 u_1 u_2 \\\\ 2 u_1 u_2 & 2 u_2^2 -1   \\end{bmatrix} &\n\\text{rotation} \\quad &\n\\begin{bmatrix} \\cos \\theta & -\\sin \\theta  \\\\ \\sin \\theta & \\cos \\theta  \\end{bmatrix} &  \\\\\n\\text{orthogonal projection} \\quad &\n\\frac{1}{w_1^2+w_2^2} \\begin{bmatrix} w_1^2 & w_1 w_2 \\\\ w_1 w_2 & w_2^2  \\end{bmatrix} &\n\\end{align*}\\]\n\nExample 6.7 Interpret the linear transformation \\(T(\\vec x)=\\begin{bmatrix} 1 & 1 \\\\ -1 & 1 \\end{bmatrix} \\vec x\\), geometrically. Explain. This is a rotation combined with a scaling. The transformation rotates 45 degrees counterclockwise and has a scaling factor of \\(\\sqrt{2}\\)."
  },
  {
    "objectID": "invertible-linear-transformations.html",
    "href": "invertible-linear-transformations.html",
    "title": "7  Invertible Linear Transformations",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nAn \\(n\\times n\\) matrix \\(A\\) is called invertible if and only if there exists a matrix \\(B\\) such that \\(A B=I_n\\) and \\(BA=I_n\\). Using the inverse of a matrix we also define the inverse of a linear transformation.\nLet \\(T(\\vec x)=A\\vec x\\) be a linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\). If the matrix \\(A\\) has inverse \\(A^{-1}\\), then the linear transformation defined by \\(A^{-1} \\vec x\\) is called the inverse transformation of \\(T\\) and is denoted by \\(T^{-1}(\\vec x)=A^{-1}.\\)\nA function \\(T\\) from \\(X\\) to \\(Y\\) is called invertible if the equation \\(T(x)=y\\) has a unique solution \\(x\\in X\\) for each \\(y\\in Y\\). A square matrix \\(A\\) is called invertible if the linear transformation \\(\\vec y=T(\\vec x)=A\\vec x\\) is invertible. In this case, then matrix of \\(T^{-1}\\) is denoted by \\(A^{-1}\\). If the linear transformation is invertible, then its inverse is \\(\\vec x = T^{-1} (\\vec y)=A^{-1}.\\)\n\nExample 7.1 Find the inverse transformation of the following linear transformation: \\[\n\\begin{array}{rl}\ny_1 = & x_1+3x_2+3x_3 \\\\\ny_2 = & x_1+4x_2+8x_3 \\\\\ny_3 = & 2x_1+7x_2+12x_3\n\\end{array}.\n\\] To find the inverse transformation we solve for \\(x_1, x_2, x_3\\) in terms of \\(y_1,.y_2,y_3\\). To do this we find the inverse matrix of \\[\nA=\n\\begin{bmatrix}\n1 & 3 & 3 \\\\\n1 & 4 & 8 \\\\\n2 & 7 & 12\n\\end{bmatrix}.\n\\] Applying elementary-row operations, \\[\n\\begin{bmatrix}\n1 & 3 & 3 & 1 & 0 & 0 \\\\\n1 & 4 & 8 & 0 & 1 & 0 \\\\\n2 & 7 & 12 & 0 & 0 & 1\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{R_2-R_1} \\\\\n\\stackrel{\\longrightarrow}{-2R_1+R_3}\n\\end{array}\n\\begin{bmatrix}1 & 3 & 3 & 1 & 0 & 0 \\\\\n0 & 1 & 5 & 1 & 1 & 0 \\\\\n0 & 1 & 6 & -2 & 0 & 1\n\\end{bmatrix}\n\\] \\[\n\\stackrel{\\longrightarrow}{-R_2+R_3}\n\\begin{bmatrix}\n1 & 3 & 3 & 1 & 0 & 0 \\\\\n0 & 1 & 5 & -1 & 1 & 0 \\\\\n0 & 0 & 1 & -1 & -1 & 1\n\\end{bmatrix}\\stackrel{\\longrightarrow}{-5R_3+R_2}\n\\begin{bmatrix}1 & 3 & 3 & 1 & 0 & 0 \\\\\n0 & 1 & 0 & 4 & 6 & -5 \\\\\n0 & 0 & 1 & -1 & -1 & 1\n\\end{bmatrix}\n\\] \\[\n\\stackrel{\\longrightarrow}{-3R_3+R_1}\n\\begin{bmatrix}\n1 & 3 & 0 & 4 & -3 & 3 \\\\\n0 & 1 & 0 & 4 & 6 &-5 \\\\\n0 & 0 & 1 & -1 & -1 & 1\n\\end{bmatrix}\\stackrel{\\longrightarrow}{-3R_2+R_1}\n\\begin{bmatrix}\n1 & 0 & 0 & -8 & -15 & 12 \\\\\n0 & 1 & 0 & 4 & 6 & -5 \\\\\n0 & 0 & 1 & -1 & -1 & 1\n\\end{bmatrix}\\] we find \\[\nA^{-1}=\n\\begin{bmatrix}\n-8 & -15 & 12 \\\\\n4 & 6 & -5 \\\\\n1 & -1 & 1\n\\end{bmatrix}.\n\\] Therefore the requested linear transformation is \\[\n\\begin{array}{rl}\nx_1 = & -8y_1-15y_2+12y_3 \\\\\nx_2 = & 4y_1+6y_2-5y_3 \\\\\nx_3 = & -y_1-y_2+y_3.\n\\end{array}\n\\]\n\nOf course inverse transformations makes sense in terms of inverse functions; that is, if \\(T^{-1}\\) is the inverse transformation of \\(T\\) then \\((T\\circ T^{-1})(\\vec x)=\\vec x\\) and \\((T^{-1 }\\circ T)(\\vec x)=\\vec x\\). For example, for \\(T\\) given in \\(\\ref{invtranseq}\\) we illustrate \\[\n(T^{-1}\\circ T)\\vectorthree{1}{2}{3}=T^{-1}\\vectorthree{2}{4}{-5}=\\vectorthree{1}{2}{3}\n\\] as one can verify.\n\nExample 7.2 Find the inverse of the linear transformation \\[\\begin{align*}\n& y_1 = 3x_1 +5x_2 \\\\\n& y_2 =3x_1+4x_2.\n\\end{align*}\\] Reducing the system \\[\n\\begin{bmatrix}\n3x_1+5x_2& =y_1 \\\\\n3x_1+4x_2 & =y_2\n\\end{bmatrix}\n\\] we obtain \\[\n\\begin{bmatrix}\nx_1 & =-\\frac{4}{3}y_1+\\frac{5}{3}y_2 \\\\\nx_2 & = y_1-y_2\n\\end{bmatrix}.\n\\]\n\n\nTheorem 7.1 Let \\(A\\) be an \\(n\\times n\\) matrix. Then\n\n\\(A\\) is invertible if and only if rref(\\(A\\))\\(=I_n\\),\n\\(A\\) is invertible if and only if \\(\\text{rank}(A)=n\\), and\n\\(A\\) is invertible if and only if \\(A^{-1} A= I_n\\) and \\(A A^{-1}=I_n\\).\n\n\n\nProof. The proof is left for the reader.\n\nTo find the inverse of an \\(n \\times n\\) matrix \\(A\\), form the augmented matrix \\([ \\, A \\, | \\, I_n \\, ]\\) and compute $(, [ , A , | , I_n , ] , ) $. If $(, [ , A , | , I_n , ] , ) $ is of the form $(, [ , I_n , | , B , ] , ) $, then \\(A\\) is invertible and \\(A^{-1}=B\\). Otherwise \\(A\\) is not invertible. For example \\[\\begin{equation}\n\\label{invtranseq}\n=\\text{rref}\\left(\n\\begin{bmatrix}\n1 & -1 & 1 & 1 & 0 & 0 \\\\\n1 & 0 & 1 & 0 & 1 & 0 \\\\\n-1 & -2 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\right)\n=\n\\begin{bmatrix}\n1 & 0 & 0 & 2 & -2 & -1 \\\\\n0 & 1 & 0 & -1 & 1 & 0 \\\\\n0 & 0 & 1 & -2 & 3 & 1\n\\end{bmatrix}\n= [ \\,  I_3 \\,  |  \\,  B \\, ]\n\\end{equation}\\] shows \\(B=A^{-1}\\) where \\[\nB=\n\\begin{bmatrix}\n2 & -2 & -1 \\\\\n-1 & 1 & 0 \\\\\n-2 & 3 & 1\n\\end{bmatrix}\n\\] and \\[A=\n\\begin{bmatrix}\n1 & -1 & 1  \\\\\n1 & 0 & 1 \\\\\n-1 & -2 & 0\n\\end{bmatrix}\n\\] as one can verify, by showing \\(AB=I_3\\) and \\(BA=I_3\\).\n\nTheorem 7.2 Let \\(A\\) and \\(B\\) be \\(n \\times n\\) matrices. Then\n\nif \\(A\\) and \\(B\\) are invertible, then \\(B A\\) is invertible as well and \\[\n(B A)^{-1}= A^{-1}B^{-1}\n\\]\nif \\(B A= I_n\\), then \\(A\\) and \\(B\\) are both invertible, \\[\nA^{-1}=B, \\qquad B^{-1}=A, \\qquad \\text{ and } \\qquad AB = I_n.\n\\]\n\n\n\nProof. The proof is left for the reader.\n\n\nExample 7.3 Find the inverse matrices of \\[\nA=\n\\begin{bmatrix} 2 & 3 \\\\ 6 & 9 \\end{bmatrix}\n\\] and \\[\nB=\n\\begin{bmatrix} 1 & 2 \\\\ 3 & 9 \\end{bmatrix}\n\\] Since \\(\\text{rref}(A)=\\begin{bmatrix} 1/2 & 3/2 \\\\ 0 & 0 \\end{bmatrix}\\neq I_2\\), \\(A^{-1}\\) does not exist. The inverse of \\(B\\) does exist and \\(B^{-1}=\\begin{bmatrix} 3 & -2/3 \\\\ -1 & 1/3 \\end{bmatrix}\\) since \\(B^{-1}B=I_2\\) and \\(B B^{-1}=I_2\\).\n\n\nExample 7.4  Show that \\(A=\\begin{bmatrix} a & b \\\\ c& d \\end{bmatrix}\\) is invertible if and only if \\(a d- b c \\neq 0\\) and when possible \\[\\begin{equation}\n\\label{twodet}\nA^{-1}=\\frac{1}{a d - b c}\\begin{bmatrix} d & -b \\\\ -c & a \\end{bmatrix}.\n\\end{equation}\\] We proceed to find the inverse: \\[\n\\begin{bmatrix}\na & b  & 1 & 0 \\\\\nc & d & 0 & 1\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\frac{1}{a} R_1}\n\\\\\n\\stackrel{\\longrightarrow}{\\frac{1}{c} R_2}\n\\end{array}\n\\begin{bmatrix}\n1 & \\frac{b}{a} & \\frac{1}{a} & 0 \\\\\n1 & \\frac{d}{c} & 0 & \\frac{1}{c}\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{-R_1+R_2}\n\\end{array}\n\\begin{bmatrix}\n1 & \\frac{b}{a} & \\frac{1}{a} & 0 \\\\\n0 & \\frac{ad-bc}{ac} & \\frac{-1}{a} & \\frac{1}{c}\n\\end{bmatrix}\n\\] \\[\n\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\frac{ac}{ad-bc} R_2}\n\\end{array}\n\\begin{bmatrix}\n1 & \\frac{b}{a} & \\frac{1}{a} & 0 \\\\\n0 & 1 & \\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{\\frac{-b}{a}R_2+R_1}\n\\end{array}\n\\begin{bmatrix}\n1 & 0 & \\frac{d}{ad-bc} & \\frac{-b}{ad-bc} \\\\\n0 & 1 & \\frac{-c}{ad-bc} & \\frac{a}{ad-bc}\n\\end{bmatrix}\n\\] Therefore, \\(A\\) is invertible if and only if \\(a d- b c \\neq 0\\) and \\(\\ref{twodet}\\) holds.\n\n\nExample 7.5 For which values of constants \\(a, b, c,\\) is the matrix \\[\nA=\n\\begin{bmatrix}\n0 & a & b \\\\\n-a & 0 & c \\\\\n-b & -c & 0\n\\end{bmatrix}\n\\] invertible? Suppose \\(a\\neq 0\\). Applying row-operations \\[\n\\begin{bmatrix}\n0 & a & b & 1 & 0 & 0\\\\\n-a & 0 & c & 0 & 1 & 0 \\\\\n-b & -c & 0 & 0 & 0 & 1\n\\end{bmatrix}\\begin{array}{c}\n\\stackrel{\\longrightarrow}{R_2\\leftrightarrow R_1}\n\\end{array}\n\\begin{bmatrix}\n-a & 0 & c & 0 & 1 & 0 \\\\\n0 & a & b & 1 & 0 & 0\\\\\n-b & -c & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\] \\[\n\\stackrel{\\longrightarrow}{-\\frac{1}{a}R_1}\n\\begin{bmatrix}\n1 & 0 & -\\frac{c}{a} & 0 & -\\frac{1}{a} & 0 \\\\\n0 & a & b & 1 & 0 & 0\\\\\n-b & -c & 0 & 0 & 0 & 1\n\\end{bmatrix}\\stackrel{\\longrightarrow}{bR_1+R_3}\n\\begin{bmatrix}\n1 & 0 & -\\frac{c}{a} & 0 & -\\frac{1}{a} & 0 \\\\\n0 & a & b & 1 & 0 & 0\\\\\n0 & -c & \\frac{-bc}{a} & 0 & \\frac{-b}{a} & 1\n\\end{bmatrix}\n\\] \\[\n\\stackrel{\\longrightarrow}{\\frac{1}{a}R_2}\n\\begin{bmatrix}\n1 & 0 & -\\frac{c}{a} & 0 & -\\frac{1}{a} & 0 \\\\\n0 & 1 & \\frac{b}{a} & \\frac{1}{a} & 0 & 0\\\\\n0 & -c & \\frac{-bc}{a} & 0 & \\frac{-b}{a} & 1\n\\end{bmatrix}\\stackrel{\\longrightarrow}{cR_2+R_3}\n\\begin{bmatrix}\n1 & 0 & -\\frac{c}{a} & 0 & -\\frac{1}{a} & 0 \\\\\n0 & 1 & \\frac{b}{a} & \\frac{1}{a} & 0 & 0\\\\\n0 & 0 & 0 & \\frac{c}{a} & \\frac{-b}{a} & 1\n\\end{bmatrix}\n\\] Thus, if \\(a\\neq 0\\) then \\(A\\) is not invertible, since \\(\\text{rref}{(A)}\\neq I_3\\). If \\(a=0\\), then clearly, \\(\\text{rref}{(A)}\\neq I_3\\), and so \\(A\\) is not invertible in either case. Therefore, there are no constants \\(a, b, c\\) for which \\(A\\) is invertible.\n\n\nCorollary 7.1 Let \\(A\\) be an \\(n \\times n\\) matrix.\n\nConsider a vector \\(\\vec b\\) in \\(\\mathbb{R}^n\\). If \\(A\\) is invertible, then the system \\(A \\vec x = \\vec b\\) has the unique solution \\(\\vec x = A^{-1} b\\). If \\(A\\) is non-invertible, then the system \\(A \\vec x = \\vec b\\) has infinitely many solutions or none.\nThe system \\(A \\vec x = \\vec 0\\) has \\(\\vec x = \\vec 0\\) as a solution. If \\(A\\) is invertible, then this is the only solution. If \\(A\\) is non-invertible, then the system \\(A \\vec x= \\vec 0\\) has infinitely many solutions.\n\n\n\nProof. The proof is left for the reader.\n\n\nExample 7.6 Find all invertible matrices \\(A\\) such that \\(A^2=A\\). Since \\(A\\) is invertible we multiply by \\(A^{-1}\\) to obtain: \\[\nA=IA=(A^{-1}A)A=A^{-1}(A^2)=A^{-1}A=I_n\n\\] and therefore \\(A\\) must be the identity matrix.\n\n\nExample 7.7 For which values of constants \\(b\\) and \\(c\\) is the matrix \\[\nB=\n\\begin{bmatrix}0 & 1 & b \\\\\n-1 & 0 & c \\\\\n-b & -c & 0\n\\end{bmatrix}\n\\] invertible? The matrix \\(B\\) is not invertible for any \\(b\\) and \\(c\\) since\\[\n\\text{rref}(B)=\n\\begin{bmatrix}1 & 0 & -c \\\\\n0 & 1 & b \\\\\n0 & 0 & 0\n\\end{bmatrix}\\neq I_3\n\\] for all \\(b\\) and \\(c\\).\n\n\nExample 7.8 Find the matrix \\(A\\) satisfying the equation \\[\n\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\nA\n\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\n=\n\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\n.\\] Let \\(B=\\begin{bmatrix} 1 & 0 \\\\ 0 & -1 \\end{bmatrix}\\) and \\(C=\\begin{bmatrix} 2 & 0 \\\\ 0 & -2 \\end{bmatrix}\\). Then \\[\nB^{-1}=\\begin{bmatrix} 1& 0 \\\\ 0 &-1\\end{bmatrix}\n\\qquad \\text{and}\\qquad\nC^{-1}=\\begin{bmatrix} 1/2 & 0 \\\\ 0 & -1/2 \\end{bmatrix}.\n\\] Multiplying on the right by \\(B^{-1}\\) and on the left by \\(C^{-1}\\) we find \\[\nA=B^{-1}\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}C^{-1}\n=\\begin{bmatrix} 1/2 & -1/2 \\\\ -1/2 & 1/2\\end{bmatrix}.\n\\]\n\n\nExample 7.9 Suppose that \\(A\\), \\(B\\), and \\(C\\) are \\(n\\times n\\) matrices and that both \\(A\\) and \\(B\\) commute with \\(C\\). Show that \\(AB\\) commutes with \\(C\\).\nTo show that \\(AB\\) commutes with \\(C\\) we need to show \\((AB)C=C(AB)\\). This is easy since \\[\n(AB)C=A(BC)=A(CB)=(AC)B=(CA)B=C(AB).\n\\] Can you justify each step?\n\n\nExample 7.10 Show that \\(AB=BA\\) if and only if \\((A-B)(A+B)=A^2-B^2\\). Suppose \\(AB=BA\\) we will show \\((A-B)(A+B)=A^2-B^2\\). Starting with the left-hand side we obtain \\[\\begin{align*}\n(A-B)(A+B)\n& =(A-B)A+(A-B)B\n=A^2-BA+AB-B^2 \\\\\n& =A^2-BA+BA-B^2\n=A^2-B^2\n\\end{align*}\\] Now suppose \\((A-B)(A+B)=A^2-B^2\\), we will show \\(AB=BA\\). This is easy since \\[\n(A-B)(A+B)\n%=(A-B)A+(A-B)B\n=A^2-BA+AB-B^2\n=A^2-B^2\n\\] implies \\(-BA+AB=0\\) as desired."
  },
  {
    "objectID": "subspaces.html",
    "href": "subspaces.html",
    "title": "8  Subspaces",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\n\nDefinition 8.1  A subset \\(U\\) of a vector space \\(V\\) is called a subspace of \\(V\\) if it has the following three properties\n\n\\(U\\) contains the zero vector in \\(V\\),\n\\(U\\) is closed under addition: if \\(\\vec{u}\\) and \\(\\vec{v}\\) are in \\(U\\) then so is \\(\\vec{u}+\\vec{v}\\), and\n\\(U\\) is closed under scalar multiplication: if \\(\\vec{v}\\) is in \\(U\\) and \\(a\\) is any scalar, then \\(a\\vec{v}\\) is in \\(U\\).\n\n\n\nExample 8.1 Let \\(U\\) be the subset of \\(\\mathbb{R}^5\\) defined by \\[\nU=\\{(x_1,x_2,x_3,x_4,x_5)\\in\\mathbb{R}^5 \\, \\mid \\,x_1=3x_2 \\text{ and } x_3=7x_4\\}.\n\\] Show \\(U\\) is a subspace of \\(\\mathbb{R}^5.\\) The zero vector \\(\\vec{0}=(0,0,0,0,0,0)\\) is in \\(U\\) since \\(0=3(0)\\) and \\(0=7(0)\\). Let \\(\\vec{u}=(u_1, u_2, u_3, u_4, u_5)\\) and \\(\\vec{v}=(v_1, v_2, v_3, v_4, v_5)\\) be vectors in \\(U\\) and let \\(a\\) be a scalar. Then \\[\n\\vec{u}+\\vec{v}=(u_1+v_1, u_2+v_2, u_3+v_3, u_4+v_4, u_5+v_5)\n\\] is in \\(U\\) since \\[\nu_1=3 u_2 \\text{ and } v_1=3 v_2 \\text{ imply } u_1+v_1=3 (u_2+v_2)\\] and \\[\nu_3=7 u_4 \\text{ and } v_3=7 v_4 \\text{ imply } u_3+v_3=7 (u_4+v_4).\\] Also, \\(a \\vec{u}\\) is in \\(U\\) since \\(u_1=3u_2\\) implies \\(a u_1=3 (a u_2)\\). By \\(\\ref{subdef}\\), \\(U\\) is a subspace of \\(\\mathbb{R}^5\\).\n\n\nExample 8.2 Give an example of a nonempty subset \\(U\\) of \\(\\mathbb{R}^2\\) such that \\(U\\) is closed under addition and under taking additive inverses, but \\(U\\) is not a subspace of \\(\\mathbb{R}^2\\). The subset \\(\\mathbb{Z}^2\\) of \\(\\mathbb{R}^2\\) is closed under additive inverses and addition, however \\(\\mathbb{Z}^2\\) is not a subspace of \\(\\mathbb{R}^2\\) since \\(\\sqrt{2} \\in \\mathbb{R}, (1,1)\\in \\mathbb{Z}^2\\) however \\((\\sqrt{2} , \\sqrt{2}) \\not \\in \\mathbb{Z}^2\\).\n\n\nExample 8.3 Give an example of a nonempty subset \\(U\\) of \\(\\mathbb{R}^2\\) such that \\(U\\) is closed under scalar multiplication, but \\(U\\) is not a subspace of \\(\\mathbb{R}^2\\). The set \\(\\{(x_1,x_2)\\in \\mathbb{R}^2 \\mid x_1 x_2=0\\}=M\\) is closed under scalar multiplication because, if \\(\\lambda\\in \\mathbb{R}\\) and \\((x_1,x_2)\\in M\\), then \\((\\lambda x_1, \\lambda x_2)\\in M\\) holds since \\(\\lambda x_1 \\lambda x_2=0\\). However, \\(M\\) is not a subspace because \\((0,1)+(1,0)=(1,1)\\not \\in M\\) even though \\((0,1),(1,0)\\in M\\).\n\n\nExample 8.4 Show that the set of all solutions of an \\(m\\times n\\) homogenous linear system of equations is a subspace of \\(V\\) (called the null space ). Let \\(A\\vec{x}=\\vec{0}\\) be an \\(m\\times n\\) homogenous system of linear equations and let \\(U\\) be the set of solutions to this system. Of course \\(A\\vec{0}=\\vec{0}\\) and so the zero vector is in \\(U\\). Let \\(\\vec{u}\\) and \\(\\vec{v}\\) be in \\(U\\) and let \\(a\\) be a scalar. Then \\[\nA(\\vec{u}+\\vec{v})=A\\vec{u}+A\\vec{v}=\\vec{0}+\\vec{0}=\\vec{0}\n\\] and \\[\nA(a \\vec{u})=a(A\\vec{u})=a\\vec{0}=\\vec{0}\n\\] shows \\(\\vec{u}+\\vec{v}\\) and \\(a\\vec{u}\\) are in \\(U\\). By \\(\\ref{subdef}\\), \\(U\\) is a subspace of \\(V\\).\n\n\nDefinition 8.2  Let \\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_m\\) be vectors in the vector space \\(V\\). The set of all linear combinations \\[\n\\text{span}(\\vlist{v}{m})\n=\\left\\{\n\\lincomb{c}{v}{m}\n\\mid \\vlist{c}{m}\\in k \\right\\}\n\\] is called the spanning set of the vectors \\(\\vlist{v}{m}\\).\n\n\nExample 8.5 Show that the spanning set of the vectors \\(\\vlist{v}{m}\\) in \\(V\\) is a subspace of \\(V\\).\nLet \\(U=\\text{span}(\\vlist{v}{m})\\). Notice \\(\\vec{0}\\in U\\) since \\(\\vec{0}=\\lincomb{0}{v}{m}\\) where \\(0\\in k\\). Let \\(\\vec{u}\\) and \\(\\vec{v}\\) be vectors in \\(U\\) and let \\(a\\) be a scalar. By \\(\\ref{spandef}\\), there exists scalars \\(\\vlist{c}{m}\\) and scalars\n\\(\\vlist{d}{m}\\) such that \\[\n\\vec{u}\n=\\lincomb{c}{v}{m}\n\\quad \\text{ and } \\quad\n\\vec{v}\n=\\lincomb{d}{v}{m}\n\\] Then \\[\n\\vec{u}+\\vec{v}=\\sum_{i=1}^m c_i \\vec{v}_i+\\sum_{i=1}^m d_i \\vec{v}_i=\\sum_{i=1}^m (c_i+d_i) \\vec{v}_i\n\\] and \\[\na\\vec{u}=a\\left(\\sum_{i=1}^m c_i \\vec{v}_i\\right)=\n\\sum_{i=1}^m (a c_i) \\vec{v}_i\n\\] show \\(\\vec{u}+\\vec{v}\\) and \\(a\\vec{u}\\) are in \\(U\\); and thus \\(U\\) is a subspace of \\(V\\).\n\nWe say that a nonzero \\(\\vec{v}_i\\) is in the list \\(\\vec{v}_1 , \\ldots, \\vec{v}_i, ..., \\vec{v}_m\\) if \\(\\vec v_i\\) can be written as a linear combination of the other nonzero vectors in the list. An equation of the form \\(a_1 \\vec{v}_1 + \\cdots +a_m \\vec{v}_m = \\vec{0}\\) is called a linear relation among the vectors \\(\\vec{v}_1 , ..., \\vec{v}_m\\); and is called a nontrivial relation if at least one of the \\(a_i\\)’s is nonzero.\n\nDefinition 8.3  The vectors \\(\\vlist{v}{m}\\) in \\(V\\) are called linear independent if the only choice for \\[\n\\lincomb{a}{v}{m}= \\vec{0}\n\\] is \\(a_1 = a_2=\\cdots =a_m = 0\\). Otherwise the vectors \\(\\vlist{v}{m}\\) are called linear dependent.\n\n\nLemma 8.1  Show that any set \\(S=\\{\\vlist{v}{m}\\}\\) of vectors in \\(V\\) is a linearly dependent set of vectors if and only if at least one of the vectors in the set can be written as a linear combination of the others.\n\n\nProof. Assume the vectors in the set \\(S\\) are linearly dependent. By \\(\\ref{lindepdef}\\), there exists scalars \\(c_1, c_2, ..., c_m\\) (not all zero) such that \\(\\lincomb{c}{v}{m}=\\vec{0}\\). Let \\(i\\) be the least index such that \\(c_i\\) is nonzero. Thus \\(c_1=c_2=\\cdots =c_{i-1}=0\\). So \\[\nc_i \\vec{v}_i=-c_{i+1}\\vec{v}_{i+1}-\\cdots -c_m \\vec{v}_m\n\\] for some \\(i\\). Since \\(c_i\\neq 0\\) and \\(c_i\\in k\\), \\(c_i^{-1}\\) exists and thus \\[\n\\vec{v}_i\n=\\left(\\frac{-c_{i+1}}{c_i}\\right)\\vec{v}_{i+1}\n+ \\cdots +\n\\left(\\frac{-c_m}{c_i}\\right)\\vec{v}_m\n\\] which shows \\(\\vec{v}_i\\) is a linear combination of the others, via \\[\n\\vec{v}_i= 0\\vec{v}_1+\\cdots+0\\vec{v}_{i-1}+\\left(\\frac{-c_{i+1}}{c_i}\\right)\\vec{v}_{i+1}\n+ \\cdots +\n\\left(\\frac{-c_m}{c_i}\\right)\\vec{v}_m.\n\\] Now assume one of the vectors in the set \\(S\\) can be written as a linear combination of the others, say \\[\n\\vec{v}_k\n=c_1 \\vec{v}_1+\\cdots\n+c_{k-1} \\vec{v}_{k-1}\n+c_{k+1} \\vec{v}_{k+1}\n+\\cdots\n+ c_m \\vec{v}_m\n\\] where \\(c_1, c_2, \\ldots, c_m\\) are scalars. Thus, \\[\n\\vec{0}=c_1 \\vec{v}_1+\\cdots\n+ c_{k-1} \\vec{v}_{k-1}\n+(-1) \\vec{v}_k\n+c_{k+1} \\vec{v}_{k+1}\n+\\cdots + c_m \\vec{v}_m\n\\] and so by \\(\\ref{lindepdef}\\), \\(\\vec{v}_1, ..., \\vec{v}_m\\) are linearly dependent.\n\nFor a list of vectors \\(\\vlist{v}{m}\\) in \\(V\\) the following equivalent statements follow from the appropriate definitions:\n\nvectors \\(\\vlist{v}{m}\\) are linearly independent,\nnone of the vectors \\(\\vlist{v}{m}\\) are redundant,\nnone of the vectors \\(\\vlist{v}{m}\\) can be written as a linear combination of the other vectors in the list,\nthere is only the trivial relation among the vectors \\(\\vlist{v}{m}\\),\nthe only solution to the equation \\(\\lincomb{a}{v}{m}\\) is \\(a_1 = a_2=\\cdots =a_m= 0\\), and\n\\(\\text{rank}(A)=n\\) where \\(A\\) is the \\(n\\times m\\) matrix whose columns are the vectors \\(\\vlist{v}{m}\\).\n\n\nExample 8.6 Determine whether the following vectors \\(\\vec{u}\\), \\(\\vec{v}\\), and \\(\\vec{w}\\) are linearly independent. \\[\n\\vec{u}=\\vectorfour{1}{1}{1}{1}\n\\qquad\n\\vec{v}=\\vectorfour{1}{2}{3}{4}\n\\qquad\n\\vec{w}=\\vectorfour{1}{4}{7}{10}\n\\] Without interchanging rows, we use elementary row operations to find \\[\n\\text{rref}\n\\begin{bmatrix}\n\\vec{u} & \\vec{v} & \\vec{w}\n\\end{bmatrix}\n=\\begin{bmatrix}1 & 0 & -2 \\\\ 0 & 1 & 3 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix}.\n\\] From this we infer the nontrivial relation \\(\\vec 0=(-2)\\vec{u}+(3)\\vec{v}+(-1)\\vec{w}.\\) Therefore the given vectors are linearly dependent.\n\n\nTheorem 8.1 Let \\[\n\\vec{v}_1=\\vectorfour{a_{11}}{a_{21}}{\\vdots}{a_{n1}}\n\\quad\n\\vec{v}_2=\\vectorfour{a_{12}}{a_{22}}{\\vdots}{a_{n2}}\n\\quad\n\\cdots\n\\quad\n\\vec{v}_m=\\vectorfour{a_{1s}}{a_{2m}}{\\vdots}{a_{nm}}\n\\] be \\(s\\) vectors in \\(V\\). These vectors are linearly dependent if and only if there exists a solution to the system of linear equations \\[\\begin{equation}\n\\label{lincomsys}\n\\begin{cases}\na_{11}x_1+a_{12}x_2+\\cdots+a_{1m}x_m=0 \\\\\na_{21}x_1+a_{22}x_2+\\cdots+a_{2m}x_m=0 \\\\\n\\qquad \\qquad \\vdots \\\\\na_{n1}x_1+a_{n2}x_2+\\cdots+a_{nm}x_m=0 \\\\\n\\end{cases}\n\\end{equation}\\] different from \\(x_1=x_2=\\cdots=x_m=0\\).\n\n\nProof. Assume \\(\\vlist{v}{m}\\) are linear dependent. By \\(\\ref{lindepdef}\\), there exists scalars \\(\\vlist{c}{m}\\) such that \\[\\begin{equation}\n\\label{lincomeq}\n\\lincomb{c}{v}{m}=\\vec{0}\n\\end{equation}\\] and not all \\(c_i\\)’s are zero. \\(\\ref{lincomeq}\\) yields a system \\[\n\\begin{cases}\na_{11}c_1+a_{12}c_2+\\cdots+a_{1m}c_m=0 \\\\\na_{21}c_1+a_{22}c_2+\\cdots+a_{2m}c_m=0 \\\\\n\\qquad \\qquad \\vdots \\\\\na_{n1}c_1+a_{n2}c_2+\\cdots+a_{nm}c_m=0 \\\\\n\\end{cases}\n\\] with solution \\(x_1=c_1\\), \\(x_2=c_2\\), …, \\(x_m=c_m\\). Since not all \\(c_i\\)’s are zero we have a solution different from \\(x_1=x_2=\\cdots=x_m=0\\). Assume the system in \\(\\ref{lincomsys}\\) has a solution \\(\\vec{x}^*\\) with \\(\\vec{x}^*\\neq \\vec{0}\\), say \\(x_i^*\\). By \\(\\ref{colvecmat}\\) we can write \\[\\begin{equation}\n\\vec{0}=A\\vec{x}=\n\\lincomb{x^*}{v}{m}\n\\end{equation}\\] Isolating the term \\(x_i^*\\vec{v}_i\\) yields \\[\\begin{equation}\nx_i^* \\vec{v}_i=-x_i^*\\vec{v}_1-\\cdots -x_{i-1}\\vec{v}_{i-1}-x_{i+1}\\vec{v}_{i+1} -\\cdots -x_m^*\\vec{v}_m\n\\end{equation}\\] and since \\(x_i^*\\neq 0\\), \\((x_i^*)^{-1}\\) exists. Therefore, \\[\\begin{equation}\n\\vec{v}_1=\\left(-\\frac{x_1^*}{x^*_i}\\right)\\vec{v}_1-\\cdots -\\left(-\\frac{x_{i-1}^*}{x^*_i}\\right)\\vec{v}_{i+1}-\\cdots - \\left(-\\frac{x_{m}^*}{x^*_i}\\right)\\vec{v}_{m}\n\\end{equation}\\] shows the vectors \\(\\vlist{v}{m}\\) are linearly dependent.\n\n\nTheorem 8.2 The \\(n\\times m\\) linear system of equations \\(A\\vec{x}=\\vec{b}\\) has a solution if and only if the vector \\(\\vec{b}\\) is contained in the subspace of \\(V\\) generated by the column vectors of \\(A\\).\n\n\nProof. Let \\(\\vec{x}\\) be a solution to \\(A\\vec{x}=\\vec{b}\\) with \\(A=\\begin{bmatrix}\\vec{v}_1 & \\vec{v}_2 & \\cdots & \\vec{v}_m \\end{bmatrix}\\). By \\(\\ref{colvecmat}\\), \\(\\vec{b}=A\\vec{x}=\\lincomb{x}{v}{m}\\) and thus \\(\\vec{b}\\in \\text{span}(\\vlist{v}{m})\\) as needed. Conversely, assume \\(\\vec{b}\\) is in the subspace generated by the column vectors of \\(A\\); that is assume \\(\\vec{b}\\in \\text{span}(\\vlist{v}{m})\\). By \\(\\ref{spandef}\\), there exists scalars \\(\\vlist{c}{m}\\) such that \\(\\vec{b}=\\lincomb{c}{v}{m}\\). By \\(\\ref{spandef}\\), \\(\\vec{b}=\\lincomb{c}{v}{m}=A\\vec{c}\\) where the components of \\(\\vec{c}\\) are the \\(c_i\\)’s. Thus the system \\(A\\vec{x}=\\vec{b}\\) has a solution, namely \\(\\vec{c}\\).\n\n\nExample 8.7 Let \\(U\\) and \\(V\\) be finite subsets of a vector space \\(V\\) with \\(U\\subseteq V\\).\n\nIf \\(U\\) is linear dependent, then so is \\(V\\).\nIf \\(V\\) is linear independent, then so is \\(U\\).\n\nLet \\(U=\\{\\vlist{u}{s}\\}\\) \\(V=\\{\\vlist{v}{t}\\}\\).\n\nIf \\(U\\) is linear dependent, then thee exists a vector, say \\(\\vec{u}_k\\) such that \\(\\vec{u}_k\\) is a linear combination of the other \\(\\vec{u}_i\\)’s. Since \\(U\\subseteq V\\) all \\(\\vec{u}_i\\)’s are in \\(V\\). Thus we have a vector \\(\\vec{u}_k\\) in \\(V\\) that is a linear combination of other vectors in \\(V\\). Therefore, by \\(\\ref{lindepothers}\\), \\(V\\) is linear dependent.\nLet \\(\\vlist{c}{s}\\) be scalars such that \\[\\begin{equation}\n\\label{lincombcus}\n\\lincomb{c}{u}{s}=\\vec{0}.\n\\end{equation}\\] Since \\(U\\subseteq V\\), we know \\(u_i\\in V\\) for \\(1\\leq i \\leq s\\). Since \\(V\\) is linear independent, \\(\\ref{lincombcus}\\) implies \\(c_1=c_2=\\cdots =c_m=0\\). By \\(\\ref{lindepdef}\\), \\(U\\) is linear independent as well.\n\n\n\nCorollary 8.1  Any vector in \\(V\\), written as a column matrix, can be expressed (uniquely) as a linear combination of \\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_m\\) if and only if \\(A \\vec{x}=\\vec{v}\\) has unique solution, where \\(A=\\begin{bmatrix}\\vec{v}_1 & \\vec{v}_2 & \\cdots & \\vec{v}_m\\end{bmatrix}\\). When there is a solution, the components \\(x_1, x_2, ...., x_m\\) of \\(\\vec{x}\\) give the coefficients for the linear combination.\n\n\nProof. This proof is left for the reader as Exercise \\(\\ref{ex:explincomb}\\).\n\n\nTheorem 8.3  The vectors \\(\\vlist{v}{n}\\) in \\(V\\) form a linearly independent set of vectors if and only if \\(\\begin{bmatrix}\\vec{v}_1& \\vec{v}_2 & \\cdots & \\vec{v}_n \\end{bmatrix}\\) is row equivalent to \\(I_n\\).\n\n\nProof. This proof is left for the reader as Exercise \\(\\ref{ex:roweqtoidn}\\).\n\n\nTheorem 8.4  Let \\(V\\) be a vector space and assume that the vectors \\(\\vlist{v}{n}\\) are linearly independent and \\(\\text{span}(\\vlist{s}{m})=V\\). Then \\(n\\leq m\\).\n\n\nProof. We are given \\[\n\\text{span}(\\vlist{s}{m})=V\n\\quad \\text{and} \\quad\n\\vlist{v}{n} \\text{ are linearly independent.}\n\\] Since \\(\\vec{v}_1\\) is a linear combination of the vectors \\(\\vec{s}_1\\), \\(\\vec{s}_2\\), …., \\(\\vec{s}_m\\) we obtain \\[\n\\text{span}(\\vec{v}_1,\\vec{s}_2,...,\\vec{s}_m)=V\n\\quad \\text{and} \\quad\n\\vec{v}_2, ..., \\vec{v}_n \\text{ are linearly independent,}\n\\] respectively. Since \\(\\vec{v}_2\\) is a linear combination of \\(\\vec{v}_1\\), \\(\\vec{s}_2\\), …, \\(\\vec{s}_m\\) we can obtain \\[\n\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{s}_3,...,\\vec{s}_m)=V\n\\quad \\text{and} \\quad\n\\vec{v}_3, ..., \\vec{v}_n \\text{ are linearly independent,}\n\\] respectively. Now if \\(m<n\\) then repeating this process will eventually exhaust the \\(\\vec{s}_i\\)’s and lead to \\[\n\\text{span}(\\vec{v}_1,\\vec{v}_2,...,\\vec{v}_m)=V\n\\quad \\text{and} \\quad\n\\vec{v}_{m+1}, ..., \\vec{v}_n \\text{ are linearly independent.}\n\\] This is a contradiction since \\(\\vec{v}_n\\) is not in \\(\\text{span}(\\vlist{v}{m});\\) and whence \\(n\\leq m\\).\n\n\nTheorem 8.5 A set \\(S=\\{\\vlist{v}{m}\\}\\) of vectors in \\(V\\) is linearly independent if and only if for any vector \\(\\vec{u}\\), if \\(\\vec{u}=\\lincomb{u}{v}{m}\\), then this representation is unique.\n\n\nProof. Assume the vectors in \\(S\\) are linearly independent and assume \\(\\vec{u}\\) is an arbitrary vector with \\[\\begin{equation}\n\\vec{u}=\\lincomb{a}{v}{m}\n\\qquad \\text{and} \\qquad\n\\vec{u}=\\lincomb{b}{v}{m}\n\\end{equation}\\] as both representations of \\(\\vec{u}\\) as linear combinations of the vectors in \\(S\\). Then \\[\n\\vec{0}=\\vec{u}-\\vec{u}\n=(a_1-b_1)\\vec{v}_1+(a_2-b_2)\\vec{v}_2+\\cdots +(a_m-b_m)\\vec{v}_m.\n\\] Since \\(S\\) is linearly independent \\(a_1-b_1=a_2-b_2=\\cdots =a_m-b_m=0\\) and thus \\(a_1=b_1\\), \\(a_2=b_2\\), …, \\(a_m=b_m\\). Therefore, the representation of \\(\\vec{u}\\) as a linear combination of the vectors in \\(S\\) is unique. Conversely, assume for nay vector \\(\\vec{u}\\) which can be written as a linear combination of the vectors in \\(S\\), the representation is unique. If \\(\\vlist{c}{m}\\) are scalars such that \\(\\lincomb{c}{v}{m}=\\vec{0}\\) then \\(c_1=c_2=\\cdots =c_m=0\\) much hold since \\(0\\vec{v}_1+0\\vec{v}_2+\\cdots +0\\vec{v}_m=\\vec{0}\\) and this representation is unique. Therefore, the vectors in \\(S\\) are linearly independent.\n\n\nDefinition 8.4  The vectors \\(\\vlist{v}{m}\\) in \\(V\\) are called a basis of a linear subspace \\(V\\) if they span \\(V\\) and are linearly independent.\n\n\nExample 8.8 Find a basis for \\(V\\) for \\(n=1,2,3,...m\\). For \\(n=2\\), the vectors \\(\\vectortwo{1}{0}\\), \\(\\vectortwo{0}{1}\\) form a basis for \\(k^2\\). For \\(n=3\\), the vectors \\(\\vectorthree{1}{0}{0}\\), \\(\\vectorthree{0}{1}{0}\\), \\(\\vectorthree{0}{0}{1}\\) form a basis for \\(k^3\\). In general, for a positive integer \\(n\\), the following \\(n\\) vectors of \\(V\\) form a basis (called the standard basis ) of \\(V\\). \\[\\begin{equation}\n\\label{stba}\n\\vec{e}_1=\\vectorfour{1}{0}{\\vdots}{0}\n\\qquad\n\\vec{e}_2=\\vectorfour{0}{1}{\\vdots}{0}\n\\qquad\n\\cdots\n\\qquad\n\\vec{e}_n=\\vectorfour{0}{0}{\\vdots}{1}\n\\end{equation}\\] The vectors in a standard basis are linearly independent by \\(\\ref{prop:roweqtoidn}\\). Given any vector \\(\\vec{v}\\) in \\(V\\) with components \\(v_i\\), we can write \\[\n\\vec{v}=\\lincomb{v}{e}{n},\n\\] and thus \\(k^n=\\text{span}(\\vlist{e}{n})\\) which shows that any standard basis is in fact a basis.\n\n\nExample 8.9 Show the following vectors \\(\\vec{v}_1\\), \\(\\vec{v}_2\\), \\(\\vec{v}_3\\), and \\(\\vec{v}_4\\) form a basis for \\(\\mathbb{R}^4\\). \\[\n\\vec v_1=\\begin{bmatrix} 1 \\\\ 1\\\\ 1 \\\\ 1 \\end{bmatrix}\n\\qquad\n\\vec v_2=\\begin{bmatrix} 1 \\\\ -1\\\\ 1 \\\\ -1 \\end{bmatrix}\n\\qquad\n\\vec v_3=\\begin{bmatrix} 1 \\\\ 2\\\\ 4 \\\\ 8 \\end{bmatrix}\\qquad\n\\vec v_4=\\begin{bmatrix} 1 \\\\ -2\\\\ 4 \\\\ -8 \\end{bmatrix}\n\\] We determine \\(\\text{rref}(A)=I_4\\) where \\(A\\) is the matrix with column vectors \\(\\vec v_1, \\vec v_2, \\vec v_3, \\vec v_4\\). By \\(\\ref{prop:roweqtoidn}\\), \\(\\vec v_1, \\vec v_2, \\vec v_3, \\vec v_4\\) are linearly independent. Since \\(\\vec v_1, \\vec v_2, \\vec v_3, \\vec v_4\\) also span \\(\\mathbb{R}^4\\), they form a basis of \\(\\mathbb{R}^4\\).\n\n\nExample 8.10 Let \\(U\\) be the subspace of \\(\\mathbb{R}^5\\) defined by \\[\nU=\\{(x_1,x_2,x_3,x_4,x_5)\\in\\mathbb{R}^5  \\mid x_1=3x_2 \\text{ and } x_3=7x_4\\}.\n\\] Find a basis of \\(U\\). The following vectors belong to \\(U\\) and are linearly independent in \\(\\mathbb{R}^5\\). \\[\nv_1=\\vectorfive{3}{1}{0}{0}{0}\n\\qquad\nv_2=\\vectorfive{0}{0}{7}{1}{0}\n\\qquad\nv_3=\\vectorfive{0}{0}{0}{0}{1}\n\\] If \\(u\\in U\\), then the representation \\[\nu\n=\\vectorfive{u_1}{u_2}{u_3}{u_4}{u_5}\n=\\vectorfive{3u_2}{u_2}{7u_4}{u_4}{u_5}\n=u_2\\vectorfive{3}{1}{0}{0}{0}+u_4\\vectorfive{0}{0}{7}{1}{0}+u_5\\vectorfive{0}{0}{0}{0}{1}\n\\] shows that they also span \\(U\\), and thus form a basis of \\(U\\) by \\(\\ref{basisdef}\\).\n\n\nTheorem 8.6  Let \\(S=\\{\\vlist{v}{n}\\}\\) be a set of vectors in a vector space \\(V\\) and let \\(W=\\text{span}(S)\\). Then some subset of \\(S\\) is a basis for \\(W\\).\n\n\nProof. Assume \\(W=\\text{span}(S)\\) and suppose \\(S\\) is a linearly independent set of vectors. Thus, in this case, \\(S\\) is a basis of \\(W\\), by \\(\\ref{basisdef}\\). So we can assume \\(S\\) is a linearly dependent set of vectors. By \\(\\ref{lindepothers}\\), there exists \\(i\\) such that \\(1\\leq i \\leq m\\) and \\(\\vec{v}_i\\) is a linear combination of the other vectors in \\(S\\). It is left for Exercise \\(\\ref{ex:spnlinbasis}\\) to show that \\[\nW=\\text{span}(S)=\\text{span}(S_1)\n\\] where \\(S_1=S/\\{\\vec{v}_i\\}\\). If \\(S_1\\) is linearly independent set of vectors, then \\(S_1\\) is a basis of \\(W\\). Otherwise, \\(S_1\\) is a linear dependent set and we can delete a vector from \\(S_1\\) that is a linear combination of the other vectors in \\(S_1\\). We obtain another subset \\(S_2\\) of \\(S\\) with \\[\nW=\\text{span}(S)=\\text{span}(S_1)=\\text{span}(S_2).\n\\] Since \\(S\\) is finite, if we continue, we find a linearly independent subset of \\(S\\) and thus a basis of \\(W\\).\n\n\nCorollary 8.2 All bases of a subspace \\(U\\) of a vector space \\(V\\) consists of the same number of vectors.\n\n\nProof. Let \\(S=\\{\\vlist{v}{n}\\}\\) and \\(T=\\{\\vlist{w}{m}\\}\\) be bases of a subspace \\(U\\). Then \\(\\text{span}(S)=V\\) and \\(T\\) is a lineal indecent set of vectors. By \\(\\ref{inpspanine}\\), \\(m\\leq n\\). Similarly, since \\(\\text{span}(T)=V\\) and \\(S\\) is a linearly independent set of vectors, \\(n\\leq m\\). Therefore, \\(m=n\\) as desired.\n\n\nCorollary 8.3  The vectors \\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\) form a basis of \\(V\\) if and only if the reduced row echelon form of the \\(n\\times n\\) matrix \\(\\begin{bmatrix}\\vec{v}_1& \\vec{v}_2 & \\cdots & \\vec{v}_n \\end{bmatrix}\\) is \\(I_n\\).\n\n\nProof. Suppose the vectors \\(\\vlist{v}{n}\\) form a basis of \\(V\\) and consider the \\(n\\times n\\) linear system \\[\n\\begin{cases}\nv_{11} x_1+v_{12} x_2+\\cdots +v_{1n} x_n=0 \\\\\nv_{21} x_1+v_{22} x_2+\\cdots +v_{2n} x_n=0 \\\\\n\\qquad \\qquad \\vdots \\\\\nv_{n1} x_1+v_{n2} x_2+\\cdots +v_{nn} x_n=0\n\\end{cases}\n\\] where the \\(v_{ij}\\)’s are the components of the \\(\\vec{v}_j\\)’s. Since \\(\\{\\vlist{v}{n}\\}\\) is a basis, \\(\\vec{v}_1=\\vec{v}_2=\\cdots =\\vec{v}_n=\\vec{0}\\) and this linear system can not have another solution. By \\(\\ref{cor:linsystmecor2}\\), \\(\\text{rref}(A)=I_n\\) where \\(A=\\begin{bmatrix}\\vec{v}_1& \\vec{v}_2 & \\cdots & \\vec{v}_n \\end{bmatrix}\\).\n\n\nDefinition 8.5 The number of vectors in a basis of a subspace \\(U\\) of \\(V\\) is called the dimension of \\(U\\), and is denoted by \\(\\text{dim} U\\).\n\n\nExample 8.11 Find a basis of the subspace of \\(\\mathbb{R}^4\\) that consists of all vectors perpendicular to both of the following vectors \\(\\vec{v}_1\\) and \\(\\vec v_2\\). \\[\n\\vec v_1=\\vectorfour{1}{0}{-1}{1}\n\\qquad\n\\vec v_2=\\vectorfour{0}{1}{2}{3}\n\\] We need to find all vectors \\(\\vec x\\) in \\(\\mathbb{R}^4\\) such that \\(\\vec x \\cdot \\vec v_1=0\\) and \\(\\vec x \\cdot \\vec v_2=0\\). We solve both \\[\n\\vectorfour{x_1}{x_2}{x_3}{x_4}\\cdot \\vectorfour{1}{0}{-1}{1}=0\n\\qquad \\text{and} \\qquad\n\\vectorfour{x_1}{x_2}{x_3}{x_4}\\cdot \\vectorfour{0}{1}{2}{3}=0\n\\] which leads to the system and matrix \\[\n\\begin{cases}\nx_1-x_3+x_4 =0 \\\\\nx_2+2x_3+3x_4 =0\n\\end{cases}\n\\qquad\n\\text{and}\n\\qquad\nA=\n\\begin{bmatrix}\n1 & 0 & -1 & 1 \\\\\n0 & 1 & 2 & 3\n\\end{bmatrix}.\n\\] All solutions are given by \\[\n\\vectorfour{x_1}{x_2}{x_3}{x_4}\n=t \\vectorfour{-1}{-3}{0}{1}+u\\vectorfour{1}{-2}{1}{0} \\quad \\text{ where $t, u\\in\\mathbb{R}$}.\n\\] It follows the vectors \\(\\vectorfour{-1}{-3}{0}{1}\\), \\(\\vectorfour{1}{-2}{1}{0}\\) form a basis of the desired subspace.\n\n\nTheorem 8.7  Let \\(U\\) be a subspace of \\(k^m\\) with \\(\\dim U=n\\), then\n\nany list of linearly independent vectors contains \\(n\\) elements,\nany list of vectors that spans \\(U\\) contains at least \\(n\\) elements,\nif \\(n\\) vectors are linearly independent then they form a basis, and\nif \\(n\\) vectors span \\(U\\), then they form a basis of \\(U\\).\n\n\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:sumprop}\\)\n\n\nExample 8.12 Determine the values of \\(a\\) for which the following vectors \\(\\vec{u}_1\\), \\(\\vec{u}_2\\), \\(\\vec{u}_3\\), and \\(\\vec{u}_4\\) form a basis of \\(\\mathbb{R}^4\\). \\[\n\\vec{u}_1=\\vectorfour{1}{0}{0}{4} \\qquad\n\\vec{u}_2=\\vectorfour{0}{1}{0}{6} \\qquad\n\\vec{u}_3=\\vectorfour{0}{0}{1}{8} \\qquad\n\\vec{u}_4=\\vectorfour{4}{5}{6}{a}\\] Let \\(A=\\begin{bmatrix}\\vec{u}_1 & \\vec{u}_2 & \\vec{u}_3& \\vec{u}_4\\end{bmatrix}\\). Using row operations we find the row-echelon form of \\(A\\) to be the following matrix. \\[\n\\begin{bmatrix}\n1 & 0 & 0 & 4 \\\\\n0 & 1 & 0 & 5 \\\\\n0 & 0 & 1 & 6\\\\\n0 & 0 & 8 & a-94\n\\end{bmatrix}\n\\] Thus, \\(\\text{rref}(A)=I_4\\) if and only if \\(a=95\\). Therefore, by \\(\\ref{rrefbasis}\\), \\(B=\\{\\vec{u}_1, \\vec{u}_2, \\vec{u}_3, \\vec{u}_4\\}\\) is a basis if and only if \\(a=95\\).\n\n\nTheorem 8.8 The dimension of the row space of a matrix \\(A\\) is equal to the dimension of the column space of \\(A\\).\n\n\nProof. Gerber pg 226.\n\n\nExercise 8.1 Determine whether the following collection of vectors in \\(\\mathbb{R}^3\\) are linearly independent or linearly dependent.   - \\((0,1,1), (1,2,1), (0,4,6), (1,0,-1)\\) - \\((0,1,0), (1,2,1), (0,-4,6), (-1,1,-1)\\)\n\n\nExercise 8.2 Determine whether the following collection of vectors in \\(\\mathbb{R}^4\\) are linearly independent or linearly dependent.   - \\((0,1,1,1), (1,2,1,1), (0,4,6,2), (1,0,-1, 2)\\) - \\((0,1,0,1), (1,2,1,3), (0,-4,6,-2), (-1,1,-1, 2)\\)\n\n\nExercise 8.3 Show that the given vectors do not form a basis for the vector space \\(V\\).   - \\((21,-7), (-6, 1)\\); \\(V=\\mathbb{R}^2\\) - \\((21,-7,14), (-6, 1,-4), (1,0,0)\\); \\(V=\\mathbb{R}^3\\) - \\((48,24,108,-72), (-24, -12,-54,36), (1,0,0,0), (1,1,0,0)\\); \\(V=\\mathbb{R}^4\\)\n\n\nExercise 8.4 Reduce the vectors to a basis of the vector space \\(V\\).   - \\((1,0), (1,2), (2,4)\\), \\(V=\\mathbb{R}^2\\) - \\((1,2,3), (-1, -10, 15), (1, 2, -3), (2,0,6), (1, -2, 3)\\), \\(V=\\mathbb{R}^3\\)\n\n\nExercise 8.5 Which of the following collection of vectors in \\(\\mathbb{R}^3\\) are linearly dependent? For those that are express one vector as a linear combination of the rest.\n\n\\((1,1,0), (0,2,3), (1,2,3)\\)\n\\((1,1,0), (3,4,2), (0,2,3)\\)\n\n\n\nExercise 8.6  Prove \\(\\ref{PropertiesofVectorAddition1}\\).\n\n\nExercise 8.7  Prove \\(\\ref{PropertiesofVectorAddition2}\\).\n\n\nExercise 8.8 Let \\(S=\\{v_1, v_2, ..., v_k\\}\\) be a set of vectors in a a vector space \\(V\\). Prove that \\(S\\) is linearly dependent if and only if one of the vectors in \\(S\\) is a linear combination of all other vectors in \\(S\\).\n\n\nExercise 8.9 Suppose that \\(S=\\{v_1, v_2, v_3\\}\\) is a linearly independent set of vector in a vector space \\(V\\). Prove that \\(T=\\{u_1, u_2, u_3\\}\\) is also linearly independent where \\(u_1=v_1\\), \\(u_2=v_1+v_2\\), and \\(u_3=v_1+v_2+v_3\\).\n\n\nExercise 8.10 Which of the following sets of vectors form a basis for the vector space \\(V\\).   - \\((1,3), (1,-1)\\); \\(V=\\mathbb{R}^2\\) - \\((1,3),(-2,6)\\); \\(V=\\mathbb{R}^2\\) - \\((3,2,2), (-1,2,1), (0,1,0)\\); \\(V=\\mathbb{R}^3\\) - \\((3,2,2), (-1,2,0), (1,1,0)\\); \\(V=\\mathbb{R}^3\\) - \\((2,2,2,2), (3,3,3,2), (1,0,0,0), (0,1,0,0)\\); \\(V=\\mathbb{R}^4\\) - $(1,1,2,0), (2,2,4,0), (1,2,3,1), (2,1,3,-1), (1,2,3,-1) $; \\(V=\\mathbb{R}^4\\)\n\n\nExercise 8.11 Find a basis for the subspace of the vector space \\(V\\).   - All vectors of the form \\((a,b,c)\\) where \\(b=a+c\\) where \\(V=\\mathbb{R}^3\\). - All vectors of the form \\((a,b,c)\\) where \\(b=a-c\\) where \\(V=\\mathbb{R}^3\\). - All vectors of the form \\(\\vectorfour{b-a}{a+c}{b+c}{c}\\) where \\(V=\\mathbb{R}^4\\).\n\n\nExercise 8.12 Let \\(\\vec{v}_1=\\vectorthree{0}{1}{1}\\), \\(\\vec{v}_2=\\vectorthree{1}{0}{0}\\) and \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2)\\).\n\nIs \\(S\\) a subspace of \\(\\mathbb{R}^3\\)?\nFind a vector \\(\\vec{u}\\) in \\(S\\) other than \\(\\vec{v}_1\\), \\(\\vec{v}_2\\).\nFind scalars which verify that \\(3\\vec{u}\\) is in \\(S\\).\nFind scalars which verify that \\(\\vec{0}\\) is in \\(S\\).\n\n\n\n\nExercise 8.13 Let \\(\\vec{u}_1=\\vectorthree{0}{2}{2}\\), \\(\\vec{u}_2=\\vectorthree{2}{0}{0}\\) and \\(T=\\text{span}(\\vec{u}_1,\\vec{u}_2)\\). Show \\(S=T\\) by showing \\(S\\subseteq T\\) and \\(T\\subseteq S\\) where \\(S\\) is defined in Exercise \\(\\ref{vecex1}\\).\n\n\nExercise 8.14 Prove that the non-empty intersection of two subspaces of \\(\\mathbb{R}^3\\) is a subspace of \\(\\mathbb{R}^3\\).\n\n\nExercise 8.15 Let \\(S\\) and \\(T\\) be subspaces of \\(\\mathbb{R}^3\\) defined by \\[\nS=\\text{span}\\left(\\vectorthree{1}{0}{2},\\vectorthree{0}{2}{1}\\right)\n\\qquad \\text{and} \\qquad\nT=\\text{span}\\left(\\vectorthree{2}{-2}{3},\\vectorthree{3}{-4}{4}\\right).\n\\] Show they are the same subspace of \\(\\mathbb{R}^3\\).\n\n\nExercise 8.16 Let \\({\\vec{v}_1,\\vec{v}_2, \\vec{v}_3}\\) be a linearly independent set of vectors. Show that if \\(\\vec{v}_4\\) is not a linear combination of \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_3\\), then \\({\\vec{v}_1,\\vec{v}_2, \\vec{v}_3},\\vec{v}_4\\) is a linearly independent set of vectors.\n\n\nExercise 8.17 If\n\\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) is a linearly independent set of vectors in \\(V\\), show that\n\\(\\{\\vec{v}_1,\\vec{v}_1+\\vec{v}_2, \\vec{v}_1+\\vec{v}_2+\\vec{v}_3\\}\\) is also a linearly independent set of vectors in \\(V\\).\n\n\nExercise 8.18 If\n\\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) is a linearly independent set of vectors in \\(V\\), show that\n\\(\\{\\vec{v}_1+\\vec{v}_2,\\vec{v}_2+\\vec{v}_3, \\vec{v}_3+\\vec{v}_1\\}\\) is also a linearly independent set of vectors in \\(V\\).\n\n\nExercise 8.19 Let \\(\\{\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\}\\) be a linearly dependent set. Show that at least one of the \\(\\vec{v}_i\\) is a linear combination of the others.\n\n\nExercise 8.20 Prove or provide a counterexample to the following statement. If a set of vectors \\(T\\) spans the vector space \\(V\\), then \\(T\\) is linearly independent.\n\n\nExercise 8.21 Which of the following are not a basis for \\(\\mathbb{R}^3\\)?   - \\(\\vec{v_1}=\\vectorthree{1}{0}{0}, \\vec{v_2}=\\vectorthree{0}{1}{1}, \\vec{v_3}=\\vectorthree{1}{-1}{-1}\\) - \\(\\vec{u_1}=\\vectorthree{0}{0}{1}, \\vec{u_2}=\\vectorthree{1}{0}{1}, \\vec{u_3}=\\vectorthree{2}{3}{4}\\)\n\n\nExercise 8.22 Let \\(S\\) be the space spanned by the vectors \\[\n\\vec{v}_1=\\vectorfour{1}{0}{1}{1}\n\\quad\n\\vec{v}_1=\\vectorfour{-1}{-3}{1}{0}\n\\quad\n\\vec{v}_1=\\vectorfour{2}{3}{0}{1}\n\\quad\n\\vec{v}_1=\\vectorfour{2}{0}{2}{2}\n\\] Find the dimension of \\(S\\) and a subset of \\(T\\) which could serve as a basis for \\(S\\).\n\n\nExercise 8.23 Let \\(\\{\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\}\\) be a basis for \\(V\\), and suppose that \\(\\vec{u} =a_1 \\vec{v_1}+a_2 \\vec{v_2}+\\cdots + a_n \\vec{v_n}\\) with \\(a_1\\neq 0\\). Prove that \\(\\{\\vec{u}, \\vec{v}_2, ..., \\vec{v}_n\\}\\) is also a basis for \\(V\\).\n\n\nExercise 8.24 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) and \\(T=\\text{span}(\\vec{u}_1,\\vec{u}_2,\\vec{u}_3)\\) where \\(\\vec{v}_i\\) and \\(\\vec{u}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{2}{0}\n\\quad\n\\vec{v}_2=\\vectorfour{2}{1}{1}{1}\n\\quad\n\\vec{v}_3=\\vectorfour{3}{-1}{2}{-1}\n\\qquad\n\\vec{u}_1=\\vectorfour{3}{0}{3}{1}\n\\quad\n\\vec{u}_2=\\vectorfour{1}{2}{-1}{1}\n\\quad\n\\vec{u}_3=\\vectorfour{4}{-1}{5}{1}\n\\] Is one of these two subspaces strictly contained in the other or are they equal?\n\n\nExercise 8.25 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) where \\(\\vec{v}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{2}{3}{1}\n\\qquad\n\\vec{v}_2=\\vectorfour{2}{-1}{1}{-3}\n\\qquad\n\\vec{v}_3=\\vectorfour{1}{3}{4}{2}\n\\qquad \\text{and}\\qquad\n\\vec{u}=\\vectorfour{1}{2}{3}{1}\n\\] Is the vector \\(\\vec{u}\\) in \\(S\\)?\n\n\nExercise 8.26 If possible, find a value of \\(a\\) so that the vectors \\[\n\\vectorthree{1}{2}{a}\n\\qquad\n\\vectorthree{0}{1}{a-1}\n\\qquad\n\\vectorthree{3}{4}{5}\n\\qquad\n\\] are linearly independent.\n\n\nExercise 8.27 Let \\(S=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3)\\) where \\(\\vec{v}_i\\) are defined as follows. \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{2}{3}\n\\qquad\n\\vec{v}_2=\\vectorfour{1}{0}{1}{0}\n\\qquad\n\\vec{v}_3=\\vectorfour{3}{-2}{5}{7}\n\\qquad \\text{and}\\qquad\n\\vec{u}=\\vectorfour{1}{1}{0}{-1}\n\\] Find a basis of \\(S\\) which includes the vector \\(\\vec{u}\\).\n\n\nExercise 8.28 Find a vector \\(\\vec{u}\\) in \\(\\mathbb{R}^4\\) such that \\(\\vec{u}\\) and the vectors \\[\n\\vec{v}_1=\\vectorfour{1}{-1}{-1}{1}\n\\qquad\n\\vec{v}_2=\\vectorfour{1}{0}{1}{1}\n\\qquad\n\\vec{v}_3=\\vectorfour{1}{2}{1}{1}\n\\] for a basis of \\(\\mathbb{R}^4\\).\n\n\nExercise 8.29 Show that every subspace of \\(V\\) has no more than \\(n\\) linearly independent vectors.\n\n\nExercise 8.30 Find two bases of \\(\\mathbb{R}^4\\) that have only the vectors \\(\\vec{e}_3\\) and \\(\\vec{e}_4\\) in common.\n\n\nExercise 8.31 Prove that if a list of vectors is linearly independent so is any sublist.\n\n\nExercise 8.32 Suppose \\(\\vec{v}_1,\\vec{v}_2, \\vec{v}_3\\) and \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_4\\) are two sets of linearly dependent vectors, and suppose that \\(\\vec{v}_1\\) and \\(\\vec{v}_2\\) are linearly independent. Prove that any set of three vectors chosen from \\(\\vec{v}_1, \\vec{v}_2, \\vec{v}_3, \\vec{v}_4\\) is linearly dependent.\n\n\nExercise 8.33 If \\(\\vec{u}\\) and \\(\\vec{v}\\) are linearly independent vectors in \\(V\\), prove that the vectors \\(a\\vec{u}+b\\vec{v}\\) and \\(c\\vec{u}+d\\vec{v}\\) are also linearly independent if and only if \\(ad-bc\\neq 0\\).\n\n\nExercise 8.34  Complete the proof of \\(\\ref{prop:spnlinbasis}\\).\n\n\nExercise 8.35 Let \\(U\\) be the collection of vectors that satisfy the equations \\(x+y+z=0\\) and \\(x+2y-z=0\\). Show \\(U\\) is a subspace of \\(\\mathbb{R}^3\\), find a basis for \\(U\\), and find \\(\\dim(U)\\).\n\n\nExercise 8.36 Let \\(U\\) be the collection of vectors that satisfy the equations \\(x+y+z=0\\), \\(x+2y-z=0\\), and \\(y-2z=0\\). Show \\(U\\) is a subspace of \\(\\mathbb{R}^3\\), find a basis for \\(U\\), and find \\(\\dim(U)\\).\n\n\nExercise 8.37 Show that the only subspaces of \\(\\mathbb{R}\\) are \\(\\{\\vec{0}\\}\\) and \\(\\{\\mathbb{R}\\}\\).\n\n\nExercise 8.38 Show that the only subspaces of \\(\\mathbb{R}^2\\) are \\(\\{\\vec{0}\\}\\), \\(\\{\\mathbb{R}^2\\}\\), and any set consisting of all scalar multiples of a nonzero vector. Describe these subspaces geometrically.\n\n\nExercise 8.39 Determine the various types of subspaces of \\(\\mathbb{R}^3\\) and describe them geometrically.\n\n\nExercise 8.40 For \\(\\vec{b}\\neq\\vec{0}\\), show that the set of solutions of the \\(n\\times m\\) linear system \\(A \\vec{x}=\\vec{b}\\), is not a subspace of \\(V\\).\n\n\nExercise 8.41 Suppose that \\(\\vec{v}_1, \\vec{v}_2, ..., \\vec{v}_n\\) are linearly independent in \\(\\mathbb{R}^n\\). Show that if \\(A\\) is an \\(n\\times n\\) matrix with \\(\\text{rref}(A)=I_n\\), then \\(A\\vec{v}_1, A\\vec{v}_2, ..., A\\vec{v}_n\\) are also linearly independent in \\(\\mathbb{R}^n\\).\n\n\nExercise 8.42 Let \\(S=\\{\\vlist{v}{s}\\}\\) and \\(T=\\{\\vlist{u}{t}\\}\\) be two sets of vectors in \\(V\\) where each \\(\\vec{u}_i\\), \\((i=1,2,...,t)\\) is a linear combination of the vectors in \\(S\\). Show that \\(\\vec{w}=\\lincomb{a}{u}{t}\\) is a linear combination of the vectors in \\(S\\).\n\n\nExercise 8.43 Let \\(S=\\{\\vlist{v}{m}\\}\\) be a set of non-zero vectors in a vector space \\(V\\) such that every vector in \\(V\\) can be uniquely as a linear combination of the vectors in \\(S\\). Prove that \\(S\\) is a basis for \\(V\\).\n\n\nExercise 8.44 Find a basis for the solution space of the homogeneous system \\((\\lambda I_n-A)\\vec{x}=\\vec{0}\\) for the given \\(\\lambda\\) and \\(A\\).\n\n\\(\\lambda=1, A=\\begin{bmatrix} 0 & 0 & 1 \\\\ 1 & 0 & -3 \\\\ 0 & 1 & 3 \\end{bmatrix}\\)\n\\(\\lambda=2, A=\\begin{bmatrix} -2 & 0 & 0 \\\\ 0 & -2 & -3 \\\\ 0 & 4 & 5 \\end{bmatrix}\\)\n\n\n\nExercise 8.45  Prove \\(\\ref{prop:roweqtoidn}\\).\n\n\nExercise 8.46  Prove \\(\\ref{cor:explincomb}\\).\n\n\nExercise 8.47  Prove \\(\\ref{sumprop}\\)."
  },
  {
    "objectID": "kernel-and-image-of-a-linear-transformation.html",
    "href": "kernel-and-image-of-a-linear-transformation.html",
    "title": "9  Kernel and Image of a Linear Transformation",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nIn this section we introduce the kernel and image of a linear transformation. We show how they can be realized as geometric objects and demonstrate how to find spanning sets for them.\n\nDefinition 9.1 Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\) with \\(n\\times m\\) matrix \\(A\\).\n\nThe kernel of \\(T\\), denoted by \\(\\ker(T)\\), is the set of all vectors \\(\\vec x\\) in \\(\\mathbb{R}^n\\) such that \\(T(\\vec x)=A \\vec x = \\vec 0\\).\nThe image of \\(T\\), denoted by \\(\\text{im}(T)\\), is the set of all vectors in \\(\\mathbb{R}^n\\) of the form \\(T (\\vec x)=A\\vec x\\).\n\nThe kernel and image of a matrix \\(A\\) of \\(T\\) is defined as the kernel and image of \\(T\\).\n\nNow to adequately describe the kernel and image of a linear transformation we need the concept of the span of a collection of vectors. We will see that one of the best ways to describe the kernel and image of a linear transformation is to describe them in terms of collections of linear combinations of vectors.\nThe next three lemmas describe basic properties of kernel and image.\n\nLemma 9.1 The kernel of any linear transformation \\(T\\) has the following properties:\n\n\\(\\vec 0 \\in \\ker(T)\\),\nif \\(\\vec v, \\vec w\\in \\ker(T)\\), then \\(\\vec v+\\vec w\\in \\ker(T)\\), and\nif \\(\\vec v\\in \\ker(T)\\) and \\(k\\in \\mathbb{R}\\), then \\(k\\vec v\\in \\ker(T)\\).\n\n\n\nProof. Let \\(A\\) be the matrix for \\(T\\). Then \\(A\\vec 0=\\vec 0\\) which shows \\(\\vec 0 \\in \\ker(T)\\). If \\(\\vec v, \\vec w\\in \\ker(T)\\), then \\(A\\vec v=\\vec 0\\) and \\(A\\vec w=\\vec 0\\). Thus, \\(A\\vec v+A\\vec w=A(\\vec v+\\vec w)=\\vec 0\\) implying \\(\\vec v+\\vec w\\in \\ker(T)\\). If \\(\\vec v\\in \\ker(T)\\) and \\(k\\in \\mathbb{R}\\), then \\(\\vec 0=A\\vec v\\) implying \\(A(k \\vec v)=\\vec 0\\). Thus, \\(k\\vec v\\in \\ker(T)\\).\n\n\nLemma 9.2 Let \\(T\\) be a linear transformation with matrix \\(A\\).\n\nIf \\(A\\) is an \\(n\\times n\\) matrix, then \\(\\ker(A)=\\{\\vec 0\\}\\) if and only if \\(\\text{rank} (A)=n\\).\nIf \\(A\\) is an \\(n\\times m\\) matrix, then\n\\(\\ker(A)=\\{\\vec 0\\} \\implies m\\geq n\\)\n\\(m>n \\implies \\ker(A)\\) contains non-zero vectors\nIf \\(A\\) is a square matrix, then \\(\\ker(A)=\\{\\vec 0\\}\\) if and only if \\(A\\) is invertible.\n\n\n\nProof. If \\(T\\) is a linear transformation \\(T(\\vec x )=A\\vec x\\) from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\) where \\(m>n\\), then there will be free variables for the equation \\(T(\\vec x)=A\\vec x=\\vec 0\\); that is the system will have infinitely many solutions. Therefore, the kernel of \\(T\\) will consists of infinitely many vectors. If \\(m=n\\) and for an invertible \\(n\\times n\\) matrix \\(A\\), how do we find \\(\\ker(A)\\)? Since \\(A\\) is invertible, \\(A\\vec x=\\vec 0\\) can be solved by \\(A^{-1}(A \\vec x)=A^{-1}\\vec 0\\) showing \\(\\vec x=\\vec 0\\); that is the only solution to the system \\(A \\vec x=\\vec 0\\) is \\(\\vec 0\\) so that \\(\\ker(A)=\\{\\vec 0\\}\\) whenever \\(A\\) is invertible.\n\n\nLemma 9.3 The image of any linear transformation \\(T\\) has the following properties:\n\n\\(\\vec 0 \\in \\text{im}(T)\\),\nif \\(\\vec v, \\vec w\\in \\text{im}(T)\\), then \\(\\vec v+\\vec w\\in \\text{im}(T)\\), and\nif \\(\\vec v\\in \\text{im}(T)\\) and \\(k\\in \\mathbb{R}\\), then \\(k\\vec v\\in \\text{im}(T)\\).\n\n\n\nProof. Let \\(A\\) be the matrix for \\(T\\). Then \\(A\\vec 0=\\vec 0\\) which shows \\(\\vec 0 \\in \\text{im}(T)\\). If \\(\\vec v, \\vec w\\in \\text{im}(T)\\), then there exists \\(\\vec x\\) and \\(\\vec y\\) such that \\(A\\vec x=\\vec v\\) and \\(A\\vec y=\\vec w\\). Thus, \\(\\vec v+\\vec w=A\\vec x+A\\vec y=A(\\vec x+\\vec y)\\) implying \\(\\vec v+\\vec w\\in \\text{im}(T)\\). If \\(\\vec v\\in \\text{im}(T)\\) and \\(k\\in \\mathbb{R}\\), then there exists \\(\\vec u\\) such that \\(\\vec v=A\\vec u\\) implying \\(A(k \\vec u)=k \\vec v\\). Thus, \\(k\\vec v\\in \\text{im}(T)\\).\n\n\nTheorem 9.1  Let \\(T:\\mathbb{R}^m\\to \\mathbb{R}^n\\) be a linear transformation with matrix \\(A\\). Then \\(\\text{im}(T)=\\text{span}(\\vec v_1, ....,\\vec v_m)\\) where \\(\\vec v_1, ..., \\vec v_m\\) are the column vectors of \\(A\\).\n\n\nProof. Since \\(\\vec v_1,...,\\vec v_m\\) are the column vectors of \\(A\\) \\[\\begin{equation}\n\\label{lineq}\nT(\\vec x)=\nA \\vec x =\n\\begin{bmatrix}\n\\vec v_1 & \\cdots & \\vec v_m\n\\end{bmatrix}\n\\vectorthree{x_1}{\\vdots}{x_m}=x_1 v_1+\\cdots + x_m v_m.\n\\end{equation}\\] If \\(\\vec u\\in \\text{span}(\\vec v_1, ....,\\vec v_m)\\) then there exists \\(x_1, ..., x_m\\) such that \\(u=x_1 v_1+\\cdots +x_m v_m\\). By \\(\\ref{lineq}\\), \\(u=T(\\vec x)\\) for \\(\\vec x\\in \\mathbb{R}^m\\). Thus, \\(\\vec u\\in \\text{im}(T)\\). Conversely, assume \\(\\vec u\\in\\text{im}(T)\\). Then there exists \\(\\vec x\\) such that \\(\\vec u=T(\\vec x)\\). By \\(\\ref{lineq}\\), there exists \\(x_1, ...,x_m\\) such that \\(u=x_1 v_1+\\cdots +x_m v_m\\). Therefore, \\(\\vec u \\in \\text{span}(\\vec v_1, ....,\\vec v_m)\\) and so \\(\\text{im}(T)=\\text{span}(\\vec v_1, ....\\vec v_m)\\) follows.:::\n\nExample 9.1 Find vectors that span \\(\\ker(A)\\) and \\(\\text{im}(A)\\) given \\[\nA=\n\\begin{bmatrix}\n2 & 1 & 3 \\\\\n3 & 4 & 2 \\\\\n6 & 5 & 7\n\\end{bmatrix}.\n\\] Describe \\(\\text{im}(A)\\) geometrically. To first find a spanning set of \\(\\ker(A)\\) we solve the system \\(A\\vec x=\\vec 0\\). We use the augmented matrix and elementary row operations and find reduced row-echelon form \\[\n\\text{rref}(A)=\n\\begin{bmatrix}\n1 & 0 & 2 & 0 \\\\\n0 & 1 & -1 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}.\n\\] Thus the solution set is \\[\n\\vectorthree{x_1}{x_2}{x_2}=t\\vectorthree{-2}{1}{1}:=t\\vec{w}\n\\] where \\(t\\in \\mathbb{R}\\). Therefore \\(\\ker(A)=\\text{span}(\\vec{w})\\). Next we find \\(\\text{im}(A).\\) To do so let \\(\\vec v_1, \\vec v_2, \\vec v_3\\) be the column vectors of the matrix \\(A\\). Since \\(\\text{im}(A)\\) is spanned by the columns of \\(A\\) and \\(\\vec v_3=2\\vec v_1+(-1)\\vec v_2\\), we find \\(\\text{im}(A)=\\text{span}\\left(\\vec v_1, \\vec v_2\\right)\\). Therefore the image of \\(A\\) is a plane in \\(\\mathbb{R}^3\\) that passes through the origin.\n\n\nExample 9.2 Give an example of a matrix \\(A\\) such that \\(\\text{im}(A)\\) is the plane with normal vector \\(\\vec w=\\vectorthree{1}{3}{2}\\) in \\(\\mathbb{R}^3\\). Since \\(\\vec w\\) is a normal vector we let \\(A=\\begin{bmatrix} 1& 3 & 2\\end{bmatrix}\\). First we find \\(\\ker(A)\\) because \\[\n\\begin{bmatrix} 1& 3 & 2\\end{bmatrix} \\vectorthree{x}{y}{z}=\\vectorthree{0}{0}{0}\n\\] is the plane \\(x+3y+2z=0\\) where \\(\\vec w\\) is a normal vector. Let \\(z=t\\) and \\(y=s\\), then \\(x=-3s-2t\\) so the solutions to the system \\(A\\vec x=\\vec 0\\) are \\[\n\\vectorthree{x}{y}{z}=\\vectorthree{-3s-2t}{s}{t}=s\\vectorthree{0}{1}{-3}+t\\vectorthree{1}{0}{-2}\n:=s\\vec u+t \\vec v\n\\] where \\(s\\) and \\(t\\) are real numbers; and thus \\(\\ker(A)=\\text{span}(\\vec{u}, \\vec{v})\\). These vectors yield the image of \\(A\\) since \\(\\text{im}(A)\\) is the plane with normal vector \\(\\vec w\\) so \\(A\\) is one such matrix; and \\(\\text{im}(A)=\\text{span}(\\vec{u}, \\vec{v})\\) is the plane in \\(\\mathbb{R}^3\\) with normal vector \\(\\vec{w}\\).\n\nThe kernel, being the most important subspace, has a special name for its dimension; namely, the dimension of \\(\\ker A\\) is called the nullity of \\(A\\).\n\nExample 9.3 Find the reduced row-echelon form of the matrix \\[\nA=\n\\begin{bmatrix}\n1 & 2 & 3 & 2 & 1 \\\\\n3 &  6 & 9 & 6 & 3 \\\\\n1 & 2 & 4 & 1 & 2 \\\\\n2 & 4 & 9 & 1 & 2\n\\end{bmatrix}.\n\\] Find a basis and state the dimension for the image and kernel of \\(A\\).\n\n\nExample 9.4 Find vectors that span \\(\\ker(A)\\) and \\(\\text{im}(A)\\) given \\[\nA=\n\\begin{bmatrix}\n1 & -1 & -1 & 1 & 1 \\\\\n-1 & 1 & 0 & -2 & 2 \\\\\n1 & -1 & -2 & 0 & 3 \\\\\n2 & -2 & -1 & 3 & 4\n\\end{bmatrix}.\n\\] We will solve the system \\(A\\vec x=\\vec 0\\) to find \\(\\ker(A)\\). Using the augmented matrix and elementary row operations, we find \\[\n\\text{rref}(A)=\n\\begin{bmatrix}\n1 & -1 & 0 & 2 & 0 & 0 \\\\\n0 & 0 & 1 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0\n\\end{bmatrix}.\n\\] So the solutions to the system \\(A\\vec x=\\vec 0\\) are \\[\n\\vectorfive{x_1}{x_2}{x_3}{x_4}{x_5}=\\vectorfive{s-2t}{s}{-t}{t}{0}=s\\vectorfive{1}{1}{0}{0}{0}+t\\vectorfive{-2}{0}{-1}{1}{0}:=s\\vec{u}+t\\vec{v}\n\\] where \\(s,t\\in \\mathbb{R}\\). Therefore \\(\\ker(A)=\\text{span}(\\vec{u},\\vec{v}).\\) Since \\(\\text{im}(A)\\) is the span of the column vectors of \\(A\\), we let \\(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3,\\vec{v}_4,\\vec{v}_5\\) be the column vectors of \\(A\\). By \\(\\ref{imspan}\\), \\(\\text{im}(A)=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_3,\\vec{v}_4,\\vec{v}_5).\\) Using \\(\\text{rref}(A)\\) as a guide, we notice \\(\\vec{v}_2=(-1)\\vec{v}_1\\) so we eliminate \\(\\vec{v}_2\\). Also, \\(\\vec{v}_4=(2)\\vec{v}_1+\\vec{v}_3\\) so we also eliminate \\(\\vec{v}_4\\). Therefore, \\(\\text{im}(A)=\\text{span}(\\vec{v}_1,\\vec{v}_2,\\vec{v}_5)\\).\n\n\nExample 9.5 Give an example of a linear transformation whose kernel is the line spanned by \\(\\vec{w}=\\begin{bmatrix}-1 \\\\ 1 \\\\2\\end{bmatrix}\\). Considering the intersection of the planes \\(x+y=0\\) and \\(2x+z=0\\), we try to use the linear transformation, \\[\nT(\\vec{x})\n=T\\vectorthree{x}{y}{z}=\\vectortwo{x+y}{2x+z}\n=\\begin{bmatrix}\n1 & 1 & 0\\\\\n2 & 0 & 1\n\\end{bmatrix}\\vec{x}:=A\\vec{x}. \\] To find the kernel of \\(T\\) we solve \\(A \\vec x= \\vec 0\\). Since \\(\\text{rref}(A)=\\begin{bmatrix}  1 & 0 & 1/2 \\\\  0 & 1 & -1/2 \\end{bmatrix}\\) the solutions are of the form \\(t \\vec{w}\\) where \\(t\\) is a real number. Therefore it suffices to let \\(T\\) be the requested linear transformation.\n\n\nExample 9.6 Express the line \\(L\\) in \\(\\mathbb{R}^3\\) spanned by the vector \\(\\vec{w}=\\begin{bmatrix} 1 \\\\1 \\\\1 \\end{bmatrix}\\) as the image of a matrix \\(A\\) and as the kernel of a matrix \\(B\\). Let \\(A=\\vec{w}\\), then \\(L=\\text{im}(A)=\\text{span}(\\vec{w})\\). Therefore it suffices to let \\(A\\) be the requested matrix. Considering the intersection of the planes \\(x=y\\) and \\(y=z\\), we try to find \\(B\\) using the linear transformation \\[\nT(\\vec{x})\n=T\\vectorthree{x}{y}{z}=\\vectortwo{x-y}{y-z}\n=\\begin{bmatrix}\n1 & -1 & 0\\\\\n0 & 1 & -1\n\\end{bmatrix}\n:= B \\vec{x}\n\\] To find the kernel of \\(T\\) we solve \\(B \\vec x= \\vec 0\\). Since \\(\\text{rref}(B)=\\begin{bmatrix}  1 & 0 & -1 \\\\  0 & 1 & -1 \\end{bmatrix}\\) the solutions are of the form \\(t \\vec{w}\\) where \\(t\\) is a real number. Therefore it suffices to let \\(B\\) be the other requested matrix.\n\n\nExample 9.7 Find a basis for the kernel and the image of the linear transformations defined by \\[\nT_1 = \\left \\{\n\\begin{array}{rl}\ny_1 = & x_1+x_2+3x_3 \\\\\ny_2 = & 2x_1+x_2+4x_3\n\\end{array}\n\\right .\n\\qquad \\text{with} \\qquad\n\\text{rref}(T_1)  = \\begin{bmatrix}\n1 & 0 &1 \\\\\n0 & 1 & 2\n\\end{bmatrix}.\n\\] All solutions to the system \\(A\\vec x=\\vec 0\\) are \\(\\vectorthree{x_1}{x_2}{x_3}=t\\vectorthree{-1}{-2}{1}\\) where \\(t\\in\\mathbb{R}\\). Since \\(\\vectorthree{-1}{-2}{1}\\) is linearly independent and spans the kernel, the vector \\(\\vectorthree{-1}{-2}{1}\\) forms a basis of \\(\\ker(A)\\). Since the columns of \\(A\\) spans the image of \\(A\\) and \\(\\vectortwo{3}{4}=(1)\\vectortwo{1}{2}+(2)\\vectortwo{1}{1}\\) we find the vector \\(\\vectortwo{3}{4}\\) redundant. Since the remaining vectors are linearly independent and span \\(\\text{im}(A)\\), they form a basis for \\(\\text{im}(A)\\).\n\n\nExample 9.8 Find a basis for the kernel and the image of the linear transformations defined by \\[\nT_2 =\n\\left \\{\n\\begin{array}{rlll}\ny_1= & x_1 +3x_2 +9x_3  \\\\\ny_2= & 4x_1+5 x_2 +8x_3  \\\\\ny_3= & 7x_1+6 x_2 +3x_3  \\\\\n\\end{array}\n\\right .\n\\qquad \\text{with} \\qquad\n\\text{rref}(T_2)  = \\begin{bmatrix}\n1 & 0 & -3 \\\\\n0 & 1 & 4 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\] To find a basis of the kernel we solve \\(A\\vec x = \\vec 0\\) where \\(A\\) is the matrix of the given transformation. Since \\[\n\\text{rref(A)}=\n\\begin{bmatrix} 1 & 0 &-3 \\\\ 0 & 1 & 4 \\\\ 0 & 0 & 0 \\end{bmatrix}\n\\] all solutions have the form \\(\\vectorthree{3t}{-4t}{t}\\). Therefore a basis of the kernel of \\(A\\) is \\(\\vectorthree{3}{-4}{1}\\). In the original matrix \\(A\\) the third column is redundant since \\(\\vectorthree{9}{8}{3}=(-3)\\vectorthree{1}{4}{7}+(4)\\vectorthree{3}{5}{6}\\), and since the vectors \\(\\vectorthree{1}{4}{7}\\) and \\(\\vectorthree{3}{5}{6}\\) are linearly independent and span, they form a basis of \\(\\text{im}(A)\\).\n\n\nExample 9.9 Find a basis for the kernel and the image of the linear transformations defined by \\[\nT_3 =\n\\left \\{\n\\begin{array}{rlllll}\ny_1= & 4x_1+8 x_2 +x_3 +x_4+6x_5 \\\\\ny_2= & 3x_1+6 x_2 +x_3 +2x_4+5x_5 \\\\\ny_3= & 2x_1+4 x_2 +x_3 +9x_4+10x_5 \\\\\ny_4= & x_1+ 2x_2 + 3x_3 +2x_4 \\\\\n\\end{array}\n\\right .\n\\quad \\text{with} \\quad\n\\text{rref}(T_3)  = \\begin{bmatrix}\n1 & 2 & 0 & 0 & 0 \\\\\n0 & 0 & 1 & 0 & 0 \\\\\n0 & 0 & 0 & 1 & 0 \\\\\n0 & 0 & 0 & 0 &1\n\\end{bmatrix}\n\\] The solutions to the system \\(A\\vec x=\\vec 0\\) are \\[\n\\vec x=\\vectorfive{x_1}{x_2}{x_3}{x_4}{x_5}=t \\vectorfive{-2}{1}{0}{0}{0}=t \\vec v \\qquad \\text{where $t\\in\\mathbb{R}$.}\n\\] The vector \\(\\vec v\\) is linearly independent and spans \\(\\ker(A)\\); thus it forms a basis for \\(\\ker(A)\\). Since \\(\\vectorfour{8}{6}{4}{2}=2\\vectorfour{4}{3}{2}{1}\\) the vector \\(\\vectorfour{8}{6}{4}{2}\\) is redundant and since the column vectors of \\(A\\) are linearly independent and span \\(\\text{im}(A)\\), we have that the vectors \\(\\vectorfour{4}{3}{2}{1}\\), \\(\\vectorfour{1}{1}{1}{3}\\), \\(\\vectorfour{1}{2}{9}{2}\\), and \\(\\vectorfour{6}{5}{10}{0}\\) form a basis of \\(\\text{im}(A)\\).\n\n\nExample 9.10 Show \\(\\ker(A)\\neq \\ker(B)\\) where \\[\nA=\n\\begin{bmatrix}\n1 & 0 & 2 & 0 & 4 & 0 \\\\\n0 & 1 & 3 & 0 & 5 & 0 \\\\\n0 & 0 & 0 & 1 & 6 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 1\n\\end{bmatrix}\n\\qquad \\text{and} \\qquad\n%$ and $\nB=\n\\begin{bmatrix}\n1 & 0 & 2 & 0 & 0 & 4 \\\\\n0 & 1 & 3 & 0 & 0 & 5 \\\\\n0 & 0 & 0 & 1 & 0 & 6 \\\\\n0 & 0 & 0 & 0 & 1 & 7\n\\end{bmatrix}.\n\\] We solve the system \\(A\\vec x=\\vec 0\\), written out we find: \\[\n\\begin{bmatrix}\nx_1 \\\\ x_2 \\\\ x_3 \\\\ x_4 \\\\ x_5 \\\\ x_6\n\\end{bmatrix}\n= s\n\\begin{bmatrix}\n-4 \\\\ -5 \\\\ 0 \\\\ 6 \\\\ 1 \\\\ 0\n\\end{bmatrix}\n+t\n\\begin{bmatrix}\n-2 \\\\ -3 \\\\ 1\\\\ 0 \\\\ 0 0\n\\end{bmatrix}\n=s\\vec v_1+t\\vec v_2\n\\quad \\text{ where $s,t\\in \\mathbb{R}$}.\n\\] Since \\(v_1\\) and \\(v_2\\) are linearly independent and span \\(\\ker(A)\\), they form a basis of the kernel of \\(A\\). By noticing \\(B \\vec v_1\\neq \\vec 0\\) we conclude \\(\\ker(A)\\neq \\ker(B)\\).\n\n\nTheorem 9.2  For any matrix \\(A\\), \\(\\dim(\\text{im} A )=\\text{rank}(A)\\).\n\n\nProof. \n\n::: {#thm- } [Rank-Nullity] Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^m\\) to \\(\\mathbb{R}^n\\) with \\(n\\times m\\) matrix \\(A\\). Then \\[\n\\text{rank}(A)+\\text{nullity}(A)\n=\\dim(\\text{im} A)+\\dim(\\ker A)=m.\n\\]\n\n\nProof. Let \\(\\vec{y}=A\\vec{x}\\) be the corresponding system of linear equations. Recall,\n\\[\\begin{equation}\n\\label{varequ} \\small\n\\begin{tabular}{ccccccc}\n$\\left(\\parbox[c]{1.55cm}{\\begin{center}\nnumber of free variables\n\\end{center}}\\right)$ & $=$  &\n$\\left(\\parbox[c]{1.55cm}{\\begin{center}\ntotal number of variables\n\\end{center}}\\right)$ & $-$  &\n$\\left(\\parbox[c]{1.55cm}{\\begin{center}\nnumber of leading variables\n\\end{center}}\\right)$ & $=$  \n& $\\parbox[c]{1.75cm}{\\begin{center}\n\\normalsize\n$m-\\text{rank}(A)$\n\\end{center}}$\n\\end{tabular}\n\\end{equation}\\] Apparently, the number of free variables is the dimension of the kernel of \\(A\\). Thus \\(\\ref{varequ}\\) becomes \\(\\text{nullity}(A)=m-\\text{rank}(A)\\). By \\(\\ref{dimenimgrank}\\) we arrive at the conclusion \\(\\text{rank}(A)+\\text{nullity}(A)=m\\).\n\n\nExample 9.11 If possible, find a \\(3\\times 3\\) matrix such that \\(\\text{im} A=\\ker A\\).\n\n\nExample 9.12 If possible, find a \\(4\\times 4\\) matrix such that \\(\\text{im} A=\\ker A\\).\n\n\nExample 9.13 Give an example of a \\(4\\times 5\\) matrix \\(A\\) with \\(\\dim(\\ker A)=3\\).\n\n\nTheorem 9.3 The vectors \\(\\vec{v}_1,\\ldots,\\vec{v}_n\\) in \\(\\mathbb{R}^n\\) form a basis of \\(\\mathbb{R}^n\\) if and only if the matrix whose columns consists of \\(\\vec{v}_1,...,\\vec{v}_n\\) is invertible.\n\n\nProof. \n\nFor example, consider the matrix \\[\nA=\n\\begin{bmatrix}\n1 & 2 & 1 & 2 \\\\\n1 & 2 & 2 & 3 \\\\\n1 & 2 & 3 & 4\n\\end{bmatrix}\n\\] What is the smallest number of vectors needed to span the image of \\(A\\)? Of course we know, \\[\n\\text{im}(A)=\\text{span}\\left(\\vectorthree{1}{1}{1},\\vectorthree{2}{2}{2},\\vectorthree{1}{2}{3},\\vectorthree{2}{3}{4}\\right).\n\\] However, it is easy to show that \\(\\vectorthree{2}{3}{4}\\) and \\(\\vectorthree{2}{2}{2}\\) are redundant; and that the remaining vectors are linearly independent. Thus, \\[\n\\text{im}(A)=\\text{span}\\left(\\vectorthree{1}{1}{1},\\vectorthree{1}{2}{3}\\right).\n\\] Clearly, the image of the linear transformation defined by \\(A\\) is more easily understood by having a spanning set of linearly independent vectors."
  },
  {
    "objectID": "coordinates.html",
    "href": "coordinates.html",
    "title": "10  Coordinates",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\n\nDefinition 10.1 Let \\(\\mathcal{B}=(v_1,...,v_n)\\) be a basis of a subspace \\(V\\) of \\(\\mathbb{R}^n\\). For any \\(\\vec x \\in V\\) we can write \\(\\vec v= c_1 \\vec v_1 + \\cdots +c_m \\vec v_m\\). The scalars \\(c_1,...,c_m\\) are called the \\(\\mathcal{B}\\)-coordinates of \\(\\vec x\\) and the vector \\[\n\\left [ \\vec x \\right ]_{\\mathcal{B}}:= \\vectorthree{c_1}{\\hdots}{c_m}\n\\] is called the \\(\\mathcal{B}\\)-coordinate vector of \\(\\vec x\\)\n\nFor example the coordinates of \\(\\vectortwo{-2}{4}\\) with respect the standard basis \\(\\mathcal{B}=(\\vec{e}_1,\\vec{e}_2)\\) is \\(\\vectortwo{-2}{4}\\) since \\(\\vectortwo{-2}{4}=-2\\vec{e}_1+4\\vec{e}_2\\) which written as \\(\\vectortwo{-2}{4}_{\\mathcal{B}}=\\vectortwo{-2}{4}.\\) Notice the coordinates of \\(\\vectortwo{-2}{4}\\) with respect the basis \\(\\mathcal{B}'=\\left(\\vectortwo{2}{0},\\vectortwo{0}{2}\\right)\\) is \\(\\vectortwo{-1}{2}\\) since \\(\\vectortwo{-2}{4}=(-1)\\vectortwo{2}{0}+2\\vectortwo{0}{2}\\) which written as \\(\\vectortwo{-2}{4}_{\\mathcal{B}'}=\\vectortwo{-1}{2}.\\)\n\nExample 10.1 Consider the plane \\(2x_1-3x_2+4x_3=0\\) with basis \\[\n\\mathcal{B}=\\left(\\vectorthree{8}{4}{-1}, \\vectorthree{5}{2}{-1}\\right).\n\\] Let \\([\\vec x]_{\\mathcal{B}} = \\vectortwo{2}{-1}\\). Find \\(\\vec x\\). By definition of coordinates \\[\n\\vec x= 2\\vectorthree{8}{4}{-1} +(-1)\\vectorthree{5}{2}{-1}=\\vectorthree{11}{6}{-1}.\n\\]\n\n\nLemma 10.1  If \\(\\mathcal{B}=(\\vec{v}_1,...,\\vec{v}_n)\\) is a basis of a subspace \\(V\\) of \\(\\mathbb{R}^n\\), then\n\n\\([\\vec x]_{\\mathcal{B}}+[\\vec y]_{\\mathcal{B}}=[\\vec x+\\vec y]_{\\mathcal{B}}\\), for all vectors \\(\\vec x, \\vec y\\)\n\\([k\\vec x]_{\\mathcal{B}}=k[\\vec x]_{\\mathcal{B}}\\), for all vectors \\(\\vec x\\), and all scalars \\(k\\).\n\n\n\nProof. Let \\([\\vec x]_{\\mathcal{B}}=c_1 \\vec{v}_1+\\cdots + c_n \\vec{v}_n\\)\nand \\([\\vec y]_{\\mathcal{B}}=d_1 \\vec{v}_1+\\cdots + d_n \\vec{v}_n\\) be the representation of \\(\\vec{x}\\) and \\(\\vec{y}\\) with respect to \\(\\mathcal{B}\\). Then \\[\n[\\vec x]_{\\mathcal{B}}+[\\vec y]_{\\mathcal{B}}\n=\\vectorthree{c_1}{\\vdots}{c_3}+\\vectorthree{d_1}{\\vdots}{d_3}\n=\\vectorthree{c_1+d_1}{\\vdots}{c_n+d_n}\n=[\\vec x+y]_{\\mathcal{B}}\n\\] where the last equality holds since \\[\n\\vec{x}+\\vec{y}=(c_1+d_1)\\vec{v}_1+\\cdots (c_n+d_n)\\vec{v}_n.\\] We leave the proof of the second part for the reader.\n\n\nExample 10.2 Determine whether the vector \\(\\vec x = \\vectorthree{1}{-2}{-2}\\) is in \\(\\text{span} V\\) of the vectors \\(\\vec{v}_1=\\vectorthree{8}{4}{-1}\\) and \\(\\vec{v}_2=\\vectorthree{5}{2}{-1}\\) and if so write the coordinates of \\(\\vec x\\) with respect to this basis of \\(V\\). We need to find scalars \\(c_1\\) and \\(c_2\\) such that \\(\\vec x=c_1 \\vec v_1+c_2 \\vec v_2\\). Solving this system we find \\(c_1=-3\\) and \\(c_2=5\\). Therefore we find, \\([\\vec x]_{\\mathcal{B}}=\\vectortwo{-3}{5}\\) where \\(\\mathcal{B}=(\\vec v_1, \\vec v_2)\\).\n\n\nExample 10.3 Consider the plane \\(x+2y+z=0\\). Find a basis of this plane. Find another basis \\(\\mathcal{B}\\) of this plane such that \\([\\vec x]_{\\mathcal{B}}=\\vectortwo{2}{-1}\\) for \\(\\vec x = \\vectorthree{1}{-1}{1}\\). We find a basis by letting \\(z=t\\) and \\(y=s\\) be free variables. Then \\(x=-t-2s\\). All solutions to the equation \\(x+2y+z=0\\) are \\[\n\\vectorthree{x}{y}{z}\n=\\vectorthree{-t-2s}{s}{t}\n=t\\vectorthree{-1}{0}{1}+s\\vectorthree{-2}{1}{0}.\n\\] So a basis for the plane is \\(\\mathcal{B}=\\left(\\vectorthree{-1}{0}{1},\\vectorthree{-2}{1}{0}\\right).\\) Notice \\(\\vectorthree{1}{-1}{1}=(1)\\vectorthree{-1}{0}{1}+(-1)\\vectorthree{-2}{1}{0}\\) and thus \\(\\vectorthree{1}{-1}{1}_{\\mathcal{B}}=\\vectortwo{1}{-1}\\). Thus this is not the basis we seek. However, notice \\[\n\\vectorthree{1}{-1}{1}\n=2\\vectorthree{x_1}{0}{-x_1}+(-1)\\vectorthree{-2y_2}{y_2}{0}\n\\] holds when \\(y_2=1\\) and \\(x_1=-1/2\\). Also notice the vectors \\(\\vectorthree{-1/2}{0}{1/2}\\) and \\(\\vectorthree{-2}{1}{0}\\) span the plane and are linearly independent. Therefore we have the basis we seek, namely \\[\n\\mathcal{B}\n=\\left(\\vectorthree{-1/2}{0}{1/2},\\vectorthree{-2}{1}{0}\\right).\n\\]\n\n\nLemma 10.2  Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) and \\(\\mathcal{B}=(\\vec{v}_1,...,\\vec{v}_n)\\) a basis of \\(\\mathbb{R}^n\\). The \\(n\\times n\\) matrix \\(B\\) that transforms \\([ \\vec{x}]_{\\mathcal{B}}\\) into \\([T(\\vec x)]_{\\mathcal{B}}\\) is called the \\(\\mathcal{B}\\)-matrix of \\(T\\), written as \\([T(\\vec x)]_{\\mathcal{B}}=B [ \\vec x ]_{\\mathcal{B}}\\) for all \\(\\vec x\\) in \\(\\mathbb{R}^n\\) and \\[\nB=\n\\begin{bmatrix}\n[T(\\vec v_1)]_{\\mathcal{B}} &  \\cdots & [T(\\vec v_n)]_{\\mathcal{B}}\n\\end{bmatrix}.\n\\]\n\n\nProof. Since \\(\\mathcal{B}\\) is a basis of \\(\\mathbb{R}^n\\), there exists scalars \\(c_1, ..., c_n\\) such that \\(\\vec{x}=c_1 \\vec{v}_1+c_2\\vec{v}_2+\\cdots + c_n \\vec{v}_n\\). Using the linearity of \\(T\\) we find \\[\nT(\\vec{x})=c_1 T(\\vec{v}_1)+c_2 T(\\vec{v}_2)+\\cdots +c_n T(\\vec{v}_n).\n\\] By \\(\\ref{lincoord}\\) we find \\[\\begin{align*}\n[T(\\vec{x})]_{\\mathcal{B}}\n& = c_1 [T(\\vec{v}_1)]_{\\mathcal{B}} + c_2 [T(\\vec{v}_2)]_{\\mathcal{B}} + \\cdots +c_n [T(\\vec{v}_n)]_{\\mathcal{B}} \\\\\n& =\n\\begin{bmatrix}\n[T(\\vec v_1)]_{\\mathcal{B}} &  \\cdots & [T(\\vec v_n)]_{\\mathcal{B}}\n\\end{bmatrix}\n[\\vec{x}]_{\\mathcal{B}}\n\\end{align*}\\] as desired.\n\n\nExample 10.4 Find the matrix \\(\\mathcal{B}\\) of the linear transformation \\(T(\\vec x)=A \\vec x\\) where \\[\nA=\\begin{bmatrix}\n5 & -4 & -2 \\\\\n-4 & 5 & -2 \\\\\n-2 & -2 & 8\n\\end{bmatrix}\n\\] with respect to the basis \\(\\mathcal{B}=\\left ( \\vectorthree{2}{2}{1}, \\vectorthree{1}{-1}{0}, \\vectorthree{0}{1}{-2} \\right )\\). By \\(\\ref{lem:bmatrix}\\), \\(T\\) with respect to \\(\\mathcal{B}\\) has the following matrix \\(B\\). \\[\n\\begin{bmatrix}\n\\left[T\\vectorthree{2}{2}{1}\\right]_{\\mathcal{B}} &\n\\left[T\\vectorthree{1}{-1}{0}\\right]_{\\mathcal{B}} &\n\\left[T\\vectorthree{0}{1}{-2}\\right]_{\\mathcal{B}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\vectorthree{0}{0}{0}_{\\mathcal{B}} &\n\\vectorthree{9}{-9}{0}_{\\mathcal{B}} &\n\\vectorthree{0}{9}{-18}_{\\mathcal{B}}\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n0 & 0 & 0 \\\\\n0 & 9 & 0 \\\\\n0 & 0 & 9\n\\end{bmatrix}.\n\\]\n\n\nTheorem 10.1 Let \\(T\\) be a linear transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) with standard matrix \\(A\\) and let \\(B\\) be the \\(\\mathcal{B}\\)-matrix of \\(T\\) where \\(\\mathcal{B}= (\\vec{v}_1,...,\\vec{v}_n)\\) then \\(A S= S B\\) where \\[\nS= \\begin{bmatrix}\n\\vec{v}_1 & \\cdots &  \\vec{v}_n\n\\end{bmatrix}\n.\\]\n\n\nProof. The proof is left for the reader.\n\n\nDefinition 10.2 Two \\(n\\times n\\) matrices \\(A\\) and \\(B\\) are called similar if there exists an invertible matrix \\(S\\) such that \\(A S= S B\\).\n\n\nExample 10.5 Determine whether the following two matrices are similar. \\[\nA=\\begin{bmatrix} 1 & 2 \\\\ 4 &3 \\end{bmatrix}\n\\qquad \\text{and} \\qquad\nB=\\begin{bmatrix} 5 & 0 \\\\ 0 & -1 \\end{bmatrix}\\] We are looking for a matrix \\[\nS=\n\\begin{bmatrix} x & y \\\\ z & t\\end{bmatrix}\n\\] such that \\(AS=SB\\). Writing out we find \\[\n\\begin{bmatrix} 1 & 2 \\\\ 4 &3 \\end{bmatrix}\n\\begin{bmatrix} x & y \\\\ z & t\\end{bmatrix}\n=\n\\begin{bmatrix} x & y \\\\ z & t\\end{bmatrix}\n\\begin{bmatrix} 5 & 0 \\\\ 0 & -1 \\end{bmatrix}\n\\quad \\implies \\quad\n\\begin{bmatrix} x+2z & y+2t \\\\ 4x+3z & 4y+3t \\end{bmatrix}\n=\n\\begin{bmatrix} 5x & -y  \\\\ 5z & -t \\end{bmatrix}.\n\\] This leads to \\(z=2x\\) and \\(t=-y\\) so that \\(S\\) is any invertible matrix of the form \\[\\begin{equation}\n\\label{simform}\n\\begin{bmatrix}x & y \\\\ 2x & -y\\end{bmatrix}.\n\\end{equation}\\] Since \\(S= \\begin{bmatrix}1 & 1 \\\\ 2 & -1 \\end{bmatrix}\\) is invertible and of the form in \\(\\eqref{simform}\\), we can say, yes \\(A\\) is similar to \\(B\\).\n\n\nTheorem 10.2 If \\(A\\) is similar to \\(B\\) then \\(A^k\\) is similar to \\(B^k\\) for any positive integer \\(k\\).\n\n\nProof. If \\(A\\) is similar to \\(B\\) then there exists an invertible matrix \\(S\\) such that \\(B=S^{-1}A S\\). Then \\[\nB^k=(S^{-1}AS)(S^{-1}AS)\\cdots (S^{-1}AS)=S^{-1}A^k S\n\\] which shows that \\(B^k\\) is similar to \\(A^k\\).\n\n\nTheorem 10.3  Two \\(n\\times n\\) matrices \\(A\\), \\(A'\\) are similar if and only if they are matrices of the same linear transformation \\(T\\) from \\(\\mathbf{R}^n\\) to \\(\\mathbf{R}^n\\) with respect to two bases for \\(\\mathbf{R}^n\\)\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 10.4 Similar matrices have the same rank.\n\n\nProof. Let \\(A\\) and \\(B\\) be similar matrices. By \\(\\ref{simsamelin}\\) \\(A\\) and \\(B\\) represent the same linear transformation. Thus they must have the same rank.\n\n\nExample 10.6 Are the following similiar matrices? \\[\nA=\n\\begin{bmatrix}\n-1 & 2 & 1 \\\\\n2 & 0 & 1 \\\\\n2 & 2 & 2\n\\end{bmatrix}\n\\qquad \\text{and}\\qquad\nB=\n\\begin{bmatrix}\n-1 & 2 & 1 \\\\\n2 & 0 & 1 \\\\\n1 & 2 & 2\n\\end{bmatrix}\n\\] Since \\(\\text{rank}(A)=3\\) and \\(\\text{rank}(B)=2\\), these are not similar matrices.\n\n\nTheorem 10.5  Similarity of matrices is an equivalence relation.\n\n\nProof. To show transitivity, assume \\(A\\) is similar to \\(B\\) and \\(B\\) is similar to \\(C\\), then there exists invertible matrices \\(E\\) and \\(F\\) such that \\(AE=EB\\) and \\(BF=FC\\). Then \\[\nA=EBE^{-1}=E(FCF^{-1})E^{-1}=(EF)C(F^{-1}E^{-1})=(EF)C(EF)^{-1}\n\\] which shows that \\(A\\) is similar to \\(C\\). The proof of the reflexive property and the symmetric property are left for the reader.\n\nBy \\(\\ref{simequiv}\\), a linear transformation represents a whole class of (similar) matrices. That is the collection of \\(n\\times n\\) matrices is partitioned into non overlapping sets of matrices, with all matrices in any such set being similar and representing the same linear transformation from \\(\\mathbf{R}^n\\) to \\(\\mathbf{R}^n\\)."
  },
  {
    "objectID": "introduction-to-linear-spaces.html",
    "href": "introduction-to-linear-spaces.html",
    "title": "11  Introduction to Linear Spaces",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\n\nExample 11.1 Show that the set of solution to a homogenous system form a linear space with standard operations.\n\n\nExample 11.2 Show that the set of vectors for which a particular linear system has a solution is a linear space.\n\n\nDefinition 11.1 Let \\(\\mathbb{F}\\) be a field (whose elements are called scalars ) and let \\(V\\) be a nonempty set (whose elements are called vectors) on which two operations, called addition and scalar multiplication, have been defined. The addition operation (denoted by \\(+\\)), assigns to each pair \\((u,v)\\in V\\times V\\), a unique vector \\(u+v\\) in \\(V\\). The scalar multiplication operation (denoted by juxtaposition), assigns to each pair \\((a,v)\\in \\mathbb{F}\\times V\\) a unique vector \\(a v\\) in \\(V\\).\nWe call \\(V\\) a linear space if the following axioms (A1)-(A8) are also satisfied.\n\nFor all \\(u, v\\in V\\), \\(u+v=v+u\\).\nFor all \\(u, v, w \\in V\\), \\((u+v)+w=u+(v+w)\\).\nThere exists \\(0\\in V\\) such that \\(v+0=v\\) for all \\(v\\in V\\).\nFor every \\(v\\in V\\), there exists \\(w\\in V\\) such that \\(v+w=0\\).\nFor all \\(v\\in V\\), \\(1 v=v\\).\nFor all \\(a, b\\in \\mathbb{F}\\) and \\(u\\in V\\), \\((a b) v=a (b v)\\).\nFor all \\(a \\in \\mathbb{F}\\) and \\(u, v\\in V\\), \\(a(u+v)=a u+av\\).\nFor all \\(a, b \\in \\mathbb{F}\\) and \\(u\\in V\\), \\((a+b)u=a u+ b u\\).\n\nIf \\(\\mathbb{F}=\\mathbb{R}\\) then \\(V\\) is a called a real linear space . If \\(\\mathbb{F}=\\mathbb{C}\\) then \\(V\\) is called a complex linear space . We denote the zero vector (A3) as \\(0\\), to distinguish between the zero vector and the zero \\(0\\) in the field of scalars.\n\n\nExample 11.3 Let \\(V=\\{(x,y)\\mid y=mx\\}\\), where \\(m\\) is a fixed real number and \\(x\\) is an arbitrary real number. Show that \\(V\\) is a linear space.\n\n\nExample 11.4 Let \\(V=\\{(x,y,x)\\mid ax+by+cz=0\\}\\) where \\(a, b\\) and \\(c\\) are fixed real numbers. Show that \\(V\\) is a linear space with the standard operations.\n\n::: {#exm- } [Matrix Space] Show that the set \\(M_{m\\times n}\\) of all \\(m\\times n\\) matrices, with ordinary addition of matrices and scalar multiplication, forms a linear space.\n:::\n::: {#exm- } [Polynomial Space] Show that the set \\(P(t)\\) of all polynomials with real coefficients, under the ordinary operations of addition of polynomials and multiplication of a polynomial by a scalar, forms a linear space. Show that the set of all polynomials with real coefficients of degree less than or equal to \\(n\\), under the ordinary operations of addition of polynomials and multiplication of a polynomial by a scalar, forms a linear space.\n:::\n::: {#exm- } [Function Space] Show that the set \\(F(x)\\) of all functions that map the real numbers into itself is a linear space. Show that the set \\(F[a,b]\\) of all functions on the interval \\([a,b]\\) using the standard operations is a linear space.\n:::\n::: {#exm- } [The Space of Infinite Sequences] Show that the set of all infinite sequences of real numbers is a linear space, where addition and scale multiplication are defined term by term.\n:::\n::: {#exm- } [The Space of Linear Equations] Show that the set \\(L_n\\) of all linear equations with \\(n\\) variables, forms a linear space.\n:::\n\nLemma 11.1 Every linear space \\(V\\) has a unique additive identity (denoted by \\(0\\)).\n\n\nProof. Let \\(u_1\\) and \\(u_2\\) be additive identities in \\(V\\), then \\(v+u_1=v\\) and \\(v+u_2=v\\) for every \\(v\\in V\\). Thus, \\(u_1=u_1+u_2=u_2+u_1=u_2\\) as desired.\n\n\nLemma 11.2 Every \\(v\\in V\\) has a unique additive inverse, (denoted by \\(-v\\)).\n\n\nProof. Let \\(v_1\\) and \\(v_2\\) be additive inverses of \\(w\\) in \\(V\\), then \\(w+v_1=\\mathbf{0}\\) and \\(w+v_2=\\mathbf{0}\\). Thus, \\[\nv_1=v_1+\\mathbf{0}=v_1+(w+v_2)=(v_1+w)+v_2=(w+v_1)+v_2=\\mathbf{0}+v_2=v_2\n\\] as desired.\n\n\nLemma 11.3 If \\(v\\in V\\), then \\(0\\, v=0\\).\n\n\nProof. Let \\(v\\in V\\), then \\(v=1 v=(1+0) v= 1 v+0 v= v+0v\\) which shows that \\(0 v\\) is the additive identity of \\(V\\), namely \\(0 v=\\mathbf{0}\\).\n\n\nLemma 11.4 If \\(a\\in \\mathbb{F}\\), then \\(a\\, 0=0\\).\n\n\nProof. Let \\(a\\in \\mathbb{F}\\), then \\[\na \\mathbf{0}=a(\\mathbf{0}+\\mathbf{0})=a\\mathbf{0}+a\\mathbf{0}\n\\] which shows that \\(a \\mathbf{0}\\) is the additive identity of \\(V\\), namely \\(a \\mathbf{0}=\\mathbf{0}\\).\n\n\nLemma 11.5 If \\(v\\in V\\), then \\(-(-v)=v\\).\n\n\nProof. Let \\(v\\in V\\), then \\[\nv+(-1)v=1 v+(-1) v=(1+(-1)) v=0 v= \\mathbf{0}\n\\] which shows that \\((-1)v\\) is the unique additive inverse of \\(v\\) namely, \\((-1)v=-v\\).\n\n\nLemma 11.6 If \\(v\\in V\\), then \\((-1)\\, v=-v\\).\n\n\nProof. Since \\(-v\\) is the unique additive inverse of \\(v\\), \\(v+(-v)=\\mathbf{0}\\). Then \\((-v)+v=\\mathbf{0}\\) shows that \\(v\\) is the unique additive inverse of \\(-v\\), namely, \\(v=-(-v)\\) as desired.\n\n\nLemma 11.7 If \\(a\\,v=0\\), then \\(a=0\\) or \\(v=0\\).\n\n\nProof. Suppose \\(a\\neq 0\\). If \\(a v =\\mathbf{0}\\) then \\(v=1 v=(a^{-1} a) v=a^{-1} (a v)=a^{-1} \\mathbf{0}=\\mathbf{0}\\). Otherwise \\(a=0\\) as desired.\n\n\nExample 11.5 Let \\(V\\) be a linear space with \\(u\\in V\\) and let \\(a\\) and \\(b\\) be scalars. Prove that if \\(a u=bu\\) and \\(u\\neq 0\\), then \\(a=b\\).\n\nLet \\(V\\) be a linear space and \\(U\\) a nonempty subset of \\(V\\). If \\(U\\) is a linear space with respect to the operations on \\(V\\), then \\(U\\) is called a subspace of \\(V\\).\n\nTheorem 11.1 A subset \\(U\\) of \\(V\\) is a linear subspace of \\(V\\) if and only if \\(U\\) has the following properties:\n\n\\(U\\) contains the zero vector of \\(V\\),\n\\(U\\) is closed under the addition defined on \\(V\\), and\n\\(U\\) is closed under the scalar multiplication defined on \\(V\\).\n\n\n\nProof. Koman pg 103.\n\nMore generally, a subset \\(U\\) of \\(V\\) is called a subspace of \\(V\\) if \\(U\\) is also a vector space using the same addition and scalar multiplication as on \\(V\\). Any vector space is a subspace of itself. The set containing just the \\(0\\) vector is also a subspace of any vector space. Given any vector space with a nonzero vector \\(v\\), the scalar multiples of \\(v\\) is a vector subspace of \\(V\\) and is denoted by \\(\\langle v \\rangle\\). Because any linear space \\(V\\) has \\(V\\) and \\(0\\) as subspaces, these subspaces are called the trivial subspaces of \\(V\\). All other subspaces are called proper subspaces of \\(V\\).\n\nExample 11.6 Give an example of a real linear space \\(V\\) and a nonempty set \\(S\\) of \\(V\\) such that, whenever \\(u\\) and \\(v\\) are in \\(S\\), \\(u+v\\) is in \\(S\\) but \\(S\\) is not a subspace of \\(V\\).\n\n\nExample 11.7 Give an example of a real linear space \\(V\\) and a nonempty set \\(S\\) of \\(V\\) such that, whenever \\(u\\) and \\(v\\) are in \\(S\\), \\(c u\\) is in \\(S\\) for every scalar \\(c\\) but \\(S\\) is not a subspace of \\(V\\).\n\n\nExample 11.8 Show that \\(P_n[0,1]\\) is a proper subspace of \\(C[0,1]\\).\n\n\nExample 11.9 Show that \\(C'[0,1]\\) (continuous first derivative) is a proper subspace of \\(C[0,1]\\).\n\n\nExample 11.10 Show that \\(R[0,1]\\) (Riemann integrable) is a proper subspace of \\(C[0,1]\\).\n\n\nExample 11.11 Show that \\(D[0,1]\\) (Differenable functions) is a proper subspace of \\(C[0,1]\\).\n\n\nDefinition 11.2 A linear combination of a list of vectors \\((\\vlist{v}{m})\\) in \\(V\\) is a vector of the form \\(\\lincomb{a}{v}{m}\\) where \\(\\vlist{a}{m} \\in k\\).\n\n\nLemma 11.8 Let \\(U\\) be a nonempty subset of a vector space \\(V\\). Then \\(U\\) is a subspace of \\(V\\) if and only if every linear combination of vectors in \\(U\\) is also in \\(U\\).\n\n\nProof. If \\(U\\) is a subspace of \\(V\\), then \\(U\\) is a vector space and so is closed under linear combinations by definition of vector space. Conversely, suppose every linear combination of vectors in \\(U\\) is also in \\(U\\). Thus for any \\(a, b \\in k\\), \\(a u+b v \\in U\\) for every \\(u, v\\in U\\). In particular, when \\(a=b=1\\) then \\(u+v \\in U\\) and so \\(U\\) is closed with respect to addition. Notice when \\(b=0\\) and \\(a=-1\\), then \\(-u\\in U\\) for every \\(u\\in U\\) and so \\(U\\) is closed under inverses. Notice when \\(u=v\\), \\(a=1\\), and \\(b=-1\\) then \\(u+(-u)=0\\in U\\) so \\(U\\) contains the identity element. The rest of the axioms in the definition of a vector space hold by containment.\n\n\nDefinition 11.3 The intersection and union of subspaces is just the intersection and union of the subspaces as sets. The sum of subspaces \\(\\vlist{U}{m}\\) of a vector space \\(U\\) is defined by\n\\[\nU_1+ U_2+\\cdots +U_m\n= \\{ u_1 + u_2+\\cdots + u_m \\mid u_i \\in U_i \\text{ for } 1\\leq i \\leq m \\}.\n\\]\n\n\nLemma 11.9 Let \\(V\\) be a linear space over a field \\(k\\). The intersection of any collection of subspaces of \\(V\\) is a subspace of \\(V\\).\n\n\nProof. Let \\(\\{U_i\\, |\\, i \\in I\\}\\) be a collection of subspaces where \\(I\\) is some indexed set. Let \\(a,b\\in k\\) and \\(u,v\\in \\cap_{i\\in I} U_i\\). Since each \\(U_i\\) is a subspace of \\(V\\), \\(a u +b v\\in U_i\\) for every \\(i\\in I\\). Thus \\(a u+b v\\in \\cap_{i\\in I} U_i\\) and therefore \\(\\cap_{i\\in I} U_i\\) is a subspace of \\(V\\).\n\n\nExample 11.12 Show that the \\(x\\)-axis and the \\(y\\)-axis are subspaces on \\(\\mathbb{R}^2\\), yet the union of these axis is not.\n\n\nLemma 11.10 Let \\(V\\) be a linear space over a field \\(k\\). The union of two subspaces of \\(V\\) is a subspace of \\(V\\) if and only if one of the subspaces is contained in the other.\n\n\nProof. Suppose \\(U\\) and \\(W\\) are subspaces of \\(V\\) with \\(U\\subseteq W\\). Then \\(U\\cup W=W\\) and so \\(U\\cup W\\) is also a subspace of \\(V\\). Conversely, suppose \\(U\\), \\(W\\), \\(U\\cup W\\) are subspaces of \\(V\\) and suppose \\(u\\in U\\). If \\(u\\in W\\) then \\(U\\) is contained in \\(W\\) as desired. Thus we assume, \\(u\\not\\in W\\). If \\(w\\in W\\), then \\(u+w \\in U\\cup W\\) and either \\(u+w\\in U\\) or \\(u+w\\in W\\). Notice \\(u+w\\in W\\) and \\(w\\in W\\) together yield \\(u\\in W\\) which is a contradiction. Thus \\(u+w\\in U\\) and so \\(w\\in U\\) which yields \\(W\\subseteq U\\) as desired.\n\n\nLemma 11.11 Let \\(V\\) be a linear space over a field \\(k\\). The sum \\(U_{1}+ U_2+\\cdots +U_{m}\\) is the smallest subspace containing each of the subspaces \\(\\vlist{U}{m}\\).\n\n\nProof. The sum of two subspaces is a subspace since the sum of two subspaces is closed under linear combinations. Thus \\(\\vlist{U}{m}\\) is a subspace containing \\(U_i\\) for each \\(1\\leq i \\leq m\\). Let \\(U\\) be another subspace containing \\(U_{i}\\) for each \\(1\\leq i \\leq m\\). If \\(u\\in U_{1}+ \\cdots +U_{m}\\), then \\(u\\) has the form \\(u=u_1+\\cdots + u_m\\) where each \\(u_i\\in U_i\\subseteq U\\). Since \\(U\\) is a subspace \\(u\\in U\\) and so \\(U_{1}+U_{2}+ \\cdots +U_{m}\\) is the smallest such subspace.\n\n\nDefinition 11.4 If \\(\\{\\vlist{v}{m}\\}\\) is a subset of a linear space \\(V\\), then the subspace of all linear combinations of these vectors is called the subspace generated ( spanned) by \\(\\vlist{v}{m}.\\) The spanning set of the list of vectors \\((\\vlist{v}{m})\\) in \\(V\\) is denoted by \\[\n\\text{span}(\\vlist{v}{m})=\n\\{\\lincomb{a}{v}{m} \\mid \\vlist{a}{m}\\in \\mathbb{F} \\}.\n\\]\n\n\nLemma 11.12 The span of a list of vectors in \\(V\\) is the smallest subspace of \\(V\\) containing all the vectors in the list.\n\n\nProof. Let \\((\\vlist{v}{n})\\) be a list of vectors in \\(V\\) and let \\(S\\) denote \\(\\text{span}(\\vlist{v}{n})\\). Clearly, \\(S\\) contains \\(v_i\\) for each \\(1\\leq i \\leq n\\). Let \\(u,v \\in S\\) and \\(a,b\\in k\\). Then there exists \\(\\vlist{a}{n}\\) in \\(k\\) and \\(\\vlist{b}{n}\\) in \\(k\\) such that \\(u=a_1 v_1+\\cdots a_n v_n\\) and \\(v=b_1 v_1+ \\cdots b_n v_n\\).\nThen \\[\na u+b v\n=(a a_1 +b b_1) v_1+ \\cdots +(a a_n+b b_n) v_n\n\\] which shows \\(a u+b v\\in S\\) since \\(a a_i+b b_i \\in k\\) for each \\(1\\leq i \\leq n\\). Thus \\(S\\) is a subspace containing each of the \\(v_i\\). Let \\(T\\) be a subspace containing \\(v_i\\) for \\(1 \\leq i \\leq n\\). If \\(s\\in S\\), then there exists \\(\\vlist{c}{n} \\in k\\) such that \\(s=c_1 v_1+\\cdots + c_n v_n\\). Since \\(v_i\\in T\\) for each \\(i\\) and \\(T\\) is closed under linear combinations (since \\(T\\) is a subspace), \\(s\\in T\\). Meaning \\(S \\subseteq T\\), so indeed \\(S\\) is the smallest subspace of \\(V\\) containing all the vectors \\(v_i\\).\n\n\nDefinition 11.5 Let \\(\\emptyset\\) denote the empty set. Then \\(\\text{span}(\\emptyset)=\\{0\\}\\).\n\n\nExample 11.13 Let \\(A=\\begin{matrix}1 & 1 \\ 0 & 0 \\end{matrix}\\). Show that \\(S=\\{X\\in M_{2\\times 2} \\mid AX=XA\\}\\) is a subspace of \\(M_{2\\times 2}\\) under the standard operations.\n\n\nExample 11.14 Let \\(f_1=x^2+1, f_2=3x-1, f_3=2\\). Determine the subspace generated by \\(f_1, f_2, f_3\\) in \\(P_4\\).\n\n\nExample 11.15 Let \\(A_1=\\begin{bmatrix} 1 & 0 & 0 & 3 \\\\ 0 & 0 & 2 & 0\\end{bmatrix}\\) and \\(A_2=\\begin{bmatrix} 0 & 2 & 0 & 1 \\\\ 0 & 0 & 0 & 1 \\end{bmatrix}.\\) Determine the subspace generated by \\(A_1\\) and \\(A_2\\) in \\(R_{2\\times 4}\\).\n\n\nExample 11.16 Describe \\(\\text{span}(0)\\).\n\n\nExample 11.17 Consider the subset \\(S=\\{x^3-2x^2+x-3, 2x^3-3x^2+2x+5, 4x^3-7x^2+4x-1, 4x^2+x-3\\}\\) of \\(P\\). Show that \\(3x^3-8x^2+2x+16\\) is in \\(\\text{span} (S)\\) by expressing it as a linear combination of the elements of \\(S\\).\n\n\nExample 11.18 Determine if the matrices \\[\n\\begin{bmatrix} 2 & -1 \\\\ 0 & 2 \\end{bmatrix},\n\\begin{bmatrix} -4 & 2 \\\\ 3 & 0 \\end{bmatrix},  \n\\begin{bmatrix} -1 & 0 \\\\ 2 & 1 \\end{bmatrix},  \n\\begin{bmatrix} 0 & 0 \\\\ 0 & 3 \\end{bmatrix}\n\\] span \\(M_{2\\times 2}\\)."
  },
  {
    "objectID": "linear-independence-and-bases.html",
    "href": "linear-independence-and-bases.html",
    "title": "12  Linear Independence and Bases",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nLet \\(V\\) be a linear save over a field \\(\\mathbb{F}\\). A subset \\(S\\) of \\(V\\) is said to be linearly dependent if there exist distinct \\(\\vlist{v}{n}\\) in \\(S\\) and scalars \\(\\vlist{a}{n}\\) in \\(\\mathbb{F}\\), not all zero, such that \\(\\lincomb{a}{v}=0.\\) If a set \\(S\\) cntains only finitely many \\(v_i\\) we sometimes say that \\(\\vlist{v}{n}\\) are dependent.\n\nDefinition 12.1 A list of vectors \\((\\vlist{v}{m})\\) in \\(V\\) is called linearly independent if the only choice of \\(\\vlist{a}{m}\\in k\\) is \\(a_1=\\cdots=a_m=0\\). A list of vectors that are not linearly independent is called linearly dependent.\n\nNotice from the definition we can conclude that any set which contains a linearly dependent set is linearly dependent. Any subset of a linearly independent set is linearly independent. Any set which contains the zero vector is linearly dependent. A set \\(S\\) of vectors is linearly independent if and only if each finite subset of \\(S\\) is linearly independent. (Show!)\n::: {#lem- } [Linear Dependence] If \\((\\vlist{v}{m})\\) is linearly dependent in \\(V\\) and \\(v_1\\neq 0\\) then there exists \\(j\\in {2,\\ldots,m}\\) such that the following hold: \\(v_j\\in\\text{span}(v_1,\\ldots,v_{j-1})\\) and if the \\(j^{th}\\) term is removed from \\((\\vlist{v}{m})\\), the span of the remaining list equals \\(\\text{span}(\\vlist{v}{m})\\). :::\n\nProof. The proof is left for the reader as Exercise \\(\\ref{ex:Linear Dependence Lemma}\\).\n\n\nExample 12.1 Show that the following subset of \\(M_{2\\times 2}\\) is a linear dependent set. \\[\\begin{equation}\n\\label{lindeexample}\n\\left\\{\n\\begin{bmatrix}\n2 & 3 \\\\\n-1 & 4\n\\end{bmatrix},\n\\begin{bmatrix}\n-11 & 3 \\\\\n-2 & 2\n\\end{bmatrix},\n\\begin{bmatrix}\n6 & -1 \\\\\n3 & 4\n\\end{bmatrix},\n\\begin{bmatrix}\n-1 & 0\\\\2 & 2\n\\end{bmatrix}\n\\right\\}\n\\end{equation}\\]\n\n\nExample 12.2 Suppose that \\(S\\) is the subset \\[\nS=\\{2x^3-x+3, 3x^3+2x-2, x^3-4x+8, 4x^3+5x-7\\}\n\\] of \\(P_3\\).\n\nShow that \\(S\\) is linear dependent.\nShow that every three element subset of \\(S\\) is linear dependent.\nShow that every two element subset of \\(S\\) is linear independent.\n\n\n\nExample 12.3 Show that no linear independent set can contain zero.\n\n::: {#lem- } [Linear Independence Lemma] Let \\(S\\) be a linearly independent subset of a vector space \\(V\\). Suppose \\(v\\) is a vector in \\(V\\) which is not in the subspace spanned by \\(S\\). Then the set obtained by adjoining \\(v\\) to \\(S\\) is linearly independent. :::\n\nProof. Suppose \\(u_1,\\ldots,u_n\\) are distinct vectors in \\(S\\) and that \\(a_1 u_1+\\cdots + a_n u_n+a v =0.\\) Then \\(a=0\\); for otherwise, \\[\nv=\\left (-\\frac{a_1}{a}\\right ) u_1+\\cdots + \\left (-\\frac{a_n}{a} \\right )u_n\n\\] and \\(v\\) is in the subspace spanned by \\(S\\). Thus \\(a_1 u_1+\\cdots + a_n u_n=0\\), and since \\(S\\) is a linearly independent set each \\(a_i=0\\).\n\n\nTheorem 12.1 If \\(S\\) is a set of vectors in a vector space \\(V\\) over a field \\(F\\) then the following are equivalent.\n\n\\(S\\) is linearly independent and spans \\(V\\)\nFor every vector \\(v\\in V\\), there is a unique set of vectors \\(v_1,\\ldots,v_n\\) in \\(S\\), along with a unique set of scalars in \\(\\mathbb{F}\\) for which \\(v=a_1 v_1+\\cdots+a_n v_n\\)\n\\(S\\) is a minimal spanning set in the sense that \\(S\\) spans \\(V\\), and any proper subset of \\(S\\) does not span \\(V\\)\n\\(S\\) is a maximal linearly independent set in the sense that \\(S\\) is linearly independent, but any proper superset of \\(S\\) is not linearly independent.\n\n\n\nProof. \\((i) \\Leftrightarrow (ii)\\): Then \\(S\\) is a spanning set. If some proper subset \\(S'\\) of \\(S\\) also spanned \\(V\\), than any vector in \\(S-S;\\) would be a linear combination of the vectors in \\(S'\\). contradicting the fact that the vectors in \\(S\\) are linearly independent. Conversely, if \\(S\\) is a minimal spanning set, then it must be linearly independent. For if not, some vector \\(s\\in S\\) would be a linear of the other vectors in \\(S\\), and so \\(S-S'\\) would be a proper spanning subset of \\(S\\), which is not possible.\n\\((i) \\Leftrightarrow (iv)\\): Then \\(S\\) is linearly independent. If \\(S\\) were not maximal, there would be a vector \\(v\\in V-S\\) for which the set \\(S\\cup \\{v\\}\\) is linear independent. But then \\(V\\) is not in the span of \\(S\\), contradicting the fact that \\(S\\) is a spanning set. Hence, \\(S\\) is a maximal linearly independent set, and so (i) implies (iv). Conversely, if \\(S\\) is a maximal independent set,. then it must span \\(V\\), for if not, we could find a vector \\(v\\in S-S'\\) that is not a linear combination of the vectors in \\(S\\). Hence, \\(S\\cup \\{v\\}\\) would be a contradiction.\n\n\nDefinition 12.2 A basis of \\(V\\) is a list of vectors in \\(V\\) that is linearly independent and spans \\(V\\).\n\n\nExample 12.4 Find a basis for the space of all \\(2\\times 2\\) matrices \\(S\\) such that \\[\n\\begin{bmatrix} 1 & 1 \\\\ 1 & 1\\end{bmatrix}S=S.\n\\] Let \\(S=\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\). Then \\[\n\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix} \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix} = \\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\n\\] meaning \\[\n\\begin{bmatrix} a+c & b+d \\\\ a+c & b+d \\end{bmatrix}=\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}.\n\\] So \\(a+c=a, b+d=b, a+c=c,\\) and \\(b+d=d\\). These imply, respectively that \\(a=b=c=d=0\\).\n\n\nCorollary 12.1 A list \\((v_1,\\ldots,v_m)\\) of vectors in \\(V\\) is a basis of \\(V\\) if and only if every \\(v\\in V\\) can be written uniquely in the form \\(v=a_1 v_1+\\ldots+a_m v_m\\) where \\(\\vlist{a}{m}\\in k\\).\n\n\nProof. \n\n\nCorollary 12.2 Any \\(n\\) linearly independent vectors in a linear space \\(V\\) of dimension \\(n\\) constitute a basis for \\(V\\).\n\n\nProof. \n\n\nExample 12.5 Show that the space of all \\(m\\times n\\) matrices over a field \\(\\mathbb{F}\\) has dimension \\(mn\\).\n\n\nCorollary 12.3 Any finitely generated linear space, generated by a asset of nonzero vectors, has a basis.\n\n\nProof. \n\nThe zero linear space has no basis, because any subset contains the zero vector and must be linearly dependent.\n\nExample 12.6 Show that \\[\nB=\\{[2,3,0,-1],[-1,1,1,-1],[3,2,-1,0],[2,3,0,-1]\\}\n\\] is a maximal linearly independent subset of \\[\nS=\\{[1,4,1,-2],[-1,1,1,-1],[3,2,-1,0],[2,3,0,-1]\\}.\n\\] Determine \\(\\dim\\text{span}(S)\\) and determine whether or not \\(\\text{span}(S)=P_3\\)."
  },
  {
    "objectID": "finite-dimensional-linear-spaces.html",
    "href": "finite-dimensional-linear-spaces.html",
    "title": "13  Finite-dimensional Linear Spaces",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\n\nDefinition 13.1 The vector space \\(V\\) is a direct sum of subspaces \\(\\vlist{U}{m}\\) of \\(V\\), written \\(V=U_{1} \\oplus \\cdots \\oplus U_{m}\\), if each element of \\(V\\) can be written uniquely as a sum \\(u_1 + u_2+\\cdots + u_m\\) where each \\(u_j\\in U_j\\).\n\n\nLemma 13.1 If \\(\\vlist{U}{m}\\) are subspaces of \\(V\\), then \\(V=U_{1} \\oplus \\cdots \\oplus U_{m}\\) if and only if both conditions hold:\n\n\\(V=U_{1} + \\cdots + U_{m}\\).\nthe only way to write \\(0\\) as a sum \\(u_{1} + u_{2}+\\cdots+ u_{m}\\) where each \\(u_{j} \\in U_{j}\\), is by taking all the \\(u_{j}\\)’s equal to \\(0\\).\n\n\n\nProof. If \\(V=U_{1} \\oplus U_2 \\oplus \\cdots \\oplus U_{m}\\), then every element in \\(V\\) can be written uniquely in the form \\(u_1+ u_2+\\cdots +u_m\\) where each \\(u_j\\in U_j\\) for \\(1\\leq i \\leq m\\). Thus both conditions listed above are satisfied. Conversely, suppose both conditions hold and assume \\(u=u_1 + \\cdots +u_m\\) where \\(u_i \\in U_i\\) and \\(u=v_1 + \\cdots + v_m\\) where \\(v_i\\in U_i\\). Since \\[\nu-u=(u_1 + \\cdots +u_m)-(v_1 + \\cdots +v_m)=(u_1-v_1)+\\cdots +(u_m-v_m)=0\n\\] it follows \\(u_i=v_i\\) for \\(1\\leq i \\leq m\\); and so uniqueness is established.\n\n\nExample 13.1 Let \\(V\\) be the linear space of all functions from \\(\\mathbb{R}\\) to \\(\\mathbb{R}\\) and let \\(V_e\\) and \\(V_o\\) be the set of all even functions (\\(f(-x)=f(x)\\)) and the set of odd functions (\\(f(-x)=-f(x)\\)), respectively.\n\nProve that \\(V_e\\) and \\(V_o\\) are subspaces of \\(V\\).\nProve that \\(V_e+V_o=V\\)\nProve that \\(V_e\\cap V_o=\\{0\\}\\).\n\n\n\nLemma 13.2 If \\(U\\) and \\(W\\) are subspaces of \\(V\\), then \\(V=U \\oplus W\\) if and only if \\(V=U+W\\) and \\(U \\cap W= \\{0\\}\\).\n\n\nProof. If \\(V=U \\oplus W\\) then \\(V=U+W\\) and \\(\\{0\\} \\subseteq U\\cap W\\) are immediate. Suppose \\(v\\in U \\cap W\\). Since \\(v\\in U \\oplus W\\), there exists unique \\(u\\in U\\) and \\(w\\in W\\) such that \\(v=u+w\\). Assume for a contradiction, \\(u\\not = 0\\). Then \\(u=u+0\\) where \\(0\\in W\\) and \\(u \\in U\\) shows that \\(u\\not \\in W\\) since \\(V=U\\oplus W\\). Also, \\(v=u+w\\in U\\cap W\\) so \\(u+w\\in W\\) and \\(w\\in W\\) implies \\(u\\in W\\). This contradiction leads to \\(u=0\\). Similarly, \\(w=0\\) and so \\(v=u+w=0\\) which yields \\(U \\cap W= \\{0\\}\\). Conversely, suppose \\(V=U+W\\) and \\(U \\cap W= \\{0\\}\\). If \\(0=u+w\\) then \\(u=-w\\in W\\) and together with \\(u\\in U\\) yields \\(u=0\\). Thus, \\(w=0\\) also. So the only way to write 0 as a sum \\(u+w\\) is to have \\(u=w=0\\) therefore \\(V=U \\oplus W\\).\n\n\nExample 13.2 Prove or give a counterexample: if \\(U_1, U_2\\), and \\(W\\) are subspaces of \\(V\\) such that \\(U_1 +W=U_2+W\\), then \\(U_1=U_2\\). False, here is a counterexample. Let \\(V=\\mathbb{R}^2=W\\) with \\(U_1=\\{(x,0) \\mid x \\in \\mathbb{R}\\}\\) and \\(U_2=\\{(0,y) \\mid y\\in \\mathbb{R}\\}\\). Then \\(U_1+W=U_2+W=\\mathbb{R}^2\\). However, \\(U_1\\neq U_2\\).\n\n\nExample 13.3 Prove or give a counterexample: if \\(U_1, U_2, W\\) are subspaces of \\(V\\) such that \\(V=U_1 \\oplus W\\) and \\(V=U_2 \\oplus W\\), then \\(U_1=U_2\\). False, here is a counterexample. Let \\(V=\\mathbb{R}^2\\) with \\(W=\\{(x,x) \\mid x\\in \\mathbb{R}\\}\\), \\(U_1=\\{(x,0) \\mid x \\in \\mathbb{R}\\}\\), and \\(U_2=\\{(0,y) \\mid y\\in \\mathbb{R}\\}\\). All of these sets are subspaces. Let \\(u\\in U_1\\cap W\\), then \\(u=(x,0)\\) and \\(u=(z,z)\\) so \\(z=0\\) and \\(x=0\\) which implies \\(u=(0,0)\\). In fact \\(U_1\\cap W=\\{0\\}\\). Thus, \\(\\mathbb{R}^2=U_1\\oplus W\\). Also \\(\\mathbb{R}^2=U_2\\oplus W\\). However, \\((1,0)\\in U_1\\) and \\((1,0)\\not \\in U_2\\) showing \\(U_1\\neq U_2\\).\n\n\nExample 13.4 Suppose \\(m\\) is a positive integer. Is the set consisting of 0 and all polynomials with coefficients in \\(F\\) and with degree equal to \\(m\\) a subspace of \\(\\mathcal{P}(\\mathbb{F})\\)?\n\n\nDefinition 13.2 We call a linear space \\(V\\) a finite-dimensional linear space if there is a finite list of vectors \\((v_1, \\ldots, v_m)\\) with \\(\\text{span}(v_1, \\ldots, v_m)=V.\\) If a linear space is not a finite-dimensional vector space it is called an infinite-dimensional vector space.\n\n\nLemma 13.3  Let \\(V\\) be a finite-dimensional vector space. Then the length of every linearly independent list of vectors in \\(V\\) is less than or equal to the length of every spanning list of vectors in \\(V\\).\n\n\nProof. \n\n\nLemma 13.4 Let \\(V\\) be a finite-dimensional vector space. Then \\(V\\) only has finite-dimensional subspaces.\n\n\nProof. \n\n\nTheorem 13.1  Let \\(V\\) be a finite-dimensional vector space. Then every spanning list \\((v_1,\\ldots,v_m)\\) of a vector space can be reduced to a basis of the vector space,\n\n\nProof. \n\n\nTheorem 13.2 Let \\(V\\) be a finite-dimensional vector space. Then if \\(V\\) is finite-dimensional then \\(V\\) has a basis.\n\n\nProof. By definition, every finite-dimensional vector space has a finite spanning set. By \\(\\ref{spanning reduce lemma}\\), every spanning set can be reduced to a basis, and so every finite-dimensional vector space does indeed have a basis.\n\n\nTheorem 13.3 Let \\(V\\) be a finite-dimensional vector space. Then if \\(W\\) is a subspace of \\(V\\), every linearly independent subset of \\(W\\) is finite and is part of a finite basis for \\(W\\).\n\n\nProof. \n\n\nTheorem 13.4 Let \\(V\\) be a finite-dimensional vector space. Then every linearly independent list of vectors in \\(V\\) can be extended to a basis of \\(V\\),\n\n\nProof. \n\n\nTheorem 13.5 Let \\(V\\) be a finite-dimensional vector space. Then if \\(U\\) is a subspace of \\(V\\), then there is a subspace \\(W\\) of \\(V\\) such that \\(V=U \\bigoplus W\\), and\n\n\nProof. \n\n\nTheorem 13.6 Let \\(V\\) be a finite-dimensional vector space. Then any two bases of \\(V\\) have the same length.\n\n\nProof. \n\n\nDefinition 13.3 The dimension of a finite-dimensional vector space is defined to be the length of any basis of the vector space.\n\n\nTheorem 13.7 Let \\(V\\) be a finite-dimensional vector space. Then if \\(U\\) is a subspace of \\(V\\) then \\(\\text{dim} U \\leq \\text{dim} V\\).\n\n\nProof. \n\n\nTheorem 13.8 Let \\(V\\) be a finite-dimensional vector space. Then every spanning list of vectors in \\(V\\) with length dim \\(V\\) is a basis of \\(V\\).\n\n\nProof. \n\n\nTheorem 13.9 Let \\(V\\) be a finite-dimensional vector space. Then every linearly independent list of vectors in \\(V\\) with length dim \\(V\\) is a basis of \\(V\\).\n\n\nProof. \n\n\nTheorem 13.10 Prove that if \\(U_1\\) and \\(U_2\\) are subspaces of a finite-dimensional vector space, then \\[\n\\text{dim}  (U_1+U_2)= \\text{dim}  U_1 + \\text{dim}  U_2 -\\text{dim}  (U_1 \\cap U_2).\n\\]\n\n\nProof. \n\n\nTheorem 13.11 If \\(V\\) is finite-dimensional, and \\(U_1, \\ldots, U_m\\) are subspaces of \\(V\\) such that\n\n\\(V=U_1+\\cdots+U_m\\) and\n\\(\\text{dim} V=\\text{dim} U_1+\\cdots+\\text{dim} U_m\\),\n\nthen \\(V=U_1 \\oplus \\cdots \\oplus U_m.\\)\n\n\nProof. Choose a basis for each \\(U_j\\). Put these bases together in one list, forming a list that spans \\(V\\) and has length \\(\\text{dim} (V)\\). Thus this list is a basis of \\(V\\) and in particular it is linearly independent. Now suppose that \\(u_j\\in U_j\\) for each \\(j\\) are such that \\(0=u_1+\\cdots + u_m\\). We can write each \\(u_j\\) as a linear combination of the basis vectors of \\(U_j\\). Substituting these expressions above, we have written \\(0\\) as a linear combination of the basis vectors of \\(V\\). Thus all scalars used in the linear combinations must be zero. Thus each \\(u_j=0\\) which proves \\(V=U_1 \\oplus \\cdots \\oplus U_m.\\)"
  },
  {
    "objectID": "infinite-dimensional-linear-spaces.html",
    "href": "infinite-dimensional-linear-spaces.html",
    "title": "14  Infinite-dimensional Linear Spaces",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\n\nExample 14.1 Prove that \\(k^\\infty\\) is infinite-dimensional. Suppose \\(\\mathbb{F}^\\infty\\) is finite-dimensional with dimension \\(n\\). Let \\(v_i\\) be the vector in \\(\\mathbb{F}^\\infty\\) consisting of all 0’s except with 1 in the \\(i\\)-th position for \\(i=1,...,n\\). The vectors \\((v_1,...,v_n)\\) are linearly independent in \\(\\mathbb{F}^\\infty\\) and so they must form a basis; however they do not span since the vector \\(v_{n+1}\\) consisting of all 0’s except with \\(1\\) in the \\((n+1)\\)-th position. Thus \\(\\mathbb{F}^\\infty\\) can not be finite-dimensional.\n\n\nExample 14.2 Prove that the real vector space consisting of all continuous real-valued functions on the interval \\([0,1]\\) is infinite-dimensional. Notice that \\(f(x)=x^n\\) is continuous on \\([0,1]\\) for every positive integer \\(n\\). If this vector space is finite-dimensional of say dimension \\(n\\) then the list of vectors \\((1,x,x^2,...,x^{n-1})\\) must form a basis since they are linearly independent and there are \\(n\\) of them. However, \\(x^n\\) is not in the span of this list and so this list can not be a basis. This contradiction shows that this vector space must be infinite-dimensional.\n\n\nExample 14.3 Prove that \\(V\\) is infinite-dimensional if and only if there is a sequence \\(v_1, v_2,...\\) of vectors in \\(V\\) such that \\((v_1, ..., v_n)\\) is linearly independent for every positive integer \\(n\\). Suppose \\(V\\) is infinite-dimensional. Then \\(V\\) has no finite spanning list. Pick \\(v_1\\neq 0\\). For each positive integer \\(n\\) choose \\(v_{n+1}\\not \\in \\text{ span}(v_1,...,v_n)\\), by the linear independence lemma, and for each positive integer \\(n\\), the list \\((v_1,...,v_n)\\) is linearly independent. Conversely, suppose there is a sequence \\(v_1,v_2,...,\\) of vectors in \\(V\\) such that \\((v_1,...,v_n)\\) is linearly independent for every positive integer \\(n\\). If \\(V\\) is finite-dimensional, then it has a spanning list with \\(M\\) elements. By the previous theorem, every linearly independent list has no more than \\(M\\) elements. Therefore, \\(V\\) is infinite-dimensional.\n\n\nTheorem 14.1 Every vector space has a basis. Moreover, any two bases have the same carnality.\n\n\nProof. Let \\(V\\) be a nonzero vector space and consider the collection \\(A\\) of all linearly independent subsets of \\(V\\). This collection is nonempty, since any single nonzero vector forms a linearly independent set. Now, if \\(I_1\\subset I_2 \\subset \\cdots\\) is a chain of linearly independent subsets of \\(V\\), then the union \\(U\\) is also a linearly independent set. Hence, every chain in \\(A\\) has an upper bound in \\(A\\), and according to Zorn’s lemma, \\(A\\) must contain a maximal element, that is, \\(V\\) has a maximal linearly independent set, which is a basis for \\(V\\).\nWe may assume that all bases for \\(V\\) are infinite sets, for if any basis is finite, then \\(V\\) has a finite spanning set and so is a finite-dimensional vector space. Let \\(B\\) be a basis for \\(V\\). We may write \\(B=\\{b_i \\mid i\\in I\\}\\) where \\(I\\) is some indexed set, used to index the vectors in \\(B\\). Note that \\(|I|=|B|\\). Now let \\(C\\) be another basis for \\(V\\). Then any vector \\(c\\in C\\) can be written as a finite linear combination of the vectors in \\(B\\), where all the coefficients are nonzero, say \\(c=\\sum_{i\\in U_c} r_i b_i\\). Here \\(U_c\\) is a finite subset of the index set \\(I\\). Now, because \\(C\\) is a basis for \\(V\\), the union of all of the \\(U_c\\)’s as \\(C\\) varies over \\(C\\) must be in \\(I\\), in symbols, \\(\\bigcup_{c\\in C} U_c=I\\). For if all vectors in the basis \\(C\\) can be expressed as a finite linear combination of the vectors \\(B-\\{ b_k\\}\\) spans \\(V\\), which is not the case. Therefore, \\(|B|=|I|\\leq |C| \\alpha_0=|C|.\\) But we may also reverse the roles of \\(B\\) and \\(C\\) we obtain the reverse inequality. Therefore, \\(|B|=|C|\\) as desired."
  },
  {
    "objectID": "introduction-to-linear-maps.html",
    "href": "introduction-to-linear-maps.html",
    "title": "15  Introduction to Linear Maps",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\n\nDefinition 15.1 Let \\(V\\) and \\(W\\) be linear spaces. A function \\(T\\) from \\(V\\) to \\(W\\) is called a linear map if \\[\nT(f+g)=T(f)+T(g)\n\\qquad \\text{and}\\qquad\nT(k f)=k T(f)\n\\] for all elements \\(f\\) and \\(g\\) of \\(V\\) and for all scalars \\(k\\).\n\nThe collection of all linear maps from \\(V\\) to \\(W\\) is denoted by \\(\\mathcal{L}(V,W)\\) and if \\(T\\) is a linear map from \\(V\\) to \\(W\\) we denote this by \\(T\\in \\mathcal{L}(V,W)\\).\n\nDefinition 15.2 Let \\(T\\in \\mathcal{L}(V,W)\\).\n\nThen the kernel of \\(T\\) is the subset of \\(V\\) consisting of the vectors that \\(T\\) maps to 0; that is\n\\(\\ker(T)=\\{v\\in V \\, | \\, T v=0\\}.\\)\nThen the image of \\(T\\) is the subset of \\(W\\) consisting of the vectors of the form \\(Tv\\) for some \\(v\\in V\\); that is \\(\\text{im}(T)=\\{w\\in W \\, | \\, \\text{ there exists } v \\in V \\text{ such that } w=T v\\}.\\)\n\n\n\nExample 15.1 The function from \\(C^{\\infty}\\) to \\(C^{\\infty}\\) defined by \\(T(f)=f''\\) is a linear map. Find the kernel and image of \\(T\\).\n\n\nExample 15.2 The function from \\(C^{\\infty}\\) to \\(C^{\\infty}\\) defined by $T(f)=_0^1 f(x) , dx $ is a linear map. Find the kernel and image of \\(T\\).\n\n\nTheorem 15.1 If \\(T \\in \\mathcal{L}(V,W),\\) then null \\(T\\) is a subspace of \\(V\\).\n\n\nProof. By definition, \\(\\text{ker} T=\\{v\\in V \\mid T v=0\\}\\) and so \\(\\text{ker} T \\subseteq W\\). Let \\(u, v\\in \\text{ker} T\\). Then, \\(T(u+v)=T u+ Tv=0+0=0\\) which shows, \\(u+v\\in \\text{ker} T\\), for all \\(u , v \\in \\text{ker} T\\). Let \\(k\\) be a scalar and \\(u\\in \\text{ker} T\\). Then \\(T(k u)=k T u= k(0)=0\\), which shows, \\(k u\\in \\text{ker} T\\) for all scalars \\(k\\) and all \\(u\\in \\text{ker} T\\). Since \\(T(0)=T(0+0)=T(0)+T(0)\\), \\(T(0)=0\\) in \\(W\\) which shows, \\(0\\in \\text{ker} T\\). Therefore, \\(\\text{ker} T\\) is a subspace of \\(W\\).\n\n\nDefinition 15.3 A linear map \\(T: V\\rightarrow W\\) is called injective whenever \\(u, v\\in V\\) and \\(T u=T v\\), we have \\(u=v\\).\n\n\nTheorem 15.2 Let \\(T\\in \\mathcal{L}(V,W)\\), then \\(T\\) is injective if and only if null \\(T=\\{0\\}\\).\n\n\nProof. Suppose \\(T\\) is injective. Since \\(T(0)=0\\), \\(0\\in \\text{ker} T\\) and so \\(\\{0\\}\\subseteq \\text{ker} T\\). Let \\(v\\in \\text{ker} T\\). Then \\(T(0)=0=T(v)\\) yields \\(v=0\\) because \\(T\\) is injective. Thus, \\(\\text{ker} T\\subseteq \\{0\\}\\). Therefore, \\(\\text{ker} T=\\{0\\}\\). Conversely, assume \\(\\text{ker} T=\\{0\\}\\). Let \\(u, v\\in V\\). If \\(Tu=Tv\\), then \\(T(u-v)=Tu - T v=0\\) which shows \\(u-v \\in \\text{ker} T\\). Thus \\(u=v\\) and therefore, \\(T\\) is injective.\n\n\nDefinition 15.4 For \\(T\\in \\mathcal{L}(V,W)\\), the image of \\(T\\), denoted by \\(\\text{im}(T)\\), is the subset of \\(W\\) consisting of those vectors that are of the form \\(T v\\) for some \\(v\\in V\\).\n\n\nDefinition 15.5 A linear map \\(T: V\\rightarrow W\\) is called surjective if its range equals \\(W\\).\n\n\nTheorem 15.3 If \\(T\\in \\mathcal{L}(V,W)\\), then \\(\\text{im} T\\) is a subspace of \\(W\\).\n\n\nProof. By definition, \\(\\text{im} T=\\{T v \\mid v\\in V\\}\\subseteq W\\). Let \\(w_1,w_2\\in W \\in \\text{im} T\\). Then there exists \\(v_1,v_2\\in V\\) such that \\(w_1=Tv_1\\) and \\(w_2=T v_2\\). By linearity of \\(T\\), \\(w_1+w_2=Tv_1+Tv_2=T(v_1+v_2)\\) which shows \\(w_1+w_2\\in \\text{im} T\\) for all \\(w_1,w_2 \\in \\text{im} T\\). Let \\(w\\in \\text{im} T\\) and let \\(k\\) be a scalar. Then there exists \\(v\\in V\\) such that \\(w=T v\\). By linearity of \\(T\\), $ kw =k Tv =T(k v)$ which shows \\(kw\\in \\text{im} T\\) for all \\(w\\in \\text{im} T\\) and for all scalars \\(k\\). Therefore, \\(\\text{im} T\\) is a subspace of \\(W\\).\n\n::: {#thm- } [Rank-Nullity Theorem] If \\(V\\) is finite-dimensional and \\(T\\in \\mathcal{L}(V,W)\\), then range \\(T\\) is finite-dimensional and \\(\\text{dim} V= \\text{dim} (\\text{im} T) + \\text{dim} (\\text{ker} T)\\). :::\n\nProof. Since \\(V\\) is a finite-dimensional and \\(\\text{ker} T\\) is a subspace of \\(V\\), \\(\\text{ker} T\\) is finite-dimensional and so let \\((u_1,\\ldots,u_n)\\) be a basis of \\(\\text{ker} T\\). Since \\((u_1,\\ldots,u_n)\\) is linearly independent in \\(V\\), it can be extended to a basis of \\(V\\), say \\(\\mathcal{B}=(u_1,\\ldots,u_n,v_1,\\ldots,v_m)\\). It suffices to show \\((T v_1,\\ldots, T v_m)\\) is a basis of \\(\\text{im} T\\), for then \\(\\text{im} T\\) is finite-dimensional is proven and \\(\\text{dim} V=n+m=\\text{dim} \\text{ker} T+\\text{dim} \\text{im} T\\) holds as well.\nLet \\(w\\in \\text{im} T\\). Then there exists \\(v\\in V\\) and scalars \\(a_1,\\ldots,a_n, b_1,\\ldots,b_m\\) such that \\[w=Tv=T(a_1 u_1+\\cdots + a_n u_n+b_1 v_1+\\cdots + b_m v_m)=b_1 T v_1+\\cdots + b_m T v_m.\\] Thus \\((T v_1,\\ldots,T v_m)\\) spans \\(\\text{im} T\\). Suppose \\(c_1,\\ldots,c_m\\) are scalars such that \\(c T v_1+\\cdots + c_m T v_m=0\\). Then there exist scalars \\(d_1,\\ldots,d_n\\) such that \\[c_1 v_1+\\cdots + c_m v_m=d_1 u_1+\\cdots +d_n u_n.\\] Since \\(\\mathcal{B}\\) is a basis of \\(V\\) and \\(c_1 v_1+\\cdots c_m v_m+(-d_1) u_1 + \\cdots +(-d_n)u_n=0\\) it follows \\(c_1=\\cdots = c_m = d_1 = \\cdots = \\cdots =d_n=0\\). In particular, \\(c_1=\\cdots = c_m=0\\) shows \\(T v_1,\\ldots, T v_m\\) are linearly independent. Therefore, \\((T v_1,\\ldots, T v_m)\\) is a basis fo \\(\\text{im} T\\).\n\n\nCorollary 15.1 If \\(V\\) and \\(W\\) are finite-dimensional vector spaces with \\(T\\in \\mathcal{L}(V,W)\\), then\n\n\\(\\text{dim} V > \\text{dim} W \\implies T\\) is not injective, and\n\\(\\text{dim} V < \\text{dim} W \\implies T\\) is not surjective.\n\n\n\nProof. The proof of each part follows.\n\nBy the Rank-Nullity Theorem, \\(\\text{dim} V=\\text{dim} \\text{ker} T+ \\text{dim} \\text{im} T \\leq \\text{dim} \\text{ker} T +\\text{dim} W.\\) Since $ V > W $, it follows \\(0<\\text{dim} V-\\text{dim} W\\leq \\text{dim} \\text{ker} T.\\) Thus, \\(\\text{ker} T \\neq \\{0\\}\\) and so \\(T\\) is not injective.\nAgain by the Rank-Nullity Theorem, \\(\\text{dim} V=\\text{dim} \\text{ker} T+ \\text{dim} \\text{im} T\\) and so \\(\\text{dim} \\text{im} T\\leq \\text{dim} V\\). Since $ V < W $, \\(0<\\text{dim} W-\\text{dim} V\\leq \\text{dim} W-\\text{dim} \\text{im} T\\) and so there exists a nonzero element \\(w\\in W\\) with \\(w\\in \\text{im} T\\). Therefore, \\(T\\) is not surjective.\n\n\n\nExample 15.3 Suppose that \\(T\\) is a linear map from \\(V\\) to \\(\\mathbb{F}\\). Prove that if \\(u \\in V\\) is not in \\(\\text{ker} T\\), then \\(V=\\text{ker} T \\, \\oplus \\{a u:a\\in F\\}\\). Let \\(U=\\{a u \\mid a\\in \\mathbb{F}\\}\\). The following arguments show \\(V=\\text{ker} T +U\\) and \\(\\text{ker} T \\cap U=\\{0\\}\\), respectively.\n\nLet \\(v\\in V\\) with \\(Tv=b\\). Since \\(Tu\\neq 0\\) there is a \\(u_1 \\in U\\) such that \\(T u_1=b\\). Then we can write \\(v=u_1+(v-u_1)\\), and \\(v-u_1\\in \\text{ker} T\\). This gives \\(V=\\text{ker} T+ U\\).\nLet \\(v\\in \\text{ker} T \\cap U\\), then there exists \\(a \\in \\mathbb{F}\\) such that \\(v=a u\\) and so \\(T(v)=a T u=0\\) since \\(T v\\in \\text{ker} T\\). Thus \\(a=0\\) and so \\(v=0\\) meaning \\(\\text{ker} T \\cap U\\subseteq \\{0\\}\\). Since \\(0 \\in \\text{ker} T \\cap U\\), it follows \\(\\text{ker} T \\cap U= \\{0\\}\\).\n\n\n\nExample 15.4 Suppose that \\(T\\in \\mathcal{L}(U,W)\\) is injective and \\((v_1,\\ldots,v_n)\\) is linearly independent in \\(V\\). Prove that \\((T v_1,\\ldots,T v_n)\\) is linearly independent in \\(W\\). Suppose \\(a_1 T v_1+ \\cdots a_n T v_n=0\\) in \\(W\\) where \\(a_1,\\ldots,a_n \\in \\mathbb{F}\\). Then by linearity of \\(T\\), \\(T(a_1 v_1 + \\cdots + a_n v_n)=0\\). Since \\(T(0)\\) and \\(T\\) is injective, \\(a_1 v_1+\\cdots + a_n v_n=0\\). Since \\((v_1,\\ldots,v_n)\\) is linearly independent \\(a_1=\\cdots = a_n =0\\). Therefore, \\((T v_1,\\ldots, T v_n)\\) is linearly independent.\n\n\nExample 15.5 Show that every linear map from a one-dimensional vector space to itself is multiplication by some scalar. More precisely, prove that if \\(\\text{dim} V=1\\) and \\(T\\in \\mathcal{L}(V,V)\\), then there exists \\(a\\in \\mathbb{F}\\) such that \\(T v=a v\\) for all \\(v\\in V\\). Let \\(\\{w\\}\\) be a basis of \\(V\\), and let \\(v\\in V\\). Then there exists \\(c\\in \\mathbb{F}\\) such that \\(v=c w\\). Applying \\(T\\) yields, \\[\nT v=T(c w)=cTw=c(a w)=(ca)w=a(cw)=a v\n\\] where \\(Tw=aw\\) since \\(Tw\\in V\\) and \\(\\{w\\}\\) is a basis of \\(V\\).\n\n\nExample 15.6  linear extension Suppose that \\(V\\) is finite-dimensional. Prove that any linear map on a subspace of \\(V\\) can be extended to a linear map on \\(V\\). In other words, show that if \\(U\\) is a subspace of \\(V\\) and \\(S\\in \\mathcal{L}(U,W)\\), then there exists \\(T\\in \\mathcal{L}(V,W)\\) such that \\(Tu=Su\\) for all \\(u\\in U\\). Let \\((u_1,\\ldots,u_n)\\) be a basis of \\(U\\) and extend this basis of \\(U\\) to a basis of \\(V\\), say \\((u_1,\\ldots,u_n, v_1,\\ldots,v_m)\\). Define \\(T\\) as the linear extension of \\(S\\), as follows \\[\nT(u_i)=S(u_i) \\text{ for } 1\\leq i \\leq n \\hspace{.5cm} \\text{ and } \\hspace{.5cm}  T(v_j)=v_j \\text{ for } 1\\leq j \\leq m.\n\\] Then for all \\(u\\in U\\), \\[\\begin{align*}\nT(u) & =T(a_1 u_1+\\cdots + a_n u_n)\n=a_1 T u_1+\\cdots a_n T u_n \\\\\n& =a_1 S u_1+\\cdots +a_n S u_n\n=S(a_1u_1+\\cdots a_n u_n)\n=S(u)\n\\end{align*}\\] where \\(a_1,\\ldots,a_n\\in \\mathbb{F}\\). By definition of \\(T\\), \\(T\\in \\mathcal{L}(V,W)\\).\n\n\nExample 15.7 Prove that if \\(S_1,\\ldots,S_n\\) are injective linear maps such that \\(S_1 \\cdots S_n\\) makes sense, then \\(S_1 \\cdots S_n\\) is injective. Suppose \\(v\\) and \\(w\\) are any vectors and \\((S_1\\cdots S_n)v=(S_1\\cdots S_n)w\\). Then by definition of composition, \\(S_1(S_2\\cdots S_n)v=S_1(S_2\\cdots S_n)w\\), and since \\(S_1\\) is injective \\(S_2(S_3\\cdots S_n)v=S_2(S_3\\cdots S_n)w\\). Since \\(S_2,\\ldots,S_n\\) are all injective \\(v=w\\) as desired showing \\(S_1\\cdots S_n\\) is injective.\n\n\nExample 15.8 Prove that if \\((v_1,\\ldots,v_n)\\) spans \\(V\\) and \\(T\\in \\mathcal{L}(V,W)\\) is surjective, then \\((T v_1,\\ldots,T v_n)\\) spans \\(W\\). Let \\(w\\in W\\). Then, since \\(T\\) is surjective, there exists \\(v\\in V\\) such that \\(T(v)=w\\). Since \\((v_1,\\ldots,v_n)\\) spans \\(V\\) there exists scalars \\(a_1,\\ldots,a_n\\) such that \\(v=a_1 v_1+ \\dots a_n v_n\\) and by linearity of \\(T\\), \\(T(v)=a_1 T v_1+ \\cdots a_n T v_n=w.\\) Therefore, \\((T v_1,\\ldots,T v_n)\\) spans \\(W\\) because every element \\(w\\) in \\(W\\) is a linear combination of the \\(T v\\)’s.\n\n\nExample 15.9 Suppose that \\(V\\) is finite-dimensional and that \\(T\\in \\mathcal{L}(V,W)\\). Prove that there exists a subspace \\(U\\) of \\(V\\) such that \\(U\\cap \\text{ker} T=\\{0\\}\\) and range \\(T = \\{T u : u\\in U\\}\\). Since \\(V\\) is finite-dimensional so is \\(\\text{ker} T\\). Let \\((v_1,\\ldots,v_n)\\) be a basis of \\(\\text{ker} T\\) and extend this basis to a basis of \\(V\\), namely let \\(\\mathcal{B}=(v_1,\\ldots,v_n,u_1,\\ldots,u_m)\\) be a basis of \\(V\\). Let \\(U=\\text{span} (u_1,\\ldots,u_m)\\), we will show \\(U\\cap \\text{ker} T=\\{0\\}\\) and range \\(T = \\{T u : u\\in U\\}\\). Clearly, \\(\\{0\\}\\subseteq U\\cap \\text{ker} T\\) since both \\(U\\) and \\(\\text{ker} T\\) are subspaces. Let \\(v\\in U\\cap \\text{ker} T\\). Then \\(v\\in U\\) implies there exists \\(a_1,\\ldots,a_m\\) such that \\(v=a_1 u_1+\\cdots +a_m u_m\\). Then \\(v\\in \\text{ker} T\\) implies there exists \\(b_1,\\ldots,b_n\\) such that \\(v=b_1 v_1+\\cdots +b_n v_n\\). Then \\(a_1 u_1+\\cdots +a_m u_m+(-b_1)v_1+\\cdots +(-b_n)v_n=0\\) and since \\(\\mathcal{B}\\) is a basis of \\(V\\) the \\(u\\)’s and \\(v\\)’s are linearly independent and so \\(a_1=\\cdots =a_m=b_1=\\cdots =b_n=0\\) meaning \\(v=0\\); and so \\(\\text{ker} T = \\{0\\}\\). Let \\(v\\in V\\). Then \\(T(v)=a_1T v_1+\\cdots +a_n T v_n+b_1 T u_1+\\cdots + b_m T u_m=b_1 Tu_1+\\cdots +b_m T u_m\\) showing \\(\\{T v : v\\in T\\}\\subseteq \\{T v : v\\in U\\}\\) conversely is obvious, since \\(U\\subseteq V\\).\n\n\nExample 15.10 Prove that if there exists a linear map on \\(V\\) whose null space and range are both finite-dimensional, then \\(V\\) is finite-dimensional.\n\n\nExample 15.11 Suppose that \\(V\\) and \\(W\\) are finite-dimensional and \\(T\\in \\mathcal{L}(V,W)\\). Prove that there exists a surjective linear map from \\(V\\) onto \\(W\\) if and only if \\(\\text{dim} W \\leq \\text{dim} V\\). Suppose \\(T\\) is a linear map of \\(V\\) onto \\(W\\), then \\(\\text{dim} V=\\text{dim} \\text{ker} T+\\text{dim} \\text{im} T\\). Since \\(\\text{dim} \\text{ker} T \\geq 0\\), \\(\\text{dim} V\\geq \\text{dim} \\text{im} T=\\text{dim} W\\), since \\(T\\) is onto. Conversely, assume \\(m=\\text{dim} W\\leq \\text{dim} V=n\\) with bases of \\(W\\) say \\((w_1,\\ldots,w_m)\\) and of \\(V\\) say \\((v_1,\\ldots,v_n)\\). Define \\(T\\) to be the linear extension of: \\[\n\\begin{cases}\nT(v_i)=w_i & \\text{ if } 1\\leq i\\leq m \\\\\nT(v_i)=0 & \\text{ if } i>m\n\\end{cases}\n\\] Then \\(T\\) is surjective since: if \\(w\\in W\\) then there exists \\(a_i\\in \\mathbb{F}\\) such that \\(w=a_1 w_1+\\cdots + a_m w_m=a_1 T(v_1)+\\cdots + a_m T(v_m)=T(a_1 v_1+\\cdots + a_m v_m)\\) showing every element in \\(W\\) is in \\(\\text{im} T\\).\n\n\nExample 15.12 Suppose that \\(W\\) is finite-dimensional and \\(T\\in \\mathcal{L}(V,W)\\). Prove that \\(T\\) is injective if and only if there exists \\(S\\in \\mathcal{L}(W,V)\\) such that \\(S T\\) is the identity map on \\(V\\). Suppose \\(S\\in\\mathcal{L}(W,V)\\) and \\(ST\\) is the identity on \\(V\\). Then \\(\\text{ker} T\\subseteq \\text{ker} ST=\\{0\\}\\) and so \\(\\text{ker} T=\\{0\\}\\). Therefore, \\(T\\) is injective. Conversely, suppose \\(T\\) is injective. So we can define \\(S_1\\in\\mathcal{L}(W,V)\\) by the following: \\(S_1(w)=v\\) where \\(v=T^{-1}(w)\\) (since \\(T\\) is injective). So \\(S_1:\\text{im} T \\rightarrow V\\) and since \\(W\\) is finite-dimensional, by \\(\\ref{Linear Extension}\\), \\(S_1\\) can be extended to \\(S: W\\rightarrow V\\) and by definition of \\(S_1\\), if \\(v\\in V\\), then \\(ST(v)=S(Tv)=T^{-1}(Tv)=v\\).\n\n\nExample 15.13 Suppose that \\(V\\) is finite-dimensional and \\(T\\in \\mathcal{L}(V,W)\\). Prove that \\(T\\) is surjective if and only if there exists \\(S\\in \\mathcal{L}(W,V)\\) such that \\(T S\\) is the identity map on \\(W\\). We will present two proofs.\n\nSuppose \\(S\\in \\mathcal{L}(W,V)\\) and \\(TS=I_W\\). Let \\(w\\in W\\). Then \\(v=S(w)\\in V\\) is such that \\(T(v)=TS(w)=w\\) and therefore \\(T\\) is surjective. Conversely, suppose \\(T\\) is surjective. Since \\(V\\) is a finite-dimensional vector space and \\(T\\) is a linear map, \\(W\\) is finite-dimensional. Let \\((v_1,\\ldots,v_n)\\) be a basis for \\(V\\). Since \\(T\\) is surjective, \\((T v_1,\\ldots,T v_n)\\) spans \\(W\\). Also since \\(T\\) is surjective \\(\\text{dim} W\\leq \\text{dim} V=n\\) Any spanning set reduced to a basis say \\((T v_1,\\ldots,T v_m)\\) is a basis of \\(W\\) where \\(m\\leq n\\). Define \\(S\\) as the linear extension of \\(S(T v_i)=v_i\\) for each \\(1\\leq i \\leq m\\). Then, for all \\(w\\in W\\), \\(S(w)=S(a_1 T v_1+\\cdots +a_m T v_m)=a_1 ST v_1+\\cdots + a_m ST v_m=a_1 v_1+\\cdots + a_m v_m=w\\) for scalars \\(a_1,\\ldots,a_m\\in \\mathbb{F}\\), and so \\(TS=I_W\\).\nSuppose \\(T\\) is surjective. Using \\(\\ref{Null Space Decomposition}\\), there exists a subspace \\(U\\) of \\(V\\) such that \\(U\\cap \\text{ker} T=\\{0\\}\\) and \\(\\text{im} T=\\{T u : u\\in U\\}\\). Define \\(T_1: U\\rightarrow W\\) by \\(T_1 u=T u\\) for \\(u\\in U\\). Notice \\(T_1\\) is injective and surjective and so \\(T_1\\) has an inverse. Define \\(S=T_1^{-1}\\) we have \\(TS w= T_1 T_1^{-1}w=w\\) for all \\(w\\in W\\).\n\n\nA linear map \\(T\\in \\mathcal{L}(V,W)\\) is called invertible if there exists a linear map \\(S\\in \\mathcal{L}(W,V)\\) such that \\(S T\\) equals the identity map on \\(V\\) and \\(TS\\) equal the identity map on \\(W\\). Given \\(T\\in\\mathcal{L}(V,W)\\). A linear map \\(S\\in\\mathcal{L}(W,V)\\) satisfying \\(S T=I\\) and \\(T S=I\\) is called an inverse of \\(T\\). Two vector spaces are called isomorphic if there is an invertible linear map from one vector space onto the other one.\n\nTheorem 15.4 If \\(\\mathcal{B}=(f_1,\\ldots,f_n)\\) is a basis of a linear space \\(V\\), then the coordinate transformation \\[\nL_{\\mathcal{B}}(f) = [f]_{\\mathcal{B}}\n\\] from \\(V\\) to \\(\\mathbb{R}^n\\) is an isomorphism. Thus any linear space \\(V\\) is isomorphic to \\(\\mathbb{R}^n\\).\n\n\nProof. The proof if left for the reader.\n\nAn invertible linear transformation \\(T\\) is called an isomorphism. Recall the rank-nullity theorem states that if \\(V\\) and \\(W\\) are finite-dimensional vector spaces and \\(T\\) is a linear transformation from \\(V\\) to \\(W\\) then \\(\\text{dim} V=\\text{dim} \\ker T+\\text{dim} \\text{im} T\\) where \\(\\text{dim} \\ker T\\) is called the nullity and \\(\\text{dim} \\text{im} T\\) is called the rank.\n\nTheorem 15.5 If \\(V\\) and \\(W\\) are finite-dimensional linear spaces, then\n\n\\(T\\) is an isomorphism from \\(V\\) to \\(W\\) if and only if \\(\\ker(T)= \\{0\\}\\) and \\(\\text{im} (T)=W\\),\nif \\(V\\) is isomorphic to \\(W\\), then \\(\\text{dim} (V)=\\text{dim} (W)\\),\nif \\(\\text{dim} (V)=\\text{dim} (W)\\) and \\(\\ker(T)=\\{0\\}\\), then \\(T\\) is an isomorphism, and\nif \\(\\text{dim} (V)=\\text{dim} (W)\\) and \\(\\text{im}(T)=W\\), then \\(T\\) is an isomorphism.\n\n\n\nProof. The proof of each part follows.\n\nSuppose \\(T\\) is an isomorphism from \\(V\\) to \\(W\\). This means there exists an invertible linear transformation \\(T^{-1}\\) from \\(W\\) to \\(V\\) such that \\(T(T^{-1})=I_W\\) and \\(T^{-1}(T)=I_V\\), where \\(I_W\\) and \\(I_V\\) are the identity transformations on \\(W\\) and \\(V\\) respectively. We will use \\(T^{-1}\\) to show \\(\\ker T=\\{0\\}\\) and \\(\\text{im} T= W\\). Since \\(\\ker T\\) is a subspace, \\(\\{0\\}\\subset \\ker T\\). Conversely, suppose \\(f\\in \\ker T\\). Then \\(T(f)=0\\) and so \\(T^{-1}(T(f))=f=0\\) so that \\(\\ker T\\subseteq \\{0\\}\\). Thus \\(\\ker T = \\{0\\}\\). Since \\(\\text{im} T\\) is a subspace of \\(W\\), \\(\\text{im} T\\subseteq W\\). To show conversely, let \\(w\\in W\\). Then \\(T^{-1}(w)\\in V\\) and so \\(T(T^{-1}w)=w\\) shows \\(w\\in \\text{im} T\\). Thus \\(\\text{im} T=W\\).\n\nConversely, suppose \\(\\ker T=\\{0\\}\\) and \\(\\text{im} T=W\\). We will show \\(T\\) is an isomorphism, which means we must show \\(T(f)=g\\) has a unique solution \\(f\\) for each \\(g\\in W\\). Now since \\(\\text{im} T=W\\), \\(T(f)=g\\) has at least one solution, say \\(f\\). Suppose \\(T(f)=g\\) has two solutions, say \\(T(f_1)=g=T(f_2)\\). Then \\(0=T(f_1)-T(f_2)=T(f_1-f_2)\\) shows \\(f_1-f_2\\in \\ker T\\). Since \\(\\ker T=\\{0\\}\\), \\(f_1=f_2\\) must follow, which means \\(T(f)=g\\) must have only one solution, so that \\(T^{-1}\\) exists, and therefore, \\(T\\) is an isomorphism.\n- If \\(V\\) is isomorphic to \\(W\\), then there exists a linear transformation \\(T\\) such that \\(\\ker T=\\{0\\}\\) and \\(\\text{im} T=W\\). By the rank-nullity theorem, \\(\\text{dim} V=\\text{dim} \\ker T + \\text{dim} \\text{im} T=0+\\text{dim} W\\). - By the previous part, \\(T\\) is an isomorphism when \\(\\ker =\\{0\\}\\) and \\(\\text{im} T=W\\). We are assuming \\(\\ker T=\\{0\\}\\) so it remains to show \\(\\text{im} T=W\\). By the rank-nullity theorem \\(\\text{dim} V=\\text{dim} \\ker T+\\text{dim} \\text{im} T=\\text{dim} \\ker T+\\text{dim} W=\\text{dim} W\\). Thus, \\(\\ker T=0\\) and so \\(\\ker T=\\{0\\}\\). Therefore, \\(T\\) is an isomorphism. - By the previous part, \\(T\\) is an isomorphism when \\(\\ker =\\{0\\}\\) and \\(\\text{im} T=W\\). We are assuming \\(\\text{im} T=W\\) so it remains to show \\(\\ker T=\\{o\\}\\). By the rank-nullity theorem \\(\\text{dim} V=\\text{dim} \\ker T+\\text{dim} \\text{im} T=\\text{dim} \\text{im} T=\\text{dim} W\\). Therefore, \\(\\text{im} T=W\\) and so \\(T\\) is an isomorphism.\n\n\nExample 15.14 Show that the transformation \\(T(A)=S^{-1}AS\\), from \\(\\mathbb{R}^{2\\times 2}\\) to \\(\\mathbb{R}^{2\\times 2}\\) where \\(S=\\begin{bmatrix} 1 & 2 \\\\ 3 & 4 \\end{bmatrix}\\) is an isomorphism. Notice that \\(\\text{dim} \\mathbb{R}^{2\\times 2}=\\text{dim} \\mathbb{R}^{2\\times 2}\\). Can we determine an invertible linear transformation \\(T^{-1}\\)? Checking the linearity conditions,\\[\nT(A_1+A_2)=S^{-1}(A_1+A_2)S=S^{-1}A_1S+S^{-1}A_2S=T(A_1)+T(A_2)\n\\] and\\[\nT(k A)=S^{-1}(k A)S=kS^{-1}A S=k T(A).\n\\] Now we know \\(T\\) is a linear transformation. Is it invertible? We need to solve the equation, \\(B=S^{-1} A S\\) for input \\(A\\). We can do this because \\(S\\) is an invertible matrix, so \\(S B=S(S^{-1}AS)=AS\\) and multiplying by \\(S^{-1}\\) on the right \\(S BS^{-1}=A\\) so that \\(T\\) is invertible, and the linear transformation is \\(T^{-1}(B)=S B S^{-1}\\).\n\n\nExample 15.15 Is the transformation \\(L( f)=\\vectorthree{f(1)}{f(2)}{f(3)}\\) from \\(\\mathcal{P}_3\\) to \\(\\mathbb{R}^3\\) an isomorphism? Since \\(\\text{dim} \\mathcal{P}_3=4\\) and \\(\\text{dim} \\mathbb{R}^3=3\\), the spaces \\(\\mathcal{P}_3\\) and \\(\\mathbb{R}^3\\) fail to be isomorphic, so that \\(L\\) is not an isomorphism.\n\n\nExample 15.16 Is the transformation \\(L( f)=\\vectorthree{f(1)}{f(2)}{f(3)}\\) from \\(\\mathcal{P}_2\\) to \\(\\mathbb{R}^3\\) an isomorphism? Notice \\(\\text{dim} \\mathcal{P}_2=3\\) and \\(\\text{dim} \\mathbb{R}^3=3\\) so the dimensions of the domain and the target space have the same dimension. Checking the linearity conditions, \\[\nL(f_1+f_2)\n=\\vectorthree{(f_1+f_2)(1)}{(f_1+f_2)(2)}{(f_1+f_2)(3)}\n=\\vectorthree{f_1(1)}{f_1(2)}{f_1(3)}+\\vectorthree{f_2(1)}{f_2(2)}{(f_2(3)}=L(f_1)+L(f_2)\n\\] and \\[\nT(k f)=\\vectorthree{(k f)(1)}{(k f)(2)}{(kf)(3)}\n=k\\vectorthree{f(1)}{f(2)}{f(3)}\n=k T(f).\n\\] The kernel of \\(T\\) consists of all polynomials \\(f(x)\\) in \\(\\mathcal{P}_2\\) such that \\[\nT(f(x))=\\vectorthree{f(1)}{f(2)}{f(3)}=\\vectorthree{0}{0}{0}.\n\\] Since a nonzero polynomial in \\(\\mathcal{P}_2\\) has at most two zeros, the zero polynomial is the only solution, so that \\(\\ker T=\\{0\\}\\). Therefore, \\(T\\) is an isomorphism.\n\n\nExample 15.17 Determine whether the transformation \\[T(M)=M\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}M\\] from \\(\\mathbb{R}^{2\\times 2}\\) to \\(\\mathbb{R}^{2\\times 2}\\) is an isomorphism. Notice that \\(\\text{dim} \\mathbb{R}^{2\\times 2}=\\text{dim} \\mathbb{R}^{2\\times 2}\\). Can we determine an invertible linear transformation \\(T^{-1}\\)? Checking the linearity conditions,\\[\n\\begin{array}{rl}\nT(M+N)\n&=(M+N)\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}(M+N) \\\\\n&=M\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}M\n+ N\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}N  =T(M)+T(N)\n\\end{array}\n\\] and\\[\n\\begin{array}{rl}\nT(k M)\n& =(kM)\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}(Mk) \\\\\n& =k \\left ( M\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}M \\right ) k\n= k T(M)\n\\end{array}\n\\] The kernel of \\(T\\) consists of \\(M=\\begin{bmatrix} a & b \\\\ c & d \\end{bmatrix}\\) such that\\[\n\\begin{bmatrix} a & b \\\\ c & d  \\end{bmatrix}\n\\begin{bmatrix} 2 & 0 \\\\ 0 & 3  \\end{bmatrix}-\\begin{bmatrix} 3 & 0 \\\\ 0 & 4  \\end{bmatrix}\n\\begin{bmatrix} a & b \\\\ c & d  \\end{bmatrix}\n=\n\\begin{bmatrix} -a & 0 \\\\ -2c & -d  \\end{bmatrix}\n\\begin{bmatrix} 0 & 0 \\\\ 0 & 0  \\end{bmatrix}.\n\\] Thus \\(a=c=d=0\\). However, when \\(M=\\begin{bmatrix} 0 & 1 \\\\ 0 & 0 \\end{bmatrix}\\) then \\(T(M)=0\\) so that \\(\\ker T \\neq \\{0\\}\\), and so \\(T\\) is not an isomorphism.\n\n\nExample 15.18 Determine whether the transformation \\(T(f(t))=\\vectorthree{f(1)}{f'(2)}{f(3)}\\) from \\(\\mathcal{P}_2\\) to \\(\\mathbb{R}^3\\) is a linear transformation. Then determine whether \\(T\\) is an isomorphism and if not then find its image and kernel. This transformation is linear since\\[\nT(f(t)+g(t))=\\vectorthree{f(1)+g(1)}{f'(2)+g'(2)}{f(3)+g(3)}=\n\\vectorthree{f(1)}{f'(2)}{f(3)}+ \\vectorthree{g(1)}{g'(2)}{g(3)}=\nT(f(t))+T(g(t))\n\\] and \\[\nT(k f(t))=\\vectorthree{kf(1)}{k f'(2)}{k f(3)}=\nk \\vectorthree{f(1)}{f'(2)}{f(3)}=\nk T(f(t)).\n\\] Since \\(T(t-1)(t-3))=T(t^2-4t+3)=\\vec 0\\) shows \\(\\ker T \\neq \\{0\\}\\), T is not an isomorphism.\n\n\nTheorem 15.6 A linear map is invertible if and only if it is injective and surjective.\n\n\nProof. Suppose \\(T\\in \\mathcal{L}(V,W)\\) is invertible with inverse \\(T\\). Let \\(u,v\\in V\\). If \\(Tu=Tv\\) then \\[u=(T^{-1})u)=T^{-1}(Tu)=T^{-1}(Tv)=(T^{-1}T)(v)=v\\] and so \\(T\\) is injective. If \\(w\\in W\\), then \\(v=T^{-1} w\\in V\\) with \\(T v=T(T^{-1}w)=w\\) shows \\(T\\) is surjective. Assume \\(T\\) is injective and surjective. For each \\(w\\in W\\) assign \\(T(v)=w\\). Such \\(S(w)=v\\) exists because \\(T\\) is surjective and is unique since \\(T\\) is injective. Then \\(T(v)=w\\) shows \\(ST(v)=S(w)=v\\) so that \\(ST\\) is the identity on \\(V\\). Also, \\(TS(w)=T(Sw)=Tv=w\\) shows \\(TS\\) is the identity on \\(W\\). Thus \\(S\\) and \\(T\\) are inverses.\nNow \\(S\\) is linear since:\n\nif \\(w_1,w_2\\in W\\), then there exists a unique \\(v_1\\) and \\(v_2\\) such that \\(Tv_1=w_1\\) and \\(Tv_2=w_2\\), \\(S(w_1)=v_1\\), and \\(S(w_2)=v_2\\). By linearity of \\(T\\), \\(S(w_1+w_2)=S(T v_1+Tv_2)=(ST)(v_1+v_2)=v_1+v_2=S(w_1)+S(w_2)\\).\nIf \\(w\\in W\\) and \\(k\\in \\mathbb{F}\\) then there exists a unique \\(v\\in V\\) such that \\(Tv=w\\) and \\(S(w)=v\\). By linearity of \\(T\\), \\(S(kw)=S(k Tv)=S(T k v)=k v=kS(w)\\).\n\nTherefore \\(S\\) is linear and is the inverse of \\(T\\).\n\nConsider the transformation \\[\nL \\left( \\, \\begin{bmatrix} a & b \\\\ c & d  \\end{bmatrix} \\, \\right)=\\vectorfour{a}{b}{c}{d}\n\\] from \\(\\mathbb{R}^{2\\times 2}\\) to \\(\\mathbb{R}^4\\). Note that \\(L\\) is the coordinate transformation \\(L_{\\mathcal{B}}\\) with respect to the basis \\[\n\\mathcal{B}=\n\\left(\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 1 \\\\ 0 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 \\\\ 1 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 \\\\ 0 & 1  \\end{bmatrix}\n\\right)\n\\] Being a coordinate transformation, \\(L\\) is both linear and invertible. Therefore \\(L\\) is an isomorphism.\n\nTheorem 15.7 Let \\(T\\) be the linear transformation from \\(V\\) to \\(V\\) and let \\(B\\) be the matrix of \\(T\\) with respect to a basis \\(\\mathcal{B}=(f_1,\\ldots,f_n)\\) of \\(V\\). Then \\[\nB=\n\\begin{bmatrix}[T(f_1)]_{\\mathcal{B}} & \\cdots & [T(f_n)]_{\\mathcal{B}}]\n\\end{bmatrix}\n\\] In other words, the columns of \\(B\\) are the \\(\\mathcal{B}\\)-coordinate vectors of the transformation of the basis elements \\(f_1,\\ldots,f_n\\) of \\(V\\).\n\n\nProof. The proof if left for the reader.\n\n\nTheorem 15.8 Two finite-dimensional vector spaces are isomorphic if and only if they have the same dimension.\n\n\nProof. Suppose \\(V\\) and \\(W\\) are finite-dimensional vector spaces that are isomorphic. If \\(\\text{dim} V > \\text{dim} W\\), then no linear map between \\(T\\) and \\(W\\) can be injective. Thus \\(V\\) and \\(W\\) are not isomorphic contrary to hypothesis. If \\(\\text{dim} V < \\text{dim} W\\), then no linear map between \\(V\\) and \\(W\\) can be surjective. Thus \\(V\\) and \\(W\\) are not isomorphic contrary to hypothesis. Therefore, \\(\\text{dim} V =\\text{dim} W\\).\nSuppose \\(V\\) and \\(W\\) are vector spaces with \\(\\text{dim} V=\\text{dim} W=n\\). Let \\((v_1,\\ldots,v_n)\\) ba a basis of \\(V\\) and let \\((w_1,\\ldots,w_n)\\) be a basis of \\(W\\). Define a linear map \\(T\\) by \\(T(a_1 v_1+\\cdots +a_n v_n)=a_1 w_1+\\cdots +a_n w_n\\). It’s an easy exercise to show \\(T\\) is linear, injective, and surjective. Thus \\(T\\) is an isomorphism as required.\n\n\nTheorem 15.9  If \\(A\\) and \\(B\\) are matrices of a linear transformation \\(T\\) with respect to different bases, then \\(A\\) and \\(B\\) are similar matrices.\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 15.10 Suppose \\(V\\) and \\(W\\) are finite-dimensional vector spaces, then \\(\\mathcal{M}\\) is an invertible linear map between \\(\\mathcal{L}(V,W)\\) and \\(Mat(m,n,\\mathbb{F})\\)\n\n\nProof. Suppose \\((v_1,\\ldots,v_n)\\) and \\(w_1,\\ldots,w_m)\\) are bases of \\(V\\) and \\(W\\) respectively. For each \\(T\\in \\mathcal{L}(V,W)\\) we assign,\\[\n\\mathcal{M}(T)=\\begin{bmatrix}a_{11} & & a_{1n}  \\\\ \\vdots & \\cdots & \\vdots \\\\ a_{m1} & & a_{mn} \\end{bmatrix}.\n\\] where \\(T v_k=a_{1k} w_1+\\cdots + a_{mk} w_m\\). Since \\((w_1,\\ldots,w_m)\\) is a basis of \\(W\\), This assigment is well-defined. The following arguments show \\(\\mathcal{M}\\) is linear, injective, and surjective, respectively.\n\nIf \\(S,T\\in \\mathcal{L}(V,W)\\), then \\(\\mathcal{M}(S+T)=\\mathcal{M}(S)+\\mathcal{M}(T)\\) follows by the definition of matrix addition. If \\(S\\in \\mathcal{L}(V,W)\\) and \\(k\\mathbb{F}\\), then \\(\\mathcal{M}(kS)=k\\mathcal{M}(S)\\) follows by the definition of scalar multiplication of matrices.\nIf \\(T\\in \\mathcal{L}(V,W)\\) with \\(\\mathcal{M}(T)=0\\), then for entries in \\([a_{ij}]\\) are zero showing \\(T v_k=0\\) for each \\(k\\). Since \\((v_1,\\ldots,v_n)\\) is a basis of \\(V\\), \\(T\\) must be the zero map, and so \\(\\text{ker} \\mathcal{M}=\\{0\\}\\). Therefore, \\(\\mathcal{M}\\) is injective.\nIf \\(A \\in Mat(m,n,\\mathbb{F})\\) then \\(A\\) is an \\(m\\times n\\) matrix with entries \\([a_{ij}]\\). We define \\(Tv_k=a_{1k} w_1+\\cdots a_{mk} w_m\\) for each \\(k\\). Then, by the definition of the matrix of a linear map, \\(\\mathcal{M}(T)=A\\) and so \\(\\mathcal{M}\\) is surjective.\n\n\n\nTheorem 15.11 If \\(V\\) and \\(W\\) are finite-dimensional, then \\(\\mathcal{L}(V,W)\\) is finite-dimensional and \\(\\text{dim} \\mathcal{L}(V,W) =(\\text{dim} V) (\\text{dim} W)\\).\n\n\nProof. If \\(V\\) and \\(W\\) are finite-dimensional vector spaces then \\(\\mathcal{L}(V,W)\\) is isomorphic to \\(Mat(m,n,\\mathbb{F})\\) where \\(n=\\text{dim} V\\) and \\(m=\\text{dim} W\\). If follows \\[\n\\text{dim}  \\mathcal{L}(V,W) =\\text{dim}  Mat(mn,\\mathbb{F})= m n=(\\text{dim}  W)(\\text{dim}  V).\n\\] since \\(\\text{dim} Mat(m,n,\\mathbb{F})= m n\\).\n\n\nTheorem 15.12 If \\(V\\) is finite-dimensional and \\(T\\in \\mathcal{L}(V)\\), the the following are equivalent:\n\n\\(T\\) is invertible,\n\\(T\\) is injective, and\n\\(T\\) is surjective.\n\n\n\nProof. The proof of each part follows.\n\nSuppose \\(T\\) is invertible, then by definition \\(T\\) is injective.\nSuppose \\(T\\) is injective. Thus \\(\\text{ker} T=\\{0\\}\\). Since \\(V\\) is finite-dimensional, we can apply the Rank-Nullity Theorem so \\(\\text{dim} V=\\text{dim} \\text{im} T\\). Since \\(\\text{im} T\\) is a subspace of \\(V\\) with the same dimension as \\(V\\), \\(\\text{im} T=V\\). Therefore, \\(T\\) is surjective.\nSuppose \\(T\\) is surjective. Then \\(T(V)=V\\) and so \\(\\text{dim} V=\\text{dim} \\text{im} T\\). Since \\(V\\) is finite-dimensional, \\(\\text{dim} V=\\text{dim} \\text{ker} T+\\text{dim} \\text{im}\\) T by the Rank-Nullity Theorem. Thus, \\(\\text{dim} \\text{ker} T =0\\) and so \\(\\text{ker} T=\\{0\\}\\). Therefore, \\(T\\) is injective, and since \\(T\\) is injective and surjective, \\(T\\) is invertible."
  },
  {
    "objectID": "coordinates-and-the-matrix-of-a-linear-map.html",
    "href": "coordinates-and-the-matrix-of-a-linear-map.html",
    "title": "16  Coordinates and the Matrix of a Linear Map",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nLet \\(V\\) and \\(W\\) be finite dimensional linear spaces.\n\nDefinition 16.1 (The Matrix of a Linear Map) Let \\(T\\in \\mathcal{L}(V,W)\\) and let \\(b_1=\\{v_1,\\ldots ,v_n\\}\\) be a basis for \\(V\\) and \\(b_2=\\{w_1,\\ldots ,w_m\\}\\) be a base for \\(W\\). Then the matrix of \\(T\\) with respect to the bases \\(b_1\\) and \\(b_2\\) is \\[ \\begin{bmatrix}\na_{1 1} & \\cdots & a_{1 n} \\\\\n\\vdots & \\cdots & \\vdots \\\\\na_{m 1} & \\cdots & a_{m n} \\\\\n\\end{bmatrix}\n\\] where the \\(a_{i j}\\in \\mathbb{F}\\) are determined by \\(T v_k=a_{1 k}w_1+\\cdots +a_{m k}w_m\\) for each \\(k=1,\\ldots ,n\\).\n\n\nExample 16.1 Consider the linear transformation \\(T(f)=f'+f''\\) from \\(\\mathcal{P}_2\\) to \\(\\mathcal{P}_2\\). Since \\(\\mathcal{P}_2\\) is isomorphic to \\(\\mathbb{R}^3\\) with isomorphism given by a \\(3\\times 3\\) matrix \\(B\\), how do we find this matrix \\(B\\)? Let \\(a+b x+c x^2\\), then we write \\(T\\) as \\[\\begin{align*}\nT(a+b x+c x^2)& =(a+b x+c x^2)'+(a+b x+c x^2)'' \\\\\n& =b+2c x+2c=(b+2c)+2cx.\n\\end{align*}\\] Next let’s write the input \\(f(x)=a+b x+c x^2\\) and the output \\(T(f(x))=(b+2c)+2c x\\) in coordinates with respect to the standard bases \\(\\mathcal{B}=(1, x, x^2)\\) of \\(\\mathcal{P}_2\\) Written in \\(\\mathcal{P}_2\\) coordinates, transformation \\(T\\) takes \\([f(x)]_{\\mathcal{B}}\\) to \\[\n[T(f(x))]_{\\mathcal{B}}=\\vectorthree{b+2c}{2c}{0}=\n\\begin{bmatrix}\n0 & 1 & 2 \\\\\n0 & 0 & 2 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\vectorthree{a}{b}{c}\n= \\begin{bmatrix}\n0 & 1 & 2 \\\\\n0 & 0 & 2 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n[f(x)]_{\\mathcal{B}}\n\\] The matrix \\[\nB= \\begin{bmatrix}\n0 & 1 & 2 \\\\\n0 & 0 & 2 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\] is called the \\(\\mathcal{B}\\)-matrix of the transformation \\(T\\).\n\n\nExample 16.2 Find the \\(B\\)-matrix for the linear transformation given by \\[\nT(M)=\\begin{bmatrix} 1 & 2 \\\\ 3 & 4  \\end{bmatrix} M - M \\begin{bmatrix} 5 & 0 \\\\ 0 & -1  \\end{bmatrix}M\n\\] from \\(\\mathbb{R}^{2\\times 2}\\) to \\(\\mathbb{R}^{2\\times 2}\\). Determine whether \\(T\\) is an isomorphism and if not find kernel, image, nullity, and the rank of \\(T\\). We will use the standard basis of \\(\\mathbb{R}^{2 \\times 2}\\): \\[\n\\mathcal{B}=\n\\left (\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 1 \\\\ 0 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 \\\\ 1 & 0  \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 \\\\ 0 & 1  \\end{bmatrix}\n\\right )\n\\] and we will construct the \\(\\mathcal{B}\\)-matrix column-by-column: \\[\n\\begin{array}{rl}\nB & =\n\\begin{bmatrix} \\left [ T\\begin{bmatrix} 1 & 0 \\\\ 0 & 0  \\end{bmatrix} \\right ]_{\\mathcal{B}} &\n\\left [T[\\begin{bmatrix} 0 & 1 \\\\ 0 & 0  \\end{bmatrix}\\right ]_{\\mathcal{B}} &\n\\left [ T\\begin{bmatrix} 0 & 0 \\\\ 1 & 0  \\end{bmatrix}\\right ]_{\\mathcal{B}} &\n\\left [ T\\begin{bmatrix} 0 & 0 \\\\ 0 & 1  \\end{bmatrix} \\right ]_{\\mathcal{B}}\n\\end{bmatrix}\\\\\n\\\\\n&=\n\\begin{bmatrix} \\begin{bmatrix} -4 & 0 \\\\ 4 & 0  \\end{bmatrix}_{\\mathcal{B}} &\n\\begin{bmatrix} 0 & 2 \\\\ 0 & 4  \\end{bmatrix}_{\\mathcal{B}} &\n\\begin{bmatrix} 2 & 0 \\\\ -2 & 0  \\end{bmatrix}_{\\mathcal{B}} &\n\\begin{bmatrix}  0 & 2 \\\\ 0 & 4  \\end{bmatrix}_{\\mathcal{B}}\n\\end{bmatrix}\n=\n\\begin{bmatrix} -4 & 0 & 2 & 0\\\\\n0 & 2 & 0 & 2 \\\\\n4 & 0 & -2 & 0 \\\\\n0 & 4 & 0 & 4\n\\end{bmatrix}\\end{array}\n\\] with \\[\n\\text{rref}(B)=\n\\begin{bmatrix} 1 & 0 & -\\frac{1}{2} & 0\\\\\n0 & 1 & 0 & 1 \\\\\n0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0\n\\end{bmatrix}.\n\\] After eliminating redundant columns from \\(B\\) we find a basis of \\(\\text{im} T\\) is \\[\\left(\\, \\vectorfour{-4}{0}{4}{0},\\vectorfour{0}{2}{0}{4} \\, \\right).\\] To find \\(\\ker T\\) we solve \\(B \\vec x=\\vec 0\\) and using the \\(\\text{rref}(B)\\) we find \\[\\left(\\, \\vectorfour{-\\frac{1}{2}}{0}{1}{0},\\vectorfour{0}{-1}{0}{1} \\, \\right)\\] to be a basis for \\(\\ker T.\\) Notice since the rank of \\(T\\) is \\(2=\\text{dim} \\text{im} T\\), \\(T\\) is not an isomorphism. Therefore, since \\(\\ker T\\neq \\{0\\}\\), \\(T\\) is not an isomorphism.\n\n\nExample 16.3 Find the matrix of the linear transformation \\(T(f(t))=f(3)\\) from \\(\\mathcal{P}_2\\) to \\(\\mathcal{P}_2\\) with respect to the basis \\((1,t-3,(t-3)^2)\\). Determine whether the transformation is an isomorphism, it it isn’t an isomorphism then determine the kernel and image of \\(T\\), and also determine the nullity and rank of \\(T\\). The matrix of \\(T\\) is \\[\nB=\\begin{bmatrix} [T(1)]_{\\mathcal{B}} & [T(x-3)]_{\\mathcal{B}} & [T((x-3)^2)]_{\\mathcal{B}} \\end{bmatrix}\n= \\begin{bmatrix}\n[1]_{\\mathcal{B}} & [0]_{\\mathcal{B}} & [0]_{\\mathcal{B}}\n\\end{bmatrix}\n= \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\  0 & 0 & 0  \\end{bmatrix}.\n\\] Notice the vectors \\(\\vectorthree{0}{1}{0}, \\vectorthree{0}{0}{1}\\) form a basis of the kernel of \\(B\\) and \\(\\vectorthree{1}{0}{0}\\) is a basis of the image of \\(B\\). Therefore, the rank is 1 and the nullity is 2; and therefore, \\(T\\) is not an isomorphism.\n\n\nDefinition 16.2 (The Matrix of a Vector) Let \\(b=\\{v_1,\\ldots ,v_n\\}\\) be a basis for \\(V\\) and let \\(v\\in V\\). We define the matrix of \\(v\\), denoted by \\(\\mathcal{M}(v)\\), to be the \\(n\\)-by-1 matrix \\(\\begin{bmatrix} b_{1} & \\cdots& b_{n} \\end{bmatrix}^T\\).\n\n\nTheorem 16.1 If \\(T\\in \\mathcal{L}(V,W)\\), then \\(\\mathcal{M}(Tv)=\\mathcal{M}(T) \\mathcal{M}(v)\\) for all \\(v\\in V\\).\n\n\nProof. Let \\((v_1,\\ldots ,v_n)\\) be a basis of \\(V\\) and \\((w_1,\\ldots ,w_m)\\) be a basis of \\(W\\). If \\(v\\in V\\), then there exists \\(b_1,\\ldots ,b_n\\in \\mathbb{F}\\) such that \\(v=b_1 v_1+\\cdots + b_n v_n\\) so that \\(\\mathcal{M}(v)=\\begin{bmatrix} b_{1} & \\cdots& b_{n} \\end{bmatrix}^T\\). For each \\(k\\), \\(1\\leq k \\leq n\\) we write \\(T v_k=a_{1k}w_1+ \\cdots + a_{m k} w_m\\) and so by definition of the matrix of a linear map \\(T\\): \\[\n\\mathcal{M}=\\begin{bmatrix}a_{11} & & a_{1n}  \\\\ \\vdots & \\cdots & \\vdots \\\\ a_{m1} & & a_{mn} \\end{bmatrix}.\n\\] By linearity of \\(T\\): \\[\n\\begin{array}{rl}\nTv& =b_1 T v_1+\\cdots b_n T v_n \\\\\n& = b_1 \\left(\\sum_{j=1}^m a_{j 1}w_j \\right)+\\cdots +b_n \\left(\\sum_{j=1}^m a_{j n}w_j \\right) \\\\\n& =w_1(a_{11}b_1+\\cdots + a_{1n}b_n)+\\cdots + w_m(a_{m1}b_1+\\cdots + a_{mn}b_n).\n\\end{array}\n\\] Therefore, \\[\n\\mathcal{M}(T v)=\n\\vectorthree{a_{11}b_1+\\cdots + a_{1n}b_n}{\\cdots}{a_{m1}b_1+\\cdots + a_{mn}b_n}\n=\\mathcal{M}(T)\\mathcal{M}(v).\n\\] where the last equality holds by definition of matrix multiplication.\n\nLet \\(\\overline{u}=(u_1,\\ldots ,u_p)\\) be a basis of \\(U\\), let \\(\\overline{v}=(v_1,\\ldots ,v_n)\\) be a basis of \\(V\\), and let \\(\\overline{w}=(w_1,\\ldots ,w_m)\\) be a basis of \\(W\\). If \\(T\\in \\mathcal{L}(U,V)\\) and \\(S\\in \\mathcal{L}(V,W)\\), then \\(ST\\in \\mathcal{L}(U,W)\\) and by the definition of matrix multiplication, \\[\\begin{equation}\\label{matrix multiplication}\n\\mathcal{M}(ST,\\overline{u},\\overline{w})=\\mathcal{M}(S,\\overline{v},\\overline{w}) \\mathcal{M}(T,\\overline{u},\\overline{v}).\n\\end{equation}\\]\n\nTheorem 16.2 If \\(\\overline{u}=(u_1,\\ldots ,u_p)\\) and \\(\\overline{v}=(v_1,\\ldots ,v_n)\\) are bases of \\(V\\), then \\(\\mathcal{M}(I,\\overline{u},\\overline{v})\\) is invertible and \\[\n\\mathcal{M}(I,\\overline{u},\\overline{v})^{-1}=\\mathcal{M}(I,\\overline{v},\\overline{u}).\n\\]\n\n\nProof. In \\(\\ref{matrix multiplication}\\), replace \\(U\\) and \\(W\\) with \\(V\\), replace \\(w_j\\) with \\(u_j\\), and replace \\(S\\) and \\(T\\) with \\(I\\), getting \\[\nI=\\mathcal{M}(I,(\\overline{v},\\overline{u}))\\mathcal{M}(I,\\overline{u},\\overline{v}).\n\\] Now interchange the roles of the \\(u\\)’s and \\(v\\)’s, getting \\[\nI=\\mathcal{M}(I,(\\overline{u},\\overline{v}))\\mathcal{M}(I,\\overline{v},\\overline{u}).\n\\] These equations give the desired result.\n\nFor example, obviously, \\[\n\\mathcal{M}\\left(I,\\left(\\vectortwo{4}{2},\\vectortwo{5}{3}\\right),\\left(\\vectortwo{1}{0},\\vectortwo{0}{1}\\right)\\right)=\\begin{bmatrix}  4 & 5 \\\\ 2 & 3 \\end{bmatrix} .\n\\] The inverse of the matrix above is \\(\\begin{bmatrix} 3/2 & -5/2 \\\\ -1 & 2\\end{bmatrix}\\). Thus, \\[\n\\mathcal{M}\\left(I,\\left(\\vectortwo{1}{0},\\vectortwo{0}{1}\\right),\\left(\\vectortwo{4}{2},\\vectortwo{5}{3}\\right)\\right)\n=\\begin{bmatrix} 3/2 & -5/2 \\\\ -1 & 2\\end{bmatrix}.\n\\]\n\nTheorem 16.3 Suppose \\(T\\in\\mathcal{L}(V)\\). Let \\(\\overline{u}=(u_1,\\ldots ,u_n)\\) and \\((v_1,\\ldots ,v_n)\\) be bases of \\(V\\). Let \\(A=\\mathcal{M}(I,\\overline{u},\\overline{v})\\). Then \\[\\begin{equation}\\label{change of basis}\n\\mathcal{M}(T,\\overline{u})=A^{-1}\\mathcal{M}(T,\\overline{v})A.\n\\end{equation}\\]\n\n\nProof. In \\(\\ref{matrix multiplication}\\), replace \\(U\\) and \\(W\\) with \\(V\\), replace \\(w_j\\) with \\(v_j\\), replace \\(T\\) with \\(I\\), and replace \\(S\\) with \\(T\\), getting \\[\\begin{equation}\\label{rchange}\n\\mathcal{M}(T,\\overline{u},\\overline{v})=\\mathcal{M}(T,\\overline{v})A.\n\\end{equation}\\] In \\(\\ref{matrix multiplication}\\), replace \\(U\\) and \\(W\\) with \\(V\\), replace \\(w_j\\) with \\(u_j\\), replace \\(S\\) with \\(I\\), and replace \\(S\\) with \\(T\\), getting \\[\\begin{equation}\\label{lchange}\n\\mathcal{M}(T,\\overline{u})=A^{-1}\\mathcal{M}(T,\\overline{u},\\overline{v})\n\\end{equation}\\] by \\(\\ref{matrix inversion}\\). Substitution of \\(\\ref{rchange}\\) into \\(\\ref{lchange}\\) yields \\(\\ref{change of basis}\\).\n\n\nExample 16.4 Prove that every linear map from \\(\\text{Mat}(n,1,F)\\) to \\(\\text{Mat}(m,1,F)\\) is given by matrix multiplication. In other words, prove that if \\(T\\) is a linear transformation from \\(\\text{Mat}(n,1,F)\\) to \\(\\text{Mat}(m,1,F))\\), then there exists an \\(m\\)-by-\\(n\\) matrix \\(A\\) such that \\(T B=A B\\) for every \\(B\\in \\text{Mat}(n,1,F).\\) Let \\((e_1,\\ldots ,e_n)\\) be a basis for \\(Mat(n,1,\\mathbb{F})\\) and let \\((v_1,\\ldots ,v_m)\\) be a basis for \\(Mat(m,1,\\mathbb{F})\\). For each \\(k\\), there exists \\(a_{1k},\\ldots ,a_{mk}\\in \\mathbb{F}\\) such that \\(T e_k=a_{1k} v_1+\\cdots a_{mk} v_m\\). Define the \\(m \\times n\\) matrix \\(A\\) as follows: \\[\nA=\\begin{bmatrix} T e_1 & \\cdots & T e_n \\end{bmatrix}.\n\\] If \\(B\\in Mat(n,1,\\mathbb{F})\\) there exists \\(b_1,\\ldots ,b_n\\in \\mathbb{F}\\) such that \\(B=b_1 e_1+\\cdots + b_n e_n\\), and thus \\[\nTB=T(b_1 e_1+\\cdots + b_n e_n)=b_1 T e_1+\\cdots + b_n T e_n= BA\n\\] as desired. Notice the word “the” follows since \\((v_1,\\ldots ,v_m)\\) is a basis. In other words one bases have been chosen, the matrix \\(A\\) is unique.\n\n\nExample 16.5 Suppose that \\(V\\) is finite-dimensional and \\(S,T\\in \\mathcal{L}(V)\\). Prove that \\(S T\\) is invertible if and only if both \\(S\\) and \\(T\\) are invertible. Suppose both \\(S\\) and \\(T\\) are invertible. Then both \\(S\\) and \\(T\\) are injective and surjective. Thus, \\(ST\\) is both injective and surjective showing \\(ST\\) is invertible. Conversely, suppose \\(ST\\) is invertible. Since \\(\\text{ker} T\\subseteq \\text{ker} ST =\\{0\\}\\) because \\(ST\\) is injective, \\(T\\) is also injective. Thus, \\(T\\) is invertible. Since \\(ST\\) is surjective, if \\(w\\in W\\), then there exists \\(v\\in V\\) such that \\((ST)v=w\\). Rewriting \\(S(Tv)=w\\), showing \\(S\\) is surjective. Thus, \\(S\\) is also invertible.\n\n\nExample 16.6 Suppose that \\(V\\) is finite-dimensional and \\(S,T\\in \\mathcal{L}(V)\\). Prove that \\(S T=I\\) if and only if \\(T S=I\\). Without loss of generality, we will show \\(ST=I \\text{im}plies TS=I\\). Suppose \\(ST=I\\). Since \\(I\\) is invertible, the previous exercise implies \\(S\\) and \\(T\\) are both invertible. Then \\(ST=I \\text{im}plies S^{-1}(ST)=S^{-1}I \\text{im}plies T=S^{-1}\\). Therefore, \\(TS=S^{-1}S=I\\).\n\n\nExample 16.7 Suppose that \\(V\\) is finite-dimensional and \\(T\\in \\mathcal{L}(V)\\). Prove that \\(T\\) is a scalar multiple of the identity if and only if \\(S T=T S\\) for every \\(S\\in \\mathcal{L}(V)\\). If \\(T\\) is a scalar multiple of the identity, say \\(T=\\alpha I\\), then for all \\(v\\in V\\), \\[\nS T v=S \\alpha v= \\alpha S v= TS v.\n\\] Conversely, suppose \\(ST=TS\\) for every \\(S\\in \\mathcal{L}(V)\\). Pick a basis \\(v_1,\\ldots ,v_N)\\) for \\(V\\). For \\(m=1,\\ldots ,N\\), define linear maps \\(S+m\\in \\mathcal{L}(V)\\) by\\[\nS_m=v_n=\\left\\{ \\begin{array}{rl} v_m & \\text{ if } m=n \\\\ 0 & \\text{ if } m\\neq n \\end{array} \\right.\n\\] Now if \\(v=\\sum \\alpha_n v_n\\), then\\[\nS_m\\sum \\alpha_n v_n=\\alpha_m v_m\n\\] Thus the only vectors satisfying \\(S_m v=v\\) are \\(v=\\alpha v_m\\) for some \\(\\alpha \\in \\mathbb{F}\\). The condition \\(S_m T=T S_m\\) gives\\[\nS_m T v_m=T S_m v_m=T v_m\n\\] and by the above observation \\(T v_m=\\alpha_m v_m\\). Now consider another collection of linear maps \\(A(m,n)\\) defined by\\[\nA(m,n) v_m=v_n, \\hspace{1cm} A(m,n)v_n=v_m, \\hspace{1cm} A(m,n)v_k=0, \\text{ when } k\\neq m,n.\\] The condition \\(A(m,n)T v_n=T A(m,n) v_n\\) gives \\[\nA(m,n)T v_n=TA(m,n)v_n=T v_m=\\alpha_m v_m\n\\] and \\[\nA(m,n)\\alpha_n v_n=A(m,n)\\alpha_n v_n=\\alpha_nA(m,n)v_n=\\alpha_n v_m.\n\\] Whence \\(\\alpha_m v_m=\\alpha_n v_m\\) that is, \\(\\alpha_m=\\alpha_n\\) for \\(m,n=1,\\ldots ,N\\); and thus \\(T\\) is a scalar multiple of the identity.\n\n\nExample 16.8 Prove that if \\(V\\) is finite-dimensional with \\(\\text{dim} V > 1\\), then the set of non-invertible operators on \\(V\\) is not a subspace of \\(\\mathcal{L}(V)\\). Suppose \\((v_1,\\ldots ,v_n)\\) is a basis of \\(V\\), with \\(n \\geq 2\\). Define the linear maps \\(S\\) and \\(T\\) by \\[S v_1=v_1, \\hspace{1cm} S v_k =0, \\text{ when } k\\geq 2\\] \\[T v_1=0, \\hspace{1cm} T v_k =v_k, \\text{ when } k\\geq 2.\\] Since \\(S\\) and \\(T\\) have nontrivial null spaces, they are not invertible. However, \\((S+T) v_k=v_k\\), for \\(k=1,\\ldots ,n,\\) so \\(S+T=I\\), which is invertible. Thus the set of noninvertible operators on \\(V\\) with \\(n\\geq 2\\) is not closed under addition."
  },
  {
    "objectID": "orthonormal-bases-and-orthogonal-projections.html",
    "href": "orthonormal-bases-and-orthogonal-projections.html",
    "title": "17  Orthonormal Bases and Orthogonal Projections",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nThe norm of a vector \\(\\vec v\\) in \\(\\mathbb{R}^n\\) is \\(\\norm{\\vec v}=\\sqrt{\\vec v \\cdot \\vec v}\\). A vector \\(\\vec u\\) in \\(\\mathbb{R}^n\\) is called a unit vector if \\(\\norm{\\vec u}=1\\).\n\nExample 17.1 If \\(\\vec v\\in \\mathbb{R}^n\\) and \\(k\\) is a scalar, then \\(\\norm{k \\vec v}=|k| \\norm{\\vec v}\\), and if \\(v\\) is nonzero then \\(\\vec u=\\frac{1}{\\norm{\\vec v}} \\vec v\\) is a unit vector. Since \\(\\norm{k \\vec v}^2=(k \\vec v)\\cdot (k \\vec v)=k^2(\\vec v \\cdot \\vec v)=k^2\\norm{\\vec v}^2\\), taking square roots provides \\(\\norm{k \\vec v}=|k| \\norm{\\vec v}\\). If \\(v\\) is nonzero, then \\(\\frac{1}{\\norm{\\vec v}}\\) is defined and so \\(\\norm{\\vec u}= \\frac{1}{\\norm{\\vec v}} \\norm{\\vec v}=1\\) which follows by the first part.\n\nTwo vectors \\(\\vec v\\) and \\(\\vec w\\) in \\(\\mathbb{R}^n\\) are called perpendicular or orthogonal if \\(\\vec v \\cdot \\vec w=0\\). The vectors \\(\\vec u_1,\\ldots,\\vec u_m\\) in \\(\\mathbb{R}^n\\) are called orthonormal if they are all unit vectors and orthogonal to one another. A basis of \\(\\mathbb{R}^n\\) consisting only of orthonormal vectors is called an orthonormal basis.\n\nTheorem 17.1 (Orthonormal Vectors) Orthonormal vectors are linearly independent, and thus orthonormal vectors \\(\\vec u_1,\\ldots,\\vec u_n\\in \\mathbb{R}^n\\) form a basis for \\(\\mathbb{R}^n\\).\n\n\nProof. Suppose \\((\\vec u_1,\\ldots,\\vec u_m)\\) are orthonormal vectors in \\(\\mathbb{R}^n\\). To show linear independence suppose,\n\\[\nc_1 \\vec u_1+\\cdots + c_m \\vec u_m=\\vec 0\n\\] for some scalars \\(c_1,\\ldots,c_m\\) in \\(\\mathbb{R}\\). Applying the dot product with \\(\\vec u_i\\), \\[\n\\left ( c_1 \\vec u_1 + \\cdots +c_m \\vec u_m\\right ) \\cdot \\vec u_i =\\vec 0 \\cdot \\vec u_i=0.\n\\] Because the dot product is distributive, \\(c_1(\\vec u_1 \\cdot \\vec u_i )+\\cdots + c_m (\\vec u_m \\cdot \\vec u_i)=0\\). We know that \\(\\vec u_i\\cdot \\vec u_i=1\\) and all other dot products are zero. Therefore, \\(c_i=0\\). Since this holds for all \\(i=1,\\ldots,m\\), it follows that \\(c_1=\\cdots =c_m=0\\), and therefore, \\((\\vec u_1,\\ldots,\\vec u_m)\\) are linearly independent. The second part follows since \\(n\\) linearly independent vectors in \\(\\mathbb{R}^n\\) always forms a basis.\n\n\nExample 17.2 Find three examples of an orthonormal basis for a subspace they span. The vectors \\(\\vec e_1,\\ldots, \\vec e_m\\) in \\(\\mathbb{R}^n\\) form an orthonormal basis of the subspace they span. For any scalar \\(\\theta\\), the vectors \\(\\vectortwo{\\cos \\theta}{\\sin \\theta}\\), \\(\\vectortwo{-\\sin\\theta}{\\cos \\theta}\\) form an orthonormal basis of \\(\\mathbb{R}^2\\). The vectors \\[\n\\begin{array}{ccc}\n\\vec u_1=\\vectorfour{1/2}{1/2}{1/2}{1/2}, &\n\\vec u_2=\\vectorfour{1/2}{1/2}{-1/2}{-1/2}, &\n\\vec u_3=\\vectorfour{1/2}{-1/2}{1/2}{-1/2}\n\\end{array}\n\\] in \\(\\mathbb{R}^4\\) form an orthonormal basis of the subspace they span.\n\nLet \\(V\\) be a subspace of \\(\\mathbb{R}^n\\). The orthogonal complement \\(V^\\perp\\) of \\(V\\) is the set of those vectors \\(\\vec x\\) in \\(\\mathbb{R}^n\\) that are orthogonal to all vectors in \\(V\\) namely, \\[\\begin{equation}\nV^{\\perp}=\\{ \\vec x \\in \\mathbb{R}^n \\mid \\vec v \\cdot \\vec x =0, \\text{ for all } \\vec v \\text{ in } V\\}.\n\\end{equation}\\] It is easy to verify that \\(V^\\perp\\) is always a subspace and that \\((\\mathbb{R}^n)^\\perp=\\{0\\}\\) and \\(\\{0\\}^\\perp=\\mathbb{R}^n\\). Also notice that if \\(U_1\\subseteq U_2\\) then \\(U_2^\\perp \\subseteq U_1^\\perp\\). If \\(\\vec x\\in V^{\\perp}\\) then \\(\\vec x\\) is said to be perpendicular to \\(V\\). The vector \\(\\vec x^{\\parallel}\\) in the following theorem is called the orthogonal projection of \\(\\vec x\\) on a subspace \\(V\\) of \\(\\mathbb{R}^n\\) and is denoted by \\(\\text{proj}_V (\\vec x)\\).\n\nTheorem 17.2 (Orthogonal Projection)  \n\nIf \\(V\\) is a subspace of \\(\\mathbb{R}^n\\) and \\(\\vec x\\in \\mathbb{R}^n\\), then \\(\\vec x=\\vec x^\\parallel + \\vec x^\\perp\\) where \\(\\vec x^\\perp\\) is perpendicular to \\(V\\), and this representation is unique.\nIf \\(V\\) is a subspace of \\(\\mathbb{R}^n\\) with an orthonormal basis \\(\\vec u_1 ,\\ldots, \\vec u_m\\), then \\[\\begin{equation}\n\\mathrm{proj}_V (\\vec x):=\\vec x^\\parallel = (\\vec u_1 \\cdot \\vec x) \\vec u_1 + \\cdots + (\\vec u_m \\cdot \\vec x) \\vec u_m\n\\end{equation}\\] for all \\(\\vec x\\) in \\(\\mathbb{R}^n\\).\nLet \\(\\vec u_1 ,\\ldots, \\vec u_n\\) be an orthonormal basis in \\(\\mathbb{R}^n\\), then \\[\\begin{equation}\n\\vec x = (\\vec u_1 \\cdot \\vec x) \\vec u_1 + \\cdots + (\\vec u_n \\cdot \\vec x) \\vec u_n\n\\end{equation}\\] for all \\(\\vec x\\) in \\(\\mathbb{R}^n\\).\n\n\n\nProof. The proof is left for the reader.\n\n\nExample 17.3 Find the orthogonal projection of \\(\\vectorthree{49}{49}{49}\\) onto the subspace of \\(\\mathbb{R}^3\\) spanned by \\(\\vectorthree{2}{3}{6}\\) and \\(\\vectorthree{3}{-6}{2}\\). The two given vectors spanning the subspace are orthogonal since \\(2(3)+3(-6)+6(2)=0\\), but they are not unit vectors since both have length 7. To obtain an orthonormal basis \\(\\vec u_1, \\vec u_2\\) of the subspace, we divide by 7: \\(\\vec u_1=\\frac{1}{7}\\vectorthree{2}{3}{6}\\) and \\(\\vec u_2=\\frac{1}{7}\\vectorthree{3}{-6}{2}\\). Now we can use \\(\\eqref{Orthogonal Projection}\\) with \\(\\vec x=\\vectorthree{49}{49}{49}\\). Then \\[\n\\text{proj}_V(\\vec x)=(\\vec u_1 \\cdot \\vec x)\\vec u_1+(u_2\\cdot \\vec x) \\vec u_2=\n11 \\vectorthree{2}{3}{6}+(-1)\\vectorthree{3}{-6}{2}= \\vectorthree{19}{39}{64}.\n\\]\n\n\nExample 17.4 Find the coordinates of the vector \\(\\vec x=\\vectorfour{4}{5}{6}{7}\\) with respect to the orthonormal basis \\[\n\\begin{array}{cccc}\n\\vec u_1=\\vectorfour{1/2}{1/2}{1/2}{1/2}, &\n\\vec u_2=\\vectorfour{1/2}{1/2}{-1/2}{-1/2}, &\n\\vec u_3=\\vectorfour{1/2}{-1/2}{1/2}{-1/2}, &  \n\\vec u_4=\\vectorfour{1/2}{-1/2}{-1/2}{1/2}.\n\\end{array}\n\\] Normally to find the coordinates of \\(\\vec x\\) we would solve the system \\[\n\\vectorfour{4}{5}{6}{7}=\nc_1 \\vectorfour{1/2}{1/2}{1/2}{1/2}+\nc_2 \\vectorfour{1/2}{1/2}{-1/2}{-1/2}+\nc_3 \\vectorfour{1/2}{-1/2}{1/2}{-1/2} +\nc_4 \\vectorfour{1/2}{-1/2}{-1/2}{1/2}\n\\] for \\(c_1, c_2, c_3, c_4\\). However we can use \\(\\eqref{Orthogonal Projection}\\) instead: \\[\nc_1=u_1\\cdot \\vec x=\\vectorfour{1/2}{1/2}{1/2}{1/2} \\cdot \\vectorfour{4}{5}{6}{7}=11,\n\\] \\[\nc_2=u_2\\cdot \\vec x=\\vectorfour{1/2}{1/2}{-1/2}{-1/2} \\cdot \\vectorfour{4}{5}{6}{7}=-2,\n\\] \\[\nc_3=u_3\\cdot \\vec x=\\vectorfour{1/2}{-1/2}{1/2}{-1/2} \\cdot \\vectorfour{4}{5}{6}{7}=-1,\n\\] \\[\nc_4=u_4\\cdot \\vec x=\\vectorfour{1/2}{-1/2}{-1/2}{1/2} \\cdot \\vectorfour{4}{5}{6}{7}=0.\n\\] Therefore the \\(\\mathcal{B}\\)-coordinate vector of \\(\\vec x\\) is \\(\\vectorfour{11}{-2}{-1}{0}\\).\n\n\nTheorem 17.3 (Properties of the Orthogonal Complement) Let \\(V\\) be a subspace of \\(\\mathbb{R}^n\\), then\n\n\\(\\mathrm{proj}_V(\\vec x)\\) is a linear transformation \\(\\mathbb{R}^n\\to V\\) with kernel \\(V^\\perp\\),\n\\(V \\cap V^\\perp = \\{\\vec 0\\}\\),\n\\(\\text{dim} V +\\text{dim} V^\\perp=n\\), and\n\\((V^\\perp)^\\perp = V\\).\n\n\n\nProof. The proof of each part follows.\n\nTo prove the linearity of \\(T(x):=\\mathrm{proj}_V(\\vec x)\\) we will use the definition of a projection: \\(T(\\vec x)\\) is in \\(V\\), and \\(\\vec x-T(\\vec x)\\) is in \\(V^\\perp\\). To show \\(T(\\vec x+\\vec y)=T(\\vec x)+T(\\vec y)\\), notice \\(T(\\vec x)+T(\\vec y)\\) is in \\(V\\) (since \\(V\\) is a subspace), and \\(\\vec x+\\vec y-(T(\\vec x)+T(\\vec y))=(\\vec x-T(\\vec x))+(\\vec y-T(\\vec y))\\) is in \\(V^\\perp\\) (since \\(V^\\perp\\) is a subspace). To show that \\(T(k \\vec x)=k T(\\vec x)\\), note that \\(k T(\\vec x)\\) is in \\(V\\) (since \\(V\\) is a subspace) and \\(k \\vec x-k T(\\vec x)=k(\\vec x-T(\\vec x))\\) is in \\(V^\\perp\\) (since \\(V^\\perp\\) is a subspace).\nSince \\(\\{\\vec 0\\} \\subseteq V\\) and \\(\\{\\vec 0\\} \\subseteq V^\\perp\\), \\(\\{\\vec 0\\} \\subseteq V\\cap V^\\perp.\\) If a vector \\(\\vec x\\) is in \\(V\\) as well as in \\(V^\\perp\\), then \\(\\vec x\\) is orthogonal to itself: \\(\\vec x \\cdot \\vec x =\\norm{x}^2=0\\), so that \\(\\vec x\\) must equal \\(\\vec 0\\) which shows \\(V \\subseteq \\{\\vec 0\\}\\). Therefore, \\(V\\cap V^\\perp=\\{\\vec 0\\}\\).\nApply the Rank-Nullity Theorem to the linear transformation \\(T(\\vec x)=\\text{proj}_V(\\vec x)\\) yielding \\(n=\\text{dim} \\mathbb{R}^n =\\text{dim} \\text{image} T+\\text{dim} \\ker T=\\text{dim} V+\\text{dim} V^\\perp\\).\nLet \\(\\vec v\\in V\\). Then \\(\\vec v\\cdot \\vec x=0\\) for all \\(\\vec x\\) in \\(V^\\perp\\). Since \\((V^\\perp)^\\perp\\) contains all vectors \\(\\vec y\\) such that \\(\\vec y \\cdot \\vec x =0\\), \\(\\vec v\\) is in \\((V^\\perp)^\\perp\\). So \\(V\\) is a subspace of \\((V^\\perp)^\\perp\\). Using (iii) with \\(V\\) (\\(n=\\text{dim} V+\\text{dim} V^\\perp\\)) and again with \\(V^\\perp\\) (\\(\\text{dim} V^\\perp+\\text{dim} (V^\\perp)^\\perp\\)) yielding \\(\\text{dim} V=\\text{dim} (V^\\perp)^\\perp\\); and since \\(V\\) is a subspace of \\((V^\\perp)^\\perp\\) it follows that \\(V=(V^\\perp)^\\perp\\).\n\n\n\n\nExample 17.5 Find a basis for \\(W^\\perp\\), where \\(W=\\text{span} \\left( \\vectorfour{1}{2}{3}{4}, \\vectorfour{5}{6}{7}{8} \\right)\\). The orthogonal complement \\(W^\\perp\\) of \\(W\\) consists of the vectors \\(\\vec x\\) in \\(\\mathbb{R}^4\\) such that \\[\n\\vectorfour{x_1}{x_2}{x_3}{x_4} \\cdot \\vectorfour{1}{2}{3}{4}\n=0 \\hspace{1cm}\\text{and}  \n\\vectorfour{x_1}{x_2}{x_3}{x_4} \\cdot \\vectorfour{5}{6}{7}{8}=0.\n\\] Finding these vectors amounts to solving the system \\[\n\\begin{cases}\nx_1+2x_2+3x_3+4x_4&=0\n\\\\ 5x_1+6x_2+7x_3+8x_4 & =0\n\\end{cases}\n\\] The solutions are of the form \\[\n\\vectorfour{x_1}{x_2}{x_3}{x_4}=\\vectorfour{s+2t}{-2s-3t}{s}{t}=s\\vectorfour{1}{-2}{1}{0}+t\\vectorfour{2}{-3}{0}{1}.\n\\] The two vectors on the right form a basis of \\(W^{\\perp}\\).\n\n\nTheorem 17.4 Let \\(\\vec x, \\vec y \\in \\mathbb{R}^n\\). Then\n\n( Pythagorean Theorem) \\[\n||\\vec x + \\vec y || ^2 = || \\vec x|| ^2 + || \\vec y || ^2\n\\] holds if and only if \\(\\vec x \\perp \\vec y\\),\n( Cauchy-Schwarz) \\[\n| \\vec x \\cdot \\vec y | \\leq || \\vec x || \\, || \\vec y ||\n\\] where equality holds if and only if \\(\\vec x\\) and \\(\\vec y\\) are parallel,\n( Law of Cosines) the angle \\(\\theta\\) between \\(\\vec x\\) and \\(\\vec y\\) is defined as \\[\n\\theta = \\arccos \\frac{\\vec x \\cdot \\vec y}{|| \\vec x || \\, || \\vec y||},\n\\]\n( Triangular Inequality) \\[\n\\norm{\\vec x+\\vec y}\\leq \\norm{\\vec x}+\\norm{\\vec y}.\n\\]\n\n\n\nProof. The proof of each part follows.\n\nThe verification is straightforward: \\[\\begin{align*}\n\\norm{\\vec x+\\vec y }^2\n& =(\\vec x + \\vec y)\\cdot (\\vec x + \\vec y)\n=\\vec x \\cdot \\vec x+2(\\vec x \\cdot \\vec y) +\\vec y \\cdot \\vec y \\\\\n& =\\norm{\\vec x}^2+2(\\vec x\\cdot \\vec y)+\\norm{\\vec y}^2\n=\\norm{\\vec x}^2+\\norm{\\vec y}^2\n\\end{align*}\\] where the last equality holds if and only if \\(\\vec x\\cdot \\vec y=0\\).\nLet \\(V\\) be a one-dimensional subspace of \\(\\mathbb{R}^n\\) spanned by a nonzero vector \\(\\vec y\\). Let \\(\\vec u=\\frac{1}{\\norm{\\vec y}} \\vec y\\). Then \\[\n\\norm{\\vec x}\n\\geq \\norm{\\text{proj}_V(\\vec x)}\n=\\norm{(\\vec x\\cdot \\vec u)\\vec u}\n=|\\vec x\\cdot \\vec u|\n=\\left| \\vec x \\cdot \\left( \\frac{1}{\\norm{y}}\\vec y \\right)\\right|\n= \\frac{1}{\\norm{y}}|\\vec x \\cdot \\vec y|\n\\] multiplying by \\(\\norm{\\vec y}\\), yields \\(| \\vec x \\cdot \\vec y | \\leq || \\vec x || \\, || \\vec y ||\\). Notice that \\(\\norm{\\vec x} \\geq \\norm{\\text{proj}_V(\\vec x)}\\) holds by applying the Pythagorean theorem to \\(\\vec x=\\vec x^\\parallel +\\vec x^\\perp\\) with \\(\\vec x^\\perp \\cdot \\vec x^\\parallel =0\\) so that \\(\\norm{\\vec x}^2=\\norm{\\text{proj}_V(\\vec x)}^2+\\norm{\\vec x^\\perp}^2\\) which leads to \\(\\norm{\\text{proj}_V \\vec x} \\leq \\norm{\\vec x}\\).\nWe have to make sure that \\(\\eqref{law-of-cosines}\\) is defined, that is \\(\\theta\\) is between \\(-1\\) and \\(1\\), or equivalently, \\[\n\\left | \\frac{\\vec x \\cdot \\vec y }{\\norm{x} \\, \\norm{y}} \\right | \\leq 1.\n\\] But this follows from the Cauchy-Schwarz inequality.\nUsing the Cauchy-Schwarz inequality, the verification is straightforward, \\[\n\\norm{\\vec x+\\vec y}^2=(\\vec x + \\vec y)\\cdot (\\vec x +  \\vec y)=\\norm{\\vec x}^2+\\norm{\\vec y}^2+2(\\vec x\\cdot \\vec y)\n\\] \\[\n\\leq \\norm{\\vec x}^2+\\norm{\\vec y}^2+2\\norm{\\vec x}\\norm{\\vec y}=(\\norm{\\vec x}+\\norm{\\vec y})^2\n\\] Taking the square root of both sides yields \\(\\norm{\\vec x+\\vec y}\\leq \\norm{\\vec x}+\\norm{\\vec y}\\).\n\n\n\nExample 17.6 Determine whether the angle between the vectors \\(\\vec u=\\vectorthree{2}{3}{4}\\), \\(\\vec v=\\vectorthree{2}{-8}{5}\\) is a right angle using the Pythagorean Theorem. Since \\(\\norm{\\vec u}=\\sqrt{2^2+3^2+4^2}=\\sqrt{29}\\) and \\(\\norm{\\vec v}=\\sqrt{2^2+(-8)^2+5^2}=\\sqrt{93}\\). Then \\[\n\\norm{\\vec u+\\vec v}^2=\\left|\\left| \\, \\, \\vectorthree{4}{-5}{9} \\, \\,\n\\right| \\right|^2 =122 = 29+93= || \\vec u|| ^2 + || \\vec v || ^2\n\\] shows \\(\\vec u \\perp \\vec v\\).\n\n\nExample 17.7 Consider the vectors \\(\\vec u =\\vectorfour{1}{1}{\\vdots}{1}\\) and \\(\\vec v=\\vectorfour{1}{0}{\\vdots}{0}\\) in \\(\\mathbb{R}^n\\). For \\(n=2,3,4\\), find the angle \\(\\theta\\) between \\(\\vec u\\) and \\(\\vec v\\). Then find the limit of \\(\\theta\\) as \\(n\\) approaches infinity. For any possible value of \\(n\\), \\[\n\\theta_n=\\arccos \\frac{\\vec u \\cdot \\vec v}{\\norm{\\vec u}\\norm{\\vec v}}=\\arccos \\frac{1}{\\sqrt{n}}.\n\\] Then \\[\n\\begin{bmatrix}\n\\theta_2=\\arccos \\frac{1}{\\sqrt{2}}=\\frac{\\pi}{4}, &\n\\hspace{.5cm}  \\theta_3=\\arccos \\frac{1}{\\sqrt{3}}=\\frac{\\pi}{4}\\sim 0.955 \\text{ rads}, &\n\\hspace{.5cm} \\theta_4=\\arccos \\frac{1}{\\sqrt{4}}=\\frac{\\pi}{3}.\n\\end{bmatrix}\n\\] Since \\(y=\\arccos(x)\\) is a continuous function, \\[\n\\lim_{x\\mapsto \\infty} \\theta_n\n= \\lim_{x\\mapsto \\infty} \\arccos\\left( \\frac{1}{\\sqrt{n}} \\right)\n= \\arccos \\left( \\lim_{x\\mapsto \\infty} \\frac{1}{\\sqrt{n}} \\right)\n= \\arccos(0)\n=\\frac{\\pi}{2}.\n\\]"
  },
  {
    "objectID": "gram-schmidt-process-and-qr-factorization.html",
    "href": "gram-schmidt-process-and-qr-factorization.html",
    "title": "18  Gram-Schmidt Process and QR Factorization",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nThe Gram-Schmidt process represents a change of basis from a basis \\(\\mathcal{B}=(\\vec v_1, \\vec v_2, ..,,\\vec v_m)\\) of a subspace \\(V\\) of \\(\\mathbb{R}^n\\) to an orthonormal basis \\(\\mathcal{U}=(\\vec u_1, \\vec u_2, \\ldots,\\vec u_m)\\) of \\(V\\); it is most sufficiently described in terms of the change of basis matrix \\(R\\) from \\(\\mathcal{B}\\) to \\(\\mathcal{U}\\) via, \\[\\begin{equation}\nM:=\\begin{bmatrix} \\vec v_1 & \\vec v_2 & \\cdots & \\vec v_m \\end{bmatrix}\n=\\begin{bmatrix} \\vec u_1 &\\vec u_2 & \\cdots & \\vec u_m \\end{bmatrix}R=:QR\n\\end{equation}\\]\nThe \\(QR\\) factorization is an effective way to organize and record the work performed in the Gram-Schmidt process; it is also useful for many computational and theoretical purposes.\n\nTheorem 18.1 (Gram-Schmidt Process) Let \\(\\vec v_1, \\vec v_2, \\ldots, \\vec v_m\\) be a basis of a subspace \\(V\\) of \\(\\mathbb{R}^n\\). Then \\[\\begin{equation}\n\\begin{array}{cccc}\n\\vec u_1 = \\frac{1}{||\\vec v_1 || } \\vec v_1,  &\n\\vec u_2 = \\frac{1}{||\\vec v_2^\\perp || } \\vec v_2^\\perp, & \\ldots, &\n\\vec u_m = \\frac{1}{||\\vec v_m^\\perp || } \\vec v_m^\\perp\n\\end{array}\n\\end{equation}\\] is an orthonormal basis of \\(V\\) where \\[\\begin{equation}\n\\vec v_j ^\\perp\n= \\vec v_j - \\vec v_j^\\parallel\n= \\vec v_j - (\\vec u_1 \\cdot \\vec v_j) \\vec u_1 - (\\vec u_2 \\cdot \\vec v_j) \\vec u_2 - \\cdots - (\\vec u_{j-1} \\cdot \\vec v_j) \\vec u_{j-1}.\n\\end{equation}\\]\n\n\nProof. For each \\(j\\), we resolve the vectors \\(\\vec v_j\\) into its components parallel and perpendicular to the span of the preceding vectors \\(\\vec v_1, \\vec v_2, \\ldots, \\vec v_{j-1}\\): \\[\\vec v_j = \\vec v_j^\\parallel + \\vec v_j^\\perp \\hspace{1cm} \\text{with respect to } \\text{span}(\\vec v_1, \\vec v_2, \\ldots, \\vec v_{j-1}).\\] Then use \\(\\ref{Orthogonal Projection}\\).\n\n\nExample 18.1 Perform the Gram-Schmidt process on the vectors \\[\n\\vec v_1=\\vectorthree{4}{0}{3}, \\qquad\n\\vec v_2=\\vectorthree{25}{0}{-25}, \\qquad\n\\vec v_3=\\vectorthree{0}{-2}{0}.\n\\] By Theorem \\(\\eqref{Gram-Schmidt Process}\\), we determine \\(\\vec u_1, \\vec u_2, \\vec u_3\\) as follows: \\[\n\\vec u_1= \\vectorthree{4/5}{0}{3/5},\n\\hspace{1cm}\n\\vec u_2=\\frac{\\vec v_2^\\perp}{\\norm{\\vec v_2^\\perp}}\n=\\frac{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right) \\vec u_1}{\\norm{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right)\\vec u_1}}\n=\\vectorthree{3/5}{0}{-4/5},\n\\] and \\[\n\\vec u_3=\\frac{v_3^\\perp}{\\norm{v_3^\\perp}}\n=\\frac{\\vec v_3-\\left(\\vec u_1 \\cdot \\vec v_3\\right) \\vec u_1-\\left(\\vec u_2 \\cdot \\vec v_3\\right) \\vec u_2}{\\norm{\\vec v_3-\\left(\\vec u_1 \\cdot \\vec v_3\\right) \\vec u_1-\\left(\\vec u_2 \\cdot \\vec v_3\\right) \\vec u_2}}\n=\\vectorthree{0}{-1}{0}.\n\\] Therefore \\[\n\\left( \\vectorthree{4/5}{0}{3/5},  \\vectorthree{3/5}{0}{-4/5}, \\vectorthree{0}{-1}{0}\\right).\n\\] is an orthonormal basis for \\(\\text{span} (\\vec v_1, \\vec v_2, \\vec v_3)\\).\n\n::: {#thm- } [QR Factorization] Let \\(M\\) be an \\(n \\times m\\) matrix with linearly independent columns \\(\\vec v_1 , \\vec v_2, \\ldots,\\vec v_m\\). Then there exists an \\(n\\times m\\) matrix \\(Q\\) whose columns \\(\\vec u_1, \\vec u_2, \\ldots, \\vec u_m\\) are orthonormal and an upper triangular matrix \\(R\\) with positive diagonal entries such that \\(M=Q R\\); and this representation is unique. Furthermore, the entries \\(r_{ij}\\) of \\(R\\) are given by\n\n\\(r_{11}=\\norm{\\vec v_1}\\),\n\\(r_{jj}=||\\vec v_j^\\perp||\\) (for \\(j=2,\\ldots,m\\)), and\n\\(r_{i j}=\\vec u_i \\cdot \\vec v_j\\) (for \\(i<j\\)). :::\n\n\nProof. The proof is left for the reader.\n\n\nExample 18.2 Find the \\(QR\\) factorization of the matrix and display the commutative diagram. \\[\nM=\\begin{bmatrix} 4 & 25 & 0 \\\\ 0 & 0 & -2 \\\\ 3 & -25 & 0\\end{bmatrix}\n\\] Let \\[\n\\begin{array}{ccc}\n\\vec v_1=\\vectorthree{4}{0}{3}, &\n\\vec v_2=\\vectorthree{25}{0}{-25}, &\n\\vec v_3=\\vectorthree{0}{-2}{0}\n\\end{array}\n\\] then (as determined above) an orthonormal basis for the column vectors of \\(M\\) is\n\\[\\left(\\vec u_1, \\vec u_2, \\vec u_3\\right)=\\left( \\vectorthree{4/5}{0}{3/5},  \\vectorthree{3/5}{0}{-4/5}, \\vectorthree{0}{-1}{0}\\right).\\] Determining the entries of \\(R\\) (also as determined above): \\[\n\\begin{array}{ccc}\nr_{11}=\\norm{\\vec v_1}=5, &\nr_{22}=\\norm{\\vec v_2^\\perp}=35, &\nr_{33}=\\norm{\\vec v_3^\\perp}=2\n\\end{array}\n\\] \\[\n\\begin{array}{ccc}\nr_{12}=\\vec u_1 \\cdot \\vec v_2=5, &\nr_{13}=\\vec u_1 \\cdot \\vec v_3=0, &\nr_{23}=\\vec u_2 \\cdot \\vec v_3=0\n\\end{array}\n\\] and therefore, the \\(QR\\)-factorization of \\(M\\) is \\[\nM= \\begin{bmatrix} 4 & 25 & 0 \\\\ 0 & 0 & -2 \\\\ 3 & -25 & 0\\end{bmatrix}\n= \\begin{bmatrix}4/5 & 3/5 & 0 \\\\ 0 & 0 & -1 \\\\ 3/5 & -4/5 & 0 \\end{bmatrix}\n\\begin{bmatrix}5 & 5 & 0 \\\\ 0 & 35 & 0 \\\\ 0 & 0 & 2\\end{bmatrix}\n=QR.\n\\]\n\n\nExample 18.3  \n\nPerform the Gram-Schmidt process on the vectors \\[\n\\vec v_1=\\vectorfour{1}{7}{1}{7}, \\qquad\n\\vec v_2= \\vectorfour{0}{7}{2}{7}, \\qquad\n\\vec v_3 = \\vectorfour{1}{8}{1}{6}.\n\\]\nFind the \\(QR\\) factorization of the matrix \\(M=\\begin{bmatrix} \\vec v_1 & \\vec v_2 & \\vec v_3\\end{bmatrix}.\\)\n\nBy Theorem \\(\\eqref{Gram-Schmidt Process}\\), we determine \\(\\vec u_1, \\vec u_2, \\vec u_3\\) as follows: \\[\n\\vec u_1= \\vectorfour{1/10}{7/10}{1/10}{7/10},\n\\vec u_2=\\frac{\\vec v_2^\\perp}{\\norm{\\vec v_2^\\perp}}\n=\\frac{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right) \\vec u_1}{\\norm{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right)\\vec u_1}}\n=\\vectorfour{-1/\\sqrt{2}}{0}{1/\\sqrt{2}}{0},\n\\] and \\[\n\\vec u_3=\\frac{\\vec v_3^\\perp}{\\norm{\\vec v_3^\\perp}}\n=\\frac{\\vec v_3-\\left(\\vec u_1 \\cdot \\vec v_3\\right) \\vec u_1-\\left(\\vec u_2 \\cdot \\vec v_3\\right) \\vec u_2}{\\norm{\\vec v_3-\\left(\\vec u_1 \\cdot \\vec v_3\\right) \\vec u_1-\\left(\\vec u_2 \\cdot \\vec v_3\\right) \\vec u_2}}\n=\\vectorfour{0}{1/\\sqrt{2}}{0}{-1/\\sqrt{2}}.\n\\] Therefore an orthonormal basis for \\(\\text{span} (\\vec v_1, \\vec v_2, \\vec v_3)\\) is \\[\n\\left( \\vectorfour{1/10}{7/10}{1/10}{7/10},  \\vectorfour{-1/\\sqrt{2}}{0}{1/\\sqrt{2}}{0}, \\vectorfour{0}{1/\\sqrt{2}}{0}{-1/\\sqrt{2}}\\right).\n\\] Determining the entries of \\(R\\) (also as determined above): \\[\n\\begin{array}{ccc}\nr_{11}=\\norm{\\vec v_1}=10, &\nr_{22}=\\norm{\\vec v_2^\\perp}=\\sqrt{2}, &\nr_{33}=\\norm{\\vec v_3^\\perp}=\\sqrt{2}\n\\end{array}\n\\] \\[\n\\begin{array}{ccc}\nr_{12}=\\vec u_1 \\cdot \\vec v_2=10, &\nr_{13}=\\vec u_1 \\cdot \\vec v_3=10, &\nr_{23}=\\vec u_2 \\cdot \\vec v_3=0\n\\end{array}\n\\] and therefore, the \\(QR\\)-factorization of \\(M\\) is \\[\nM=\\begin{bmatrix} 1 & 0 & 1 \\\\ 7 & 7 & 8 \\\\ 1 & 2 & 1 \\\\ 7 & 7 & 6 \\end{bmatrix}\n= \\begin{bmatrix}1/10 & -1/\\sqrt{2} & 0 \\\\ 7/10 & 0 & 1/\\sqrt{2} \\\\ 1/10 & 1/\\sqrt{2} & 0 \\\\ 7/10 & 0 & -1/\\sqrt{2}\\end{bmatrix}\n\\begin{bmatrix}10 & 10 & 10 \\\\ 0 & \\sqrt{2} & 0 \\\\ 0 & 0 & \\sqrt{2}\\end{bmatrix}\n=QR.\n\\]"
  },
  {
    "objectID": "orthogonal-transformations-and-orthogonal-matrices.html",
    "href": "orthogonal-transformations-and-orthogonal-matrices.html",
    "title": "19  Orthogonal Transformations and Orthogonal Matrices",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nA linear transformation \\(T\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) is called an orthogonal transformation if it preserves the length of vectors: \\(\\norm{T(\\vec x)}=\\norm{x}\\) for all \\(\\vec x\\in \\mathbb{R}^n.\\) If \\(T(\\vec x)=A\\vec x\\) is an orthogonal transformation, we say \\(A\\) is an orthogonal matrix .\n\nLemma 19.1 Let \\(T\\) be an orthogonal transformation from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\). If \\(\\vec v, \\vec w \\in \\mathbb{R}^n\\) are orthogonal, then \\(T(\\vec v), T(\\vec w) \\in \\mathbb{R}^n\\) are orthogonal.\n\n\nProof. We want to show \\(T(\\vec v), T(\\vec w)\\) are orthogonal, and by the Pythagorean theorem, we have to show \\[\n\\norm{T(\\vec v)+T(\\vec w)}^2=\\norm{T(\\vec v)}^2+\\norm{T(\\vec w)}^2.\n\\] This equality follows \\[\\begin{align*}\n\\norm{T(\\vec v)+T(\\vec w)}^2\n& =\\norm{T(\\vec v+\\vec w)}^2\n=\\norm{\\vec v+\\vec w}^2 \\\\\n& =\\norm{\\vec v}^2+\\norm{\\vec w}^2\n=\\norm{T(\\vec v)}^2+\\norm{T(\\vec w)}^2\n\\end{align*}\\] since \\(T\\) is linear, orthogonal and that \\(\\vec v, \\vec w\\) are orthogonal, respectively.\n\n\nTheorem 19.1 A linear transformation \\(T\\) from \\(\\mathbb{R}^n\\) to \\(\\mathbb{R}^n\\) is orthogonal if and only if the vectors \\(T(\\vec e_1)\\), , \\(T(\\vec e_n)\\) form an orthonormal basis.\n\n\nProof. If \\(T\\) is an orthogonal transformation, then by definition, the \\(T(\\vec e_i)\\) are unit vectors, and also, by \\(\\ref{orthogonal transformation}\\) they are orthogonal. Therefore, \\(T(\\vec e_1)\\), , \\(T(\\vec e_n)\\) form an orthonormal basis.\nConversely, suppose \\(T(\\vec e_1)\\), , \\(T(\\vec e_n)\\) form an orthonormal basis. Consider a vector \\(\\vec x=x_1 \\vec e_1+\\cdots +x_n \\vec e_n\\). Then \\[\\begin{align*}\n\\norm{T(\\vec x)}^2\n&=\\norm{T(x_1 \\vec e_1+\\cdots + x_n \\vec e_n)}^2\n=\\norm{x_1 T(\\vec e_1)+\\cdots + x_n T(\\vec e_n)}^2 \\\\\n&=\\norm{x_1T(\\vec e_1)}^2+\\cdots + \\norm{x_nT(\\vec e_n)}^2 = x_1^2+\\cdots + x_n^2\n=\\norm{x}^2.\n\\end{align*}\\] Taking the square root of both sides shows that \\(T\\) preserves lengths and therefore, \\(T\\) is an orthogonal transformation.\n\n\nCorollary 19.1 An \\(n \\times n\\) matrix \\(A\\) is orthogonal if and only if its columns form an orthonormal basis.\n\n\nProof. The proof is left for the reader.\n\nThe transpose \\(A^T\\) of an \\(n\\times n\\) matrix \\(A\\) is the \\(n\\times n\\) matrix whose \\(ij\\)-th entry is the \\(ji\\)-th entry of \\(A\\). We say that a square matrix \\(A\\) is symmetric if \\(A^T=A\\), and \\(A\\) is called skew-symmetric if \\(A^T=-A\\).\n\nTheorem 19.2 (Orthogonal and Transpose Properties)  \n\nThe product of two orthogonal \\(n\\times n\\) matrices is orthogonal.\nThe inverse of an orthogonal matrix is orthogonal.\nIf the products \\((A B)^T\\) and \\(B^T A^T\\) are defined then they are equal.\nIf \\(A\\) is invertible then so is \\(A^T\\), and \\((A^T)^{-1}=(A^{-1})^T\\).\nFor any matrix \\(A\\), \\(\\text{rank}\\,(A) = \\text{rank} \\,(A^T)\\).\nIf \\(\\vec v\\) and \\(\\vec w\\) are two column vectors in \\(\\mathbb{R}^n\\), then \\(\\vec v \\cdot \\vec w = \\vec v^T \\vec w\\).\nThe \\(n \\times n\\) matrix \\(A\\) is orthogonal if and only if \\(A^{-1}=A^T\\).\n\n\n\nProof. The proof of each part follows.\n\nSuppose \\(A\\) and \\(B\\) are orthogonal matrices, then \\(AB\\) is an orthogonal matrix since \\(T(\\vec x)=AB \\vec x\\) preserves length because \\(\\norm{T(\\vec x)}=\\norm{AB \\vec x}=\\norm{A(B \\vec x)}=\\norm{B \\vec x}=\\norm{\\vec x}.\\)\nSuppose \\(A\\) is an orthogonal matrix, then \\(A^{-1}\\) is orthogonal an matrix since \\(T(\\vec x)=A^{-1} \\vec x\\) preserves length because \\(\\norm{A^{-1}\\vec x}=\\norm{A(A^{-1}\\vec x)}=\\norm{\\vec x}\\).\nWe will compare entries in the matrices \\((AB)^T\\) and \\(B^T A^T\\) as follows: \\[\n\\begin{array}{rl}\ni j \\text{-th entry of }(AB)^T &= ji \\text{-th entry of }AB\\\\\n& = (j \\text{-th row of } A) \\cdot (i \\text{-th column of } B)\\\\ \\\\\ni j \\text{-th entry of }B^TA^T &=(i \\text{-th row of } B^T) \\cdot (j \\text{th column of } A^T)\\\\\n& = (i \\text{-th column of } B) \\cdot (j \\text{-th row of } A)\\\\\n& = (j \\text{-th row of } A) \\cdot (i \\text{-th column of } B).\n\\end{array}\n\\] Therefore, the \\(ij\\)-th entry of \\((AB)^T\\) is the same of the \\(ij\\)-th entry of \\(B^T A^T\\).\nSuppose \\(A\\) is invertible, then \\(A A^{-1}=I_n\\). Taking the transpose of both sides along with (iii) it yields, \\((A A^{-1})^T=(A^{-1})^T A^T=I_n\\). Thus \\(A^T\\) is invertible and since inverses are unique, it follows \\((A^T)^{-1}=(A^{-1})^T\\).\nExercise.\nIf \\(\\vec v=\\vectorthree{a_1}{\\vdots}{a_n}\\) and \\(\\vec w=\\vectorthree{b_1}{\\vdots}{b_n}\\), then \\[\n\\vec v \\cdot \\vec w=\\vectorthree{a_1}{\\vdots}{a_n}\\cdot \\vectorthree{b_1}{\\vdots}{b_n}=a_1b_1+\\cdots +a_n b_n\n=\\begin{bmatrix}a_1 & \\cdots & a_n\\end{bmatrix}  \\vectorthree{b_1}{\\vdots}{b_n}\n=\\vectorthree{a_1}{\\vdots}{a_n}^T \\vec w=\\vec v^T \\vec w.\n\\]\nLet’s write \\(A\\) in terms of its columns: \\(A=\\begin{bmatrix} \\vec v_1 & \\cdots & \\vec v_n \\end{bmatrix}\\). Then \\[\\begin{equation}\n\\label{ata}\nA^T A=\n\\begin{bmatrix}  \\vec v_1^T  \\\\  \\vdots \\\\  \\vec v_n^T \\end{bmatrix}\n\\begin{bmatrix} \\vec v_1 & \\cdots & \\vec v_n  \\end{bmatrix}\n=\\begin{bmatrix}\\vec v_1 \\cdot \\vec v_1 & & \\vec v_1 \\cdot \\vec v_n \\\\ \\vdots & \\cdots & \\vdots \\\\ \\vec v_n \\cdot \\vec v_1 & & \\vec v_n \\cdot \\vec v_n\\end{bmatrix}.\n\\end{equation}\\] Now \\(A\\) is orthogonal, by \\(\\ref{oob}\\), if and only if \\(A\\) has orthonormal columns, meaning \\(A\\) is orthogonal if and only if \\(A^TA=I_n\\) by \\(\\ref{ata}\\). Therefore, \\(A\\) is orthogonal if and only if \\(A^{-1}=A^T\\).\n\n\n\nTheorem 19.3 (Orthogonal Projection Matrix)  \n\nLet \\(V\\) be a subspace of \\(\\mathbb{R}^n\\) with orthonormal basis \\(\\vec u_1\\), , \\(\\vec u_m\\). The matrix of the orthogonal projection onto \\(V\\) is \\(Q Q^T\\) where \\(Q= \\begin{bmatrix} \\vec u_1 & \\cdots & \\vec u_m \\end{bmatrix}.\\)\nLet \\(V\\) be a subspace of \\(\\mathbb{R}^n\\) with basis \\(\\vec v_1,\\ldots,\\vec v_m\\) and let \\(A=\\begin{bmatrix}\\vec v_1 & \\cdots \\vec v_m \\end{bmatrix}\\), then the orthogonal projection matrix onto \\(V\\) is \\(A(A^T A)^{-1}A^T\\).\n\n\n\nProof. The proof of each part follows.\n\nSince \\(\\vec u_1\\), , \\(\\vec u_m\\) is an orthonormal basis of \\(V\\) we can, by \\(\\ref{Orthogonal Projection}\\), write, \\[\\begin{align*}\n\\text{proj}_V (\\vec x)\n& =(\\vec u_1 \\cdot \\vec x) \\vec u_1 + \\cdots + (\\vec u_m \\cdot \\vec x) \\vec u_m\n=\\vec u_1 \\vec u_1^T \\vec x + \\cdots +\\vec u_m \\vec u_m^T \\vec x &  \\\\\n&=(\\vec u_1 \\vec u_1^T  + \\cdots +\\vec u_m \\vec u_m^T) \\vec x\n= \\begin{bmatrix} \\vec u_1 & \\cdots & \\vec u_m  \\end{bmatrix}\n\\begin{bmatrix} \\vec u_1^T  \\\\  \\vdots \\\\  \\vec u_m^T  \\end{bmatrix} \\vec x\n=QQ^T\\vec x.\n\\end{align*}\\]\nSince \\(\\vec v_1,\\ldots,\\vec v_m\\) form a basis of \\(V\\), there exists unique scalars \\(c_1,\\ldots,c_m\\) such that \\(\\text{proj}_V(\\vec x)=c_1 \\vec v_1+\\cdots +c_m \\vec v_m\\). Since \\(A=\\begin{bmatrix}\\vec v_1 & \\cdots & \\vec v_m \\end{bmatrix}\\) we can write \\(\\text{proj}_V(\\vec x)=A \\vec c\\). Consider the system \\(A^TA\\vec c =A^T \\vec x\\) where \\(A^TA\\) is the coefficient matrix and \\(\\vec c\\) is the unknown. Since \\(\\vec c\\) is the coordinate vector of \\(\\text{proj}_V(\\vec x)\\) with respect to the basis \\((v_1,\\ldots,v_m)\\), the system has a unique solution. Thus, \\(A^TA\\) must be invertible, and so we can solve for \\(\\vec c\\), namely \\(\\vec c=(A^T A)^{-1}A^T\\vec x\\). Therefore, \\(\\text{proj}_V(\\vec x)=A \\vec c =A (A^T A)^{-1}A^T\\) as desired. Notice it suffices to consider the system \\(A^TA\\vec c =A^T \\vec x\\), or equivalently \\(A^T(\\vec x-A \\vec c)=\\vec 0\\), because \\[\nA^T(\\vec x -A \\vec c)=A^T(\\vec x-c_1 \\vec v_1-\\cdots - c_m \\vec v_m)\n\\] is the vector whose \\(i\\)th component is \\[\n(\\vec v_i)^T(\\vec x-c_1 \\vec v_1-\\cdots -c_m \\vec v_m)=\\vec v_i\\cdot(\\vec x-c_1\\vec v_1-\\cdots -c_m \\vec v_m)\n\\] which we know to be zero since \\(\\vec x-\\text{proj}_V(\\vec x)\\) is orthogonal to \\(V\\).\n\n\n\nExample 19.1 Is there an orthogonal transformation \\(T\\) from \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}^3\\) such that \\[\nT\\vectorthree{2}{3}{0}=\\vectorthree{3}{0}{2} \\hspace{1cm} \\text{and}  \\hspace{1cm} T\\vectorthree{-3}{2}{0}=\\vectorthree{2}{-3}{0}?\n\\] No, since the vectors \\(\\vectorthree{2}{3}{0}\\) and \\(\\vectorthree{-3}{2}{0}\\) are orthogonal, whereas \\(\\vectorthree{3}{0}{2}\\) and \\(\\vectorthree{2}{-3}{0}\\) are not, by \\(\\ref{orthogonal transformation}\\).\n\n\nExample 19.2 Find an orthogonal transformation \\(T\\) from \\(\\mathbb{R}^3\\) to \\(\\mathbb{R}^3\\) such that \\[\nT\\vectorthree{2/3}{2/3}{1/3}=\\vectorthree{0}{0}{1}.\n\\] Let’s think about the inverse of \\(T\\) first. The inverse of \\(T\\), if it exists, must satisfy \\(T^{-1}(\\vec e_3)=\\vectorthree{2/3}{2/3}{1/3}=\\vec v_3\\). Furthermore, the vectors \\(\\vec v_1, \\vec v_2, \\vec v_3\\) must form an orthonormal basis of \\(\\mathbb{R}^3\\) where \\(T^{-1}\\vec x=\\begin{bmatrix}\\vec v_1 & \\vec v_2 & \\vec v_3\\end{bmatrix} \\vec x\\). We require a vector \\(\\vec v_1\\) with \\(\\vec v_1\\cdot \\vec v_3=0\\) and \\(\\norm{\\vec v_1}=1\\). By inspection, we find \\(\\vec v_1=\\vectorthree{-2/3}{1/3}{2/3}\\). Then \\[\n\\vec v_2=\\vec v_1\\times \\vec v_3=\\vectorthree{-2/3}{1/3}{2/3} \\times \\vectorthree{2/3}{2/3}{1/3}=\\vectorthree{1/9-4/9}{-(-2/9-4/9)}{-4/9-2/9}=\\vectorthree{-1/3}{2/3}{-2/3}\n\\] does the job since \\(\\norm{v_1}=\\norm{v_2}=\\norm{v_3}=1\\) and \\(\\vec v_1\\cdot \\vec v_2=\\vec v_1\\cdot \\vec v_3=\\vec v_2\\cdot \\vec v_3=0\\). In summary \\[\nT^{-1}=\\begin{bmatrix}-2/3 & -1/3 & 2/3 \\\\ 1/3 & 2/3 & 2/3 \\\\\\ 2/3 & -2/3 & 1/3\\end{bmatrix}\\vec x.\n\\] By \\(\\ref{Orthogonal and Transpose Properties}\\) the matrix of \\(T^{-1}\\) is orthogonal and the matrix \\(T=(T^{-1})^{-1}\\) is the transpose of the matrix of \\(T^{-1}\\). Therefore, it suffices to use \\[\nT=\\begin{bmatrix}-2/3 & -1/3 & 2/3 \\\\ 1/3 & 2/3 & 2/3 \\\\\\ 2/3 & -2/3 & 1/3\\end{bmatrix}^T\\vec x=\\begin{bmatrix}-2/3 & 1/3 & 2/3 \\\\ -1/3 & 2/3 & -2/3 \\\\\\ 2/3 & 2/3 & 1/3 \\end{bmatrix} \\vec x.\n\\]\n\n\nExample 19.3 Show that a matrix with orthogonal columns need not be an orthogonal matrix. For example \\(A=\\begin{bmatrix}4 & -3 \\\\ 3 & 4 \\end{bmatrix}\\) is not an orthogonal matrix \\(T\\vec x=A\\vec x\\) does not preserve length by comparing the lengths of \\(\\vec x\\) and \\(T\\vec x\\) with \\(\\vectortwo{-3}{4}\\).\n\n\nExample 19.4 Find all orthogonal \\(2\\times 2\\) matrices. Write \\(A=\\begin{bmatrix}\\vec v_1 & \\vec v_2\\end{bmatrix}\\). The unit vector \\(\\vec v_1\\) can be expressed as \\(\\vec v_1=\\vectortwo{\\cos \\theta}{\\sin \\theta}\\), for some \\(\\theta\\). Then \\(v_2\\) will be one of the two unit vectors orthogonal to \\(\\vec v_1\\), namely \\(\\vec v_2=\\vectortwo{-\\sin \\theta}{\\cos \\theta}\\) or \\(\\vec v_2=\\vectortwo{\\sin \\theta}{-\\cos \\theta}\\). Therefore, an orthogonal \\(2\\times 2\\) matrix is either of the form \\[\nA=\\begin{bmatrix}\\cos \\theta & -\\sin \\theta \\\\ \\sin \\theta & \\cos \\theta \\end{bmatrix}\\hspace{1cm} \\text{or} \\hspace{1cm} A=\\begin{bmatrix}\\cos \\theta & \\sin \\theta \\\\ \\sin \\theta & -\\cos \\theta \\end{bmatrix}\n\\] representing a rotation or a reflection, respectively.\n\n\nExample 19.5 Given \\(n\\times n\\) matrices \\(A\\) and \\(B\\) which of the following must be symmetric?\n\n\\(B B^T\\)\n\\(A^T B^TB A\\)\n\\(B(A+A^T)B^T\\)\n\nThe solution to each part follows.\n\n\\(\\ref{Orthogonal and Transpose Properties}\\), \\(B B^T\\) is symmetric because \\[\n(B B^T)^T=(B^T)^TB^T=B B^T.\n\\]\n\\(\\ref{Orthogonal and Transpose Properties}\\), \\(A^T B^TB A\\) is symmetric because \\[\n(A^TB^TBA)^T=A^TB^T(B^T)^T(A^T)^T=A^TB^TBA.\n\\]\n\\(\\ref{Orthogonal and Transpose Properties}\\), \\(B(A+A^T)B^T\\) is symmetric because \\[\n(B(A+A^T)B^T)^T=((A+A^T)B^T)^TB^T=B(A+A^T)^TB^T\n\\] \\[\n=B(A^T+A)^TB^T=B((A^T)^T+A^T)B^T=B(A+A^T)B^T.\n\\]\n\n\n\nExample 19.6 If the \\(n\\times n\\) matrices \\(A\\) and \\(B\\) are symmetric which of the following must be symmetric as well?\n\n\\(2I_n+3A-4 A^2\\),\n\\(A B^2 A\\).\n\nThe solution to each part follows.\n\nFirst note that \\((A^2)^T=(A^T)^2=A^2\\) for a symmetric matrix \\(A\\). Now we can use the linearity of the transpose, \\[\n(2I_n+3A-4 A^2)^T=2I_n^T+3A^T-4 (A^2)^T=2I_n+3A-4 A^2\n\\] showing that the matrix \\(2I_n+3A-4 A^2\\) is symmetric.\nThe matrix \\(A B^2 A\\) is symmetric since, \\[\n(AB^2A)^T=(ABBA)^T=(BA)^T(AB)^T=A^TB^TB^TA^T=AB^2A.\n\\]\n\n\n\nExample 19.7 Use \\(\\ref{Orthogonal Projection Matrix}\\) to find the matrix \\(A\\) of the orthogonal projection onto \\[\nW=\\text{span} \\left(\\vectorfour{1}{1}{1}{1},\\vectorfour{1}{9}{-5}{3}\\right).\n\\] Then find the matrix of the orthogonal projection onto the subspace of \\(\\mathbb{R}^4\\) spanned by the vectors \\(\\vectorfour{1}{1}{1}{1}\\) and \\(\\vectorfour{1}{2}{3}{4}\\). First we apply \\(\\ref{Gram-Schmidt Process}\\), the Gram-Schmidt process, to \\(W=\\text{span}(\\vec v_1, \\vec v_2)\\), to find that the vectors \\[\n\\vec u_1=\\frac{\\vec v_1}{\\norm{\\vec v_1}}\n=\\vectorfour{1/2}{1/2}{1/2}{1/2}, \\hspace{1cm}\\vec u_2\n=\\frac{\\vec v_2^\\perp}{\\norm{\\vec v_2^\\perp}}\n=\\frac{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right) \\vec u_1}{\\norm{\\vec v_2-\\left(\\vec u_1 \\cdot \\vec v_2\\right)\\vec u_1}}\n=\\vectorfour{-1/10}{7/10}{-7/10}{1/10}\n\\] form an orthonormal basis of \\(W\\). By \\(\\ref{Orthogonal Projection Matrix}\\), the matrix of the projection onto \\(W\\) is \\(A=Q Q^T\\) where \\(Q=\\begin{bmatrix}\\vec u_1 & \\vec u_2\\end{bmatrix}\\). Therefore the orthogonal projection onto \\(W\\) is \\[\nA=\n\\begin{bmatrix} 1/2 & -1/10 \\\\ 1/2 & 7/10 \\\\ 1/2 & -7/10 \\\\ 1/2 & 1/10 \\end{bmatrix}\n\\begin{bmatrix} 1/2 & 1/2 & 1/2 & 1/2  \\\\ -1/10 & 7/10 & -7/10 & 1/10 \\end{bmatrix}\n=\\frac{1}{100}\n\\begin{bmatrix}\n26 & 18 & 32 & 24 \\\\\n18 & 74 & -24 & 32 \\\\\n32 & -24 & 74 & 18 \\\\\n24 & 32 & 18 & 26\n\\end{bmatrix}.\n\\] Let \\(A=\\begin{bmatrix}1 & 1 \\\\ 1 & 2 \\\\1 & 3 \\\\1 & 4 \\end{bmatrix}\\) and then the orthogonal projection matrix is \\[\nA(A^TA)^{-1}A^T\n=\\frac{1}{10}\\begin{bmatrix}7 & 4 & 1 & -2 \\\\4 & 3 & 2 & 1 \\\\ 1 & 2 & 3 & 4 \\\\ -2 & 1 & 4 & 7 \\end{bmatrix}.\n\\]"
  },
  {
    "objectID": "inner-products.html",
    "href": "inner-products.html",
    "title": "20  Inner Products",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nRecall that the norm of \\(x\\in \\mathbb{R}^n\\) defined by \\(\\norm{x}=\\sqrt{x_1^2+x_2^2}\\) is not linear. To injective linearity into the discussion we introduce the dot product: for \\(x,y\\in \\mathbb{R}^n\\) the dot product of \\(x\\) and \\(y\\) is defined as \\(x \\cdot y=x_1 y_1+\\cdots +x_n y_n\\). Obviously \\(x \\cdot x=\\norm{x}^2\\), and with the dot product being so useful, so we generalize the dot product into an inner product on a vector space \\(V\\).\nAn inner product on \\(V\\) is a function that takes each ordered pair \\((u,v)\\) of elements of \\(V\\) to a number \\(\\ip{u,v} \\in\\mathbb{F}\\) and has the following properties:\n\n() positivity \\(\\ip{ v,v }\\geq 0\\) for all \\(v\\in V\\);\n() definiteness \\(\\ip{ v,v }=0\\) if and only if \\(v=0\\);\n( additivity in the first slot) \\(\\ip{ u+v,w } = \\ip{u,w} +\\ip{ v,w }\\) for all \\(u,v,w\\in V\\);\n( homogeneity in the first slot) \\(\\ip{av,w} =a\\ip{v,w}\\) for all \\(a\\in \\mathbb{F}\\) and all \\(v,w,\\in V\\);\n( conjugate symmetry) \\(\\ip{v,w} = \\overline{\\ip{w,v} }\\) for all \\(v,w\\in V\\).\n\nRecall that for \\(z\\in \\mathbb{C}^n\\), we define the norm of \\(z\\) by \\[\n\\norm{z}=\\sqrt{|z_1|^2+\\cdots + |z_n|^2}\n\\] where the absolute values are needed because we want \\(\\norm{z}\\) to be a non-negative number. Then \\[\\begin{equation}\\label{cp}\n\\norm{z}^2=z_1 \\overline{z_1}+\\cdots + z_n \\overline{z_n}\n\\end{equation}\\] because every \\(\\lambda\\in\\mathbb{C}\\) satisfies \\(|\\lambda|^2 =\\lambda \\overline{\\lambda}\\). Since \\(\\norm{z^2}\\) is the inner-product of \\(z\\) with itself, as in \\(\\mathbb{R}^n\\), the Equation \\(\\eqref{cp}\\) suggests that the inner product of \\(w\\in \\mathbb{C}^n\\) with \\(z\\) should equal \\[\nw_1 \\overline{z_1}+\\cdots+w_n \\overline{z_n}.\n\\] We should expect that the inner product of \\(w\\) with \\(z\\) equals the complex conjugate of the inner product of \\(z\\) with \\(w\\), thus motivating the definition of conjugate symmetry.\nAn inner-product space is a vector space \\(V\\) along with an inner product on \\(V\\). The inner product defined on \\(\\mathbb{F}^n\\) by \\[\n\\ip{ (w_1,\\ldots,w_n),(z_1,\\ldots,z_n) } = w_1 \\overline{z_1}+\\cdots + w_n \\overline{z_n}\n\\] is called the Euclidean inner product.\nContinue to let \\(V\\) denote a complex or real vector space. In this section we develop the basic theorems for norms. For \\(v\\in V\\), the norm of \\(v\\) is defined by \\(||v||=\\sqrt{\\ip{v,v} }\\). Two vectors \\(u,v\\in V\\) are orthogonal if \\(\\ip{u,v}= 0\\).\n\nTheorem 20.1 [Pythagorean Theorem] If \\(u\\) and \\(v\\) are orthogonal vectors in \\(V\\), then \\[\n\\norm{u + v}^2 = \\norm{u}^2 + \\norm{y}^2.\n\\]\n\n\nProof. Suppose that \\(u,v\\) are orthogonal vectors in \\(V\\). Then \\[\n\\norm{u+v}^2=\\ip{u+v,u+v}=\\norm{u}^2+\\norm{v}^2+\\ip{u,v}+\\ip{v,u}=\\norm{u}^2+\\norm{v}^2,\n\\] as desired.\n\n::: {#thm- } [Orthogonal Decomposition] If \\(v\\) is a nonzero vector in \\(V\\), then \\(u\\) can be written as a scalar multiple of \\(v\\) plus a vector orthogonal to \\(v\\). :::\n\nProof. Let \\(a\\in \\mathbb{F}\\). Then \\[\nu=a v+(u-av).\n\\] Thus we need to choose \\(a\\) so that \\(v\\) is orthogonal to \\((u-a v)\\). In other words, we want \\[\n0=\\ip{u-av,v}=\\ip{u,v}-a\\ip{v,v}=\\ip{u,v}-a\\norm{v}^2.\n\\] The equation above shows that we should choose \\(a\\) to be \\(\\ip{u,v}/\\norm{v}^2\\) (assume that \\(v\\ne 0\\) to avoid division by 0). Making this choice of \\(a\\), we can write \\[\nu=\\frac{\\ip{u,v}}{\\norm{v}^2} v+\\left(u-\\frac{\\ip{u,v}}{\\norm{v}^2}v\\right).\n\\] Thus, if \\(v\\neq 0\\) then the equation above writes \\(u\\) as a scalar multiple of \\(v\\) plus a vector orthogonal to \\(v\\).\n\n::: {#thm- } [Cauchy-Schwarz] If \\(u, v \\in V\\), then \\[\n| \\ip{u,v} | \\leq \\norm{u} \\, \\norm{v}\n\\] where equality holds if and only if one of \\(u, v\\) is a scalar multiple of the other. :::\n\nProof. Let \\(u,v\\in V\\). If \\(v=0\\), then both sides and the desired inequality holds. Thus we can assume that \\(v\\neq 0\\). Consider the orthogonal decomposition \\[\nu=\\frac{\\ip{u,v}}{\\norm{v}^2}v+w\n\\] where \\(w\\) is orthogonal to \\(v\\). By the Pythagorean theorem, \\[\\begin{align*}\n\\norm{u}^2\n=\\norm{\\frac{\\ip{u,v}}{\\norm{v}^2} v}^2+\\norm{w}^2\\\\\n=\\frac{|\\ip{u,v}|^2}{\\norm{v}^2}+\\norm{w}^2\\\\\n\\geq \\frac{|\\ip{u,v}|^2}{\\norm{v}^2}\n\\end{align*}\\] Multiplying both sides by \\(\\norm{v}^2\\) and then taking square roots gives the Cauchy-Schwarz inequality. Notice that there is equality if and only if \\(w=0\\), that is, if and only if \\(u\\) is a multiple of \\(v\\).\n\n::: {#thm- } [Triangular Inequality] If \\(u, v \\in V\\), then \\[\n\\norm{u+v}\\leq \\norm{u}+\\norm{v}\n\\] where equality holds if and only if one of \\(u, v\\) is a nonnegative multiple of the other. :::\n\nProof. Let \\(u,v\\in V\\). Then \\[\\begin{align}\n\\norm{u+v}^2 \\notag\n&=\\ip{u+v,u+v}  \\notag\\\\\n&=\\ip{u,u}+\\ip{v,v}+\\ip{u,v}+\\ip{v,u}  \\notag \\\\\n&=\\ip{u,u}+\\ip{v,v}+\\ip{u,v}+\\overline{\\ip{u,v}}  \\notag \\\\\n&=\\norm{u,u}^2+\\norm{v,v}^2+2 \\text{remark}\\ip{u,v}  \\notag \\\\\n&\\leq \\norm{u,u}^2+\\norm{v,v}^2+2 | \\ip{u,v} | \\label{ti1} \\\\\n&\\leq \\norm{u,u}^2+\\norm{v,v}^2+2 \\norm{u}\\norm{v} \\label{ti2}\\\\\n&= \\left(\\norm{u}+\\norm{v}\\right)^2  \\notag\n\\end{align}\\] and so by taking square root of both sides yields the triangular inequality. This proof shows that the triangle inequality is an equality if and only if we have equality in \\(\\eqref{ti1}\\) and \\(\\eqref{ti2}\\). Thus we have equality in the triangular inequality if and only if \\[\\begin{equation}\\label{ti3}\n\\ip{u,v}=\\norm{u}\\norm{v}.\n\\end{equation}\\] If one of \\(u,v\\) is a nonnegative multiple of the other, then \\(\\eqref{ti3}\\) holds. Conversely, suppose \\(\\eqref{ti3}\\) holds. The the condition for equality in the Cauchy-Schwarz inequality implies that one of \\(u,v\\) must be a scalar multiple of the other. Clearly, then \\(\\eqref{ti3}\\) forces the scalar in question to be nonnegative, as desired.\n\n::: {#thm- } [Parallelogram Equality] If \\(u, v \\in V\\), then \\[\n\\norm{u+v}^2+\\norm{u-v}^2= 2 \\left( \\norm{u}^2+\\norm{v}^2 \\right).\n\\] :::\n\nProof. If \\(u, v \\in V\\), then\n\\[\\begin{align*}\n\\norm{u+v}^2+\\norm{u-v}^2\n&= \\ip{u+v,u+v}+\\ip{u-v,u-v} \\\\\n& = \\norm{u}^2+\\norm{v}^2+\\ip{u,v}+\\ip{v,u}+\\norm{u}^2+\\norm{v}^2-\\ip{u,v}-\\ip{v,u} \\\\\n& =2 \\left( \\norm{u}^2+\\norm{v}^2 \\right )\n\\end{align*}\\] as desired.\n\nA list of vectors is called orthonormal if the vectors in it are pairwise orthogonal and each vector has norm 1.\n\nTheorem 20.2 If \\((e_1,\\ldots,e_m)\\) is an orthonormal list of vectors in \\(V\\), then \\[\n\\norm{a_1 e_1+\\cdots +a_m e_m}^2=|a_1|^2+\\cdots + |a_n|^2\n\\] for all \\(a_1,\\ldots,a_m\\in\\mathbb{F}\\).\n\n\nProof. Because each \\(e_j\\) has norm 1, this follows easily from repeated by application of the Pythagorean theorem.\n\n::: {#thm- } Every orthonormal list of vectors is linearly independent. :::\n\nProof. Suppose \\((e_1,\\ldots,e_n)\\) is an orthonormal list of vectors in \\(V\\) and \\(a_1,\\ldots,a_n\\in \\mathbb{F}\\) are such that \\(a_1 e_1+\\cdots + a_n e_n=0\\). Then \\(|a_1|^2+\\cdots + |a_n|^2=0\\), which means that all the \\(a_j\\)’s are 0, as desired.\n\nAn orthonormal basis of \\(V\\) is an orthonormal list of vectors in \\(V\\) that is also a basis of \\(V\\).\nThe importance of orthonormal bases stems mainly from the following proposition.\n::: {#thm- } Suppose \\((e_1,\\ldots,e_n)\\) is an orthonormal basis of \\(V\\). Then \\[\\begin{equation} \\label{inim1}\nv=\\ip{ v,e_1 } e_1+\\cdots + \\ip{ v,e_n } e_n\n\\end{equation}\\] and \\[\\begin{equation}\\label{inim2}\n\\norm{v}=|\\ip{ v,e_1 } |^2+\\cdots + |\\ip{ v,e_n } |^2\n\\end{equation}\\] for every \\(v\\in V\\). :::\n\nProof. Let \\(v\\in V\\). Because \\((e_1,\\ldots,e_n)\\) is a basis of \\(V\\), there exist scalars \\(a_1,\\ldots,a_n\\) such that \\(v=a_1 e_1+\\cdots + a_n e_n\\). Take the inner product of both sides of this equation with \\(e_j\\), getting \\(\\ip{v,e_j}=a_j\\). Thus \\(\\eqref{inim1}\\) holds. Clearly \\(\\eqref{inim2}\\) holds by \\(\\eqref{inim1}\\) and \\(\\eqref{innorm}\\).\n\n::: {#thm- } [Gram-Schmidt] If \\((v_1,\\ldots,v_m)\\) is a linearly independent list of vectors in \\(V\\), then there exists an orthonormal list \\((e_1,\\ldots,e_m)\\) of vectors in \\(V\\) such that \\[\\begin{equation} \\label{gmeq}\n\\text{span}(v_1,\\ldots,v_j)=\\text{span}(e_1,\\ldots,e_j)\n\\end{equation}\\] for \\(j=1,\\ldots,m\\) :::\n\nProof. Suppose \\((v_1,\\ldots,v_m)\\) is a linearly independent list of vectors in \\(V\\). To construct the \\(e\\)’s, start by setting \\(e_1=\\frac{v_1}{\\norm{v_1}}\\). This satisfies \\(\\eqref{gmeq}\\) for \\(j=1\\). We will choose \\(e_2,\\ldots e_m\\) inductively, as follows. Suppose \\(j>1\\) and an orthonormal list \\((e_1,\\ldots,e_{j-1})\\) has been chosen so that \\[\\begin{equation}\\label{gspan}\n\\text{span}(v_1,\\ldots,v_{j-1})=\\text{span}(e_1,\\ldots,e_{j-1}).\n\\end{equation}\\] Let \\[\\begin{equation}\\label{gsproj}\ne_j=\\frac{v_j-\\ip{v_j,e_1}e_1-\\cdots - \\ip{v_j,e_{j-1}} e_{j-1} }{\\norm{v_j-\\ip{v_j,e_1}e_1-\\cdots - \\ip{v_j,e_{j-1}} e_{j-1}}}.\n\\end{equation}\\] Note that \\(v_j \\not\\in \\text{span}(v_1,\\ldots,v_{j-1})\\) (because \\((v_1,\\ldots,v_m)\\) is linearly independent) and thus \\(v_j\\not \\in \\text{span}(e_1,\\ldots,e_{j-1})\\). Hence we are not dividing by 0 in the equation above, and so \\(e_j\\) is well-defined. Dividing a vector by its norm produces a new vector with norm 1; thus \\(\\norm{e_j}=1\\).\nLet \\(1\\leq k <j\\). Then \\[\\begin{align*}\n\\ip{e_j,e_k}\n&=\\ip{\\frac{v_j-\\ip{v_j,e_1}e_1-\\cdots - \\ip{v_j,e_{j-1}} e_{j-1} }{\\norm{v_j-\\ip{v_j,e_1}e_1-\\cdots - \\ip{v_j,e_{j-1}} e_{j-1}}},e_k} \\\\\n&= \\frac{\\ip{v_j,e_k}-\\ip{v_j,e_k}\\ip{e_k,e_k}}{\\norm{v_j-\\ip{v_j,e_1}e_1-\\cdots - \\ip{v_j,e_{j-1}} e_{j-1}}} \\\\\n&=0.\n\\end{align*}\\] Thus \\((e_1,\\ldots,e_j)\\) is an orthonormal list.\nFrom \\(\\eqref{gsproj}\\), we see that \\(v_j\\in \\text{span}(e_1,\\ldots,e_j)\\). Combining this information with \\(\\eqref{gspan}\\) shows that \\[\n\\text{span}(v_1,\\ldots,v_{j-1})\\subset \\text{span}(e_1,\\ldots,e_{j}).\n\\] Both lists above are linearly independent (the \\(v\\)’s by hypothesis, the \\(e\\)’s by orthonormality and \\(\\eqref{oind}\\). Thus both subspaces above have dimension \\(j\\), and hence must be equal, completing the proof.\n\n\nTheorem 20.3 Every finite-dimensional inner-product space has an orthonormal basis.\n\n\nProof. Choose a basis of \\(V\\). Apply the Gram-Schmidt procedure to it, producing an orthonormal list. This list is linearly independent and it spans \\(V\\). Thus it is an orthonormal basis.\n\n\nTheorem 20.4 Every orthonormal list of vectors in \\(V\\) can be extended to an orthonormal basis of \\(V\\).\n\n\nProof. Suppose \\((e_1, \\ldots ,e_m)\\) is an orthonormal list of vectors in \\(V\\). Then \\((e_1, \\ldots , e_m)\\) is linearly independent and so can be extended to a basis \\[\n\\mathcal{B}=(e_1, \\ldots, e_m, v_1, \\ldots, v_n)\n\\] of \\(V\\). Now apply the Gram-Schmidt procedure to \\(\\mathcal{B}\\) producing an orthonormal list \\((e_1,\\ldots,e_m,f_1,\\ldots,f_n)\\); here the Gram-Schmidt procedure leaves the first \\(m\\) vectors unchanged because they are already orthonormal. Clearly \\(\\mathcal{B}\\) is an orthonormal basis of \\(V\\) because it is linearly independent and its span equals \\(V\\). Hence we have our extension of \\((e_1,\\ldots,e_m)\\) to an orthonormal basis of \\(V\\).\n\nRecall that if \\(V\\) is a complex vector space, then for each operator on \\(V\\) there is a basis with respect to which the matrix of the operator is upper-triangular. Now for inner-product spaces we would like to know the same question.\n::: {#thm- } Suppose \\(T\\in\\mathcal{L}(V)\\). If \\(T\\) has an upper-triangular matrix with respect to some basis of \\(V\\), then \\(T\\) has an upper-triangular matrix with respect to some orthonormal basis of \\(V\\). :::\n\nProof. Suppose \\(T\\) has upper-triangular matrix with respect to some basis \\((v_1,\\ldots,v_n)\\) of \\(V\\). Thus \\(\\text{span}(v_1,\\ldots,v_j)\\) is invariant under \\(T\\) for each \\(j=1,\\ldots,n\\). Apply the Gram-Schmidt procedure to \\((v_1,\\ldots,v_n)\\), producing an orthonormal basis \\((e_1,\\ldots,e_n)\\) of \\(V\\). Because \\[\n\\text{span}(e_1,\\ldots,e_j)=\\text{span}(v_1,\\ldots,v_j)\n\\] for each \\(j\\), we conclude that \\(\\text{span}(e_1,\\ldots,e_j)\\) is invariant under \\(T\\) for each \\(j=1,\\ldots,n\\). Thus, by \\(\\eqref{utm}\\), \\(T\\) has an upper-triangular matrix with respect to the orthonormal basis \\((e_1,\\ldots,e_n)\\).\n\n\nTheorem 20.5 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Then \\(T\\) has an upper-triangular matrix with respect to some orthonormal basis of \\(V\\).\n\n\nProof. This follows immediately from \\(\\eqref{cutm}\\) and \\(\\eqref{outm}\\).\n\nIf \\(U\\) is a subset of an inner-product space \\(V\\), then the orthogonal complement of \\(U\\) is defined as \\(U^\\bot=\\{v\\in V\\, : \\, \\ip{v,u} =0 \\text{ for all } u\\in U\\}.\\)\n::: {#thm- } [Orthogonal Decomposition] If \\(U\\) is a subspace of an inner-product space \\(V\\), then \\(v=U\\oplus U^\\perp\\). :::\n\nProof. Suppose that \\(U\\) is a subspace of \\(V\\). First we will show that \\[\\begin{equation}\\label{sumfirst}\nV=U + U^\\perp.\n\\end{equation}\\] To do this, suppose \\(v\\in V\\). Let \\((e_1,\\ldots,e_m)\\) be an orthonormal basis of \\(U\\). Obviously, \\[\nv=\\underbrace{ \\ip{v,e_1}e_1+\\cdots +\\ip{v,e_m}e_m}_u+\\underbrace{v-\\ip{v,e_1}e_1-\\cdots -\\ip{v,e_m}e_m}_w.\n\\] Clearly, \\(u\\in U\\). Because \\((e_1,\\ldots,e_m)\\) is an orthonormal list, for each \\(j\\) we have \\[\n\\ip{w,e_j}=\\ip{v,e_j}-\\ip{v,e_j}=0.\n\\] Thus \\(w\\) is orthogonal to every vector in \\(\\text{span}(e_1,\\ldots,e_m)\\). In other words, \\(w\\in U^\\perp\\), completing the proof of \\(\\eqref{sumfirst}\\).\nIf \\(v\\in U\\cap U^\\perp\\), then \\(v\\) (which is in \\(U\\)) is orthogonal to every vector in \\(U\\) (including \\(v\\) itself), which implies that \\(\\ip{v,v}=0\\), which implies that \\(v=0\\). Thus \\[\\begin{equation}\\label{orthogonalss}\nU\\cap U^\\perp=\\{0\\}.\n\\end{equation}\\] Now \\(\\eqref{sumfirst}\\) and \\(\\eqref{orthogonalss}\\) imply that \\(U\\oplus U^\\perp\\).\n\n\nTheorem 20.6 If \\(U\\) is a subspace of an inner-product space \\(V\\), then \\(U=(U^\\perp)^\\perp\\).\n\n\nProof. Suppose that \\(U\\) is a subspace of \\(V\\). First we will show that \\[\\begin{equation}\\label{subsetorth}\nU\\subseteq (U^\\perp)^\\perp.\n\\end{equation}\\] To do this, suppose that \\(u\\in U\\). Then \\(\\ip{u,v}=0\\) for every \\(v\\in U^\\perp\\) (by definition of \\(U^\\perp\\)). Because \\(u\\) is orthogonal to every vector in \\(U^\\perp\\), we have \\(u\\in (U^\\perp)^\\perp\\), completing the proof of \\(\\eqref{subsetorth}\\).\nTo prove the inclusion in the other direction, suppose \\(v\\in (U^\\perp)^\\perp\\). By \\(\\eqref{ip-orthogonal-decomposition}\\), we can write \\(v=u+w\\), where \\(u\\in U\\) and \\(w\\in U^\\perp\\). We have \\(v-u=w\\in U^\\perp\\). Because \\(v\\in (U^\\perp)^\\perp\\) and \\(u\\in(U^\\perp)^\\perp\\) (from \\(\\eqref{subsetorth}\\)), we have \\(v-u\\in (U^\\perp)^\\perp\\). Thus \\(v-u\\in U^\\perp\\cap (U^\\perp)^\\perp\\), which implies that \\(v=u\\), which implies that \\(v\\in U\\). Thus \\((U^\\perp)^\\perp \\subseteq U\\), which along with \\(\\eqref{subsetorth}\\) completes the proof.\n\nLet \\(V=U \\oplus U ^\\bot\\) and for \\(v\\in V\\) let \\(v=u+w\\) where \\(w\\in U ^\\bot\\). Then \\(u\\) is called the orthogonal projection of \\(V\\) onto \\(U\\) and is denoted by \\(P_U v\\).\n\nTheorem 20.7 If \\(U\\) is a subspace of an inner-product space \\(V\\) and \\(v\\in V\\). Then \\(\\norm{v-P_U v}\\leq \\norm{v-u}\\) for every \\(u\\in U\\). Furthermore, if \\(u\\in U\\) and the inequality above is an equality; then \\(u=P_U v\\).\n\n\nProof. Suppose \\(u\\in U\\). Then \\[\\begin{align}\n\\norm{v-P_U v}^2\n& \\leq \\norm{v-P_Uv}+ \\norm{P_U v-u}^2 \\label{mp1} \\\\\n& = \\norm{v-P_U v+P_Uv-u}^2 = \\norm{v-u}^2   \\label{mp2}\n\\end{align}\\] where \\(\\eqref{mp2}\\) comes from the Pythagorean theorem, which applies because \\(v-P_U v\\in U^\\perp\\) and \\(P_U v-u\\in U\\). Taking the square root gives the desired inequality. The inequality is an equality if and only if \\(\\eqref{mp1}\\) is an equality, which happens if and only if \\(\\norm{P_U v-u}=0\\), which happens if and only if \\(u=P_u v\\).\n\n\nExample 20.1 Show that if \\(c_1,\\ldots,c_n\\) are positive numbers, then \\[\n\\ip{ (w_1,\\ldots,w_n),(z_1,\\ldots,z_n) } = c_1 w_1 \\overline{z_1}+\\cdots + c_n w_n \\overline{z_n}\n\\] defines an inner product on \\(\\mathbb{F}^n\\).\n\n\nExample 20.2 Show that if \\(p,q\\in \\mathcal{P}_m(\\mathbb{F})\\), then \\[\n\\ip{ p ,q } = \\int_0^1 p(x)\\overline{q(x)}dx\n\\] is an inner product on the vector space \\(\\mathcal{P}_m(\\mathbb{F})\\).\n\n\nExample 20.3 Show that every inner product is a linear map in the first slot, as well as a linear map in the second slot.\n\n\nExample 20.4 If \\(v\\in V\\) and \\(a\\in \\mathbb{F}\\), then \\(\\norm{av}=|a| \\norm{v}\\), and if \\(v\\) is nonzero then \\(u=\\frac{1}{\\norm{v}} v\\) is a unit vector. Since \\(\\norm{a v}^2=\\ip{av,av} =a \\overline{a} \\ip{v,v} =|a|^2 \\norm{v}^2\\), taking square roots provides \\(\\norm{a v}=|a| \\norm{v}\\).\n\n\nExample 20.5 Prove that if \\(x, y\\) are nonzero vectors in \\(\\mathbb{R}^2\\), then \\[\n\\ip{x,y} =\\norm{x}\\norm{y} \\cos \\theta,\n\\] where \\(\\theta\\) is the angle between \\(x\\) and \\(y\\). The law of cosines gives \\[\n\\norm{x-y}^2=\\norm{x}^2+\\norm{y}^2-2\\norm{x}\\norm{y} \\cos \\theta.\n\\] The left hand side of this equation is \\[\n\\norm{x-y}^2=(x-y)\\cdot (x-y)=\\norm{x}^2-2 (x \\cdot y) +\\norm{y}^2\n\\] so \\[\nx\\cdot y= \\norm{x}\\norm{y}\\cos \\theta.\n\\]\n\n\nExample 20.6 Suppose \\(u,v \\in V\\). Prove that \\(\\ip{ u,v } =0\\) if and only if \\(||u||\\leq ||u+a v||\\) for all \\(a\\in F\\). If \\(\\ip{u,v}=0\\), then by the Pythagorean theorem \\[\n\\norm{u+\\alpha v}^2=\\norm{u}^2+\\norm{\\alpha v}^2\\geq \\norm{u}.\n\\] Conversely, we will prove the contrapositive, that is we will prove: if \\(\\ip{u,v}\\neq0\\) then there exists \\(a \\in \\mathbb{F}\\) such that \\(\\norm{u}>\\norm{u+av}\\). Suppose \\(\\ip{u,v}\\neq 0\\) then \\(u\\) and \\(v\\) are both nonzero vectors. By the orthogonal decomposition, we can write \\[\\begin{equation} \\label{ex3}\nu=\\alpha v+w\n\\end{equation}\\] for some \\(\\alpha \\in \\mathbb{F}\\) and where \\(\\ip{w,v}=0\\). Notice \\(\\alpha \\neq 0\\) since \\(\\ip{u,v}\\neq 0\\). Since \\(v\\) and \\(w\\) are orthogonal \\[\n\\norm{u}^2=|\\alpha|^2\\norm{v}^2+\\norm{w}^2\n\\] Let \\(a=-\\alpha\\). Then by equation \\(\\eqref{ex3}\\) \\[\n\\norm{u+a v}^2=\\norm{w}^2\n\\] and so \\[\n\\norm{u}^2=|a|^2\\norm{v}^2 +\\norm{u+a v}^2 >  \\norm{u+a v}^2\n\\] which implies \\[\n\\norm{u}>\\norm{u+a v}\n\\] as desired.\n\n\nExample 20.7 Prove that \\[\n\\left (\\sum_{k=1}^{n} a_k b_k \\right )^2\n\\leq \\left ( \\sum_{k=1}^n k a_k^2 \\right ) \\left ( \\sum_{k=1}^n \\frac{b_k^2}{k}\\right )\n\\] for all real numbers \\(a_1,\\ldots,a_n\\) and \\(b_1,\\ldots,b_n\\). This is a simple trick. \\[\n\\left (\\sum_{k=1}^{n} a_k b_k \\right )^2\n= \\left ( \\sum_{k=1}^n \\sqrt{k} a_k \\frac{b_k}{\\sqrt{k}} \\right )^2\n\\leq \\left ( \\sum_{k=1}^n k a_k^2 \\right ) \\left ( \\sum_{k=1}^n \\frac{b_k^2}{k}\\right )\n\\] where the last inequality is from the Cauchy-Schwarz inequality.\n\n\nExample 20.8 Suppose \\(u, v\\in V\\) are such that \\(||u||=3\\), \\(||u+v||=4\\), and \\(||u-v||=6\\). What number must \\(||v||\\) equal? Using the parallelogram equality \\[\n\\norm{u+v}^2+\\norm{u-v}^2=2 \\left(\\norm{u}^2+\\norm{v}^2\\right)\n\\] to get \\[\n16+36=2(9+\\norm{v}^2), \\qquad \\norm{v}=\\sqrt{17}.\n\\]\n\n\nExample 20.9 Prove or disprove: there is an inner product on \\(\\mathbb{R}^2\\) such that the associated norm is given by \\(||(x_1,x_2)||=|x_1|+|x_2|\\) for all \\((x_1,x_2)\\in \\mathbb{R}^2\\). There is no such inner product. Take for instance, \\[\nu=(1/4,0), \\qquad v=(0,3/4), \\qquad u+v=(1/4,3/4).\n\\] Then we have equality in the triangular inequality \\[\n1=\\norm{u+v}\\leq \\norm{u}+\\norm{v}=1/4+3/4.\n\\] By the triangular inequality, we must have \\(u=a v\\) or \\(v=a v\\), with \\(a\\geq 0\\). But clearly no such \\(a\\in \\mathbb{F}\\) exists.\n\n\nExample 20.10 Prove that if \\(V\\) is a real inner-product space, then \\[\n\\ip{ u,v } =\\frac{||u+v||^2-||u-v||^2}{4}\n\\] for all \\(u,v\\in V\\). Expressing the norms as inner products \\[\\begin{align*}\n%\\ip{ u,v }\n\\frac{\\norm{u+v}^2- \\norm{u-v}^2}{4}\n&=\\frac{\\ip{u+v,u+v}-\\ip{u-v,u-v} }{4} \\\\\n&=\\frac{\\ip{u,u}+\\ip{v,v}+\\ip{u,v}+\\ip{v,u} - \\ip{u,u}-\\ip{v,v}+\\ip{u,v}+\\ip{v,u}}{4} \\\\\n&=\\frac{2\\ip{u,v}+2\\ip{v,u} }{4} \\\\\n&=\\frac{2\\ip{u,v}+2\\ip{u,v} }{4} \\quad \\text{(because $V$ is a real inner product space)} \\\\\n&=\\ip{u,v}\n\\end{align*}\\] as desired.\n\n\nExample 20.11 Prove that if \\(V\\) is a complex inner-product space, then \\[\n\\ip{ u,v }\n\\] is \\[\n\\frac{||u+v||^2-||u-v||^2 + ||u+i v||^2 i - ||u-i v||^2 i}{4}\n\\] for all \\(u,v\\in V\\).\n\n\nExample 20.12 A norm on a vector space \\(U\\) is a function \\(||\\text{  }|| : U\\rightarrow [0,\\infty)\\) such that \\(||u||=0\\) if and only if \\(u=0\\), \\(||\\alpha u ||= |\\alpha | ||u||\\) for all \\(\\alpha \\in F\\) and all \\(u \\in U\\), and \\(||u+v|| \\leq ||u||+||v||\\) for all \\(u,v \\in U\\). Prove that a norm satisfying the parallelogram equality comes from an inner product (in other words, show that if \\(|| \\text{  }||\\) is a norm on \\(U\\) satisfying the parallelogram equality, then there is an inner product \\(\\ip{ \\text{ } , \\text{ } }\\) on \\(U\\) such that \\(||u||=\\ip{ u, u } ^{1/2}\\) for all \\(u \\in U\\)).\n\n\nExample 20.13 Suppose \\(n\\) is a positive integer. Prove that \\[\n\\left (\n\\frac{1}{\\sqrt{2\\pi}},\\frac{\\sin x}{\\sqrt{\\pi}},\\frac{\\sin 2x}{\\sqrt{\\pi}},\\ldots,\\frac{\\sin n x}{\\sqrt{\\pi}}, \\frac{\\cos x}{\\sqrt{\\pi}},\\frac{\\cos 2x}{\\sqrt{\\pi}},\\ldots,\\frac{\\cos n x}{\\sqrt{\\pi}}\n\\right )\n\\] is an orthonormal list of vectors in \\(\\mathcal{C}[-\\pi,\\pi]\\), the vector space of continuous real-valued functions on \\([-\\pi,\\pi]\\) with inner product \\[\n\\ip{ f,g } = \\int_{-\\pi}^{\\pi} f(x)g(x) \\, d x .\n\\] Computation of these integrals is based on the product-to-sum formulas from trigonometry: \\[\\begin{align*}\n\\sin(A)\\sin(B)&=\\frac{1}{2}\\cos(A-B)-\\frac{1}{2}\\cos(A+B) \\\\\n\\cos(A)\\cos(B)&=\\frac{1}{2}\\cos(A-B)+\\frac{1}{2}\\cos(A+B) \\\\\n\\sin(A)\\cos(B)&=\\frac{1}{2}\\sin(A-B)+\\frac{1}{2}\\sin(A+B).\n\\end{align*}\\] Here is a sample computation, valid for \\(m,n=1,2,3,,\\ldots\\) when \\(m\\neq n\\). \\[\\begin{align*}\n\\ip{\\sin(mx),\\cos(nx)}\n&= \\int_{-\\pi}^{\\pi}\\sin(mx)\\cos(nx) \\, dx \\\\\n&= \\int_{-\\pi}^{\\pi}\\left(\\frac{1}{2}\\sin((m-n)x)+\\frac{1}{2}\\sin((m+n)x)\\right)\\, dx \\\\\n&= \\left. \\frac{-1}{2(m-n)}\\cos\\left((m-n)x\\right)+\\frac{-1}{2(m+n)}\\cos\\left((m+n)x\\right) \\right|^{\\pi}_{-\\pi}, \\\\\n&= \\frac{-1}{2(m-n)}[-1-(-1)]+\\frac{-1}{2(m-n)}[-1-(-1)] \\\\\n&=0.\n\\end{align*}\\]\n\n\nExample 20.14 On \\(\\mathcal{P}_2(\\mathbb{R})\\), consider the inner product given by \\[\n\\ip{ p,q } = \\int_o^1 p(x) q(x) \\, d x.\n\\] Apply the Gram-Schmidt procedure to the basis \\((1,x,x^2)\\) to produce an orthonormal basis of \\(\\mathcal{P}_2(\\mathbb{R})\\). Computing \\(e_1\\): A calculation gives \\[\n\\ip{1,1}=\\int_0^1 1 \\, dx=1,\n\\] so \\(e_1=1\\). Computing \\(e_2\\): A calculation gives \\[\n\\ip{x,1}=\\int_0^1 x \\, dx =\\frac{1}{2}.\n\\] Let \\[\nf_2=x-1/2.\n\\] Then \\[\n\\norm{f_2}^2=\\ip{x-1/2,x-1/2}=\\int_0^1\\left(x^2-x+\\frac{1}{4}\\right)\\, dx =\\frac{1}{12},\n\\] so \\[\ne_2=\\frac{f_2}{\\norm{f_2}}=2\\sqrt{3}\\left(x-\\frac{1}{2}\\right).\n\\] Computing \\(e_3\\): A calculation gives \\[\n\\ip{x^2,1}=\\int_0^1 x^2 \\, dx =\\frac{1}{3},\n\\] and \\[\n\\ip{x^2,2\\sqrt{3}\\left(x-\\frac{1}{2}\\right)}=2\\sqrt{3}\\int_0^1\\left(x^3-\\frac{x^2}{2}\\right) \\, dx=\\frac{1}{2\\sqrt{3}}.\n\\] Let \\[\nf_3=x^2-\\frac{1}{2\\sqrt{3}}\\left(x-\\frac{1}{2}\\right)-\\frac{1}{3}=x^2-x+\\frac{1}{6}.\n\\] Then \\[\n\\norm{f_3}^2=\\int_0^1\\left(x^2-x+\\frac{1}{6}\\right)^2 \\, dx\n\\] and \\[\ne_3=\\frac{f_3}{\\norm{f_3}}.\n\\]\n\n\nExample 20.15 What happens if the Gram-Schmidt procedure is applied to a list of vectors that is not linearly independent? By examining the proof, notice that the numerator in the Gram-Schmidt formula is the difference between \\(v_j\\) and the orthogonal projection \\(P_u\\) of \\(v_j\\) onto the subspace \\[\nU=\\text{span}(v_1,\\ldots,v_{j-1})=\\text{span}(e_1,\\ldots,e_{j-1}).\n\\] If \\(v_j\\in U\\), then \\(v_j-P_U v_j=0\\), so the numerator has norm 0 and division by the denominator is not defined. The algorithm can be adapted to handle this case by testing for 0 in the denominator. If 0 is found, throw \\(v_j\\) out of the list and continue. The result will be an orthonormal basis for \\(\\text{span}(v_1,\\ldots,v_N)\\).\n\n\nExample 20.16 Suppose \\(V\\) is a real inner-product space and \\((v_1,\\ldots,v_m)\\) is a linearly independent list of vectors in \\(V\\). Prove that there exist exactly \\(2^m\\) orthonormal lists \\((e_1,\\ldots,e_m)\\) of vectors in \\(V\\) such that span \\((v_1,\\ldots,v_j)=\\) span \\((e_1,\\ldots,e_j)\\) for all \\(j\\in \\{1,\\ldots,m\\}\\).\n\n\nExample 20.17 Suppose \\((e_1,\\ldots,e_M)\\) is an orthonormal list of vectors in \\(V\\). Let \\(v \\in V\\). Prove that \\[\n||v||^2=\\sum_{n=1}^M \\left|\\ip{v,e_n}\\right|^2\n\\] if and only if $v $ span \\((e_1,\\ldots,e_M)\\). Extend \\((e_1,\\ldots,e_M)\\) to an orthonormal basis for \\(V\\). Then \\[\n\\sum_{n=1}^N\\ip{v,e_n} e_n\n\\] and \\[\n\\norm{v}^2=\\sum_{n=1}^N \\left|\\ip{v,e_n}\\right|^2.\n\\] If \\(v\\in \\text{span}(e_1,\\ldots,e_M)\\) then \\(\\ip{v,e_n}=0\\) for \\(n>M\\), and \\[\n\\norm{v}^2=\\sum_{m=1}^M \\left|\\ip{v,e_n}\\right|^2.\n\\] If \\(v \\not\\in\\text{span}(e_1,\\ldots,e_M)\\) then for some \\(n>M\\) we have \\(\\ip{v,e_n}\\neq 0\\). This gives \\[\n\\norm{v}^2=\\sum_{n=1}^N\\left|\\ip{v,e_n}\\right|^2>\\sum_{m=1}^M\\left|v,e_n\\right|^2.\n\\]\n\n\nExample 20.18 Find an orthonormal basis of \\(\\mathcal{P}_2(\\mathbb{R})\\), such that the differentiation operator on \\(\\mathcal{P}_2(\\mathbb{R})\\) has an upper-triangular matrix with respect to this basis.\n\n\nExample 20.19 Suppose \\(U\\) is a subspace of \\(V\\). Prove that \\(\\text{dim} U^{\\bot} = \\text{dim} V - \\text{dim} U\\).\n\n\nExample 20.20 Suppose \\(U\\) is a subspace of \\(V\\). Prove that \\(U^{\\bot} = \\{0\\}\\) if and only if \\(U=V\\). If \\(U=V\\) and \\(v\\in U^\\perp\\) then \\(v\\in U\\cap U^\\perp\\), and \\(\\ip{v,v}=0\\), so \\(v=0\\). Therefore, \\(V=U\\oplus U^\\perp\\). If \\(U^\\perp=\\{0\\}\\), then \\(U=V\\).\n\n\nExample 20.21 Prove that if \\(P\\in \\mathcal{L}(V)\\) is such that \\(P^2=P\\) and every vector in null \\(P\\) is orthogonal to every vector in range \\(P\\), then \\(P\\) is an orthogonal projection.\n\n\nExample 20.22 Prove that if \\(P\\in \\mathcal{L}(V)\\) is such that \\(P^2=P\\) and \\(|| P v ||\\leq ||v||\\) for every \\(v\\in V\\), then \\(P\\) is an orthogonal projection.\n\n\nExample 20.23 Suppose \\(T\\in \\mathcal{L}(V)\\) and \\(U\\) is a subspace of \\(V\\). Prove that \\(U\\) is invariant under \\(T\\) if and only if \\(P_U T= T P_U\\).\n\n\nExample 20.24 Suppose \\(T\\in \\mathcal{L}(V)\\) and \\(U\\) is a subspace of \\(V\\). Prove that \\(U\\) and \\(U^\\bot\\) are both invariant under \\(T\\) if and only if \\(P_U T= T P_U\\).\n\n\nExample 20.25 In \\(\\mathbb{R}^4\\), let \\(U=\\text{span} \\left ( (1,1,0,0),(1,1,1,2) \\right )\\). Find \\(u\\in U\\) such that \\(||u-(1,2,3,4)||\\) is as small as possible. in \\(\\mathbb{R}^4\\) let \\(U=\\text{span}((1,1,0,0),(1,1,1,2)).\\) Find \\(u\\in U\\) such that \\(\\norm{u-(1,2,3,4)}\\) is as small as possible. We want the orthogonal projection \\(P_U(1,2,3,4)\\). Notice that \\(U=\\text{span}((1,1,0,0),(0,0,1,2)).\\) An orthonormal basis for \\(U\\) is \\[\n\\left( \\frac{1}{\\sqrt{2}},\\frac{1}{\\sqrt{2}} ,0,0\\right ),\\left(0,0,\\frac{1}{\\sqrt{5}},\\frac{2}{\\sqrt{5}} \\right)\n\\] Thus the desired vector is \\[\nP_U(1,2,3,4)=\\left(\\frac{3}{2},\\frac{3}{2},0,0\\right)+\\left(0,0,\\frac{11}{5},\\frac{2}{5}\\right).\n\\]\n\n\nExample 20.26 Find a polynomial \\(p\\in \\mathcal{P}_3(\\mathbb{R})\\) such that \\(p(0)=0\\), \\(p'(0)=0\\), and \\[\n\\int_0^1 |2+3x-p(x) |^2 \\, dx\n\\] is as small as possible.\n\n\nExample 20.27 Find a polynomial \\(p\\in \\mathcal{P}_5(\\mathbb{R})\\) that makes \\[\n\\int_{-\\pi}^{\\pi} |\\sin x - p(x) |^2 \\, dx\n\\] is as small as possible.\n\n\nExample 20.28 Find a polynomial \\(p\\in \\mathcal{P}_2(\\mathbb{R})\\) such that \\[\n\\phi(p)=p \\left( \\frac{1}{2} \\right) = \\int_{0}^{1} p(x) \\, q(x) \\, dx\n\\] for every \\(p\\in \\mathcal{P}_2(\\mathbb{R})\\). Here is the direct approach. Every \\(q\\in\\mathcal{P}_2(\\mathbb{R})\\) can be expressed as \\[\n\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2.\n\\] The desired polynomial \\(q\\) must satisfy \\[\np(x)=1: \\qquad \\phi(1)=p(1/2)=1=\\int_0^1\\left( \\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2 \\right) \\, dx = \\alpha+\\gamma \\frac{1}{12}.\n\\] Moving to \\(p(x)=x-1/2\\) we find \\[\\begin{align*}\np(x)&=x-1/2: \\qquad \\phi(x-1/2)=p(1/2)=0 \\\\\n&=\\int_0^1(x-1/2)[\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2] \\, dx =\\beta \\frac{1}{12},\n\\end{align*}\\] so \\(\\beta=0\\). Finally \\[\np(x)=(x-1/2)^2: \\qquad \\phi((x-1/2)^2)=p(1/2)=0\n\\] \\[\n=\\int_0^1(x-1/2)^2[\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2] \\, dx =\\alpha \\frac{1}{12}+\\gamma\\frac{1}{80}.\n\\] Solving gives \\[\n\\alpha=\\frac{27}{12}, \\qquad  \\beta=0, \\qquad \\gamma=-15.\n\\] Thus \\[\nq(x)=\\frac{27}{12}-15(x-1/2)^2.\n\\]\n\n\nExample 20.29 Find a polynomial \\(q\\in \\mathcal{P}_2(\\mathbb{R})\\) such that \\[\n\\phi(p)=\\int_{0}^{1} p(x) \\, (\\cos \\pi x)  \\, dx= \\int_{0}^{1} p(x) \\, q(x) \\, dx\n\\] for every \\(p\\in \\mathcal{P}_2(\\mathbb{R})\\). Taking the same approach as in the previous example. We compute \\[\np(x)=1: \\qquad \\phi(1)=\\int_0^1\\cos(\\pi x) \\, dx=0\n\\] \\[\n=\\int_0^1[\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2]\\,dx=\\alpha+\\gamma\\frac{1}{12}.\n\\] Moving to \\(p(x)=x-1/2\\) we find \\[\np(x)=x-1/2: \\qquad \\phi(x-1/2)=\\int_0^1(x-1/2)\\cos(\\pi x)\\, dx=\\int_0^1 x \\cos (\\pi x) \\, dx\n\\] \\[\n=-\\frac{2}{\\pi^2}=\\int_0^1(x-1/2)[\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2]\\, dx=\\beta\\frac{1}{12},\n\\] so \\(\\beta=\\frac{-24}{\\pi^2}\\). Finally, since \\(\\cos(\\pi x)\\) is odd about \\(x=1/2\\), \\[\np(x)=(x-1/2)^2: \\qquad \\phi((x-1/2)^2)=\\int_0^1(x-1/2)^2\\cos(\\pi x)\\, dx =0\n\\] \\[\n=\\int_0^1(x-1/2)^2[\\alpha+\\beta(x-1/2)+\\gamma(x-1/2)^2]=\\alpha \\frac{1}{12} +\\gamma\\frac{1}{80}.\n\\] Solving gives \\[\n\\alpha=\\gamma=0, \\qquad \\beta=\\frac{-24}{\\pi^2}.\n\\] Thus \\[\nq(x)=\\frac{-24}{\\pi^2}(x-1/2).\n\\]\n\n\nExample 20.30 Give an example of a real vector space \\(V\\) and \\(T\\in \\mathcal{L}(V)\\) such that trace\\((T^2) < 0\\).\n\n\nExample 20.31 Suppose \\(V\\) is a real vector space, \\(T\\in \\mathcal{L}(V)\\), and \\(V\\) has a basis consisting of eignvectors of \\(T\\). Prove that trace\\((T^2)\\geq 0\\).\n\n\nExample 20.32 Suppose \\(V\\) is an inner-product space and \\(v, w\\in V\\). Define \\(T\\in \\mathcal{L}(V)\\) by \\(T u=\\langle u,v \\rangle w\\). Find a formula for trace \\(T\\).\n\n\nExample 20.33 Prove that if \\(P\\in \\mathcal{L}(V)\\) satisfies \\(P^2=P\\), then trace \\(P\\) is a nonnegative integer.\n\n\nExample 20.34 Prove that if \\(V\\) is an inner-product space and \\(T\\in \\mathcal{L}(V)\\), then trace \\(T^*=\\overline{\\text{trace} T}.\\)\n\n\nExample 20.35 Suppose \\(V\\) is an inner-product space and \\(T\\in \\mathcal{L}(V)\\) is a positive operator with trace \\(T=0\\), then \\(T=0\\).\n\n\nExample 20.36 Suppose \\(T\\in \\mathcal{L}(\\mathbb{C}^3)\\) is the operator whose matrix is \\[\n\\begin{bmatrix}\n51 & -12 & -21 \\\\\n60 & -40 & -28 \\\\\n57 & -68 & 1\n\\end{bmatrix} .\n\\] If \\(-48\\) and \\(24\\) are eigenvalues of \\(T\\), find the third eigenvalue of \\(T\\).\n\n\nExample 20.37 Prove or give a counterexample: if \\(T\\in \\mathcal{L}(V)\\) and \\(c\\in F\\), then trace\\((c T ) = c\\) trace \\(T\\).\n\n\nExample 20.38 Prove or give a counterexample,: if \\(S, T\\in \\mathcal{L}(V)\\), then trace \\((S T)=(\\text{trace} S)(\\text{trace} T)\\).\n\n\nExample 20.39 Suppose \\(T\\in \\mathcal{L}(V)\\). Prove that if \\(\\text{trace} (ST)=0\\) for all \\(S\\in \\mathcal{L}(V)\\), then \\(T=0\\).\n\n\nExample 20.40 Suppose \\(V\\) is an inner-product space and \\(T\\in \\mathcal{L}(V)\\). Prove that if \\((e_1,\\ldots.,e_n)\\) is an orthonormal basis of \\(V\\), then \\[\n\\text{trace}(T^* T)=|| T e_1||^2+\\cdots + ||T e_n||^2.\n\\] Conclude that the right side of the equation above is independent of which orthonormal basis \\((e_1,\\ldots,e_n)\\) is chosen for \\(V\\).\n\n\nExample 20.41 Suppose \\(V\\) is a complex inner-product space and \\(T\\in \\mathcal{L}(V)\\). Let \\(\\lambda_1,\\ldots.,\\lambda_n\\) be the eigenvalues of \\(T\\), repeated according to multiplicity.\n\n\nExample 20.42 Suppose \\[\n\\begin{bmatrix}\na_{1,1} & \\cdots & a_{1,n} \\\\\n\\vdots &   & \\vdots \\\\\na_{n,1} & \\cdots & a_{n,n} \\\\\n\\end{bmatrix}\n\\] is the matrix of \\(T\\) with respect to some orthonormal basis of \\(V\\). Prove that \\[\n|\\lambda_1|^2+\\cdots + |\\lambda_n|^2\\leq \\sum^n_{k=1} \\sum_{j=1}^n |a_{j,k}|^2.\n\\]\n\n\nExample 20.43 Suppose \\(V\\) is an inner-product space. Prove that \\(\\langle S, T\\rangle=\\text{trace}(S T^*)\\) defines an inner-product on \\(\\mathcal{L}(V)\\).\n\n\nExample 20.44 Suppose \\(V\\) is an inner-product space and \\(T\\in \\mathcal{L}(V)\\). Prove that if \\(||T^* v ||\\leq ||T v||\\) for every \\(v \\in V\\), then \\(T\\) is normal.\n\n\nExample 20.45 Prove or give a counterexample: if \\(T\\in \\mathcal{L}(V)\\) and \\(c\\in F\\), then \\(\\det(cT)=c^{\\text{dim} V} \\det T\\).\n\n\nExample 20.46 Prove or give a counterexample: if \\(T\\in \\mathcal{L}(V)\\), then \\(\\det(S+T)=\\det S+ \\det T\\).\n\n\nExample 20.47 Suppose \\(A\\) is a block upper-triangular matrix \\[\nA=\n\\begin{bmatrix}\nA_1 & & * \\\\\n& \\ddots & \\\\\n0 & & A_m\n\\end{bmatrix} ,\n\\] where each \\(A_j\\) along the diagonal is a square matrix. Prove that \\[\n\\det A =(\\det A_1) \\cdots (\\det A_m).\n\\]"
  },
  {
    "objectID": "determinants-and-trace.html#cramer",
    "href": "determinants-and-trace.html#cramer",
    "title": "21  Determinants and Trace",
    "section": "21.1 Cramer",
    "text": "21.1 Cramer\nConsider the system of equations \\(A \\vec x=\\vec b\\) where \\(A\\) is an \\(n\\times n\\) invertible matrix. Then the unique solution to the system is given by \\[\nx_1=\\frac{\\det(A_1)}{\\det(A)},\\qquad x_2=\\frac{\\det(A_2)}{\\det(A)},\\cdots, \\qquad x_n=\\frac{\\det(A_n)}{\\det(A)}\n\\] where the matrix \\(A_i\\) is the matrix \\(A\\) with the \\(i\\)-th column replaced by \\(\\vec b\\). :::\n\nProof. The proof is left for the reader.\n\n\nExample 21.18 Solve the system using Cramer’s Rule. \\[\n\\left\\{\n\\begin{matrix}\n2x_1 +3x_2-x_3 & =1 \\\\\n4x_1+x_2+2x_3 & = 5 \\\\\nx_1-x_2+x_3 & =2\n\\end{matrix}\n\\right.\n\\] First we compute the determinant of the coefficient matrix \\(A\\), and find \\(\\det (A)=5\\) Since \\(\\det(A)\\neq 0\\), we can apply Cramer’s rule, so we compute the required determinants and obtain, according to Cramer’s rule, the solution to the system is \\[\nx_1=\\frac{\\det(A_1)}{\\det(A)}=\\frac{7}{5},\\quad x_2=\\frac{\\det(A_2)}{\\det(A)}=-\\frac{3}{5}, \\quad x_3=\\frac{\\det(A_n)}{\\det(A)}=0.\n\\]\n\n\nExample 21.19 Use paper, pencil, and Cramer’s rule to solve the system \\(A\\vec x=\\vec b\\) where \\(A\\) and \\(\\vec b\\) are the following matrices \\[\nA=\n\\begin{bmatrix}\n1 & 2 & 3 & 4 \\\\\n-5 & -6 & -7 & -8 \\\\\n9 & -10 & 11 & -12 \\\\\n-13 & 14 & -15 & 16\n\\end{bmatrix}\n\\qquad \\text{and} \\qquad\n\\vec b=\n\\begin{bmatrix}\n1 \\\\ 2 \\\\ 3 \\\\ 4\n\\end{bmatrix} .\n\\] First we find \\(\\det(A)=-256\\) and then we find the solution vector \\[\n\\vec x= \\vectorfour{-51/8}{5}{41/8}{-9/2}.\n\\]"
  },
  {
    "objectID": "eigenvalues-and-eigenvectors.html",
    "href": "eigenvalues-and-eigenvectors.html",
    "title": "22  Eigenvalues and Eigenvectors",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nLet \\(\\mathbb{F}\\) be either the real numbers or the complex numbers. A nonzero vector \\(\\vec v\\) in \\(\\mathbb{F}^n\\) is called an of an \\(n\\times n\\) matrix \\(A\\) if \\(A \\vec v\\) is a scalar multiple of \\(\\vec v\\), that is \\(A \\vec v= \\lambda \\vec v\\) for some scalar \\(\\lambda\\). Note that this scalar \\(\\lambda\\) may be zero. The scalar \\(\\lambda\\) is called the eigenvalue associated with the eigenvector \\(\\vec v\\). Even though, \\(A\\vec 0=\\lambda \\vec 0\\) we do not call \\(\\vec 0\\) an eigenvector. Of course a matrix need not have any eigenvalues or eigenvectors, but notice if \\(\\vec v\\) is an eigenvector of matrix \\(A\\), then \\(\\vec v\\) is an eigenvector of matrices \\(A^2\\), \\(A^3\\), as well, with \\(A^t\\vec v=\\lambda^t \\vec v,\\) for all positive integers \\(t\\). If \\(\\mathbb{F}=\\mathbb{C}\\), then counting multiplicities, every \\(n\\times n\\) matrix has exactly \\(n\\) eigenvalues.\nIf \\(\\vec v\\) is an eigenvector of the \\(n\\times n\\) matrix \\(A\\) with associated eigenvalue \\(\\lambda\\), what can you say about \\(\\ker(A-\\lambda I_n)\\)? Is the matrix \\(A-\\lambda I_n\\) invertible? We know \\(A \\vec v=\\lambda \\vec v\\) so \\((A-\\lambda I_n)\\vec v=A \\vec v-\\lambda I_n\\vec v=\\lambda\\vec v-\\lambda \\vec v=0\\). Thus a nonzero vector \\(\\vec v\\) is in the kernel of \\((A-\\lambda I_n)\\). Therefore, \\(\\ker(A-\\lambda I_n)\\neq \\{\\vec 0\\}\\) and so \\(A-\\lambda I_n\\) is not invertible.\n\nLemma 22.1  Let \\(A\\) be an \\(n\\times n\\) matrix \\(A\\) and \\(\\lambda\\) a scalar. Then \\(\\lambda\\) is an eigenvalue of \\(A\\) if and only if \\(\\det(A-\\lambda I_n)=0\\).\n\n\nProof. The proof follows from the chain of equivalent statements:\n\n\\(\\lambda\\) is an eigenvalue of \\(A\\),\nthere exists a nonzero vector \\(\\vec v\\) such that \\((A -\\lambda I_n ) \\vec v=0,\\)\n\\(\\ker(A -\\lambda I_n )\\neq \\{\\vec 0\\}\\),\nmatrix \\(A-\\lambda I_n\\) fails to be invertible, and\n\\(\\det(A -\\lambda I_n )=0\\).\n\n\n\nExample 22.1 Find all eigenvectors and eigenvalues of the identity matrix \\(I_n\\). Since \\(I_n \\vec v = \\lambda \\vec v= 1 \\vec v\\) for all \\(\\vec v\\in \\mathbb{R}^n\\), all nonzero vectors in \\(\\mathbb{R}^n\\) are eigenvectors of \\(I_n\\), with eigenvalues \\(\\lambda=1\\).\n\n\nLemma 22.2  The eigenvalues of a triangular matrix are its diagonal entries.\n\n\nProof. Let \\(A\\) be a triangular matrix. Then \\(A-\\lambda I_n\\) is also a triangular matrix, and so \\(\\det(A-\\lambda I_n)\\) is the product of its diagonal entries. Let \\(a_{ii}\\) be any diagonal entry of \\(A\\). Then \\(a_{ii}-\\lambda\\) is the corresponding diagonal entry of \\(A-\\lambda I_n\\). Thus \\(\\lambda\\) is an eigenvalue of \\(A\\) if and only if \\(a_{ii}-\\lambda=0\\) by \\(\\ref{eigenprop}\\).\n\n\nExample 22.2 Find a basis of the linear space \\(V\\) of all \\(2\\times 2\\) matrices for which \\(\\vec e_1\\) is an eigenvector. For an arbitrary \\(2\\times 2\\) matrix we want \\[\n\\begin{bmatrix} a & b \\\\c & d\\end{bmatrix} \\vectortwo{1}{0}=\\vectortwo{a}{c}=\\vectortwo{\\lambda}{0}=\\lambda \\vectortwo{1}{0}\n\\] for any \\(\\lambda\\). Hence \\(a, b, d\\) are free and \\(c=0\\); thus a desired basis of \\(V\\) is \\[\n\\left(\n\\begin{bmatrix} 1 & 0 \\\\ 0 & 0\\end{bmatrix},\n\\begin{bmatrix} 0 & 1 \\\\ 0 & 0\\end{bmatrix},\n\\begin{bmatrix} 0 & 0 \\\\ 0 & 1\\end{bmatrix}\n\\right).\n\\]\n\n\nExample 22.3 Find a basis of the linear space \\(V\\) of all \\(4\\times 4\\) matrices for which \\(\\vec e_2\\) is an eigenvector. We want to find all \\(4 \\times 4\\) matrices \\(A\\) such that \\(A \\vec e_2=\\lambda e_2\\). Thus the second column of an arbitrary \\(4 \\times 4\\) matrix \\(A\\) must be of the form \\(\\vectorfour{0}{\\lambda}{0}{0}^T\\), so \\[\nA=\\begin{bmatrix} a & 0 & c & d \\\\ e & \\lambda & f & g \\\\ h & 0 & i & j \\\\ k & 0 & l & m\\end{bmatrix}.\n\\] Let \\(E_{ij}\\) denote the \\(4\\times 4\\) matrix with all entries zero except for a 1 in the \\(i\\)-th row and \\(j\\)-th column. Then a basis for \\(V\\) is \\[\n\\left(\nE_{11}, E_{21}, E_{31}, E_{41}, E_{22}, E_{13}, E_{23}, E_{33}, E_{34}, E_{41}, E_{42}, E_{43}, E_{44}\n\\right)\n\\] and so the dimension of \\(V\\) is 13.\n\n\nExample 22.4 Find the eigenvalues and find a basis for each eigenspace given \\(A=\\begin{bmatrix}1 & 0 & 0 \\\\ -5 & 0 & 2 \\\\ 0 & 0 & 1 \\end{bmatrix}\\). Find an eigenbasis for \\(A\\). The eigenvalues are \\(\\lambda_1=0\\) and \\(\\lambda_2=\\lambda_3=1\\). A basis for \\(E_{0}\\) is \\(\\left(\\vectorthree{0}{1}{0}\\right)\\). A basis for \\(E_{1}\\) is \\(\\left(\\vectorthree{1}{-5}{0},\\vectorthree{0}{2}{1}\\right)\\). An eigenbasis for \\(A\\) is \\(\\left(\\vectorthree{0}{1}{0},\\vectorthree{1}{-5}{0},\\vectorthree{0}{2}{1}\\right)\\).\n\n\nExample 22.5 Find a basis of the linear space \\(V\\) of all \\(2\\times 2\\) matrices \\(A\\) for which \\(\\vectortwo{1}{-3}\\) is an eigenvector. For an arbitrary \\(2\\times 2\\) matrix we want \\[\n\\begin{bmatrix} a & b \\\\c & d \\end{bmatrix} \\vectortwo{1}{-3}=\\lambda\\vectortwo{1}{-3}=\\vectortwo{\\lambda}{-3\\lambda}.\n\\] Thus \\(a-3b=\\lambda\\), \\(c-3d=-3\\lambda\\) and so \\(c=-3a+9b+3d\\). Thus \\(A\\) must be of the form \\[\n\\begin{bmatrix} a & b \\\\ -3a+9b+3d & d\\end{bmatrix}=a\\begin{bmatrix} 1 & 0 \\\\-3 & 0 \\end{bmatrix}+b \\begin{bmatrix} 0 & 1 \\\\ 9 & 0 \\end{bmatrix}+ d\\begin{bmatrix} 0 & 0 \\\\ 3 & 1\\end{bmatrix}.\n\\] Thus a basis of \\(V\\) is \\[\n\\left( \\begin{bmatrix} 1 & 0 \\\\ -3 & 0 \\end{bmatrix}, \\begin{bmatrix} 0 & 1 \\\\ 9 & 0 \\end{bmatrix}, \\begin{bmatrix} 0 & 0 \\\\ 3 & 1\\end{bmatrix} \\right)\n\\] and so the dimension of \\(V\\) is \\(3\\).\n\n\nExample 22.6 Find a basis of the linear space \\(V\\) of all \\(3\\times 3\\) matrices \\(A\\) for which both \\(\\vectorthree{1}{0}{0}^T\\) and \\(\\vectorthree{0}{0}{1}^T\\) are eigenvectors. Since \\(A \\vectorthree{1}{0}{0}^T\\) is simply the first column of \\(A\\), the first column must be a multiple of \\(\\vec e_1\\). Similarly, the third column must be a multiple of \\(\\vec e_3\\). There are no other restrictions on the form of \\(A\\), meaning it can be any matrix of the form \\[\n\\begin{bmatrix} a & b & 0 \\\\ 0 & c & 0 \\\\ 0 & d & e \\end{bmatrix}\n\\] Thus a basis of \\(V\\) is \\[\n\\left(\n\\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix},\n\\begin{bmatrix} 0 & 1 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 0 \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 1 & 0 \\end{bmatrix},\n\\begin{bmatrix} 0 & 0 & 0 \\\\ 0 & 0 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix}\n\\right)\n\\] and so the dimension of \\(V\\) is 5.\n\n\nTheorem 22.1  If \\(A\\) is an \\(n\\times n\\) matrix, then \\(\\det(A-\\lambda I_n)\\) is a polynomial of degree \\(n\\), of the form \\[\\begin{equation} \\label{chpo}\nf_A(\\lambda)=(-\\lambda)^n+\\text{trace} (A) (-\\lambda)^{n-1}+\\cdots +\\det(A).\n\\end{equation}\\]\n\n\nProof. This proof is left for the reader.\n\nThe equation \\(\\det(A-\\lambda I_n)=0\\) is called the characteristic equation of \\(A\\). The polynomial in \\(\\ref{chpo}\\) is called the characteristic polynomial and is denoted by \\(f_A(\\lambda)\\). We say that an eigenvalue \\(\\lambda_0\\) of a square matrix \\(A\\) has algebraic multiplicity \\(k\\) if \\(\\lambda_0\\) is a root of multiplicity \\(k\\) of the characteristic polynomial \\(f_A(\\lambda)\\) meaning that we can write \\[\nf_A(\\lambda)=(\\lambda_0-\\lambda)^k g(\\lambda)\n\\] for some polynomial \\(g(\\lambda)\\) with \\(g(\\lambda_0)\\neq 0\\).\n\nExample 22.7 Find the characteristic equation for a $2 $ matrix \\(A\\). The characteristic equation of \\(A=\\begin{bmatrix} a& b \\\\c & d\\end{bmatrix}\\) is \\[\nf_A(\\lambda)\n=\\det \\begin{bmatrix} a-\\lambda& b \\\\c & d-\\lambda \\end{bmatrix}\n%=(a-\\lambda)(d-\\lambda)-bc\n=\\lambda^2-(a+d)\\lambda +(ad-bc)=0.\n\\]\n\n\nExample 22.8 Use the characteristic polynomial \\(f_A(\\lambda)\\) to determine the eigenvalues and their multiplicities of \\[\nA=\\begin{bmatrix} -1 & -1 & -1 \\\\ -1 & -1 & -1 \\\\ -1 & -1 & -1 \\end{bmatrix}.\n\\] The characteristic equation is \\(f_A(\\lambda)=-\\lambda^2(\\lambda+3)\\). So \\(\\lambda_1=0\\) with algebraic multiplicity of 2 and \\(\\lambda_2=-3\\) with algebraic multiplicity of 1 are the eigenvalues of \\(A\\).\n\n\nExample 22.9 Consider the matrix \\(A=\\begin{bmatrix} a & b \\\\ b & c \\end{bmatrix}\\), where \\(a, b, c\\) are nonzero constants. For which values of $a, b, c $ does \\(A\\) have two distinct eigenvalues? The characteristic equation is \\(f_A(\\lambda)=\\lambda^2-(a+c)\\lambda+(a c-b^2)\\). The discriminant of this quadratic equation is \\[\n(a+c)^2-4(ac-b^2)=a^2+2ac+c^2-4ac+4b^2=(a-c)^2+4b^2.\n\\] The discriminant is always positive since \\(b\\neq 0\\). Thus, the matrix \\(A\\) there will always have two distinct real eigenvalues.\n\n\nExample 22.10 In terms of eigenvalues of \\(A\\), which \\(2\\times 2\\) matrices \\(A\\) does there exist an invertible matrix \\(S\\) such that \\(AS=SD\\), where \\(D=\\begin{bmatrix} 2 & 0 \\\\ 0 & 3 \\end{bmatrix}\\)? If we let \\(S=\\begin{bmatrix} \\vec v_1 & \\vec v_2\\end{bmatrix}\\), then \\(AS=\\begin{bmatrix} A \\vec v_1 & A \\vec v_2\\end{bmatrix}\\) and \\(SD=\\begin{bmatrix} 2 \\vec v_1 & 3\\vec v_2\\end{bmatrix}\\). So that \\(\\vec v_1\\) must be an eigenvector of \\(A\\) with eigenvalue 2, and \\(\\vec v_2\\) must be an eigenvector of \\(A\\) with eigenvalue 3. Thus, the matrix \\(S\\) will exist and will have first column has an eigenvector of \\(A\\) with eigenvalue 2, and have second column is an eigenvector of \\(A\\) with eigenvalue of 3. Therefore, \\(A\\) can be any matrix satisfying these requirements.\n\n\nExample 22.11 Let \\(A\\) be a matrix with eigenvalues \\(\\lambda_1, \\ldots, \\lambda_k\\).\n\nShow the eigenvalues of \\(A^T\\) are \\(\\lambda_1, \\ldots, \\lambda_k\\).\nShow the eigenvalues of \\(\\alpha A\\) are \\(\\alpha\\lambda_1, \\ldots, \\alpha \\lambda_k\\).\n\nShow \\(A^{-1}\\) exists if and only if \\(\\lambda_1 \\cdots \\lambda_k\\neq 0\\). - Also, show that if \\(A^{-1}\\) exists then its eigenvalues are \\(1/\\lambda_1,\\ldots,1/\\lambda_k\\).\n\n\nExample 22.12 Let \\(A\\) be a matrix with eigenvalues \\(\\lambda_1, \\ldots, \\lambda_k\\) and let \\(m\\) be a positive integer. Show that the eigenvalues of \\(A^m\\) are \\(\\lambda^m_1, \\ldots, \\lambda^m_k\\).\n\n\nExample 22.13 By using the matrix \\[\n\\begin{bmatrix}\n0 & 1 & 0 & \\cdots & 0\\\\\n0 & 0 & 1 & \\cdots & 0\\\\\n\\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 1 \\\\\n\\frac{-a_n}{a_0} & \\frac{-a_{n-1}}{a_0}& \\frac{-a_{n-2}}{a_0} & \\cdots & \\frac{-a_1}{a_0}\n\\end{bmatrix}\n\\] Show that any given polynomial \\(a_o \\lambda^n+a_1\\lambda^{n-1}+\\cdots +a_{n-1}\\lambda +a_n\\) where \\(a_0\\neq 0\\), of degree \\(n\\) may be regarded as the characteristic polynomial of a matrix of order \\(n\\). This matrix is called the companion matrix of the given polynomial.\n\n\nExample 22.14 Let \\(A\\) and \\(B\\) be \\(n\\times n\\) matrices. Show that \\(AB\\) and \\(BA\\) have the same eigenvalues.\n\n\nExample 22.15 Let \\(A\\) and \\(B\\) be real \\(n\\times n\\) matrices with distinct eigenvalues. Prove that \\(AB=BA\\) if and only if \\(A\\) and \\(B\\) have the same eigenvectors.\n\n\nExample 22.16 Prove that the characteristic polynomial of the block-triangular matrix \\(A=\\begin{bmatrix} B & C\\\\ 0 & D\\end{bmatrix}\\) is the product of the characteristic polynomials of \\(B\\) and \\(D\\).\n\n\nExample 22.17 Suppose that \\(A\\) is an invertible \\(n\\times n\\) matrix. Prove that \\[\\begin{equation}\nf_{A^{-1}}(x)=(-x)^n\\det(A^{-1})f_A\\left(\\frac{1}{x}\\right).\n\\end{equation}\\]\n\n\nExample 22.18 Let \\(A\\) be an \\(n\\times n\\) matrix. Prove that \\(A\\) and \\(A^T\\) have the same characteristic polynomial and hence the same eigenvalues.\n\nIf\\(\\lambda\\) is an eigenvalue of an \\(n\\times n\\) matrix \\(A\\), then the kernel of the matrix \\(A-\\lambda I_n\\) is called the eigenspace associated with \\(\\lambda\\) and is denoted by \\(E_\\lambda\\). The dimension of the eigenspace is called the geometric multiplicity of eigenvalue \\(\\lambda\\). In other words, the geometric multiplicity is the nullity of the matrix \\(A-\\lambda I_n\\).\n\nTheorem 22.2  Let \\(A\\) be an \\(n\\times n\\) matrix. If \\(\\lambda_1, \\ldots, \\lambda_k\\) are distinct eigenvalues of \\(A\\), and \\(\\vec v_1, \\ldots, \\vec v_k\\) are any nonzero eigenvectors associated with these eigenvalues respectively, then \\(\\vec v_1, \\ldots, \\vec v_k\\) are linearly independent.\n\n\nProof. Suppose there exists constants \\(c_1, \\ldots, c_k\\) such that \\[\\begin{equation}\n\\label{eigenveclin}\nc_1 \\vec v_1+\\cdots +c_k \\vec v_k=0\n\\end{equation}\\] Using the fact that \\(A \\vec v_i=\\lambda_i \\vec v_i\\) we multiply \\(\\ref{eigenveclin}\\) by \\(A\\) to obtain \\[\\begin{equation}\n\\label{eigenveclin2}\nc_1 \\lambda_1\\vec v_1+\\cdots c_k \\lambda_k \\vec v_k=0.\n\\end{equation}\\] Repeating this again we obtain \\[\\begin{equation}\n\\label{eigenveclin3}\nc_1 \\lambda^2_1\\vec v_1+\\cdots +c_k \\lambda^2_k \\vec v_k=0.\n\\end{equation}\\] Repeating, we are lead to the system in the vector unknowns \\(\\vec v_1, \\ldots, \\vec v_k\\) \\[\\begin{equation}\n\\begin{bmatrix}\nc_1 \\vec v_1 & \\cdots & c_k \\vec v_k\n\\end{bmatrix}_{n\\times k}\n\\begin{bmatrix}\n1 & \\lambda_1 & \\lambda_1^2 & \\cdots \\lambda_1^{k-1} \\\\\n1 & \\lambda_2 & \\lambda_2^2 & \\cdots \\lambda_2^{k-1} \\\\\n1 & \\lambda_3 & \\lambda_3^2 & \\cdots \\lambda_3^{k-1} \\\\\n\\vdots & \\vdots & \\vdots & \\ddots \\vdots \\\\\n1 & \\lambda_k & \\lambda_k^2 & \\cdots \\lambda_k^{k-1} \\\\\n\\end{bmatrix}_{k\\times k}\n=0_{n\\times k}.\n\\end{equation}\\] Since the eigenvalues are distinct, the coefficient matrix is an invertible Vandermonde matrix. Multiplying on the right by its inverse shows that \\[\n\\begin{bmatrix}\nc_1 \\vec v_1 & \\cdots & c_k \\vec v_k\n\\end{bmatrix}\n=0_{n\\times k}.\n\\] It follows that every \\(c_i\\) must be zero. Hence \\(\\vec v_1, \\ldots, \\vec v_k\\) are linearly independent.\n\nA basis of \\(\\mathbb{F}^n\\) consisting of eigenvectors of \\(A\\) is called an eigenbasis for \\(A\\).\nIn particular, if an \\(n\\times n\\) matrix \\(A\\) has \\(n\\) distinct eigenvalues, then there exists an eigenbasis for \\(A\\), namely, construct an eigenbasis by finding an eigenvector for each eigenvalue.\n\nExample 22.19 Find the characteristic equation, the eigenvalues, and a basis for the eigenspace. \\[A=\\begin{bmatrix}\n3 & 2 & 4 \\\\\n2 & 0 & 2\\\\\n4 & 2 & 3\n\\end{bmatrix}\n\\] The eigenvalues are \\(\\lambda_1=8\\) (with algebraic multiplicity 1) and \\(\\lambda_2=-1\\) (with algebraic multiplicity 2) since \\[\n\\det(A-\\lambda I)\n=\\begin{vmatrix}\n3-\\lambda & 2 & 4 \\\\\n2 & -\\lambda & 2\\\\\n4 & 2 & 3-\\lambda\n\\end{vmatrix}\n=-\\lambda^3+6\\lambda^2+15\\lambda+8=0.\n\\] For \\(\\lambda_1=8\\) we obtain \\[\n\\begin{bmatrix}\n-5 & 2 & 4 \\\\\n2 & -8 & 2\\\\\n4 & 2 & -5\n\\end{bmatrix}\n\\vectorthree{x_1}{x_2}{x_3}=\\vectorthree{0}{0}{0}\n\\] and we obtain the eigenvector \\(\\vec v_1=\\vectorthree{2}{1}{2}^T\\) with \\(E_8=\\text{span}(\\vec v_1)\\). Therefore the geometric multiplicity of \\(\\lambda_1=8\\) is 1. For \\(\\lambda_1=-1\\) we obtain \\[\n\\begin{bmatrix}\n4 & 2 & 4 \\\\\n2 & 1 & 2\\\\\n4 & 2 & 4\n\\end{bmatrix}\n\\vectorthree{x_1}{x_2}{x_3}=\\vectorthree{0}{0}{0}\n\\] and we obtain the eigenvectors \\(\\vec v_2=\\vectorthree{1}{-2}{0}^T\\) and \\(\\vec v_3=\\vectorthree{0}{-2}{1}^T\\) with \\(E_{-1}=\\text{span}(\\vec v_2, \\vec v_3)\\). Therefore the geometric multiplicity of \\(\\lambda_2=-1\\) is 2.\n\n\nExample 22.20 Show that for each of the following matrices, \\(\\lambda=3\\) is an eigenvalue of algebraic multiplicity 4. In each case, compute the geometric multiplicity of \\(\\lambda\\). \\[\n\\begin{bmatrix}\n3 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 3 & 0\\\\\n0 & 0 & 0 & 3\n\\end{bmatrix}\n\\qquad\n\\begin{bmatrix}\n3 & 1 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 3 & 0\\\\\n0 & 0 & 0 & 3\n\\end{bmatrix}\n\\qquad\n\\begin{bmatrix}\n3 & 1 & 0 & 0 \\\\\n0 & 3 & 1 & 0 \\\\\n0 & 0 & 3 & 0\\\\\n0 & 0 & 0 & 3\n\\end{bmatrix}\n\\qquad\n\\begin{bmatrix}\n3 & 1 & 0 & 0 \\\\\n0 & 3 & 1 & 0 \\\\\n0 & 0 & 3 & 1\\\\\n0 & 0 & 0 & 3\n\\end{bmatrix}\n\\]\n\n\nTheorem 22.3  Similar matrices \\(A\\) and \\(B\\) have the same determinant, trace, characteristic polynomial, rank, nullity, and the same eigenvalues with the same algebraic multiplicities.\n\n\nProof. The case for the determinant and trace are proven in \\(\\ref{propdettrace}\\). Since \\(A\\) and \\(B\\) are similar, there exists an invertible matrix \\(P\\) such that \\(B=P^{-1}AP\\). Using \\(\\ref{propdettrace}\\) we find \\[\\begin{align*}\n\\det(B-\\lambda I)\n&=\\det(P^{-1}AP-\\lambda I)\n=\\det(P^{-1}AP-P^{-1}\\lambda I P)\n=\\det(P^{-1}(A-\\lambda I) P) \\\\\n& =\\det(P^{-1})\\det(A-\\lambda I) \\det(P)\n=\\det(P^{-1})\\det(P) \\det(A-\\lambda I) \\\\\n& =\\det(P^{-1}P)\\det(A-\\lambda I)\n=\\det(A-\\lambda I)\n\\end{align*}\\] Thus \\(A\\) and \\(B\\) have the same characteristic equation. Therefore also the same eigenvalues with the same algebraic multiplicities according to \\(\\ref{eigenprop}\\) and \\(\\ref{charform}\\).\n\nIn light of \\(\\ref{invsimmatrix}\\), if \\(T\\) is a linear transformation from \\(V\\) to \\(V\\) then a scalar \\(\\lambda\\) is called an eigenvalue of \\(T\\) if there exists a nonzero element \\(\\vec v\\) in \\(V\\) such that \\(T(\\vec v)=\\lambda \\vec v\\). Assuming \\(V\\) is finite-dimensional then a basis \\(\\mathcal{D}\\) of \\(V\\) consisting of eigenvectors of \\(T\\) is called an eigenbasis for \\(T\\).\n\nTheorem 22.4 Let \\(T\\) be a linear transformation on a finite-dimensional vector space \\(V\\), and let \\(\\lambda\\) be an eigenvalue of \\(T\\). The geometric multiplicity of \\(\\lambda\\) is less than or equal to the algebraic multiplicity of \\(\\lambda\\).\n\n\nProof. Let \\(k\\) represent the geometric multiplicity of \\(\\lambda\\) and assume \\(\\dim V=n\\). First notice, by definition, the eigenspace \\(E_{\\lambda}\\) must contain at least one nonzero vector, and thus \\(k=\\dim E_{\\lambda} \\geq 1\\). Choose a basis \\(\\vec v_1, \\ldots,\\vec v_k\\) for \\(E_{\\lambda}\\) and, by \\(\\ref{basisspacethm}\\), extend it to a basis \\(\\mathcal{B}=(\\vec v_1, \\ldots, \\vec v_k, \\vec v_{k+1},\\ldots,\\vec v_n)\\) of \\(V\\). For \\(1\\leq i \\leq k\\), notice \\[\n[T(\\vec v_i)]_{\\mathcal{B}}\n=[\\lambda \\vec v_i]_{\\mathcal{B}}\n=\\lambda[\\vec v_i]_{\\mathcal{B}}\n=\\lambda \\vec e_i.\n\\] Thus the matrix representation for \\(T\\) with respect to \\(\\mathcal{B}\\) has the form \\[\\begin{equation}\nB=\n\\begin{bmatrix}\n\\lambda I_k & C\\\\ 0 & D\n\\end{bmatrix}\n\\end{equation}\\] where \\(C\\) is a \\(k\\times (n-k)\\) submatrix, \\(O\\) is an \\((n-k)\\times k\\) zero submatrix, and \\(D\\) is an \\((n-k)\\times (n-k)\\) submatrix.\nUsing \\(\\ref{blockdetprod}\\) we determine the characteristic polynomial of \\(T\\) \\[\nf_T(x)\n%=f_B(x)\n=|x I_n-B|\n=\\left|xI_n-\n\\begin{bmatrix}\n\\lambda I_k & C\\\\ 0 & D\n\\end{bmatrix}\n\\right|\n=\n\\begin{vmatrix}\n(x-\\lambda)I_k & C\\\\ 0 & xI_{n-k}-D\n\\end{vmatrix}\n=(x-\\lambda)^k f_D(x).\n\\] It follows that \\(f_T(x)=(x-\\lambda)^{k+m} g(x)\\) where \\(g(\\lambda)\\neq 0\\) and \\(m\\) is the number of factors of \\(x-\\lambda\\) in \\(f_D(x)\\). Hence \\(k\\leq k+m\\) leading to the desired conclusion.\n\n\nExample 22.21 Let \\(T(M)=M-M^T\\) be a linear transformation from \\(\\mathbb{R}^{2\\times2}\\) to \\(\\mathbb{R}^{2\\times2}\\). For each eigenvalue find a basis for the eigenspace and state the geometric multiplicity. Since \\(A=A^T\\) for every symmetric matrix, we notice \\(T(M)=M-M^T=M-M=0\\) whenever \\(M\\) is a symmetric matrix. Thus the nonzero symmetric matrices are eigenmatrices with eigenvalue \\(0\\).\nAlso notice the nonzero skew-symmetric matrices have eigenvalue \\(2\\) since \\(L(M)=M-M^T=M+M=2M\\). For eigenvlaue \\(\\lambda=0\\) we have eigenspace \\(E_0\\) with basis \\[\n\\left(\n\\begin{bmatrix}\n1 & 0 \\\\ 0 & 0\n\\end{bmatrix},\n\\begin{bmatrix}\n0 & 1 \\\\ 1 & 0\n\\end{bmatrix},\n\\begin{bmatrix}\n0 & 0 \\\\  0 & 1\n\\end{bmatrix}\n\\right).\n\\] This follow from the condition \\(A=A^T\\) in \\(\\mathbb{R}^{2\\times2}\\). Therefore the geometric multiplicity of \\(\\lambda=0\\) is 3. For eigenvalue \\(\\lambda=2\\) we have eigenspace \\(E_2\\) with basis \\[\n\\left(\n\\begin{bmatrix}\n0 & 1 \\\\ -1 & 0\n\\end{bmatrix}\n\\right).\n\\] which follows from the condition \\(A=-A^T\\) in \\(\\mathbb{R}^{2\\times2}\\). Therefore the geometric multiplicity of \\(\\lambda=2\\) is 1. By \\(\\ref{eigenveceigenvallemma}\\) we have an eigenbasis \\[\n\\left(\n\\begin{bmatrix}\n1 & 0 \\\\ 0 & 0\n\\end{bmatrix},\n\\begin{bmatrix}\n0 & 1 \\\\ 1 & 0\n\\end{bmatrix},\n\\begin{bmatrix}\n0 & 0 \\\\  0 & 1\n\\end{bmatrix},\n\\begin{bmatrix}\n0 & 1 \\\\ -1 & 0\n\\end{bmatrix}\n\\right).\n\\] for \\(T\\)."
  },
  {
    "objectID": "diagonalization.html",
    "href": "diagonalization.html",
    "title": "23  Diagonalization",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nAn \\(n\\times n\\) matrix \\(A\\) is called diagonalizable if \\(A\\) is similar to some diagonal matrix \\(D\\). If the matrix of a linear transformation \\(T\\) with respect to some basis is diagonal then we call \\(T\\) diagonalizable .\n\nTheorem 23.1  An \\(n\\times n\\) matrix \\(A\\) is diagonalizable if and only if it has \\(n\\) linearly independent eigenvectors. In that case, the diagonal matrix \\(D\\) is similar to \\(A\\) and is given by \\[\\begin{equation}\n\\label{diagmat}\nD=\n\\begin{bmatrix}\n\\lambda_1 & 0 & \\cdots & 0 \\\\\n0 & \\lambda_2 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\ddots& \\vdots \\\\\n0 & 0 & \\cdots & \\lambda_n\n\\end{bmatrix}\n\\end{equation}\\] where \\(\\lambda_1, ..., \\lambda_n\\) are the eigenvalues of \\(A\\). If \\(C\\) is a matrix whose columns are linearly independent eigenvectors of \\(A\\), then \\(D=C^{-1}A C\\).\n\n\nProof. The proof is left for the reader.\n\n\nCorollary 23.1  Let \\(T\\) be a linear transformation given by \\(T(\\vec x)=A\\vec x\\) where \\(A\\) is a square matrix. If \\(\\mathcal{D}=(\\vec v_1, ....,\\vec v_n)\\) is an eigenbasis for \\(T\\), with \\(A\\vec v_i=\\lambda_i \\vec v_i\\), then the \\(\\mathcal{D}\\)-matrix \\(D\\) of \\(T\\) given in \\(\\ref{diagmat}\\) is \\(D=[\\vec v_1, ...., \\vec v_n]^{-1}A [\\vec v_1, ...., \\vec v_n]\\).\n\n\nProof. The proof follows from \\(\\ref{dialineigvec}\\) and \\(\\ref{eigenveceigenvallemma}\\).\n\n\nCorollary 23.2  A matrix \\(A\\) is diagonalizable if andy only if there exists an eigenbasis for \\(A\\). In particular, if an \\(n\\times n\\) matrix \\(A\\) has \\(n\\) distinct eigenvalues, then \\(A\\) is diagonalizable.\n\n\nProof. The proof follows from \\(\\ref{dialineigvec}\\) and \\(\\ref{eigenveceigenvallemma}\\).\n\n\nExample 23.1 Let \\(T: \\mathcal{P}_2\\to \\mathcal{P}_2\\) be the linear transformation defined by\n\\[\nT(a_0+a_1 x+a_2x^2)=(a_0+a_1+a_2)+(a_1+a_2)x+a_2x^2.\n\\] Show that \\(T\\) is not diagonalizable. The matrix of \\(T\\) with respect to the usual basis \\((, x, x^2)\\) for \\(\\mathcal{P}_2\\) is easily seen to be \\[\nA=\n\\begin{bmatrix}\n1 & 1 & 1 \\\\\n0 & 1 & 1 \\\\\n0 & 0 & 1\n\\end{bmatrix}.\n\\] The characteristic polynomial is \\(f_A(x)=-(x-1)^3\\) since \\(A\\) is upper triangular. So \\(T\\) has only one (repeated) eigenvalue \\(\\lambda=1\\). A nonzero polynomial \\(g\\) with \\(g(x)=a_0+a_1 x+a_2 x^2\\) is an eigenvector if and only if \\[\n\\label{notdiageq}\n\\begin{bmatrix}\n0 & 1 & 1 \\\\\n0 & 0 & 1 \\\\\n0 & 0 & 0\n\\end{bmatrix}\n\\vectorthree{a_0}{a_1}{a_2}=\\vectorthree{0}{0}{0}.\n\\] Thus \\(a_1=0\\) and \\(a_2=0\\), so there is only one linearly independent eigenvector for \\(\\lambda=1\\). Thus \\(T\\) is not diagonalizable by \\(\\ref{diagonalizablechar}\\).\n\n\nExample 23.2 Let \\(T:\\mathcal{P}_2\\to \\mathcal{P}_2\\) be the linear transformation defined by\n\\[\\begin{equation}\nT(f(x))=x^2f''(x)+(3x-2)f'(x)+5 f(x).\n\\end{equation}\\] Find a basis for \\(\\mathcal{P}_2\\) such that the matrix representation of \\(T\\) with respect to \\(\\mathcal{B}\\) is diagonal. Since \\(T(x^2)=13x^2-4x\\), \\(T(x)=8x-2\\), and \\(T(1)=5\\) the matrix representation of \\(T\\) with respect to the basis \\(\\mathcal{B}=(x^2,x,1)\\) is \\[\nA=\\begin{bmatrix}13 & 0 & 0 \\\\ -4 & 8 & 0 \\\\ 0 & -2 & 5 \\end{bmatrix}.\n\\] Hence \\[\\begin{equation}\nf_T(x)=f_A(x)=\\begin{vmatrix} x-13  & 0 & 0 \\\\ 4 & x-8 & 0 \\\\ 0 & 2 & x-5 \\end{vmatrix}=(x-13)(x-8)(x-5).\n\\end{equation}\\] The eigenvalues of \\(T\\) are \\(\\lambda_1=13\\), \\(\\lambda_2=8\\) and \\(\\lambda_3=5\\). Solving each of the homogenous systems \\((A-13I_3)\\vec x=\\vec 0\\), \\((A-8I_3)\\vec x=\\vec 0\\), and \\((A-5I_3)\\vec x=\\vec 0\\) yields the eigenvectors \\(\\vec v_1=5x^2-4x+1\\), \\(\\vec v_2=3x-2\\), and \\(\\vec v_3=1\\), respectively. Notice \\(\\vec v_1, \\vec v_2, \\vec v_3\\) are 3 linearly independent vectors, so by \\(\\ref{dialineigvec}\\), \\(T\\) is diagonalizable. We let \\(\\mathcal{D}=(\\vec v_1, \\vec v_2, \\vec v_3)\\) and since \\(T(\\vec v_1)=13\\vec v_1\\),\\(T(\\vec v_2)=8\\vec v_2\\), and \\(T(\\vec v_3)=5\\vec v_3\\), the matrix representation of \\(T\\) with respect to \\(\\mathcal{D}\\) is the diagonal matrix \\[\n\\begin{bmatrix}13 & 0 & 0 \\\\ 0 & 8 & 0 \\\\ 0 & 0 & 5 \\end{bmatrix}\n\\] according to \\(\\ref{dialineigvec}\\).\n\n\nExample 23.3 Let \\(T:\\mathcal{P}_3\\to \\mathcal{P}_3\\) be the linear transformation defined by\n\\[\\begin{equation}\nT(f(x))=xf'(x)+f(x+1).\n\\end{equation}\\] Find a basis for \\(\\mathcal{P}_3\\) such that the matrix representation of \\(T\\) with respect to \\(\\mathcal{B}\\) is diagonal. Since \\(T(x^3)=4x^3+3x^2+3x+1\\), \\(T(x^2)=3x^2+2x+1\\), \\(T(x)=2x+1\\), and \\(T(1)=1\\) the matrix representation of \\(T\\) with respect to the basis \\(\\mathcal{B}=(x^3,x^2,x,1)\\) is \\[\nA=\\begin{bmatrix} 4 & 0 & 0 & 0 \\\\ 3 & 3 & 0 & 0 \\\\ 3 & 2 & 2 & 0 \\\\ 1 & 1 & 1 & 1  \\end{bmatrix}.\n\\] Since \\(A\\) is lower triangular, \\(f_T(x)=f_A(x)=(x-4)(x-3)(x-2)(x-1)\\); and so the eigenvalues are \\(\\lambda_1=4\\), \\(\\lambda_2=3\\), \\(\\lambda_3=2\\), and \\(\\lambda_4=1\\). Solving for a basis for each eigenspace of \\(A\\) yields \\[\nE_{\\lambda_1}=\\left(\\vectorfour{6}{18}{27}{17}\\right), \\quad\nE_{\\lambda_2}=\\left(\\vectorfour{0}{2}{4}{3}\\right), \\quad\nE_{\\lambda_3}=\\left(\\vectorfour{0}{0}{1}{1}\\right),\\quad\nE_{\\lambda_4}=\\left(\\vectorfour{0}{0}{0}{1}\\right).\n\\] By taking the polynomial corresponding to the basis vectors, we let \\(\\mathcal{D}=(\\vec v_1, \\vec v_2, \\vec v_3, \\vec v_4)\\) where \\(\\vec v_1=6x^3+18x^2+27x+17\\), \\(\\vec v_2=2x^2+4x+3\\), \\(\\vec v_3=x+1\\), and \\(\\vec v_4=1\\). The diagonal matrix \\[\n\\begin{bmatrix}\n4 & 0 & 0 & 0 \\\\\n0 & 3 & 0 & 0 \\\\\n0 & 0 & 2 & 0 \\\\\n0 & 0 & 0 & 1\n\\end{bmatrix}\n\\] is the matrix representation of \\(T\\) in \\(\\mathcal{D}\\)-coordinates and has the eigenvalues of \\(T\\) on its main diagonal. The transition matrix \\(P\\) from \\(\\mathcal{B}\\)-coordinates to \\(\\mathcal{D}\\)-coordinates is \\[\nP=\\begin{bmatrix}\n6 & 0 & 0 & 0 \\\\\n18 & 2 & 0 & 0 \\\\\n27 & 4 & 1 & 0 \\\\\n17 & 3 & 1 & 1\n\\end{bmatrix}\n\\]\nand satisfies the required relation \\(D=P^{-1}AP\\) as can be verified.\n\n\nExample 23.4 If \\(A\\) is similar to \\(B\\), show that \\(A^n\\) is similar to \\(B^n\\), for any positive integer \\(n\\).\n\n\nExample 23.5 Suppose that \\(C^{-1}AC=D\\). Show that for any integer \\(n\\), \\(A^n=CD^nC^{-1}\\).\n\n\nExample 23.6 Let \\(a\\) and \\(b\\) be real numbers. By diagonalizing \\[\nM=\n\\begin{bmatrix}\na & b-a \\\\\n0 & b\n\\end{bmatrix},\n\\] prove that \\[\nM^n=\n\\begin{bmatrix}\na^n & b^n-a^n \\\\\n0 & b^n\n\\end{bmatrix}\n\\] for all positive integers \\(n\\). We need a basis of \\(\\mathbb{R}^2\\) consisting of eigenvectors of \\(M\\). One such basis is \\(\\vec v_1=\\vec e_1\\) and \\(\\vec v_2=\\vec e_1+\\vec e_2\\) where \\(a\\) and \\(b\\) are eigenvalues for corresponding to these eigenvectors, respectively. Let \\(P=\\begin{bmatrix}\\vec v_1 & \\vec v_2\\end{bmatrix},\\) then by \\(\\ref{eigendmatrix}\\), the diagonalization is \\[\\begin{equation*}\nD=\\begin{bmatrix}\\vec v_1 & \\vec v_2\\end{bmatrix}^{-1}M\\begin{bmatrix}\\vec v_1 & \\vec v_2\\end{bmatrix}=\\begin{bmatrix}a & 0 \\\\ 0 & b \\end{bmatrix}.\n\\end{equation*}\\] Therefore \\[\\begin{equation*}\nM^n=(PDP^{-1})^n=\\underbrace{(PDP^{-1})\\cdots (PDP^{-1})}_{n\\text{-times}}=PD^n P^{-1}\n=\\begin{bmatrix}\na^n & b^n-a^n \\\\\n0 & b^n\n\\end{bmatrix}.\n\\end{equation*}\\]"
  },
  {
    "objectID": "invariant-subspaces.html",
    "href": "invariant-subspaces.html",
    "title": "24  Invariant Subspaces",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nIn this section we let \\(V\\) and \\(W\\) denote real or complex vector spaces.\nSuppose \\(T \\in \\mathcal{L}(V)\\) and \\(U\\) a subspace of \\(V\\), then we say \\(U\\) is an invariant subspace of \\(T\\) if \\(u\\in U\\) implies \\(Tu\\in U\\).\n\nLemma 24.1 Suppose \\(T \\in \\mathcal{L}(V)\\) and \\(U\\) a subspace of \\(V\\). Then all of the following hold\n\n\\(U\\) is invariant under \\(T\\) if and only if \\(T|_U\\) is an operator on \\(U\\),\n\\(\\text{ker} T\\) is invariant under \\(T\\), and\n\\(\\text{im} T\\) is invariant under \\(T\\).\n\n\n\nProof. The proof of each part follows.\n\nBy definition, invariant subspace \\(U\\) is invariant under \\(T\\) if and only if \\(u\\in U \\implies Tu\\in U\\), which, by definition of operator, is the same as \\(T|_U\\) being an operator.\nIf $u T $ then \\(Tu=0\\), and hence \\(Tu\\in \\text{ker} T\\).\nIf $u T $, then by the definition of range \\(Tu\\in\\text{im} T\\).\n\n\n\nTheorem 24.1 Suppose \\(T\\in \\mathcal{L}(V)\\). Let \\(\\lambda_1,\\ldots,\\lambda_m\\) denote distinct eigenvalues of \\(T\\). The the following are equivalent:\n\n\\(T\\) has a diagonal matrix with respect to some basis of \\(V\\);\n\\(V\\) has a basis consisting of eigenvectors of \\(T\\);\nthere exist one-dimensional \\(T\\)-invariant subspaces \\(U_1, \\ldots, U_n\\) of \\(V\\) such that \\(V=U_1 \\oplus \\cdots \\oplus U_n\\);\n\\(V=\\null (T-\\lambda_1 I) \\oplus \\cdots \\oplus \\null (T-\\lambda_m I)\\);\n\\(\\text{dim} V = \\text{dim} \\null (T-\\lambda_1 I) + \\cdots + \\text{dim} \\null (T-\\lambda_m I)\\).\n\n\n\n\nProof. \n\\(\\Longleftrightarrow\\) (ii): Exercise.\n\\(\\Longleftrightarrow\\) (iii): Suppose (ii) holds; thus suppose \\(V\\) has a basis consisting of eigenvectors of \\(T\\). For each \\(j\\), let \\(U_j=\\text{span}(v_j)\\). Obviously each \\(U_j\\) is a one-dimensional subspace of \\(V\\) that is invariant under \\(T\\) (because each \\(v_j\\) is an eigenvector of \\(T\\)). Because \\((v_1,\\ldots,v_n)\\) is a basis of \\(V\\), each vector in \\(V\\) can be written uniquely as a linear combination of \\((v_1,\\ldots,v_n)\\). In other words, each vector in \\(V\\) can be written uniquely as a sum \\(u_1+\\cdots +u_n\\), where each \\(u_j\\in U_j\\). Thus \\(V=U_1 \\oplus \\cdots \\oplus U_n\\). Hence (ii) implies (iii). Conversely, suppose now that (iii) holds; thus there are one-dimensional subspaces \\(U_1,\\ldots,U_n\\) of \\(V\\), each invariant under \\(T\\), such that \\(V=U_1 \\oplus \\cdots \\oplus U_n\\). For each \\(j\\), let \\(v_j\\) be a nonzero vector in \\(U_j\\). Then each \\(v_j\\) is an eigenvector of \\(T\\). Because each vector in \\(V\\) can be written uniquely as a sum \\(u_1+\\cdots + u_n\\), where each \\(u_j\\in U_j\\) ( so each \\(u_j\\) is a scalar multiple of \\(v_j\\)), we see that \\((v_1,\\ldots,v_n)\\) is a basis of \\(V\\). Thus (iii) implies (ii).\n\\(\\Longrightarrow\\) (iv): Suppose (ii) holds; thus thus suppose \\(V\\) has a basis consisting of eigenvectors of \\(T\\). Thus every vector in \\(V\\) is a linear combination of eigenvectors of \\(T\\). Hence \\(V=\\null (T-\\lambda_1)I + \\cdots + \\null (T-\\lambda_m)I.\\) To show that the sum above is direct, suppose that \\(0=u_1+\\cdots + u_m\\), where each \\(u_j\\in \\null (T-\\lambda_j I)\\). Because nonzero eigenvectors correspond to distinct eigenvalues are linearly independent, this implies that each \\(u_j\\) equals 0. This implies that the sum is a direct sum, completing the proof that\nimplies (iii).\n\\(\\Longrightarrow\\) (v): Exercise.\n\\(\\Longrightarrow\\) (ii): Suppose (v) holds; thus \\(\\text{dim} V=\\text{dim} \\null (T-\\lambda_1I)+\\cdots + \\text{dim} \\null (T-\\lambda_m I)\\). Choose a basis of each \\(\\null(T-\\lambda_j I)\\); put all these bases together to form a list \\((v_1,\\ldots,v_n)\\) of eigenvectors of \\(T\\), where \\(n=\\text{dim} V\\). To show that this list is linearly independent, suppose \\(a_1 v_1+\\cdots + a_n v_n=0\\), where \\(a_1,\\ldots,a_n\\) are scalars. For each \\(j=1, \\ldots., m\\), let \\(u_j\\) denote the sum of all the terms \\(a_k v_k\\) such that \\(v_k\\in \\null(T-\\lambda_j I)\\). Thus each \\(u_j\\) is an eigenvector of \\(T\\) with eigenvalue \\(\\lambda_j\\), and \\(u_1+\\cdots + u_m=0\\). Because nonzero eigenvectors corresponding to distinct eigenvalues are linearly independent, this implies that each \\(u_j\\) equals to 0. Because each \\(u_j\\) is a sum of terms \\(a_k v_k\\) where the \\(v_k\\)’s where chosen to be a basis of \\(\\null (T-\\lambda_j I)\\), this implies that all the \\(a_k\\)’s equal to 0. Thus \\((v_1,\\ldots,v_n)\\) is linearly independent and hence a basis of \\(V\\). Thus (v) implies (ii).\n\n\nA vector \\(v\\) is called a generalized eigenvector of \\(T\\) corresponding to \\(\\lambda\\), where \\(\\lambda\\) is an eigenvalue of \\(T\\), if \\((T-\\lambda I)^j v=0\\) for some positive integer \\(j\\).\n\nLemma 24.2 If \\(T\\in \\mathcal{L}(V)\\). Then, if \\(m\\) is a nonnegative integer such that \\(\\text{ker} T^m=\\text{ker} T^{m+1}\\), then\n\n\\(\\{0\\}=\\text{ker} T^0 \\subseteq \\text{ker} T^1 \\subseteq \\cdots \\subseteq \\text{ker} T^m = \\text{ker} T^{m+1} = \\text{ker} T^{m+2} = \\cdots\\)\n\\(\\text{ker} T^{\\text{dim} V}=\\text{ker} T^{\\text{dim} V+1} =\\text{ker} T^{\\text{dim} V+2}\\cdots\\)\n\\(V=\\text{im} T^0 \\supseteq \\text{im} T^1 \\supseteq \\cdots \\supseteq \\text{im} T^k \\supseteq \\text{im} T^{k+1} \\supseteq \\cdots\\)\n\\(\\text{im} T^{\\text{dim} V}= \\text{im} T^{\\text{dim} V+1} = \\text{im} T^{\\text{dim} V+2}\\cdots\\)\n\n\n\nLemma 24.3 Suppose \\(T\\in \\mathcal{L}(V)\\) and \\(\\lambda\\) is an eigenvalue of \\(T\\). Then the set of generalized eigenvalues of \\(T\\) corresponding to \\(\\lambda\\) equals \\(\\text{ker}(T-\\lambda I)^{\\text{dim} V}\\).\n\n\nProof. The proof is left for the reader.\n\nAn operator is called nilpotent if some power of it equal 0.\n\nLemma 24.4 Suppose \\(N\\in \\mathcal{L}(V)\\) is nilpotent, then \\(N^{\\text{dim} V}=0\\).\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 24.2 Let \\(T\\in \\mathcal{L}(V)\\) and \\(\\lambda\\in\\mathbb{F}\\). Then for every basis of \\(V\\) with respect to which \\(T\\) has an upper-triangular matrix, \\(\\lambda\\) appears on the diagonal of the matrix of \\(T\\) precisely \\(\\text{dim} (T-\\lambda I)^{\\text{dim} V}\\) times.\n\n\nProof. The proof is left for the reader.\n\nThe multiplicity of an eigenvalue \\(\\lambda\\) of \\(T\\) is defined to be the dimension of the subspace of generalized eigenvectors corresponding to \\(\\lambda\\), that is the multiplicity of \\(\\lambda\\) is equal to \\(\\text{dim} \\text{ker}(T-\\lambda I)^{\\text{dim} V}\\).\n\nTheorem 24.3 If \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\), then the sum of the multiplicities of all the eigenvalues of \\(T\\) equals \\(\\text{dim} V\\).\n\n\nProof. The proof is left for the reader.\n\nLet \\(d_j\\) denote the multiplicity of \\(\\lambda_j\\) as an eigenvalue of \\(T\\), the polynomial \\[\n(z-\\lambda_1)^{d_1} \\cdots (z-\\lambda_m)^{d_m}\n\\] is called the characteristic polynomial of \\(T\\).\n\nTheorem 24.4 Suppose that \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Let \\(q\\) denote the characteristic polynomial of \\(T\\). Then \\(q(T)=0\\).\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 24.5 If \\(T\\in \\mathcal{L}(V)\\) and \\(p\\in \\mathcal{P}(\\mathbb{F})\\), then \\(\\text{ker} p(T)\\) is invariant under \\(T\\).\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 24.6 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Let \\(\\lambda_1,\\ldots,\\lambda_m\\) be the distinct eigenvalues of \\(T\\), and let \\(U_1,\\ldots,U_m\\) be the corresponding subspaces of generalized eigenvectors. Then\n\n\\(V=U_1\\oplus \\cdots \\oplus U_m\\);\neach \\(U_J\\) is invariant under \\(T\\);\neach \\(\\left.(T-\\lambda_j I) \\right|_{U_j}\\) is nilpotent.\n\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 24.7 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Then there is a basis of \\(V\\) consisting of generalized eigenvectors of \\(T\\).\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 24.8 Suppose \\(N\\) is a nilpotent operator on \\(V\\). Then there is a basis of \\(V\\) with respect to which the matrix of \\(N\\) has the form \\[\n\\begin{bmatrix} 0 & & * \\\\ & \\ddots & \\\\ 0 & & 0\\end{bmatrix};\n\\] here all entries on and below the diagonal are 0’s.\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 24.9 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Let \\(\\lambda_1,\\ldots,\\lambda_m\\) be the distinct eigenvalues of \\(T\\). Then there is a basis of \\(V\\) with respect to which \\(T\\) has block diagonal matrix of the form \\[\n\\begin{bmatrix} A_1 & & 0\\\\ & \\ddots & \\\\ 0 & & A_m\\end{bmatrix},\n\\] where each \\(A_j\\) is an upper-triangular matrix of the form \\[\n\\begin{bmatrix} \\lambda_1 & & * \\\\ & \\ddots & \\\\ 0 & & \\lambda_j\\end{bmatrix}.\n\\]\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 24.10 Suppose \\(N\\in\\mathcal{L}(V)\\) is nilpotent. Then \\(I+N\\) has a square root.\n\n\nProof. The proof is left for the reader.\n\nOn real vector spaces there exist invertible operators that have no square roots. For example, the operator of multplication by \\(-1\\) on \\(\\mathbb{R}\\) has no square root because no real number has its square equal to \\(-1\\).\n\nTheorem 24.11 Suppose \\(V\\) is a complex vector space. If \\(T\\in \\mathcal{L}(V)\\) is invertible, then \\(T\\) has a square root.\n\n\nProof. The proof is left for the reader.\n\nThe minimal polynomial of \\(T\\) is the monic polynomial \\(p\\in \\mathcal{P}(\\mathbb{F}\\) of smallest degree such that \\(p(T)=0\\).\n\nTheorem 24.12 Let \\(T\\in \\mathcal{L}(V)\\) and let \\(q\\in \\mathcal{P}(\\mathbb{F})\\). Then \\(q(T)=0\\) if and only if the minimal polynomial of \\(T\\) divided \\(q\\).\n\n\nProof. The proof is left for the reader.\n\n\nTheorem 24.13 Let \\(T\\in \\mathcal{L}(V)\\). Then the roots of the minimal polynomial of \\(T\\) are precisely the eigenvalues of \\(T\\).\n\n\nProof. The proof is left for the reader.\n\nEvery \\(T\\in \\mathcal{L}(V)\\) where \\(V\\) is a complex vector space, there is a basis of \\(V\\) with respect to which \\(T\\) has a nice upper-triangular matrix. We can do even better. There is a basis of \\(V\\) with respect to which the matrix of \\(T\\) contains zeros everywhere except possibly on the diagonal and the line directly above the diagonal.\nSuppose \\(N\\in \\mathcal{L}(V)\\) is nilpotent. For each nonzero vector \\(v\\in V\\), let \\(m(v)\\) denote the largest nonnegative integer such that \\(N^{m(v)}\\neq 0\\).\n\nTheorem 24.14 If \\(N\\in \\mathcal{L}(V)\\) is nilpotent, then there exist vectors \\(v_1,\\ldots,v_k \\in V\\) such that\n\n\\(\\left(v_1,N v_1,\\ldots,N^{m(v_1)},\\ldots,v_k, N v_k, \\ldots, N^{m(v_k)}v_k\\right)\\) is a basis of \\(V\\);\n\\(\\left(N^{m(v_1)}v_1,\\ldots,N^{m(v_k)}v_k\\right)\\) is a basis of \\(\\text{ker} N\\).\n\n\n\nProof. The proof is left for the reader.\n\n\nExample 24.1 Suppose \\(T\\in \\mathcal{L}(V)\\). Prove that if \\(U_1,\\ldots,U_m\\) are subspaces of \\(V\\) invariant under \\(T\\), then \\(U_1 + \\cdots +U_m\\) is invariant under \\(T\\). Suppose \\(v\\in U_1+\\cdots + U_m\\). Then there exists \\(u_1,\\ldots,u_m\\) such that \\(v=u_1+\\cdots +u_m\\) with \\(u_j\\in U_j\\). Then \\(Tv=T u_1+\\cdots + T u_m\\). Since each \\(U_j\\) is invariant under \\(T\\), \\(T u_j \\in U_j\\), so \\(T v \\in U_1+ \\cdots + U_m\\).\n\n\nExample 24.2 Suppose \\(T\\in \\mathcal{L}(V)\\). Prove that the intersection of any collection of subspaces of \\(V\\) invariant under \\(T\\) is invariant under \\(T\\). Suppose we have subspaces \\(\\{U_j\\}\\) with each \\(U_j\\) invariant under \\(T\\). Let \\(v\\in \\cap_j U_j\\). Then \\(Tv\\in U_j\\) for each \\(j\\), and so \\(\\cap_j U_j\\) is invariant under \\(T\\).\n\n\nExample 24.3 Prove or give a counterexample: if \\(U\\) is a subspace of \\(V\\) that is invariant under every operator on \\(V\\), then \\(U=\\{0\\}\\) or \\(U=V\\). We will prove the contrapositive: if \\(U\\) is a subspace of \\(V\\) and \\(U\\neq \\{0\\}\\) and \\(U\\neq V\\), then there exists an operator \\(T\\) on \\(V\\) such that \\(U\\) is not invariant under \\(T\\). Let \\((u_1,\\ldots,u_m)\\) be a basis for \\(U\\), which we extend to a basis \\((u_1,\\ldots,u_m, v_1,\\ldots,v_n)\\) of \\(V\\). The assumption \\(U\\neq \\{0\\}\\) and \\(U\\neq V\\) means that \\(m\\geq 1\\) and \\(n\\geq 1\\). Define a linear map \\(T\\) by \\(Tu_1=v_1\\) and for \\(j>1\\), \\(T u_j=0\\). Since \\(v_1\\not \\in U\\), the subspace \\(U\\) is not invariant under the operator \\(T\\).\n\n\nExample 24.4 Suppose that \\(S,T\\in \\mathcal{L}(V)\\) are such that \\(S T= T S\\). Prove that \\(\\text{null }(T-\\lambda I)\\) is invariant under \\(S\\) for every \\(\\lambda \\in F\\). Suppose \\(v\\in \\text{ker}(T-\\lambda I)\\). Then \\(Tv = \\lambda v\\) and using \\(TS=ST\\), \\(Sv\\) satisfies $ T(S v)=S(T v)=S v)=(S v). $ Thus \\(S v\\in \\text{ker}(T-\\lambda I)\\) and so \\(\\text{ker} (T-\\lambda I)\\) is invariant under \\(S\\).\n\n\nExample 24.5 Define \\(T\\in \\mathcal{L}(F^2)\\) by \\(T(w,z)=(z,w)\\). Find all eigenvalues and eigenvectors of \\(T\\). Suppose \\((w,z)\\neq (0,0)\\) and \\(T(w,z)=(z,w)=\\lambda(w,z)\\). Then \\(z=\\lambda w\\) and \\(w=\\lambda z\\). Of course this leads to \\(w=\\lambda z=\\lambda^2w\\), \\(z=\\lambda w=\\lambda^2 z\\). Since \\(w\\neq 0\\) or \\(z\\neq 0\\), we see that \\(\\lambda^2=1\\) so that \\(\\lambda =\\pm 1\\). A basis of eigenvectors is \\((w_1,z_1)=(1,1)\\), \\((w_2,z_2)=(-1,1)\\) and they have eigenvalues 1 and \\(-1\\) respectively.\n\n\nExample 24.6 Define \\(T\\in \\mathcal{L}(F^3)\\) by \\(T(z_1,z_2,z_3)=(2z_2,0,5z_3)\\). Find all eigenvalues and eigenvectors of \\(T\\). Suppose \\((z_1,z_2,z_3)\\neq (0,0,0)\\) and \\[\nT(z_1,z_2,z_3)=(2z_2,0,5z_3)=\\lambda (z_1,z_2,z_3).\n\\] If \\(\\lambda=0\\) then \\(z_2=z_3=0\\), and one checks that \\(v_1=(0,0,0)\\) is an eigenvector with eigenvalue 0. If \\(\\lambda\\neq 0\\) then \\(z_2=0\\), \\(2z_2=\\lambda z_1=0\\), \\(5z_3=\\lambda z_3\\), so \\(z_1=0\\) and \\(\\lambda =5\\). The eigenvector for \\(\\lambda=5\\) is \\(v_2=(0,0,1)\\). These are the only eigenvalues and each eigenspace is one dimensional.\n\n\nExample 24.7 Suppose \\(n\\) is a positive integer and \\(T\\in \\mathcal{L}(\\mathbb{F}^n)\\) is defined by \\[\nT(x_1,\\ldots,x_n)=(x_1+ \\cdots + x_n,\\ldots,x_1+\\cdots +x_n).\n\\] Find all eigenvalues and eigenvectors of \\(T\\). First, any vector of the form \\(v_1=(\\alpha,\\ldots,\\alpha)\\), for \\(\\alpha\\in \\mathbb{F}\\), is an eigenvector with eigenvalue \\(n\\). If \\(v_2\\) is any vector $v_2=(x_1,,x_n), $ such that \\(x_1+\\cdots + x_n=0\\) then \\(v_2\\) is an eigenvector with eigenvalue 0. Here are the independent eigenvectors: \\(v_1=(1,1,\\ldots,1)\\), and \\(v_n=(1,0,\\ldots,0)-E_n\\), for \\(n\\geq 2\\) where \\(E_n\\) denoted the \\(n\\)-th standard basis vector.\n\n\nExample 24.8 Suppose \\(T\\in \\mathcal{L}(V)\\) is invertible and \\(0\\neq \\lambda \\in F\\). Prove that \\(\\lambda\\) is an eigenvalue of \\(T\\) if and only if \\(\\frac{1}{\\lambda}\\) is an eigenvalue of \\(T^{-1}\\). Suppose \\(v\\neq 0\\) and \\(T v =\\lambda v\\). Then \\(v=T^{-1}T v=\\lambda T^{-1}v\\), or \\(T^{-1}v=\\frac{1}{\\lambda}v\\), and the other direction is similar.\n\n\nExample 24.9 Suppose \\(S,T\\in \\mathcal{L}(V)\\). Prove that \\(S T\\) and \\(T S\\) have the same eigenvalues. Suppose \\(v\\neq 0\\) and \\(STv=\\lambda v\\). Multiply by \\(T\\) to get \\(T S(Tv)=\\lambda T v\\). Thus if \\(t\\neq 0\\) then \\(\\lambda\\) is also an eigenvalue of \\(TS\\), with nonzero=o eigenvector \\(Tv\\). On the other hand, if \\(T v=0\\), then \\(\\lambda =0\\) is an eigenvalue of \\(ST\\). But if \\(T\\) is not invertible, then \\(\\text{im} TS \\subset\\text{im} T\\)is not equal to \\(V\\), so \\(TS\\) has a nontrivial null space, hence 0 is an eigenvalue of \\(TS\\).\n\n\nExample 24.10 Suppose \\(T\\in \\mathcal{L}(V)\\) is such that every vector in \\(V\\) is an eigenvector of \\(T\\). Prove that \\(T\\) is a scalar multiple of the identity operator. Pick a basis \\((v_1,\\ldots,v_N)\\) for \\(V\\). By assumption, \\(T v_n=\\lambda_n v_n\\). Pick any two distance indices, \\(m,n\\). We also have \\(T(v_m+v_n)=\\lambda(v_m+v_n)=\\lambda(v_m+v_n)=\\lambda_m v_m + \\lambda_n v_n\\). Write this as \\(0=(\\lambda-\\lambda_m)v_m+(\\lambda-\\lambda_n)v_n\\). Since \\(v_m\\) and \\(v_n\\) are independent, \\(\\lambda=\\lambda_m=\\lambda_n\\), and all the \\(\\lambda_n\\) are equal.\n\n\nExample 24.11 Suppose \\(S, T\\in \\mathcal{L}(V)\\) and \\(S\\) is invertible. Prove that if \\(p \\in \\mathcal{P}(\\mathbb{F})\\) is a polynomial, then \\(p(S T S^{-1})=S p(T) S^{-1}\\). First let’s show that for positive integers \\(n\\), \\((STS^{-1})^n=S T^n S^{-1}\\). We may do this by induction, with nothing to show if \\(n=1\\). Assume it’s true for \\(n=k\\), and consider \\[\n(STS^{-1})^{k+1}=(STS^{-1})^k(STS^{-1})=ST^k S^{-1}STS^{-1}=S T^{k+1}S^{-1}.\n\\] Now suppose \\(P(z)=a_n z^N+\\cdots + a_1 z+a_0.\\) Then \\[\\begin{align*}\np(STS^{-1})&=\\sum_{n=0}^N a_n (STS^{-1})^n=\\sum_{n=0}^N a_n ST^n S^{-1}\\\\ & =S\\left( \\sum_{n=0}^N a_n T^n \\right) S^{-1}=Sp(T)S^{-1}.\n\\tag*{}\n\\end{align*}\\]\n\n\nExample 24.12 Suppose \\(F=\\mathbb{C}\\), \\(T\\in \\mathcal{L}(V)\\), \\(p\\in \\mathcal{P}(\\mathbb{C})\\), and \\(a\\in \\mathbb{C}\\). Prove that \\(a\\) is an eigenvalue of \\(p(T)\\) if and only if \\(a=p(\\lambda)\\) for some eigenvalue $$ of \\(T\\). Suppose first that \\(v\\neq 0\\) is an eigenvalue of \\(T\\) with eigenvalue \\(\\lambda\\); that is \\(T v = \\lambda v\\). Then for positive integers \\(n\\), \\(T^n v=T^{n-1} \\lambda v = \\cdots \\lambda^n v\\), and so \\(p(T)v=p(\\lambda) v\\). That is \\(\\alpha=p(\\lambda)\\) is an eigenvalue of \\(p(T)\\) if \\(\\lambda\\) is an eigenvalue of \\(T\\). Conversely, suppose now that \\(\\alpha\\) is a eigenvalue of \\(p(T)\\), so there is a \\(v\\neq 0\\) with \\(p(T)v =\\alpha v\\), or \\((p(T)-\\alpha I)v=0\\). Since \\(\\mathbb{F}=\\mathbb{C}\\), we may factor the polynomial $p(T)-I $ into linear factors \\[\n0=(p(T)-\\alpha I)v=\\prod (T-\\lambda_n I)v.\n\\] At least one of the factors is not invertible, so at least one of the \\(\\lambda_n\\), say \\(\\lambda_1\\), is an eigenvalue of \\(T\\). Let \\(w\\neq 0\\) be an eigenvector for \\(T\\) with eigenvalue \\(\\lambda_1\\). Then \\[\n0=(T-\\lambda_N I)\\cdots (T-\\lambda_1 I)w=(p(T)-\\alpha I) w,\n\\] so \\(w\\) is an eigenvalue for \\(p(T)\\) with eigenvalue \\(\\alpha\\). But by the first part of the argument, \\(p(T)w=p(\\lambda_1)w=\\alpha w\\) and \\(\\alpha=p(\\lambda_1)\\).\n\n\nExample 24.13 Show that the previous exercise does not hold with \\(F=\\mathbb{R}\\). Take \\(T: \\mathbb{R}^2 \\rightarrow \\mathbb{R}^2\\) given by \\(T(x,y)=(-y,x)\\). We’ve seen perviously that \\(T\\) has no real eigenvalues. On the other hand, \\(T^2(x,y)=(-x,-y)=-1(x,y)\\).\n\n\nExample 24.14 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Prove that \\(T\\) has an invariant subspace of dimension \\(j\\) for each \\(j=1,\\ldots, \\text{dim} V\\). Let \\((v_1,\\ldots,v_N)\\) be a basis with respect to which \\(T\\) has an upper triangular matrix. Then by a previous proposition, \\(T: \\text{span}(v_1,\\ldots,v_j) \\rightarrow \\text{span}(v_1,\\ldots,v_j)\\).\n\n\nExample 24.15 Give an example of an operator whose matrix with respect to some basis contains only 0’s on the diagonal, but the operator is invertible. Consider \\(T=\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\\).\n\n\nExample 24.16 Give an example of an operator whose matrix with respect to some basis contains only nonzero numbers on the diagonal, but the operator is not invertible. Taking \\(T=\\begin{bmatrix} 1 & 1 \\\\ 1 & 1 \\end{bmatrix}\\). If \\(v=[1,-1]\\), then \\(Tv=0\\).\n\n\nExample 24.17 Give an example of an operator on \\(\\mathbb{C}^4\\) whose characteristic and minimal polynomials both equal \\(z (z-1)^2(z-3)\\).\n\n\nExample 24.18 Give an example of an operator on \\(\\mathbb{C}^4\\) whose characteristic polynomial equals \\(z (z-1)^2(z-3)\\) and whose minimal polynomial equals \\(z (z-1)(z-3)\\).\n\n\nExample 24.19 Suppose \\(a_0, \\ldots, a_{n-1}\\in \\mathbb{C}\\). Find the minimal and characteristic polynomials of the operator on \\(\\mathbb{C}^n\\) whose matrix is \\[\n\\begin{bmatrix}\n0 & & & & & -a_0 \\\\\n1 & 0 & & & & -a_1 \\\\\n& 1 & \\ddots & & & -a_2 \\\\\n&  & \\ddots & & & \\vdots \\\\\n& 1 & & & 0 & -a_{n-2} \\\\\n&  & & & 1 & -a_{n-1}\n\\end{bmatrix}\n\\] with respect to the standard bases."
  },
  {
    "objectID": "jordan-canonical-form.html",
    "href": "jordan-canonical-form.html",
    "title": "25  Jordan Canonical Form",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)\n\n\n\n\nA basis of \\(V\\) is called a Jordan basis for \\(T\\) if with respect to this basis \\(T\\) has block diagonal matrix \\[\n\\begin{bmatrix}\nA_1 &   & 0 \\\\\n& \\ddots &  \\\\\n0 &  & A_m \\\\\n\\end{bmatrix}\n\\] where each \\(A_j\\) is an upper triangular matrix of the form \\[\nA_j =\n\\begin{bmatrix}\n\\lambda_j & 1 &  & 0 \\\\\n& \\ddots & \\ddots & \\\\\n&  &  \\ddots & 1 \\\\\n0 &  &  & \\lambda_j \\\\\n\\end{bmatrix}\n\\] where the diagonal is filled with some eigenvalue \\(\\lambda_j\\) of \\(T\\).\nBecause there exist operators on real vector spaces that have no eigenvalues, there exist operators on real vector spaces for which there is no corresponding Jordan basis.\n\nTheorem 25.1 Suppose \\(V\\) is a complex vector space. If \\(T\\in \\mathcal{L}(V)\\), then there is a basis of \\(V\\) that is a Jordan basis for \\(T\\).\n\n\nProof. The proof is left for the reader.\n\nA basis of \\(V\\) is called a Jordan basis for \\(T\\) if with respect to this basis \\(T\\) has block diagonal matrix \\[\n\\begin{bmatrix}\nA_1 &   & 0 \\\\\n& \\ddots &  \\\\\n0 &  & A_m \\\\\n\\end{bmatrix}\n\\] where each \\(A_j\\) is an upper triangular matrix of the form \\[\nA_j =\n\\begin{bmatrix}\n\\lambda_j & 1 &  & 0 \\\\\n& \\ddots & \\ddots & \\\\\n&  &  \\ddots & 1 \\\\\n0 &  &  & \\lambda_j \\\\\n\\end{bmatrix}\n\\] where the diagonal is filled with some eigenvalue \\(\\lambda_j\\) of \\(T\\).\nAn operator \\(T\\) can be put into Jordan canonical form if its characteristic and minimal polynomials factor into linear polynomials. this is always true if the vector space is complex.\n\nTheorem 25.2 Let \\(T\\in\\mathcal{L}(V)\\) whose characteristic and minimal polynomials are, respectively, \\[\nc(t)=(t-\\lambda_1)^{n_1} \\cdots (t-\\lambda_r)^{n_r})\n\\qquad \\text{and} \\qquad\nm(t)=(t-\\lambda_1)^{m_1} \\cdots (t-\\lambda_r)^{m_r})\n\\] where the \\(\\lambda_i\\) are distinct scalars. Then \\(T\\) has block diagonal matrix representation \\(J\\) whose diagonal entries are of the form \\[\nJ_{ij}=\n\\begin{bmatrix}\n\\lambda_i & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & \\lambda_i & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\lambda_i & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & \\lambda_i\n\\end{bmatrix}\n\\] For each \\(\\lambda_i\\) the corresponding blocks have the following properties:\n\nThere is at least one \\(J_{ij}\\) of order \\(m_i\\); all other \\(J_{ij}\\) are of order \\(\\leq m_i\\)\nThe sum of the orders of the \\(J_{ij}\\) is \\(n_i\\).\nThe number of \\(J_{ij}\\) equals the geometric multiplicity of \\(\\lambda_i\\).\nThe number of \\(J_{ij}\\) of each possible order is uniquely determined by \\(T\\).\n\n\n\nProof. The proof is left for the reader.\n\nThe matrix \\(J\\) in the above proposition is called the Jordan canonical form of the operator \\(T\\). A diagonal block \\(J_{ij}\\) is called a Jordan block belongng to the eigenvalue \\(\\lambda_i\\). Observe that \\[\n\\begin{bmatrix}\n\\lambda_i & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & \\lambda_i & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\lambda_i & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & \\lambda_i\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n\\lambda_i & 0 & 0 & \\cdots & 0 & 0 \\\\\n0 & \\lambda_i & 0 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & \\lambda_i & 0 \\\\\n0 & 0 & 0 & \\cdots & 0 & \\lambda_i\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n0 & 1 & 0 & \\cdots & 0 & 0 \\\\\n0 & 0 & 1 & \\cdots & 0 & 0 \\\\\n\\vdots & \\vdots & \\ddots & \\ddots & \\vdots & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 & 1 \\\\\n0 & 0 & 0 & \\cdots & 0 & 0\n\\end{bmatrix}\n\\] That is, \\(J_{ij}=\\lambda_i I+N\\) where \\(N\\) is the nilpotent block.\n\nExample 25.1 Suppose the characteristic and minimum polynomials of an operator \\(T\\) are, respectively, \\[\nc(t)=(t-2)^4(t-3)^3 \\qquad \\text{and} \\qquad m(t)=(t-2)^2(t-3)^2.\n\\] Then the Jordan canonical form of \\(T\\) is one of the following matrices: \\[\n\\begin{bmatrix}\n2 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 2 & 1 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 3 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 3\n\\end{bmatrix}\n\\qquad \\text{or} \\qquad\n\\begin{bmatrix}\n2 & 1 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 2 & 0 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 2 & 0 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 2 & 0 & 0 & 0 \\\\\n0 & 0 & 0 & 0 & 3 & 1 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 3 & 0 \\\\\n0 & 0 & 0 & 0 & 0 & 0 & 3\n\\end{bmatrix}\n\\] The first matrix occurs if \\(T\\) has two independent eigenvectors belonging to the eigenvalue 2; and the second matrix occurs if \\(T\\) has three independent eigenvectors belonging to 2.\n\n\nExample 25.2 Suppose \\(N\\in \\mathcal{L}(V)\\) is nilpotent. Prove that the minimal polynomial of \\(N\\) is \\(z^{m+1}\\), where \\(m\\) is the length of the longest consecutive string of \\(1'\\text{s}\\) that appears on the line directly above the diagonal in the matrix of \\(N\\) with respect to any Jordan basis for \\(N\\).\n\n\nExample 25.3 Suppose \\(V\\) is a complex vector space and \\(T\\in \\mathcal{L}(V)\\). Prove that there does not exist a direct sum decomposition of \\(V\\) into two proper subspaces invariant under \\(T\\) if and only if the minimal polynomial of \\(T\\) is of the form \\((z-\\lambda)^{\\text{dim} V}\\) for some \\(\\lambda \\in \\mathbb{C}\\).\n\n\nExample 25.4 Suppose \\(T\\in \\mathcal{L}(V)\\) and \\((v_1,\\ldots,v_n)\\) is a basis of \\(V\\) that is a Jordan basis for \\(T\\). Describe the matrix of \\(T\\) with respect to the basis \\((v_n,\\ldots,v_1)\\) obtained by reversing the order of the \\(v\\)’s.\n\n\nExample 25.5 Consider a 2-by-2 matrix of real numbers \\[\n\\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}.\n\\]\n\n\nExample 25.6 Suppose \\(A\\) is a block diagonal matrix \\[\nA=\\begin{bmatrix}\nA_1 & & 0 \\\\\n& \\ddots & \\\\\n0 & & A_m\n\\end{bmatrix},\n\\] where each \\(A_j\\) is a square matrix. Prove that the set of eigenvalues of \\(A\\) equals the union of the eigenvalues of \\(A_1,\\ldots,A_m\\).\n\n\nExample 25.7 Suppose \\(A\\) is a block upper-triangular matrix \\[\nA=\n\\begin{bmatrix}\nA_1 & & * \\\\\n& \\ddots & \\\\\n0 & & A_m\n\\end{bmatrix},\n\\] where each \\(A_j\\) is a square matrix. Prove that the set of eigenvalues of \\(A\\) equals the union of the eigenvalues of \\(A_1\\),,\\(A_m\\).\n\n\nExample 25.8 Suppose \\(V\\) is a real vector space and \\(T\\in \\mathcal{L}(V)\\). Suppose \\(\\alpha, \\beta \\in \\mathbb{R}\\) are such that \\(T^2+\\alpha T+\\beta I=0\\). Prove that \\(T\\) has an eigenvalue if and only if \\(\\alpha^2 \\geq 4 \\beta\\).\n\n\nExample 25.9 Suppose \\(V\\) is a real inner-product space and \\(T\\in \\mathcal{L}(V)\\). Prove that there is an orthonormal basis of \\(V\\) with respect to which \\(T\\) has a block upper-triangular matrix \\[\n\\begin{bmatrix}\nA_1 & & * \\\\\n& \\ddots & \\\\\n0 & & A_m\n\\end{bmatrix}.\n\\] where each \\(A_j\\) is a 1-by-1 matrix or a 2-by-2 matrix with no eigenvalues.\n\n\nExample 25.10 Prove that if \\(T\\in \\mathcal{L}(V)\\) and \\(j\\) is a positive integer such that \\(j \\leq \\text{dim} V\\), then \\(T\\) has an invariant subspace whose dimension equals \\(j-1\\) or \\(j\\).\n\n\nExample 25.11 Prove that there does not exist an operator \\(T\\in \\mathcal{L}(\\mathbb{R}^7)\\) such that \\(T^2+T+I\\) is nilpotent.\n\n\nExample 25.12 Give an example of an operator \\(T\\in \\mathcal{L}(\\mathbb{C}^7)\\) such that \\(T^2+T+I\\) is nilpotent.\n\n\nExample 25.13 Suppose \\(V\\) is a real vector space and \\(T\\in \\mathcal{L}(V)\\). Suppose \\(\\alpha, \\beta \\in \\mathbb{R}\\) are such that \\(\\alpha^2< 4\\beta\\). Prove that null \\((T^2+\\alpha T + \\beta I)^k\\) has even dimension for every positive integer \\(k\\).\n\n\nExample 25.14 Suppose \\(V\\) is a real vector space and \\(T\\in \\mathcal{L}(V)\\). Suppose \\(\\alpha, \\beta \\in \\mathbb{R}\\) are such that \\(\\alpha^2< 4\\beta\\) and \\(T^2+\\alpha T+\\beta I\\) is nilpotent. Prove that \\(\\text{dim} V\\) is even and \\((T^2+\\alpha T+\\beta I)^{\\text{dim} V/2}=0.\\)\n\n\nExample 25.15 Prove that if \\(T\\in \\mathcal{L}(\\mathbb{R}^3)\\) and 5, 7 are eigenvalues of \\(T\\), then \\(T\\) has no eigenpairs.\n\n\nExample 25.16 Suppose \\(V\\) is a real vector space with $ V =n $ and \\(T\\in \\mathcal{L}(V)\\) is such that null $T^{n-2}$ null \\(T^{n-1}\\). Prove that if \\(T\\) has at most two distinct eigenvalues and that \\(T\\) has no eigenpairs.\n\n\nExample 25.17 Prove that 1 is an eigenvalue of every square matrix with the property that the sum of the entries in each row equals 1.\n\n\nExample 25.18 Suppose \\(V\\) is a real vector space with $ V =2 $. Prove that if \\[\n\\begin{bmatrix}\na & c \\\\\nb & d\n\\end{bmatrix}\n\\] is the matrix of \\(T\\) with respect to some basis of \\(V\\), then the characteristic polynomial of \\(T\\) equals \\((z-a)(z-d)-b c\\).\n\n\nExample 25.19 Suppose \\(V\\) is a real inner-product space and \\(S\\in \\mathcal{L}(V)\\) is an isometry. Prove that if \\((\\alpha, \\beta)\\) is an eigenpair of \\(S\\), then \\(\\beta=1\\)."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "\\(\n\\newcommand{\\vlist}[2]{#1_1,#1_2,\\ldots,#1_#2}\n\\newcommand{\\vectortwo}[2]{\\begin{bmatrix} #1 \\\\ #2\\end{bmatrix}}\n\\newcommand{\\vectorthree}[3]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3\\end{bmatrix}}\n\\newcommand{\\vectorfour}[4]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4\\end{bmatrix}}\n\\newcommand{\\vectorfive}[5]{\\begin{bmatrix} #1 \\\\ #2 \\\\ #3 \\\\ #4 \\\\ #5 \\end{bmatrix}}\n\\newcommand{\\lincomb}[3]{#1_1 \\vec{#2}_1+#1_2 \\vec{#2}_2+\\cdots + #1_m \\vec{#2}_#3}\n\\newcommand{\\norm}[1]{\\left|\\left |#1\\right|\\right |}\n\\newcommand{\\ip}[1]{\\left \\langle #1\\right \\rangle}\n\\)"
  }
]