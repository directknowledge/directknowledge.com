---
pagetitle: "Inner Products - Learning Linear Algebra"
---
# Inner Products


Recall that the norm of $x\in \mathbb{R}^n$ defined by $\norm{x}=\sqrt{x_1^2+x_2^2}$ is not linear. To injective linearity into the discussion we introduce the dot product: for $x,y\in \mathbb{R}^n$ the \index{dot product} **dot product** of $x$ and $y$ is defined as $x \cdot y=x_1 y_1+\cdots +x_n y_n$. Obviously $x \cdot x=\norm{x}^2$, and with the dot product being so useful, so we generalize the dot product into an inner product on a vector space $V$.

An \index{inner product} **inner product** on $V$ is a function that takes each ordered pair $(u,v)$ of elements of $V$ to a number $\ip{u,v} \in\mathbb{F}$ and has the following properties: 
 
- (\index{positivity}) **positivity** $\ip{ v,v }\geq 0$ for all $v\in V$;
- (\index{definiteness}) **definiteness** $\ip{ v,v }=0$ if and only if $v=0$;
- (\index{additivity} **additivity** in the first slot) $\ip{ u+v,w } = \ip{u,w} +\ip{ v,w }$ for all $u,v,w\in V$;
- (\index{homogeneity} **homogeneity** in the first slot) $\ip{av,w} =a\ip{v,w}$ for all $a\in \mathbb{F}$ and all $v,w,\in V$;
- (\index{conjugate symmetry} **conjugate symmetry**) $\ip{v,w} = \overline{\ip{w,v} }$ for all $v,w\in V$.
 

Recall that for $z\in \mathbb{C}^n$, we define the norm of $z$ by 
$$
\norm{z}=\sqrt{|z_1|^2+\cdots + |z_n|^2}
$$
where the absolute values are needed because we want $\norm{z}$ to be a non-negative number. Then 
\begin{equation}\label{cp}
\norm{z}^2=z_1 \overline{z_1}+\cdots + z_n \overline{z_n}
\end{equation}
because every $\lambda\in\mathbb{C}$ satisfies $|\lambda|^2 =\lambda \overline{\lambda}$.
Since $\norm{z^2}$ is the inner-product of $z$ with itself, as in $\mathbb{R}^n$, the Equation \eqref{cp} suggests that the inner product of $w\in \mathbb{C}^n$ with $z$ should equal 
$$
w_1 \overline{z_1}+\cdots+w_n \overline{z_n}.
$$
We should expect that the inner product of $w$ with $z$ equals the complex conjugate of the inner product of $z$ with $w$, thus motivating the definition of conjugate symmetry. 

An \index{inner-product space} **inner-product space** is a vector space $V$ along with an inner product on $V$.
The inner product defined on $\mathbb{F}^n$ by 
$$
\ip{ (w_1,\ldots,w_n),(z_1,\ldots,z_n) } = w_1 \overline{z_1}+\cdots + w_n \overline{z_n}
$$ 
is called the \index{Euclidean inner product} **Euclidean inner product**.

Continue to let $V$ denote a complex or real vector space. In this section we develop the basic theorems for norms. For $v\in V$, the \index{norm} **norm** of $v$ is defined by $||v||=\sqrt{\ip{v,v} }$. Two vectors $u,v\in V$ are \index{orthogonal} **orthogonal** if $\ip{u,v}= 0$.

::: {#thm- } 
[Pythagorean Theorem]
If $u$ and $v$ are orthogonal vectors in $V$, then 
$$
\norm{u + v}^2 = \norm{u}^2 + \norm{y}^2.
$$
::: 

::: {.proof }
Suppose that $u,v$ are orthogonal vectors in $V$. Then
$$
\norm{u+v}^2=\ip{u+v,u+v}=\norm{u}^2+\norm{v}^2+\ip{u,v}+\ip{v,u}=\norm{u}^2+\norm{v}^2,
$$
as desired.
::: 

::: {#thm- } [Orthogonal Decomposition]
If $v$ is a nonzero vector in $V$, then $u$ can be written as a scalar multiple of $v$ plus a vector orthogonal to $v$.
::: 

::: {.proof }
Let $a\in \mathbb{F}$. Then 
$$
u=a v+(u-av).
$$
Thus we need to choose $a$ so that $v$ is orthogonal to $(u-a v)$. In other words, we want 
$$
0=\ip{u-av,v}=\ip{u,v}-a\ip{v,v}=\ip{u,v}-a\norm{v}^2.
$$
The equation above shows that we should choose $a$ to be $\ip{u,v}/\norm{v}^2$ (assume that $v\ne 0$ to avoid division by 0). Making this choice of $a$, we can write
$$
u=\frac{\ip{u,v}}{\norm{v}^2} v+\left(u-\frac{\ip{u,v}}{\norm{v}^2}v\right).
$$
Thus, if $v\neq 0$ then the equation above writes $u$ as a scalar multiple of $v$ plus a vector orthogonal to $v$.
::: 

::: {#thm- } [Cauchy-Schwarz]
If $u, v \in V$, then 
$$
| \ip{u,v} | \leq \norm{u} \, \norm{v}
$$ 
where equality holds if and only if one of $u, v$ is a scalar multiple of the other.
::: 

::: {.proof }
Let $u,v\in V$. If $v=0$, then both sides and the desired inequality holds. Thus we can assume that $v\neq 0$. Consider the orthogonal decomposition
$$
u=\frac{\ip{u,v}}{\norm{v}^2}v+w
$$
where $w$ is orthogonal to $v$. By the Pythagorean theorem, 
\begin{align*}
\norm{u}^2
=\norm{\frac{\ip{u,v}}{\norm{v}^2} v}^2+\norm{w}^2\\
=\frac{|\ip{u,v}|^2}{\norm{v}^2}+\norm{w}^2\\
\geq \frac{|\ip{u,v}|^2}{\norm{v}^2}
\end{align*}
Multiplying both sides by $\norm{v}^2$ and then taking square roots gives the Cauchy-Schwarz inequality.
Notice that there is equality if and only if $w=0$, that is, if and only if $u$ is a multiple of $v$. 
::: 

::: {#thm- } [Triangular Inequality]
If $u, v \in V$, then 
$$
\norm{u+v}\leq \norm{u}+\norm{v}
$$ 
where equality holds if and only if one of $u, v$ is a nonnegative multiple of the other.
::: 

::: {.proof }
Let $u,v\in V$. Then
\begin{align}
\norm{u+v}^2 \notag
&=\ip{u+v,u+v}  \notag\\
&=\ip{u,u}+\ip{v,v}+\ip{u,v}+\ip{v,u}  \notag \\
&=\ip{u,u}+\ip{v,v}+\ip{u,v}+\overline{\ip{u,v}}  \notag \\
&=\norm{u,u}^2+\norm{v,v}^2+2 \text{remark}\ip{u,v}  \notag \\
&\leq \norm{u,u}^2+\norm{v,v}^2+2 | \ip{u,v} | \label{ti1} \\
&\leq \norm{u,u}^2+\norm{v,v}^2+2 \norm{u}\norm{v} \label{ti2}\\
&= \left(\norm{u}+\norm{v}\right)^2  \notag
\end{align}
and so by taking square root of both sides yields the triangular inequality. This proof shows that the triangle inequality is an equality if and only if we have equality in \eqref{ti1} and \eqref{ti2}. Thus we have equality in the triangular inequality if and only if 
\begin{equation}\label{ti3}
\ip{u,v}=\norm{u}\norm{v}.
\end{equation}
If one of $u,v$ is a nonnegative multiple of the other, then \eqref{ti3} holds. Conversely, suppose \eqref{ti3} holds. The the condition for equality in the Cauchy-Schwarz inequality implies that one of $u,v$ must be a scalar multiple of the other. Clearly, then \eqref{ti3} forces the scalar in question to be nonnegative, as desired. 
::: 

::: {#thm- } [Parallelogram Equality]
If $u, v \in V$, then 
$$
\norm{u+v}^2+\norm{u-v}^2= 2 \left( \norm{u}^2+\norm{v}^2 \right).
$$
::: 

::: {.proof }
If $u, v \in V$, then  
\begin{align*}
\norm{u+v}^2+\norm{u-v}^2 
&= \ip{u+v,u+v}+\ip{u-v,u-v} \\
& = \norm{u}^2+\norm{v}^2+\ip{u,v}+\ip{v,u}+\norm{u}^2+\norm{v}^2-\ip{u,v}-\ip{v,u} \\
& =2 \left( \norm{u}^2+\norm{v}^2 \right )
\end{align*}
as desired.
::: 

A list of vectors is called \index{orthonormal} **orthonormal**  if the vectors in it are pairwise orthogonal and each vector has norm 1.
 
::: {#thm- } 
If $(e_1,\ldots,e_m)$ is an orthonormal list of vectors in $V$, then 
$$
\norm{a_1 e_1+\cdots +a_m e_m}^2=|a_1|^2+\cdots + |a_n|^2
$$
for all $a_1,\ldots,a_m\in\mathbb{F}$.
::: 

::: {.proof }
Because each $e_j$ has norm 1, this follows easily from repeated by application of the Pythagorean theorem. 
::: 

::: {#thm- } \label{oind}
Every orthonormal list of vectors is linearly independent.
::: 

::: {.proof }
Suppose $(e_1,\ldots,e_n)$ is an orthonormal list of vectors in $V$ and $a_1,\ldots,a_n\in \mathbb{F}$ are such that $a_1 e_1+\cdots + a_n e_n=0$. Then $|a_1|^2+\cdots + |a_n|^2=0$, which means that all the $a_j$'s are 0, as desired.
::: 

An \index{orthonormal basis} **orthonormal basis**  of $V$ is an orthonormal list of vectors in $V$ that is also a basis of $V$.
 
The importance of orthonormal bases stems mainly from the following proposition. 

::: {#thm- } \label{ob1}
Suppose $(e_1,\ldots,e_n)$ is an orthonormal basis of $V$. Then 
\begin{equation} \label{inim1}
v=\ip{ v,e_1 } e_1+\cdots + \ip{ v,e_n } e_n
\end{equation}
and
\begin{equation}\label{inim2}
\norm{v}=|\ip{ v,e_1 } |^2+\cdots + |\ip{ v,e_n } |^2
\end{equation}
for every $v\in V$.
::: 

::: {.proof }
Let $v\in V$. Because $(e_1,\ldots,e_n)$ is a basis of $V$, there exist scalars $a_1,\ldots,a_n$ such that $v=a_1 e_1+\cdots + a_n e_n$. Take the inner product of both sides of this equation with $e_j$, getting $\ip{v,e_j}=a_j$. Thus \eqref{inim1} holds. Clearly \eqref{inim2} holds by \eqref{inim1} and \eqref{innorm}.
::: 

::: {#thm- } [Gram-Schmidt]
If $(v_1,\ldots,v_m)$ is a linearly independent list of vectors in $V$, then there exists an orthonormal list $(e_1,\ldots,e_m)$ of vectors in $V$ such that 
\begin{equation} \label{gmeq}
\text{span}(v_1,\ldots,v_j)=\text{span}(e_1,\ldots,e_j)
\end{equation}
for $j=1,\ldots,m$
::: 

::: {.proof }
Suppose $(v_1,\ldots,v_m)$ is a linearly independent list of vectors in $V$. To construct the $e$'s, start by setting $e_1=\frac{v_1}{\norm{v_1}}$. This satisfies \eqref{gmeq} for $j=1$. We will choose $e_2,\ldots e_m$ inductively, as follows. Suppose $j>1$ and an orthonormal list $(e_1,\ldots,e_{j-1})$ has been chosen so that 
\begin{equation}\label{gspan}
\text{span}(v_1,\ldots,v_{j-1})=\text{span}(e_1,\ldots,e_{j-1}).
\end{equation}
Let 
\begin{equation}\label{gsproj}
e_j=\frac{v_j-\ip{v_j,e_1}e_1-\cdots - \ip{v_j,e_{j-1}} e_{j-1} }{\norm{v_j-\ip{v_j,e_1}e_1-\cdots - \ip{v_j,e_{j-1}} e_{j-1}}}.
\end{equation}
Note that $v_j \not\in \text{span}(v_1,\ldots,v_{j-1})$ (because $(v_1,\ldots,v_m)$ is linearly independent) and thus $v_j\not \in \text{span}(e_1,\ldots,e_{j-1})$. Hence we are not dividing by 0 in the equation above, and so $e_j$ is well-defined. Dividing a vector by its norm produces a new vector with norm 1; thus $\norm{e_j}=1$. 

Let $1\leq k <j$. Then 
\begin{align*}
\ip{e_j,e_k}
&=\ip{\frac{v_j-\ip{v_j,e_1}e_1-\cdots - \ip{v_j,e_{j-1}} e_{j-1} }{\norm{v_j-\ip{v_j,e_1}e_1-\cdots - \ip{v_j,e_{j-1}} e_{j-1}}},e_k} \\
&= \frac{\ip{v_j,e_k}-\ip{v_j,e_k}\ip{e_k,e_k}}{\norm{v_j-\ip{v_j,e_1}e_1-\cdots - \ip{v_j,e_{j-1}} e_{j-1}}} \\
&=0.
\end{align*}
Thus $(e_1,\ldots,e_j)$ is an orthonormal list. 

From \eqref{gsproj}, we see that $v_j\in \text{span}(e_1,\ldots,e_j)$. Combining this information with \eqref{gspan} shows that 
$$
\text{span}(v_1,\ldots,v_{j-1})\subset \text{span}(e_1,\ldots,e_{j}).
$$
Both lists above are linearly independent (the $v$'s by hypothesis, the $e$'s by orthonormality and \eqref{oind}. Thus both subspaces above have dimension $j$, and hence must be equal, completing the proof.
::: 

::: {#thm- } 
Every finite-dimensional inner-product space has an orthonormal basis.
::: 

::: {.proof }
Choose a basis of $V$. Apply the Gram-Schmidt procedure to it, producing an orthonormal list. This list is linearly independent and it spans $V$. Thus it is an orthonormal basis. 
::: 

::: {#thm- } 
Every orthonormal list of vectors in $V$ can be extended to an orthonormal basis of $V$.
::: 

::: {.proof }
Suppose $(e_1, \ldots ,e_m)$ is an orthonormal list of vectors in $V$. Then $(e_1, \ldots , e_m)$ is linearly independent and so can be extended to a basis 
$$
\mathcal{B}=(e_1, \ldots, e_m, v_1, \ldots, v_n)
$$ 
of $V$. Now apply the Gram-Schmidt procedure to $\mathcal{B}$ producing an orthonormal list $(e_1,\ldots,e_m,f_1,\ldots,f_n)$; here the Gram-Schmidt procedure leaves the first $m$ vectors unchanged because they are already orthonormal. Clearly $\mathcal{B}$ is an orthonormal basis of $V$ because it is linearly independent and its span equals $V$. Hence we have our extension of $(e_1,\ldots,e_m)$ to an orthonormal basis of $V$.
::: 

Recall that if $V$ is a complex vector space, then for each operator on $V$ there is a basis with respect to which the matrix of the operator is upper-triangular. Now for inner-product spaces we would like to know the same question. 

::: {#thm- } \label{outm}
Suppose $T\in\mathcal{L}(V)$. If $T$ has an upper-triangular matrix with respect to some basis of $V$, then $T$ has an upper-triangular matrix with respect to some orthonormal basis of $V$.
::: 

::: {.proof }
Suppose $T$ has upper-triangular matrix with respect to some basis $(v_1,\ldots,v_n)$ of $V$. Thus $\text{span}(v_1,\ldots,v_j)$ is invariant under $T$ for each $j=1,\ldots,n$. Apply the Gram-Schmidt procedure to $(v_1,\ldots,v_n)$, producing an orthonormal basis $(e_1,\ldots,e_n)$ of $V$. Because 
$$
\text{span}(e_1,\ldots,e_j)=\text{span}(v_1,\ldots,v_j)
$$
for each $j$, we conclude that $\text{span}(e_1,\ldots,e_j)$ is invariant under $T$ for each $j=1,\ldots,n$. Thus, by \eqref{utm}, $T$ has an upper-triangular matrix with respect to the orthonormal basis $(e_1,\ldots,e_n)$.
::: 

::: {#thm- } 
Suppose $V$ is a complex vector space and $T\in \mathcal{L}(V)$. Then $T$ has an upper-triangular matrix with respect to some orthonormal basis of $V$.
::: 

::: {.proof }
This follows immediately from \eqref{cutm} and \eqref{outm}.
::: 

If $U$ is a subset of an inner-product space $V$, then the \index{orthogonal complement} **orthogonal complement** of $U$ is defined as $U^\bot=\{v\in V\, : \, \ip{v,u} =0 \text{ for all } u\in U\}.$

::: {#thm- } [Orthogonal Decomposition]
\label{ip-orthogonal-decomposition}
If $U$ is a subspace of an inner-product space $V$, then $v=U\oplus U^\perp$.
::: 

::: {.proof }
Suppose that $U$ is a subspace of $V$. First we will show that 
\begin{equation}\label{sumfirst}
V=U + U^\perp.
\end{equation}
To do this, suppose $v\in V$. Let $(e_1,\ldots,e_m)$ be an orthonormal basis of $U$. Obviously, 
$$
v=\underbrace{ \ip{v,e_1}e_1+\cdots +\ip{v,e_m}e_m}_u+\underbrace{v-\ip{v,e_1}e_1-\cdots -\ip{v,e_m}e_m}_w.
$$
Clearly, $u\in U$. Because $(e_1,\ldots,e_m)$ is an orthonormal list, for each $j$ we have
$$
\ip{w,e_j}=\ip{v,e_j}-\ip{v,e_j}=0.
$$
Thus $w$ is orthogonal to every vector in $\text{span}(e_1,\ldots,e_m)$. In other words, $w\in U^\perp$, completing the proof of \eqref{sumfirst}. 

If $v\in U\cap U^\perp$, then $v$ (which is in $U$) is orthogonal to every vector in $U$ (including $v$ itself), which implies that $\ip{v,v}=0$, which implies that $v=0$. Thus
\begin{equation}\label{orthogonalss}
U\cap U^\perp=\{0\}.
\end{equation}
Now \eqref{sumfirst} and \eqref{orthogonalss} imply that $U\oplus U^\perp$.
::: 

::: {#thm- } 
If $U$ is a subspace of an inner-product space $V$, then $U=(U^\perp)^\perp$.
::: 

::: {.proof }
Suppose that $U$ is a subspace of $V$. First we will show that 
\begin{equation}\label{subsetorth}
U\subseteq (U^\perp)^\perp.
\end{equation}
To do this, suppose that $u\in U$. Then $\ip{u,v}=0$ for every $v\in U^\perp$ (by definition of $U^\perp$). Because $u$ is orthogonal to every vector in $U^\perp$, we have $u\in (U^\perp)^\perp$, completing the proof of \eqref{subsetorth}.

To prove the inclusion in the other direction, suppose $v\in (U^\perp)^\perp$. By \eqref{ip-orthogonal-decomposition}, we can write $v=u+w$, where $u\in U$ and $w\in U^\perp$. We have $v-u=w\in U^\perp$. Because $v\in (U^\perp)^\perp$ and $u\in(U^\perp)^\perp$ (from \eqref{subsetorth}), we have $v-u\in (U^\perp)^\perp$. Thus $v-u\in U^\perp\cap (U^\perp)^\perp$, which implies that $v=u$, which implies that $v\in U$. Thus $(U^\perp)^\perp \subseteq U$, which along with \eqref{subsetorth} completes the proof. 
::: 

Let $V=U \oplus U ^\bot$ and for $v\in V$ let $v=u+w$ where $w\in U ^\bot$. Then $u$ is called the \index{orthogonal projection} **orthogonal projection**  of $V$ onto $U$ and is denoted by $P_U v$.

::: {#thm- } 
If $U$ is a subspace of an inner-product space $V$ and $v\in V$. Then $\norm{v-P_U v}\leq \norm{v-u}$ for every $u\in U$. Furthermore, if $u\in U$ and the inequality above is an equality; then $u=P_U v$.
::: 

::: {.proof }
Suppose $u\in U$. Then 
\begin{align}
\norm{v-P_U v}^2 
& \leq \norm{v-P_Uv}+ \norm{P_U v-u}^2 \label{mp1} \\
& = \norm{v-P_U v+P_Uv-u}^2 = \norm{v-u}^2   \label{mp2}
\end{align}
where \eqref{mp2} comes from the Pythagorean theorem, which applies because $v-P_U v\in U^\perp$ and $P_U v-u\in U$. Taking the square root gives the desired inequality. The inequality is an equality if and only if \eqref{mp1} is an equality, which happens if and only if 
$\norm{P_U v-u}=0$, which happens if and only if $u=P_u v$.
::: 

::: {#exm- } 
Show that if $c_1,\ldots,c_n$ are positive numbers, then 
$$
\ip{ (w_1,\ldots,w_n),(z_1,\ldots,z_n) } = c_1 w_1 \overline{z_1}+\cdots + c_n w_n \overline{z_n}
$$
defines an inner product on $\mathbb{F}^n$.
::: 

::: {#exm- } 
Show that if $p,q\in \mathcal{P}_m(\mathbb{F})$, then 
$$
\ip{ p ,q } = \int_0^1 p(x)\overline{q(x)}dx
$$
is an inner product on the vector space $\mathcal{P}_m(\mathbb{F})$.
::: 

::: {#exm- } 
Show that every inner product is a linear map in the first slot, as well as a linear map in the second slot. 
::: 

::: {#exm- } 
If $v\in V$ and $a\in \mathbb{F}$, then $\norm{av}=|a| \norm{v}$, and if $v$ is nonzero then $u=\frac{1}{\norm{v}} v$ is a unit vector.
Since $\norm{a v}^2=\ip{av,av} =a \overline{a}  \ip{v,v} =|a|^2 \norm{v}^2$, taking square roots provides 
$\norm{a v}=|a| \norm{v}$. 
::: 

::: {#exm- } 
Prove that if $x, y$ are nonzero vectors in $\mathbb{R}^2$, then 
$$
\ip{x,y} =\norm{x}\norm{y} \cos \theta,
$$
where $\theta$ is the angle between $x$ and $y$.
The law of cosines gives
$$
\norm{x-y}^2=\norm{x}^2+\norm{y}^2-2\norm{x}\norm{y} \cos \theta.
$$
The left hand side of this equation is 
$$
\norm{x-y}^2=(x-y)\cdot (x-y)=\norm{x}^2-2 (x \cdot y) +\norm{y}^2
$$
so
$$
x\cdot y= \norm{x}\norm{y}\cos \theta.
$$
::: 

::: {#exm- } 
Suppose $u,v \in V$. Prove that $\ip{ u,v } =0$ if and only if $||u||\leq ||u+a v||$ for all $a\in F$.
If $\ip{u,v}=0$, then by the Pythagorean theorem
$$
\norm{u+\alpha v}^2=\norm{u}^2+\norm{\alpha v}^2\geq \norm{u}.
$$
Conversely, we will prove the contrapositive, that is we will prove: if $\ip{u,v}\neq0$ then there exists $a \in \mathbb{F}$ such that $\norm{u}>\norm{u+av}$. Suppose $\ip{u,v}\neq 0$ then $u$ and $v$ are both nonzero vectors. By the orthogonal decomposition, we can write 
\begin{equation} \label{ex3}
u=\alpha v+w
\end{equation}
for some $\alpha \in \mathbb{F}$ and where $\ip{w,v}=0$.  Notice $\alpha \neq 0$ since $\ip{u,v}\neq 0$. Since $v$ and $w$ are orthogonal 
$$
\norm{u}^2=|\alpha|^2\norm{v}^2+\norm{w}^2
$$
Let $a=-\alpha$. Then by equation \eqref{ex3}
$$
\norm{u+a v}^2=\norm{w}^2
$$
and so 
$$
\norm{u}^2=|a|^2\norm{v}^2 +\norm{u+a v}^2 >  \norm{u+a v}^2
$$
which implies
$$
\norm{u}>\norm{u+a v}
$$
as desired.
::: 

::: {#exm- } 
Prove that
$$
\left (\sum_{k=1}^{n} a_k b_k \right )^2 
\leq \left ( \sum_{k=1}^n k a_k^2 \right ) \left ( \sum_{k=1}^n \frac{b_k^2}{k}\right )
$$
for all real numbers $a_1,\ldots,a_n$ and $b_1,\ldots,b_n$.
This is a simple trick. 
$$
\left (\sum_{k=1}^{n} a_k b_k \right )^2 
= \left ( \sum_{k=1}^n \sqrt{k} a_k \frac{b_k}{\sqrt{k}} \right )^2
\leq \left ( \sum_{k=1}^n k a_k^2 \right ) \left ( \sum_{k=1}^n \frac{b_k^2}{k}\right )
$$
where the last inequality is from the Cauchy-Schwarz inequality.
::: 

::: {#exm- } 
Suppose $u, v\in V$ are such that $||u||=3$, $||u+v||=4$, and $||u-v||=6$. What number must $||v||$ equal?
Using the parallelogram equality
$$
\norm{u+v}^2+\norm{u-v}^2=2 \left(\norm{u}^2+\norm{v}^2\right)
$$ 
to get
$$
16+36=2(9+\norm{v}^2), \qquad \norm{v}=\sqrt{17}.
$$
::: 

::: {#exm- } 
Prove or disprove: there is an inner product on $\mathbb{R}^2$ such that the associated norm is given by $||(x_1,x_2)||=|x_1|+|x_2|$ for all $(x_1,x_2)\in \mathbb{R}^2$.
There is no such inner product. Take for instance,
$$
u=(1/4,0), \qquad v=(0,3/4), \qquad u+v=(1/4,3/4).
$$
Then we have equality in the triangular inequality
$$
1=\norm{u+v}\leq \norm{u}+\norm{v}=1/4+3/4.
$$
By the triangular inequality, we must have $u=a v$ or $v=a v$, with $a\geq 0$. But clearly no such $a\in \mathbb{F}$ exists.
::: 

::: {#exm- } 
Prove that if $V$ is a real inner-product space, then 
$$
\ip{ u,v } =\frac{||u+v||^2-||u-v||^2}{4}
$$ 
for all $u,v\in V$.
Expressing the norms as inner products
\begin{align*}
%\ip{ u,v } 
\frac{\norm{u+v}^2- \norm{u-v}^2}{4} 
&=\frac{\ip{u+v,u+v}-\ip{u-v,u-v} }{4} \\
&=\frac{\ip{u,u}+\ip{v,v}+\ip{u,v}+\ip{v,u} - \ip{u,u}-\ip{v,v}+\ip{u,v}+\ip{v,u}}{4} \\
&=\frac{2\ip{u,v}+2\ip{v,u} }{4} \\
&=\frac{2\ip{u,v}+2\ip{u,v} }{4} \quad \text{(because $V$ is a real inner product space)} \\
&=\ip{u,v}
\end{align*}
as desired.
::: 

::: {#exm- } 
Prove that if $V$ is a complex inner-product space, then
$$
\ip{ u,v } 
$$
is
$$
\frac{||u+v||^2-||u-v||^2 + ||u+i v||^2 i - ||u-i v||^2 i}{4}
$$
for all $u,v\in V$.
::: 

::: {#exm- } 
A norm on a vector space $U$ is a function $||\text{  }|| : U\rightarrow [0,\infty)$ such that $||u||=0$ if and only if $u=0$, $||\alpha u ||= |\alpha | ||u||$ for all $\alpha \in F$ and all $u \in U$, and $||u+v|| \leq ||u||+||v||$ for all $u,v \in U$. Prove that a norm satisfying the parallelogram equality comes from an inner product (in other words, show that if $|| \text{  }||$ is a norm on $U$ satisfying the parallelogram equality, then there is an inner product $\ip{ \text{ } , \text{ } }$ on $U$ such that $||u||=\ip{ u, u } ^{1/2}$ for all $u \in U$).
::: 

::: {#exm- } 
Suppose $n$ is a positive integer. Prove that
$$
\left (
\frac{1}{\sqrt{2\pi}},\frac{\sin x}{\sqrt{\pi}},\frac{\sin 2x}{\sqrt{\pi}},\ldots,\frac{\sin n x}{\sqrt{\pi}}, \frac{\cos x}{\sqrt{\pi}},\frac{\cos 2x}{\sqrt{\pi}},\ldots,\frac{\cos n x}{\sqrt{\pi}}
\right )
$$
is an orthonormal list of vectors in $\mathcal{C}[-\pi,\pi]$, the vector space of continuous real-valued functions on $[-\pi,\pi]$ with inner product
$$
\ip{ f,g } = \int_{-\pi}^{\pi} f(x)g(x) \, d x .
$$
Computation of these integrals is based on the product-to-sum formulas from trigonometry:
\begin{align*}
\sin(A)\sin(B)&=\frac{1}{2}\cos(A-B)-\frac{1}{2}\cos(A+B) \\
\cos(A)\cos(B)&=\frac{1}{2}\cos(A-B)+\frac{1}{2}\cos(A+B) \\
\sin(A)\cos(B)&=\frac{1}{2}\sin(A-B)+\frac{1}{2}\sin(A+B).
\end{align*}
Here is a sample computation, valid for $m,n=1,2,3,,\ldots$ when $m\neq n$.
\begin{align*}
\ip{\sin(mx),\cos(nx)}
&= \int_{-\pi}^{\pi}\sin(mx)\cos(nx) \, dx \\
&= \int_{-\pi}^{\pi}\left(\frac{1}{2}\sin((m-n)x)+\frac{1}{2}\sin((m+n)x)\right)\, dx \\
&= \left. \frac{-1}{2(m-n)}\cos\left((m-n)x\right)+\frac{-1}{2(m+n)}\cos\left((m+n)x\right) \right|^{\pi}_{-\pi}, \\
&= \frac{-1}{2(m-n)}[-1-(-1)]+\frac{-1}{2(m-n)}[-1-(-1)] \\
&=0.
\end{align*}
::: 

::: {#exm- } 
On $\mathcal{P}_2(\mathbb{R})$, consider the inner product given by
$$
\ip{ p,q } = \int_o^1 p(x) q(x) \, d x.
$$
Apply the Gram-Schmidt procedure to the basis $(1,x,x^2)$ to produce an orthonormal basis of  $\mathcal{P}_2(\mathbb{R})$.
Computing $e_1$: A calculation gives
$$
\ip{1,1}=\int_0^1 1 \, dx=1,
$$
so $e_1=1$. Computing $e_2$: A calculation gives
$$
\ip{x,1}=\int_0^1 x \, dx =\frac{1}{2}.
$$
Let 
$$
f_2=x-1/2.
$$
Then
$$
\norm{f_2}^2=\ip{x-1/2,x-1/2}=\int_0^1\left(x^2-x+\frac{1}{4}\right)\, dx =\frac{1}{12},
$$
so 
$$
e_2=\frac{f_2}{\norm{f_2}}=2\sqrt{3}\left(x-\frac{1}{2}\right).
$$
Computing $e_3$: A calculation gives 
$$
\ip{x^2,1}=\int_0^1 x^2 \, dx =\frac{1}{3},
$$
and 
$$
\ip{x^2,2\sqrt{3}\left(x-\frac{1}{2}\right)}=2\sqrt{3}\int_0^1\left(x^3-\frac{x^2}{2}\right) \, dx=\frac{1}{2\sqrt{3}}.
$$
Let
$$
f_3=x^2-\frac{1}{2\sqrt{3}}\left(x-\frac{1}{2}\right)-\frac{1}{3}=x^2-x+\frac{1}{6}.
$$
Then 
$$
\norm{f_3}^2=\int_0^1\left(x^2-x+\frac{1}{6}\right)^2 \, dx
$$
and 
$$
e_3=\frac{f_3}{\norm{f_3}}.
$$
::: 

::: {#exm- } 
What happens if the Gram-Schmidt procedure is applied to a list of vectors that is not linearly independent?
By examining the proof, notice that the numerator in the Gram-Schmidt formula is the difference between $v_j$ and the orthogonal projection $P_u$ of $v_j$ onto the subspace 
$$
U=\text{span}(v_1,\ldots,v_{j-1})=\text{span}(e_1,\ldots,e_{j-1}).
$$
If $v_j\in U$, then $v_j-P_U v_j=0$, so the numerator has norm 0 and division by the denominator is not defined. The algorithm can be adapted to handle this case by testing for 0 in the denominator. If 0 is found, throw $v_j$ out of the list and continue. The result will be an orthonormal basis for $\text{span}(v_1,\ldots,v_N)$.
::: 

::: {#exm- } 
Suppose $V$ is a real inner-product space and $(v_1,\ldots,v_m)$ is a linearly independent list of vectors in $V$. Prove that there exist exactly $2^m$ orthonormal lists $(e_1,\ldots,e_m)$ of vectors in $V$ such that span $(v_1,\ldots,v_j)=$ span $(e_1,\ldots,e_j)$ for all $j\in \{1,\ldots,m\}$.
::: 

::: {#exm- } 
Suppose $(e_1,\ldots,e_M)$ is an orthonormal list of vectors in $V$. Let $v \in V$. Prove that 
$$
||v||^2=\sum_{n=1}^M \left|\ip{v,e_n}\right|^2
$$ 
if and only if $v \in $ span $(e_1,\ldots,e_M)$.
Extend $(e_1,\ldots,e_M)$ to an orthonormal basis  for $V$. Then
$$
\sum_{n=1}^N\ip{v,e_n} e_n
$$
and
$$
\norm{v}^2=\sum_{n=1}^N \left|\ip{v,e_n}\right|^2.
$$
If $v\in \text{span}(e_1,\ldots,e_M)$ then $\ip{v,e_n}=0$  for $n>M$, and 
$$
\norm{v}^2=\sum_{m=1}^M \left|\ip{v,e_n}\right|^2.
$$
If $v \not\in\text{span}(e_1,\ldots,e_M)$ then for some $n>M$ we have $\ip{v,e_n}\neq 0$. This gives 
$$
\norm{v}^2=\sum_{n=1}^N\left|\ip{v,e_n}\right|^2>\sum_{m=1}^M\left|v,e_n\right|^2.
$$
::: 

::: {#exm- } 
Find an orthonormal basis of  $\mathcal{P}_2(\mathbb{R})$, such that the differentiation operator on  $\mathcal{P}_2(\mathbb{R})$ has an upper-triangular matrix with respect to this basis.
::: 

::: {#exm- } 
Suppose $U$ is a subspace of $V$. Prove that $\text{dim}  U^{\bot} = \text{dim}  V - \text{dim}  U$.
::: 

::: {#exm- } 
Suppose $U$ is a subspace of $V$. Prove that $U^{\bot} = \{0\}$ if and only if $U=V$.
If $U=V$ and $v\in U^\perp$ then $v\in U\cap U^\perp$, and $\ip{v,v}=0$, so $v=0$.
Therefore, $V=U\oplus U^\perp$. If $U^\perp=\{0\}$, then $U=V$.
::: 

::: {#exm- } 
Prove that if $P\in \mathcal{L}(V)$ is such that $P^2=P$ and every vector in null $P$ is orthogonal to every vector in range $P$, then $P$ is an orthogonal projection.
::: 

::: {#exm- } 
Prove that if $P\in \mathcal{L}(V)$ is such that $P^2=P$ and $|| P v ||\leq ||v||$ for every $v\in V$, then $P$ is an orthogonal projection.
::: 

::: {#exm- } 
Suppose $T\in \mathcal{L}(V)$ and $U$ is a subspace of $V$. Prove that $U$ is invariant under $T$ if and only if $P_U T= T P_U$.
::: 

::: {#exm- } 
Suppose $T\in \mathcal{L}(V)$ and $U$ is a subspace of $V$. Prove that $U$ and $U^\bot$ are both invariant under $T$ if and only if $P_U T= T P_U$.
::: 

::: {#exm- } 
In $\mathbb{R}^4$, let $U=\text{span} \left ( (1,1,0,0),(1,1,1,2) \right )$. Find $u\in U$ such that $||u-(1,2,3,4)||$ is as small as possible.
in $\mathbb{R}^4$ let $U=\text{span}((1,1,0,0),(1,1,1,2)).$
Find $u\in U$ such that $\norm{u-(1,2,3,4)}$ is as small as possible. We want the orthogonal projection $P_U(1,2,3,4)$. Notice that $U=\text{span}((1,1,0,0),(0,0,1,2)).$
An orthonormal basis for $U$ is
$$
\left( \frac{1}{\sqrt{2}},\frac{1}{\sqrt{2}} ,0,0\right ),\left(0,0,\frac{1}{\sqrt{5}},\frac{2}{\sqrt{5}} \right)
$$
Thus the desired vector is 
$$
P_U(1,2,3,4)=\left(\frac{3}{2},\frac{3}{2},0,0\right)+\left(0,0,\frac{11}{5},\frac{2}{5}\right).
$$
::: 

::: {#exm- } 
Find a polynomial $p\in \mathcal{P}_3(\mathbb{R})$ such that $p(0)=0$, $p'(0)=0$, and 
$$
\int_0^1 |2+3x-p(x) |^2 \, dx
$$ 
is as small as possible.
::: 

::: {#exm- } 
Find a polynomial $p\in \mathcal{P}_5(\mathbb{R})$ that makes 
$$
\int_{-\pi}^{\pi} |\sin x - p(x) |^2 \, dx
$$ 
is as small as possible.
::: 

::: {#exm- } 
Find a polynomial $p\in \mathcal{P}_2(\mathbb{R})$ such that 
$$
\phi(p)=p \left( \frac{1}{2} \right) = \int_{0}^{1} p(x) \, q(x) \, dx
$$ 
for every $p\in \mathcal{P}_2(\mathbb{R})$.
Here is the direct approach. Every $q\in\mathcal{P}_2(\mathbb{R})$ can be expressed as 
$$
\alpha+\beta(x-1/2)+\gamma(x-1/2)^2.
$$
The desired polynomial $q$ must satisfy
$$
p(x)=1: \qquad \phi(1)=p(1/2)=1=\int_0^1\left( \alpha+\beta(x-1/2)+\gamma(x-1/2)^2 \right) \, dx = \alpha+\gamma \frac{1}{12}.
$$
Moving to $p(x)=x-1/2$ we find 
\begin{align*}
p(x)&=x-1/2: \qquad \phi(x-1/2)=p(1/2)=0 \\
&=\int_0^1(x-1/2)[\alpha+\beta(x-1/2)+\gamma(x-1/2)^2] \, dx =\beta \frac{1}{12},
\end{align*}
so $\beta=0$. Finally
$$
p(x)=(x-1/2)^2: \qquad \phi((x-1/2)^2)=p(1/2)=0
$$
$$
=\int_0^1(x-1/2)^2[\alpha+\beta(x-1/2)+\gamma(x-1/2)^2] \, dx =\alpha \frac{1}{12}+\gamma\frac{1}{80}.
$$
Solving gives 
$$
\alpha=\frac{27}{12}, \qquad  \beta=0, \qquad \gamma=-15.
$$
Thus
$$
q(x)=\frac{27}{12}-15(x-1/2)^2.
$$
::: 

::: {#exm- } 
Find a polynomial $q\in \mathcal{P}_2(\mathbb{R})$ such that 
$$
\phi(p)=\int_{0}^{1} p(x) \, (\cos \pi x)  \, dx= \int_{0}^{1} p(x) \, q(x) \, dx
$$ 
for every $p\in \mathcal{P}_2(\mathbb{R})$.
Taking the same approach as in the previous example. We compute
$$
p(x)=1: \qquad \phi(1)=\int_0^1\cos(\pi x) \, dx=0
$$
$$
=\int_0^1[\alpha+\beta(x-1/2)+\gamma(x-1/2)^2]\,dx=\alpha+\gamma\frac{1}{12}.
$$
Moving to $p(x)=x-1/2$ we find
$$
p(x)=x-1/2: \qquad \phi(x-1/2)=\int_0^1(x-1/2)\cos(\pi x)\, dx=\int_0^1 x \cos (\pi x) \, dx
$$
$$
=-\frac{2}{\pi^2}=\int_0^1(x-1/2)[\alpha+\beta(x-1/2)+\gamma(x-1/2)^2]\, dx=\beta\frac{1}{12},
$$
so $\beta=\frac{-24}{\pi^2}$.
Finally, since $\cos(\pi x)$ is odd about $x=1/2$,
$$
p(x)=(x-1/2)^2: \qquad \phi((x-1/2)^2)=\int_0^1(x-1/2)^2\cos(\pi x)\, dx =0
$$
$$
=\int_0^1(x-1/2)^2[\alpha+\beta(x-1/2)+\gamma(x-1/2)^2]=\alpha \frac{1}{12} +\gamma\frac{1}{80}.
$$
Solving gives
$$
\alpha=\gamma=0, \qquad \beta=\frac{-24}{\pi^2}.
$$
Thus
$$
q(x)=\frac{-24}{\pi^2}(x-1/2).
$$
::: 

::: {#exm- } 
Give an example of a real vector space $V$ and $T\in \mathcal{L}(V)$ such that trace$(T^2) < 0$.
::: 

::: {#exm- } 
Suppose $V$ is a real vector space, $T\in \mathcal{L}(V)$, and $V$ has a basis consisting of eignvectors of $T$. Prove that trace$(T^2)\geq 0$.
::: 

::: {#exm- } 
Suppose $V$ is an inner-product space and $v, w\in V$. Define $T\in \mathcal{L}(V)$ by $T u=\langle u,v \rangle w$. Find a formula for trace $T$.
::: 

::: {#exm- } 
Prove that if $P\in \mathcal{L}(V)$ satisfies $P^2=P$, then trace $P$ is a nonnegative integer.
::: 

::: {#exm- } 
Prove that if $V$ is an inner-product space and $T\in \mathcal{L}(V)$, then trace $T^*=\overline{\text{trace} T}.$
::: 

::: {#exm- } 
Suppose $V$ is an inner-product space and $T\in \mathcal{L}(V)$ is a positive operator with trace $T=0$, then $T=0$.
::: 

::: {#exm- } 
Suppose $T\in \mathcal{L}(\mathbb{C}^3)$ is the operator whose matrix is
$$
\begin{bmatrix} 
51 & -12 & -21 \\
60 & -40 & -28 \\
57 & -68 & 1
\end{bmatrix} .
$$
If $-48$ and $24$ are eigenvalues of $T$, find the third eigenvalue of $T$.
::: 

::: {#exm- } 
Prove or give a counterexample: if $T\in \mathcal{L}(V)$ and $c\in F$, then trace$(c T ) = c$ trace $T$.
::: 

::: {#exm- } 
Prove or give a counterexample,: if $S, T\in \mathcal{L}(V)$, then trace $(S T)=(\text{trace} S)(\text{trace} T)$.
::: 

::: {#exm- } 
Suppose $T\in \mathcal{L}(V)$. Prove that if $\text{trace} (ST)=0$ for all $S\in \mathcal{L}(V)$, then $T=0$.
::: 

::: {#exm- } 
Suppose $V$ is an inner-product space and $T\in \mathcal{L}(V)$. Prove that if $(e_1,\ldots.,e_n)$ is an orthonormal basis of $V$, then
$$
\text{trace}(T^* T)=|| T e_1||^2+\cdots + ||T e_n||^2.
$$
Conclude that the right side of the equation above is independent of which orthonormal basis $(e_1,\ldots,e_n)$ is chosen for $V$.
::: 

::: {#exm- } 
Suppose $V$ is a complex inner-product space and $T\in \mathcal{L}(V)$. Let $\lambda_1,\ldots.,\lambda_n$ be the eigenvalues of $T$, repeated according to multiplicity.
::: 

::: {#exm- } 
Suppose
$$
\begin{bmatrix} 
a_{1,1} & \cdots & a_{1,n} \\
\vdots &   & \vdots \\
a_{n,1} & \cdots & a_{n,n} \\
\end{bmatrix}
$$
is the matrix of $T$ with respect to some orthonormal basis of $V$. Prove that
$$
|\lambda_1|^2+\cdots + |\lambda_n|^2\leq \sum^n_{k=1} \sum_{j=1}^n |a_{j,k}|^2.
$$
::: 

::: {#exm- } 
Suppose $V$ is an inner-product space. Prove that $\langle S, T\rangle=\text{trace}(S T^*)$ defines an inner-product on $\mathcal{L}(V)$.
::: 

::: {#exm- } 
Suppose $V$ is an inner-product space and $T\in \mathcal{L}(V)$. Prove that if $||T^* v ||\leq ||T v||$ for every $v \in V$, then $T$ is normal.
::: 

::: {#exm- } 
Prove or give a counterexample: if $T\in \mathcal{L}(V)$ and $c\in F$, then $\det(cT)=c^{\text{dim}  V} \det T$.
::: 

::: {#exm- } 
Prove or give a counterexample: if $T\in \mathcal{L}(V)$, then $\det(S+T)=\det S+ \det T$.
::: 

::: {#exm- } 
Suppose $A$ is a block upper-triangular matrix
$$
A=
\begin{bmatrix} 
A_1 & & * \\
& \ddots & \\
0 & & A_m
\end{bmatrix} ,
$$
where each $A_j$ along the diagonal is a square matrix. Prove that
$$
\det A =(\det A_1) \cdots (\det A_m).
$$
::: 


