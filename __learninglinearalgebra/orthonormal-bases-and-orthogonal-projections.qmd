---
pagetitle: "Orthonormal Bases and Orthogonal Projections - Learning Linear Algebra"
---
# Orthonormal Bases and Orthogonal Projections

The \index{norm} **norm** of a vector $\vec v$ in $\mathbb{R}^n$ is $\norm{\vec v}=\sqrt{\vec v \cdot  \vec v}$. A vector $\vec u$ in $\mathbb{R}^n$ is called a \index{unit vector} **unit vector** if $\norm{\vec u}=1$.

::: {#exm- } 
If $\vec v\in \mathbb{R}^n$ and $k$ is a scalar, then $\norm{k \vec v}=|k| \norm{\vec v}$, and if $v$ is nonzero then $\vec u=\frac{1}{\norm{\vec v}} \vec v$ is a unit vector.
Since $\norm{k \vec v}^2=(k \vec v)\cdot (k \vec v)=k^2(\vec v \cdot \vec v)=k^2\norm{\vec v}^2$, taking square roots provides 
$\norm{k \vec v}=|k| \norm{\vec v}$. If $v$ is nonzero, then $\frac{1}{\norm{\vec v}}$ is defined and so $\norm{\vec u}= \frac{1}{\norm{\vec v}} \norm{\vec v}=1$
which follows by the first part.
::: 

Two vectors $\vec v$ and $\vec w$ in $\mathbb{R}^n$ are called \index{perpendicular} **perpendicular** or \index{orthogonal} **orthogonal**  if $\vec v \cdot \vec w=0$.
The vectors $\vec u_1,\ldots,\vec u_m$ in $\mathbb{R}^n$ are called \index{orthonormal} **orthonormal**  if they are all unit vectors and orthogonal to one another. 
A basis of $\mathbb{R}^n$ consisting only of orthonormal vectors is called an  \index{orthonormal basis} **orthonormal basis**.




::: {#thm-orthonormal-vectors } 

## Orthonormal Vectors
Orthonormal vectors are linearly independent, and thus orthonormal vectors $\vec u_1,\ldots,\vec u_n\in \mathbb{R}^n$ form a basis for $\mathbb{R}^n$.
::: 

::: {.proof }
Suppose $(\vec u_1,\ldots,\vec u_m)$ are orthonormal vectors in $\mathbb{R}^n$. To show linear independence suppose,  
$$
c_1 \vec u_1+\cdots + c_m \vec u_m=\vec 0
$$
for some scalars $c_1,\ldots,c_m$ in $\mathbb{R}$. Applying the dot product with $\vec u_i$, 
$$
\left ( c_1 \vec u_1 + \cdots +c_m \vec u_m\right ) \cdot \vec u_i =\vec 0 \cdot \vec u_i=0.
$$
Because the dot product is distributive,  $c_1(\vec u_1 \cdot \vec u_i )+\cdots  + c_m (\vec u_m \cdot \vec u_i)=0$.
We know that $\vec u_i\cdot \vec u_i=1$ and all other dot products are zero. Therefore, $c_i=0$. Since this holds for all $i=1,\ldots,m$, it follows that $c_1=\cdots =c_m=0$, and therefore, $(\vec u_1,\ldots,\vec u_m)$ are linearly independent.
The second part follows since $n$ linearly independent vectors in $\mathbb{R}^n$ always forms a basis.
::: 



::: {#exm- } 
Find three examples of an orthonormal basis for a subspace they span. 
The vectors $\vec e_1,\ldots, \vec e_m$ in $\mathbb{R}^n$ form an orthonormal basis of the subspace they span.
For any scalar $\theta$, the vectors $\vectortwo{\cos \theta}{\sin \theta}$, $\vectortwo{-\sin\theta}{\cos \theta}$ form an orthonormal basis of $\mathbb{R}^2$.
The vectors
$$
\begin{array}{ccc}
\vec u_1=\vectorfour{1/2}{1/2}{1/2}{1/2}, & 
\vec u_2=\vectorfour{1/2}{1/2}{-1/2}{-1/2}, &
\vec u_3=\vectorfour{1/2}{-1/2}{1/2}{-1/2} 
\end{array}
$$
in $\mathbb{R}^4$  form an orthonormal basis of the subspace they span.
::: 

Let $V$ be a subspace of $\mathbb{R}^n$. The \index{orthogonal complement} **orthogonal complement** $V^\perp$ of $V$ is the set of those vectors $\vec x$ in $\mathbb{R}^n$ that are orthogonal to all vectors in $V$ namely,
\begin{equation}
V^{\perp}=\{ \vec x \in \mathbb{R}^n \mid \vec v \cdot \vec x =0, \text{ for all } \vec v \text{ in } V\}.
\end{equation}
It is easy to verify that $V^\perp$ is always a subspace and that $(\mathbb{R}^n)^\perp=\{0\}$ and $\{0\}^\perp=\mathbb{R}^n$. Also notice that if $U_1\subseteq U_2$ then  $U_2^\perp \subseteq U_1^\perp$. 
If $\vec x\in V^{\perp}$ then $\vec x$ is said to be \index{perpendicular} **perpendicular** to $V$. The vector $\vec x^{\parallel}$ in the following theorem is called the \index{orthogonal projection} **orthogonal projection** of $\vec x$ on a subspace $V$ of $\mathbb{R}^n$ and is denoted by $\text{proj}_V (\vec x)$.




::: {#thm-orthogonal-projection } 

## Orthogonal Projection
 
- If $V$ is a subspace of $\mathbb{R}^n$ and $\vec x\in \mathbb{R}^n$, then $\vec x=\vec x^\parallel + \vec x^\perp$ where $\vec x^\perp$ is perpendicular to $V$, and this representation is unique. 
- If $V$ is a subspace of $\mathbb{R}^n$ with an orthonormal basis $\vec u_1 ,\ldots, \vec u_m$, then
\begin{equation}
\mathrm{proj}_V (\vec x):=\vec x^\parallel = (\vec u_1 \cdot \vec x) \vec u_1 + \cdots + (\vec u_m \cdot \vec x) \vec u_m
\end{equation}
for all $\vec x$ in $\mathbb{R}^n$.
- Let $\vec u_1 ,\ldots, \vec u_n$ be an orthonormal basis in $\mathbb{R}^n$, then
\begin{equation}
\vec x = (\vec u_1 \cdot \vec x) \vec u_1 + \cdots + (\vec u_n \cdot \vec x) \vec u_n
\end{equation}
for all $\vec x$ in $\mathbb{R}^n$.
::: 

::: {.proof }
The proof is left for the reader.
::: 




::: {#exm- } 
Find the orthogonal projection of $\vectorthree{49}{49}{49}$ onto the subspace of $\mathbb{R}^3$ spanned by $\vectorthree{2}{3}{6}$ and $\vectorthree{3}{-6}{2}$. 
The two given vectors spanning the subspace are orthogonal since $2(3)+3(-6)+6(2)=0$, but they are not unit vectors since both have length 7. To obtain an orthonormal basis $\vec u_1, \vec u_2$ of the subspace, we divide by 7: $\vec u_1=\frac{1}{7}\vectorthree{2}{3}{6}$ and $\vec u_2=\frac{1}{7}\vectorthree{3}{-6}{2}$. Now we can use \eqref{Orthogonal Projection} with $\vec x=\vectorthree{49}{49}{49}$. Then 
$$
\text{proj}_V(\vec x)=(\vec u_1 \cdot \vec x)\vec u_1+(u_2\cdot \vec x) \vec u_2=
11 \vectorthree{2}{3}{6}+(-1)\vectorthree{3}{-6}{2}= \vectorthree{19}{39}{64}.
$$
::: 




::: {#exm- } 
Find the coordinates of the vector $\vec x=\vectorfour{4}{5}{6}{7}$ with respect to the orthonormal basis
$$
\begin{array}{cccc}
\vec u_1=\vectorfour{1/2}{1/2}{1/2}{1/2}, & 
\vec u_2=\vectorfour{1/2}{1/2}{-1/2}{-1/2}, &
\vec u_3=\vectorfour{1/2}{-1/2}{1/2}{-1/2}, &  
\vec u_4=\vectorfour{1/2}{-1/2}{-1/2}{1/2}. 
\end{array}
$$
Normally to find the coordinates of $\vec x$ we would solve the system
$$
\vectorfour{4}{5}{6}{7}=
c_1 \vectorfour{1/2}{1/2}{1/2}{1/2}+
c_2 \vectorfour{1/2}{1/2}{-1/2}{-1/2}+
c_3 \vectorfour{1/2}{-1/2}{1/2}{-1/2} +
c_4 \vectorfour{1/2}{-1/2}{-1/2}{1/2}
$$
for $c_1, c_2, c_3, c_4$. However we can use \eqref{Orthogonal Projection} instead:
$$
c_1=u_1\cdot \vec x=\vectorfour{1/2}{1/2}{1/2}{1/2} \cdot \vectorfour{4}{5}{6}{7}=11,
$$
$$
c_2=u_2\cdot \vec x=\vectorfour{1/2}{1/2}{-1/2}{-1/2} \cdot \vectorfour{4}{5}{6}{7}=-2,
$$
$$
c_3=u_3\cdot \vec x=\vectorfour{1/2}{-1/2}{1/2}{-1/2} \cdot \vectorfour{4}{5}{6}{7}=-1,
$$
$$
c_4=u_4\cdot \vec x=\vectorfour{1/2}{-1/2}{-1/2}{1/2} \cdot \vectorfour{4}{5}{6}{7}=0.
$$
Therefore the $\mathcal{B}$-coordinate vector of $\vec x$ is $\vectorfour{11}{-2}{-1}{0}$. 
::: 




::: {#thm- } 

## Properties of the Orthogonal Complement
Let $V$ be a subspace of $\mathbb{R}^n$, then
 
- $\mathrm{proj}_V(\vec x)$ is a linear transformation $\mathbb{R}^n\to V$ with kernel $V^\perp$,
- $V \cap V^\perp = \{\vec 0\}$,
- $\text{dim}  V +\text{dim}  V^\perp=n$, and
- $(V^\perp)^\perp = V$.
::: 

::: {.proof }
The proof of each part follows.
 
- To prove the linearity of $T(x):=\mathrm{proj}_V(\vec x)$ we will use the definition of a projection: $T(\vec x)$ is in $V$, and $\vec x-T(\vec x)$ is in $V^\perp$. 
To show $T(\vec x+\vec y)=T(\vec x)+T(\vec y)$, notice $T(\vec x)+T(\vec y)$ is in $V$ (since $V$ is a subspace), 
and $\vec x+\vec y-(T(\vec x)+T(\vec y))=(\vec x-T(\vec x))+(\vec y-T(\vec y))$ is in $V^\perp$ (since $V^\perp$ is a subspace). 
To show that $T(k \vec x)=k T(\vec x)$, note that $k T(\vec x)$ is in $V$ (since $V$ is a subspace) and $k \vec x-k T(\vec x)=k(\vec x-T(\vec x))$ is in $V^\perp$ (since $V^\perp$ is a subspace).
- Since $\{\vec 0\} \subseteq V$ and  $\{\vec 0\} \subseteq V^\perp$,  $\{\vec 0\} \subseteq V\cap V^\perp.$ If a vector $\vec x$ is in $V$ as well as in $V^\perp$, then $\vec x$ is orthogonal to itself: $\vec x \cdot \vec x =\norm{x}^2=0$, so that $\vec x$ must equal $\vec 0$ which shows $V \subseteq \{\vec 0\}$. Therefore, $V\cap V^\perp=\{\vec 0\}$. 
- Apply the Rank-Nullity Theorem to the linear transformation $T(\vec x)=\text{proj}_V(\vec x)$ yielding 
$n=\text{dim}  \mathbb{R}^n =\text{dim} \text{image} T+\text{dim}  \ker T=\text{dim}  V+\text{dim}  V^\perp$.
- Let $\vec v\in V$. Then $\vec v\cdot \vec x=0$ for all $\vec x$ in $V^\perp$. Since $(V^\perp)^\perp$ contains all vectors $\vec y$ such that $\vec y \cdot \vec x =0$, $\vec v$ is in $(V^\perp)^\perp$. So $V$ is a subspace of $(V^\perp)^\perp$. Using (iii) with $V$ ($n=\text{dim}  V+\text{dim}  V^\perp$) and again with $V^\perp$ ($\text{dim}  V^\perp+\text{dim}  (V^\perp)^\perp$) yielding $\text{dim}  V=\text{dim}  (V^\perp)^\perp$; and since $V$ is a subspace of $(V^\perp)^\perp$ it follows that $V=(V^\perp)^\perp$.  
::: 





::: {#exm- } 
Find a basis for $W^\perp$, where $W=\text{span} \left( \vectorfour{1}{2}{3}{4}, \vectorfour{5}{6}{7}{8} \right)$.
The orthogonal complement $W^\perp$ of $W$ consists of the vectors $\vec x$ in $\mathbb{R}^4$ such that
$$
\vectorfour{x_1}{x_2}{x_3}{x_4} \cdot \vectorfour{1}{2}{3}{4}
=0 \hspace{1cm}\text{and}  
\vectorfour{x_1}{x_2}{x_3}{x_4} \cdot \vectorfour{5}{6}{7}{8}=0.
$$
Finding these vectors amounts to solving the system
$$
\begin{cases}
x_1+2x_2+3x_3+4x_4&=0 
\\ 5x_1+6x_2+7x_3+8x_4 & =0 
\end{cases}
$$
The solutions are of the form
$$
\vectorfour{x_1}{x_2}{x_3}{x_4}=\vectorfour{s+2t}{-2s-3t}{s}{t}=s\vectorfour{1}{-2}{1}{0}+t\vectorfour{2}{-3}{0}{1}.
$$
The two vectors on the right form a basis of $W^{\perp}$.
::: 




::: {#thm- } 
Let $\vec x, \vec y \in \mathbb{R}^n$. Then
 
- (\index{Pythagorean Theorem} **Pythagorean Theorem**) 
$$
||\vec x + \vec y || ^2 = || \vec x|| ^2 + || \vec y || ^2 
$$
holds if and only if $\vec x \perp \vec y$, 
- (\index{Cauchy-Schwarz} **Cauchy-Schwarz**) 
$$
| \vec x \cdot \vec y | \leq || \vec x || \, || \vec y ||
$$
where equality holds if and only if $\vec x$ and $\vec y$ are parallel, 
- (\index{Law of Cosines} **Law of Cosines**) the angle $\theta$ between $\vec x$ and $\vec y$ is defined as 
$$ 
\theta = \arccos \frac{\vec x \cdot \vec y}{|| \vec x || \, || \vec y||},
$$
- (\index{Triangular Inequality} **Triangular Inequality**) 
$$
\norm{\vec x+\vec y}\leq \norm{\vec x}+\norm{\vec y}.
$$
::: 

::: {.proof }
The proof of each part follows.
 
- The verification is straightforward:
\begin{align*}
\norm{\vec x+\vec y }^2
& =(\vec x + \vec y)\cdot (\vec x + \vec y) 
=\vec x \cdot \vec x+2(\vec x \cdot \vec y) +\vec y \cdot \vec y \\
& =\norm{\vec x}^2+2(\vec x\cdot \vec y)+\norm{\vec y}^2 
=\norm{\vec x}^2+\norm{\vec y}^2 
\end{align*}
where the last equality holds if and only if $\vec x\cdot \vec y=0$. 
- Let $V$ be a one-dimensional subspace of $\mathbb{R}^n$ spanned by a nonzero vector $\vec y$. Let $\vec u=\frac{1}{\norm{\vec y}} \vec y$. Then
$$
\norm{\vec x} 
\geq \norm{\text{proj}_V(\vec x)}
=\norm{(\vec x\cdot \vec u)\vec u} 
=|\vec x\cdot \vec u|
=\left| \vec x \cdot \left( \frac{1}{\norm{y}}\vec y \right)\right|
= \frac{1}{\norm{y}}|\vec x \cdot \vec y|
$$
multiplying by $\norm{\vec y}$, yields $| \vec x \cdot \vec y | \leq || \vec x || \, || \vec y ||$. Notice that $\norm{\vec x}  \geq \norm{\text{proj}_V(\vec x)}$ holds by applying the Pythagorean theorem to $\vec x=\vec x^\parallel +\vec x^\perp$ with $\vec x^\perp \cdot \vec x^\parallel =0$ so that $\norm{\vec x}^2=\norm{\text{proj}_V(\vec x)}^2+\norm{\vec x^\perp}^2$ which leads to $\norm{\text{proj}_V \vec x} \leq \norm{\vec x}$. 
- We have to make sure that \eqref{law-of-cosines} is defined, that is $\theta$ is between $-1$ and $1$, or equivalently,
$$
\left | \frac{\vec x \cdot \vec y }{\norm{x} \, \norm{y}} \right | \leq 1.
$$
But this follows from the Cauchy-Schwarz inequality. 
- Using the  Cauchy-Schwarz inequality, the verification is straightforward,
$$
\norm{\vec x+\vec y}^2=(\vec x + \vec y)\cdot (\vec x +  \vec y)=\norm{\vec x}^2+\norm{\vec y}^2+2(\vec x\cdot \vec y)
$$
$$
\leq \norm{\vec x}^2+\norm{\vec y}^2+2\norm{\vec x}\norm{\vec y}=(\norm{\vec x}+\norm{\vec y})^2
$$
Taking the square root of both sides yields $\norm{\vec x+\vec y}\leq \norm{\vec x}+\norm{\vec y}$.
::: 





::: {#exm- } 
Determine whether the angle between the vectors $\vec u=\vectorthree{2}{3}{4}$, $\vec v=\vectorthree{2}{-8}{5}$ is a right angle using the Pythagorean Theorem. 
Since $\norm{\vec u}=\sqrt{2^2+3^2+4^2}=\sqrt{29}$ and $\norm{\vec v}=\sqrt{2^2+(-8)^2+5^2}=\sqrt{93}$. Then
$$
\norm{\vec u+\vec v}^2=\left|\left| \, \, \vectorthree{4}{-5}{9} \, \, 
\right| \right|^2 =122 = 29+93= || \vec u|| ^2 + || \vec v || ^2
$$
shows $\vec u \perp \vec v$.
::: 



::: {#exm- } 
Consider the vectors  $\vec u =\vectorfour{1}{1}{\vdots}{1}$ and $\vec v=\vectorfour{1}{0}{\vdots}{0}$
in $\mathbb{R}^n$. For $n=2,3,4$, find the angle $\theta$ between $\vec u$ and $\vec v$. Then find the limit of $\theta$ as $n$ approaches infinity. 
For any possible value of $n$, 
$$
\theta_n=\arccos \frac{\vec u \cdot \vec v}{\norm{\vec u}\norm{\vec v}}=\arccos \frac{1}{\sqrt{n}}.
$$
Then
$$
\begin{bmatrix}
\theta_2=\arccos \frac{1}{\sqrt{2}}=\frac{\pi}{4}, & 
\hspace{.5cm}  \theta_3=\arccos \frac{1}{\sqrt{3}}=\frac{\pi}{4}\sim 0.955 \text{ rads}, &
\hspace{.5cm} \theta_4=\arccos \frac{1}{\sqrt{4}}=\frac{\pi}{3}.
\end{bmatrix}
$$
Since $y=\arccos(x)$ is a continuous function,
$$
\lim_{x\mapsto \infty} \theta_n 
= \lim_{x\mapsto \infty} \arccos\left( \frac{1}{\sqrt{n}} \right)
= \arccos \left( \lim_{x\mapsto \infty} \frac{1}{\sqrt{n}} \right)
= \arccos(0)
=\frac{\pi}{2}.
$$
::: 


