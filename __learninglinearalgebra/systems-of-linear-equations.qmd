---
pagetitle: "Systems of Linear Equations - Learning Linear Algebra"
aliases:
    - on-the-solutions-of-linear-systems.html
    - matrices-and-vectors.html
---
# Systems of Linear Equations

::: {.content-visible when-format="html" .d-none }
$\newcommand{\vlist}[2]{#1_1,#1_2,\ldots,#1_#2}$
$\newcommand{\vectortwo}[2]{\begin{bmatrix} #1 \\ #2\end{bmatrix}}$
$\newcommand{\vectorthree}[3]{\begin{bmatrix} #1 \\ #2 \\ #3\end{bmatrix}}$
$\newcommand{\vectorfour}[4]{\begin{bmatrix} #1 \\ #2 \\ #3 \\ #4\end{bmatrix}}$
$\newcommand{\vectorfive}[5]{\begin{bmatrix} #1 \\ #2 \\ #3 \\ #4 \\ #5 \end{bmatrix}}$
$\newcommand{\lincomb}[3]{#1_1 \vec{#2}_1+#1_2 \vec{#2}_2+\cdots + #1_m \vec{#2}_#3}$
$\newcommand{\norm}[1]{\left|\left |#1\right|\right |}$
$\newcommand{\ip}[1]{\left \langle #1\right \rangle}$
$\newcommand{\plim}[2]{\lim_{\footnotesize\begin{array}{c} \\[-10pt] #1 \\[0pt] #2 \end{array}}}$
:::


Systems of Linear Equations (A Student's Guide) is a comprehensive guide written for students. This book provides in-depth explanations of how to solve systems of linear equations using various methods, including graphing, substitution, and elimination. It also includes examples and practice problems to help you master systems for linear equations.

A system of linear equations is a set of two or more linear equations that share the same set of unknowns. In other words, a system of linear equations is a collection of linear equations that you must solve together. There are three main types of systems of linear equations:

1. Consistent and independent: A consistent and independent system of linear equations has a unique solution. This means that there is only one way to solve the system, and the solution will not change if different values are used for the unknowns.

2. Consistent and dependent: A consistent and dependent system of linear equations has infinitely many solutions. This means that there is more than one way to solve the system, and the solution will change if different values are used for the unknowns.

3. Inconsistent: An inconsistent system of linear equations has no solution. This means that there is no way to solve the system.

The graphing method is a visual way to find the solution to a system of linear equations. To use the graphing method, graph both equations on the same coordinate plane. The point of intersection is the solution to the system.

To graph a linear equation, start by plotting the y-intercept. This is the point where the line crosses the y-axis. To find the y-intercept, set x=0 and solve for y. Then, use the slope to find additional points on the line. The slope is the ratio of the change in y to the change in x. To find the slope, pick two points on the line and calculate the rise (the change in y) divided by the run (the change in x).

Once you have the y-intercept and the slope, you can plot additional points on the line and connect them with a straight line.

The substitution method is an algebraic way to find the solution to a system of linear equations. To use the substitution method, solve one of the equations for one of the unknowns. Then, substitute this value into the other equation. This will give you a new equation with one unknown. Solve this equation for the unknown. Now, substitute the value of the unknown back into one of the original equations. This will give you the solution to the system.

The substitution method is usually easiest when one of the equations is already solved for one of the unknowns. However, it is sometimes necessary to solve one of the equations for an unknown before you can substitute.

##  The Elimination Method

The elimination method is an algebraic way to find the solution to a system of linear equations. To use the elimination method, add or subtract the equations so that one of the unknowns cancels out. This will give you a new equation with one fewer unknown. Repeat this process until you are left with an equation that has only one unknown. Solve this equation for the unknown. Now, substitute the value of the unknown back into one of the original equations. This will give you the solution to the system.

The elimination method is usually easiest when the coefficients (the numbers in front of the unknowns) are already opposites. However, it is sometimes necessary to multiply one or both equations by a constant before you can eliminate an unknown.

There are many ways to solve a system of linear equations. The best way to choose a method is to look at the equations and decide which one will be the easiest to work with. In general, the substitution method is best when one equation is already solved for one unknown. The elimination method is best when the coefficients of one unknown are opposites. The graphing method is a good way to check your work, but it can be difficult to find the intersection of two lines.

##  Row Operations and Similar Matrices

In solving systems of linear equations, we will often need to use row operations. Row operations are mathematical operations that can be performed on the rows of a matrix. There are three types of row operations:

 
- Interchange two rows
- Multiply a row by a nonzero constant
- Add a multiple of one row to another row
 

Row operations do not change the solution set of a system of linear equations. This means that if we perform row operations on a matrix, the new matrix will have the same solution set as the original matrix. We can use row operations to transform a matrix into an equivalent matrix. Two matrices are equivalent if they have the same solution set.

We can use row operations to solve systems of linear equations. In general, we will use row operations to transform the coefficient matrix into an upper triangular matrix. An upper triangular matrix is a matrix where all the elements below the main diagonal are zero. Once we have transformed the coefficient matrix into an upper triangular matrix, we can use back substitution to solve the system.

We can use row operations to transform a matrix into any equivalent matrix. However, some matrices are more difficult to work with than others. In general, it is easier to work with matrices that are in reduced row echelon form. A matrix is in reduced row echelon form if it is upper triangular and if the leading entry in each nonzero row is a 1. The leading entry in a row is the first nonzero entry in the row.

We can use row operations to transform any matrix into an equivalent matrix that is in reduced row echelon form. Once we have transformed the matrix into reduced row echelon form, we can use back substitution to solve the system of linear equations.

##  Gaussian Elimination Method

The Gaussian elimination method is another way to solve a system of linear equations. To use the Gaussian elimination method, you must first arrange the equations into row form. This will make it easier to see how to eliminate an unknown. Next, multiply each equation by a constant so that one of the unknowns cancels out. Then, add or subtract these equations so that another unknown cancels out. Finally, solve the equation resulting from this process for the last unknown. Now, substitute this value back into one of the original equations and you have the solution to the system!

The Gaussian Elimination Method is a procedure used to solve systems of linear equations. The method is named after German mathematician Carl Friedrich Gauss and Irish mathematician William Rowan Hamilton, who developed it independently in the early 19th century.

##  Gauss-Jordan Elimination Method

The Gauss-Jordan Elimination Method is similar to the traditional Gaussian Elimination Method, but it is more efficient and easier to use. The method works by first transforming the system of equations into an equivalent system with a triangular form. Then, the solution is obtained by a backward substitution.

This method is called Gauss-Jordan Elimination because it was developed by two mathematicians, Carl Friedrich Gauss and Wilhelm Jordan. Although it is not the quickest or most intuitive method, it is a robust way to solve linear equations. When done correctly, it will always give you the correct answer.

##  Conclusion

There are many ways to solve a system of linear equations. The best way to choose a method is to look at the equations and decide which one will be the easiest to work with. In general, the substitution method is best when one equation is already solved for one unknown. The elimination method is best when the coefficients of one unknown are opposites. The Gaussian elimination method is best when there are a lot of equations and unknowns. Lastly, the Gauss-Jordan elimination method is the most robust way to solve linear equations. It is not always the quickest or easiest method, but it will always give you the correct answer.

No matter which method you choose, make sure you understand the steps involved and why they work. This will help you not only solve the equations correctly but also to understand what is happening behind the scenes. With a little practice, you will be solving systems of linear equations like a pro!




A linear system of equations in two variables $x$ and $y$ has the form
$$
\begin{cases}
a x + b y =c\\
d x+ e y =f 
\end{cases}
$$
where $a, b, c, d, e, f$ are given numbers such as real numbers or complex numbers. 
Sometimes a linear system is also written using indices 

\begin{equation}
\label{2by2sys}
\begin{cases}
a_{11} x+a_{12} y=b_1\\
a_{21}x+ a_{22} y =b_2
\end{cases}
\end{equation}

to help reduce on the number of letters used. 
For simplicity let's temporally assume that the coefficients are nonzero real numbers and ask the question: 
how many solutions can there be to a $2\times 2$ system? 
The key idea is to realize that each linear equation in \ref{2by2sys} represents a line in the Cartesian plane. 
If we consider the possible ways lines can intersect in the plane we come to the conclusion that there must either 
be no solutions, one unique solution, or infinitely many points $(x,y)$ that solves the system in \ref{2by2sys}.

::: {#exm- } 
\label{example:2by2system}
Determine whether the linear system 
$$
\begin{cases}
2x+3y =0 \\
4x+5y=0.
\end{cases}
$$
has no solutions, exactly one solution, or infinitely many solutions.
If we multiple the first equation, namely $2x+3y=0$ by 2 and subtract from the second equation, $4x+5y=0$, we obtain $y=0$. Therefore the solution is unique and is $(x,y)=(0,0)$. 
::: 

A $3\times 3$ linear system has the form 
\begin{equation}
\label{3by3sys}
\begin{cases}
a_{11} x+a_{12} y+a_{13}z =b_1\\
a_{21} x+a_{22} y+a_{23}z =b_2\\
a_{31} x+a_{32} y+a_{33}z =b_3\\
\end{cases}
\end{equation}
where $a_{ij}$ and $b_1, b_2, b_3$ are numbers and $x, y, z$ are  variables. 
Geometrically, linear equations in three variables are just planes in three dimensions. So what are the different types of solution sets for the system in \ref{3by3sys}? By considering the possible ways three planes might intersect in three dimensions we come to the conclusion that there must either be no solutions, one unique solution, or infinitely many points $(x,y,z)$ that solves the system in \ref{3by3sys}.

::: {#exm- } 
\label{example:3by3system}
Determine whether the linear system 
\begin{equation}
\label{linesysex1}
\begin{cases}
x+2y+3z=0 \\
4x+5y+6z=3 \\
7x+8y+9z =0
\end{cases}
\end{equation}
has no solutions, exactly one solution, or infinitely many solutions.
Multiply the first equation by $-2$ and add to the second equation obtaining the equation $2x+y=3$. Eliminating $z$ from the first and third equations we obtain $4x+2y=0$ by multiplying the first equation by $-3$ and adding to the third equation. The $x$ and $y$ that satisfies \ref{linesysex1} must also satisfy the system 
\begin{equation}
\label{linesysex1b}
\begin{cases}
2x+y=3 \\
4x+2y=0.
\end{cases}
\end{equation}
Multiply the first equation of \ref{linesysex1b} by $2$ yields $4x+2y=6$. Notice there are no $x$ and $y$ that satisfies both $4x+2y=6$ and $4x+2y=0$. Thus the system in \ref{linesysex1b} has no solutions; therefore the system in \ref{linesysex1} also has no solutions. 
::: 



::: {#exm- } 
Given numbers $a, b$, and $c$, show that the linear system 
$$
\begin{cases}
x+2y +3z = a\\
x+3y +8z = b\\
x+2y +2z = c
\end{cases}
$$
either has no solutions, exactly one solution, or infinitely many solutions.
We choose to eliminate $x$ first and we obtain the system 
$$
\begin{cases}
-y-5z  =a-b\\
z  =a-c.
\end{cases}
$$
Next we eliminate $z$ obtaining $y=-6a+b+5c$. Therefore the unique solution is $(x,y,z)=(10a-2b-7c,-6a+b+5c,a-c)$.
::: 

Let's consider a set of linear equations that involves $n$ unknown quantities represented by 
$\vlist{x}{n}$. 
Let $a_{ij}$ represent the number that is the coefficient of $x_j$ in the $i$th equation. 
Let $\vlist{b}{m}$ be given numbers. 
The linear system of equations 
\begin{equation}
\label{syseq}
\begin{cases}
a_{11} x_1+a_{12} x_2+\cdots +a_{1n} x_n =  b_1 \\
a_{21} x_1+a_{22} x_2+\cdots +a_{2n} x_n =  b_2 \\
\hfill \vdots \hfill \\
a_{m1} x_1+a_{m2} x_2+\cdots +a_{mn} x_n =  b_m \\
\end{cases}
\end{equation}
is called a \index{system of simultaneous linear algebraic equations} **system of simultaneous linear algebraic equations**. 
A \index{solution} **solution** of this system is an ordered set of $n$ numbers that satisfies each of the $m$ statements in the system. 
A linear system of equations with no solution is called \index{inconsistent} and a system with at least one solution is called \index{consistent} **inconsistent** . 
The array  
$$
\left[
\begin{array}{l|l}
\begin{matrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n}\\
 & & \vdots \\
a_{m1} & a_{m2} & \cdots  & a_{mn}\\
\end{matrix}
& 
\begin{matrix}
b_1 \\ b_2 \\ \vdots \\ b_m
\end{matrix}
\end{array}
\right]
$$
is called the \index{augmented matrix} corresponding to the linear system in \ref{syseq} **augmented matrix**.

For example, the linear system in \ref{example:2by2system} was shown to be consistent and the linear system in \ref{example:3by3system} was shown to be inconsistent. The augmented matrices for these systems are as follows.
$$
\left[
\begin{array}{l|l}
\begin{matrix}
2 & 3 \\
4 & 5
\end{matrix}
&
\begin{matrix}
0 \\ 0
\end{matrix}
\end{array}
\right]
\qquad 
%\text{and} 
\qquad
\left[
\begin{array}{l|l}
\begin{matrix}
1& 2 & 3 \\
4 & 5 & 6 \\
7 & 8 & 9
\end{matrix}
&
\begin{matrix}
0 \\ 3 \\0 
\end{matrix}
\end{array}
\right]
$$

::: {#exm- } 
\label{example:2by3system}
Find all solutions to the following linear system.
$$
\begin{cases}
-150 x+500y=z \\
50x+100y+z =200
\end{cases}
$$
The given system is equivalent to 
$$
\begin{cases}
-150 x+500y-z=0 \\
50x+100y+z =200.
\end{cases}
$$
Adding these equations yields $-100x+600y=200$. Since we have one equation with two variables, one of the variables is free. 
We choose to let $y$ be free. 
Let $y=t$ for an arbitrary number $t$. 
Then solving for $x$ we obtain $-100x=200y-600y$, or equivalently $x=-2+6t$. 
By substituting into the original system, we find $z=-150(-2+6t)+500t=300-400t.$ 
Therefore, there are an infinitely many solutions, which can be represented by the set 
$$
\{(x,y,z) \mid x=-2+6t, y=t, z=300-400t \text{ where $t\in \mathbb{R} $} \}.
$$
::: 

The augmented matrix for the system in \ref{example:2by3system} is
$$
\left[
\begin{array}{l|l}
\begin{matrix}
-150 & 500 & -1 \\
50 & 100 & 1
\end{matrix}
&
\begin{matrix}
0 \\ 200
\end{matrix}
\end{array}
\right]
$$
Do you think it is possible to decide, by inspection of an augmented matrix, whether or not the corresponding linear system will be consistent or inconsistent? 

In our examples so far we have seen linear systems having no solutions, a unique solution, or perhaps an infinite number of solutions. 
These examples suggest the following definition. 

::: {#def- } 
Two linear systems are called \index{equivalent} **equivalent**  if they have the same solution set. 
::: 

::: {#exm- } 
Find a system of linear equations with three unknowns $x,y,z$ whose solutions are 
$x=6+5t$, $y=4+3t$, $z=2+t$
where $t$ is arbitrary.
We want to eliminate $t$. Solving for $t$ yields $t=z-2$. By substitution, $x=6+5(z-2)$ and $y=4+3(z-2)$. Thus we have a linear system 
$$
\begin{cases}
x-5z=-4\\
y-3z =-2
\end{cases}
$$
which has an infinitely many solutions.
::: 

::: {#thm- } 
\label{proprowoperations}
Linear systems of equations are equivalent if each can be obtained from the other by one or more of the following operations.
 
- Interchange the order of the equations.
- Multiply (or divide) one equation by a nonzero scalar.
- Add one multiple of one equation to another.
:::


::: {.proof }
If we consider the solution set as a geometric object, it should be very easy to understand that they way in which we write the equations that represents the object does not change the object. Thus it should be obvious that interchanging the order of the equations will not change the solutions to a linear system. Neither will multiplying (or dividing) both sides of an equation by a nonzero constant. 

Let system $A$ be an $m\times n$ system represented by 
\begin{equation}
\label{syseqA}
\begin{cases}
a_{11} x_1+a_{12} x_2+\cdots +a_{1n} x_n =  b_1 \\
\hfill \vdots \hfill \\
a_{i1} x_1+a_{i2} x_2+\cdots +a_{in} x_n =  b_i  \\
\hfill \vdots \hfill \\
a_{m1} x_1+a_{m2} x_2+\cdots +a_{mn} x_n =  b_m & \\
\end{cases}
\end{equation}
Consider system $B$ obtained from system $A$ by adding $k$ times equation $i$ to equation 
$j$ as follows:
\begin{equation}
\label{syseqB}
\begin{cases}
a_{11} x_1+a_{12} x_2+\cdots +a_{1n} x_n =  b_1  \\
 \qquad \qquad \vdots \qquad \qquad \qquad  \vdots \\
(a_{j1}+k a_{i1}) x_1+(a_{j2}+k a_{i2}) x_2+\cdots +(a_{jn}+k a_{in}) x_n =  b_j +k b_i\\ \qquad \qquad  \vdots \qquad \qquad \qquad  \vdots \\
a_{m1} x_1+a_{m2} x_2+\cdots +a_{mn} x_n =  b_m \\
\end{cases}
\end{equation}
Let $S_A$ and $S_B$ be the solutions sets of systems $A$ and $B$, respectively. 
We will show $S_A=S_B$.
Let $x_0=(\vlist{x}{n})$ be a solution of system $A$. 
Thus $x_0$ satisfies every linear equation in \ref{syseqA}. 
So $x_0$ satisfies every equation in system B except possibly the $j$th equation. 
Working with the $j$th equation in system $B$ we find 
\begin{align*}
& (a_{j1}+k a_{i1}) x_1+(a_{j2}+k a_{i2}) x_2+\cdots +(a_{jn}+k a_{in}) x_n \\
& \qquad = a_{j1}x_1+\cdots +a_{jn} x_n+k (a_{i1}x_1+\cdots a_{jn}x_n)\\
& \qquad =  b_j +k b_i.\end{align*}
This shows $x_0$ also satisfies the $j$th equation of system $B$. Since $x_0$ satisfies every equation in \ref{syseqB}, $x_0$ is also a member of $S_B$. So far we have shown $S_A\subseteq S_B$. 
Conversely, assume $x_0$ is a solutions to every equation in \ref{syseqB}.
Working with the $j$th equation of system $A$ we find,
\begin{align}
& a_{j1} x_1+a_{j2} x_2+\cdots +a_{jn}x_n=b_j \notag \\
\Longleftrightarrow \quad & a_{j1} x_1+ (k a_{i1}-ka_{i1})x_1 
%+ a_{j2} x_2+ (k a_{21}-ka_{21})x_2
+\cdots +a_{jn} + (k a_{i1}-ka_{i1})x_n=b_j \label{eqsys}\notag \\
\Longleftrightarrow \quad & (a_{j1}+k a_{i1}) x_1+(a_{j2}+k a_{i2}) x_2+\cdots +(a_{jn}+k a_{in}) x_n = b_j+k b_i. 
\end{align}
Notice \ref{eqsys} holds since $x_0$ satisfies system $B$; and thus $x_0$ satisfies the $j$th equation of system $A$. Therefore $S_A=S_B$ as desired.  
::: 



While working through the details of the following two examples, notice how the operations in \ref{proprowoperations} are being used. 

::: {#exm- } 
Let $a, b$, and $c$ be constants. Solve the linear system
$$
\begin{cases}
y+z  =a \\
x+z =b \\
x+y = c.
\end{cases}
$$
Eliminating $z$ from the first and second equations we obtain the system
$$
\begin{cases}
x-y = b-a \\
x+y=c
\end{cases}
$$
Solving for $x$ yields $x=(c+b-a)/2$. Using the original system we find $y$ and $z$ to be
$$
y= c-x=c-(c+b-a)/2=(c-b+a)/2
$$
$$
z=a-y=a-(c-b+a)/2=(a+b-c)/2
$$
Therefore, the solution to the system is 
$$
(x,y,z)=\left(\frac{c+b-a}{2},\frac{a+c-b}{2},\frac{a+b-c}{2}\right).
$$
::: 

::: {#exm- } 
Find the smallest positive integer $C$ such that $x, y, z$ are integers and satisfies the linear system of equations
$$
\begin{cases}
2x+y =C \\
3y+z =C \\
x+4z = C.
\end{cases}
$$
Multiply the third equation by $-2$ and add to the first equation, obtaining $y-8z=-C$. 
Multiplying the second equation by 8 and adding to $y-8z=-C$, yields $25y=7C$. 
Solving for $y$ we obtain $y=(7/25)C$. 
By substitution, $x=(9/25)C$ and $z=(4/25)C$. Therefore, 25 is the smallest integer $C$ such that $x,y,z$ are integers and solves the system.
::: 

We will solve a linear system of equations using elementary row operations on matrices using a procedure known as \index{Gaussian elimination} **Gaussian elimination** . The solution set will be a collection of vectors. Here is the overall strategy to solving linear systems using matrices. 
$$
\small
\begin{tabular}{c}
$\left\{\parbox[c]{1.75cm}{\begin{center}linear system of equations\end{center}}\right\}$  $\rightarrow$  
$\left\{\parbox[c]{1.75cm}{\begin{center}matrix representation of system\end{center}}\right\}$ 
$\rightarrow$ 
$\left\{\parbox[c]{1.75cm}{\begin{center}row echelon form of system\end{center}}\right\}$
$\rightarrow$  
$\left\{\parbox[c]{1.75cm}{\begin{center}solutions of system as a set of vectors\end{center}}\right\}$
\end{tabular}$$

::: {#def- } 
The following operations are collectively known as \index{elementary row operations} **elementary row operations** .
 
- Interchange two rows.
- Multiply a row by a nonzero scalar.
- Add a multiple of a row from another row.
::: 

From \ref{proprowoperations} it is obvious that applying elementary row-operations to a linear system of equations leads to an equivalent system. Reducing a linear system of equations, while preserving the solutions set, is an extremely useful idea that we will develop extensively.  

For example, in \ref{gjeunique} (see below) we find that the linear systems 
\begin{equation}
\label{refex}
\begin{cases}
x+2y+z =3 \\
2x+5y-z =-4 \\
3x-2y-z =5
\end{cases}
\qquad 
\qquad 
\begin{cases}
x=2 \\
y=-1 \\
z=3
\end{cases}
\end{equation}
are equivalent. 
In \ref{gjeunique} we will show how to apply elementary row-operations to obtain the system on the right. 
While the system on the left might be a given linear system of equations, the system on the right is solved. 

::: {#def- } 
A matrix is said to be in \index{row echelon form} **row echelon form** if it satisfies all the following conditions.
 
- All rows with at least one nonzero coefficient are above any rows of all zeroes. 
- The first nonzero number from the left, (called the \index{leading coefficient} **leading coefficient** ) of a nonzero row is always strictly to the right of the leading coefficient of the row above it.
- All entries in a column below a leading coefficient are zeroes.
 
Further, a matrix is said to be in 
\index{reduced row echelon form} **reduced row echelon form** if it is in row echelon form and the additional condition holds.
 
\setcounter{enumi}{3} 
- Every leading coefficient is 1 and is the only nonzero entry in its column.
::: 

For example, the augmented matrices for the linear systems in \ref{refex} are
$$
\begin{bmatrix}
\begin{array}{ccc|c}
1 & 2 & 1 & 3 \\
2 & 5 & -1 & -4 \\
3 & -2 & -1 & 5
\end{array}
\end{bmatrix}\qquad
\qquad
\begin{bmatrix}
\begin{array}{ccc|c}
1 & 0 & 0 & 2 \\
0 & 1 & 0 & -1 \\
0 & 0 & 1& 3
\end{array}
\end{bmatrix}.
$$
By checking conditions (i)-(iv) we can see that the matrix on the right is in reduced row echelon form. 
For more examples, consider the following matrices.
$$
A = \begin{bmatrix} 0 & 5 \\ 2 & 3 \end{bmatrix}
\qquad
B = \begin{bmatrix} 1 & 0 \\ 1 & 0 \end{bmatrix}
\qquad
C = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}
$$
$$
D = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}
\qquad 
E = \begin{bmatrix} 1 & 0 \\ 0 & 1 \end{bmatrix}
\qquad
F = \begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}
$$
Notice that matrices $D$ and $E$ are in reduced row echelon form and the others are not. 

::: {#exm- } 
\label{gjeunique}
Use Gauss-Jordan elimination to solve the system.
$$
\begin{cases}
x+2y+z =3 \\
2x+5y-z =-4 \\
3x-2y-z =5
\end{cases}
$$
Using row operations on the augmented matrix we obtain the reduced row echelon form.
\begin{align*}
& \begin{bmatrix}
\begin{array}{ccc|c}
1 & 2 & 1 & 3 \\
2 & 5 & -1 & -4 \\
3 & -2 & -1 & 5
\end{array}
\end{bmatrix}\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -2R_1+R_2}
\\
\stackrel{\longrightarrow}{\scriptstyle -3R_1+R_3}
\end{array}
\begin{bmatrix}
\begin{array}{ccc|c}
1 & 2 & 1 & 3 \\
0 & 1 & -3 & -10 \\
0 & -8 & -4 & -4
\end{array}
\end{bmatrix}
\\ & \qquad \qquad
\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle 8 R_2+R_3}
\end{array}
\begin{bmatrix}
\begin{array}{ccc|c}
1 & 2 & 1 & 3 \\
0 & 1 & -3 & -10 \\
0 & 0 & -28 & -84
\end{array}
\end{bmatrix}
\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -\frac{1}{28}R_3}
\end{array}
\begin{bmatrix}
\begin{array}{ccc|c}
1 & 2 & 1 & 3 \\
0 & 1 & -3 & -10\\
0 & 0 & 1 & 3
\end{array}
\end{bmatrix}
\\ &  \qquad \qquad
\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle 3R_3+R_2}
\\
\stackrel{\longrightarrow}{\scriptstyle -R_3+R_1}
\end{array}
\begin{bmatrix}
\begin{array}{ccc|c}
1 & 2 & 0 & 0 \\
0 & 1 & 0 & -1 \\
0 & 0 & 1 & 3
\end{array}
\end{bmatrix}\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -2R_2+R_1}
\end{array}
\begin{bmatrix}
\begin{array}{ccc|c}
1 & 0 & 0 & 2 \\
0 & 1 & 0 & -1 \\
0 & 0 & 1& 3
\end{array}
\end{bmatrix}.
\end{align*}
Therefore the unique solution is  $x=2$, $y=-1$, and $z=3$.
::: 

::: {#exm- } 
Use the Gauss-Jordan elimination method to solve the following linear system.
\begin{equation}
\label{gjee1}
\begin{cases}
x+y -2z+4t =5 \\
2x+2y-3z+t =3 \\
3x+3y-4z-2t  = 1
\end{cases}
\end{equation}
Using row operations on the augmented matrix we obtain the reduced row echelon form.
\begin{align*}
\begin{bmatrix}
\begin{array}{cccc|c}
1 & 1 & -2 & 4 & 5 \\
2 & 2 & -3 & 1 & 3 \\
3 & 3 & -4 & -2 & 1
\end{array}
\end{bmatrix} 
& \begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -2R_1+R_2}
\\
\stackrel{\longrightarrow}{\scriptstyle -3R_1+R_3}
\end{array} 
\begin{bmatrix}
\begin{array}{cccc|c}
1 & 1 & -2 & 4 & 5 \\
0 & 0 & 1 & -7  & -7 \\
0 & 0 & 2 & -14 & -14
\end{array}
\end{bmatrix} 
\\ & 
\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -2R_2+R_3}
\\
\stackrel{\longrightarrow}{\scriptstyle 2R_2+R_1}
\end{array}
\begin{bmatrix}
\begin{array}{cccc|c}
1 & 1 & 0 & -10 & -9 \\
0 & 0 & 1 & -7 & -7 \\
0 & 0 & 0 & 0 & 0 
\end{array}
\end{bmatrix}
\end{align*}
The system in \ref{gjee1} is equivalent to 
$$
\begin{cases}
x+y-10t =-9 \\
z-7t  = -7
\end{cases}
\quad \text{ or more simply } \quad 
\begin{cases}
x =-9-y+10t \\
z =-7+7t.
\end{cases}
$$
Therefore
$$
\{(x,y,z,w)\in \mathbb{R}^4 \mid x=-9-y+10t, y=s, z=-7+7t, w=t, 
\text{ for } s,t\in \mathbb{R}\}.
$$
is the solution set.
::: 

::: {#exm- } 
Use the Gauss-Jordan elimination method to solve the following linear system.
$$
\begin{cases}
x_1+x_2-2x_3+3x_4 =4 \\
2x_1+3x_2+3x_3-x_4=3 \\
5 x_1+7 x_2+4 x_3+x_4 = 5
\end{cases}
$$
Using row operations on the augmented matrix we obtain the reduced row echelon form.
\begin{align*}
\begin{bmatrix}
\begin{array}{cccc|c}
1 & 1 & -2 & 3 & 4 \\
2 & 3 & 3 & -1 & 3 \\
5 & 7 & 4 & 1 & 5 
\end{array}
\end{bmatrix} 
& \begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -2R_1+R_2}
\\
\stackrel{\longrightarrow}{\scriptstyle -5R_1+R_3}
\end{array}
\begin{bmatrix}
\begin{array}{cccc|c}
1 & 1 & -2 & 3 & 4 \\
0 & 1 & 7 & -7 & -5 \\
0 & 2 & 14 & -14 & 15
\end{array}
\end{bmatrix}
\\ & \begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -2R_2+R_3}
\end{array}
\begin{bmatrix}
\begin{array}{cccc|c}
1 & 1 & -2 & 3 & 4\\
0 & 1 & 7 & -7 & -5 \\
0 & 0 & 0 & 0 & 25
\end{array}
\end{bmatrix}
\end{align*}
Notice the last row corresponds to the equation $0x_1+0x_2+0x_3+0x_4=25$. Therefore there are no solutions to this system.
Actually using Gauss-Jordan elimination means reaching reduced 
row echelon form; however, once we encounter the row 
$[0 \, \, \,  0  \, \, \,  0  \, \, \,  0  \, \, \,  25 ]$ 
we immediately stop and reach the correct conclusion: no solutions to the system. 
::: 

::: {#exm- } 
Find the polynomial of degree 2 whose graph goes through the points $(1,-1)$, $(2,3)$, and $(3,13)$.
Let $f(t)=a+b t+ c t^2$ be such a polynomial. Using the given points we setup a system and solve.
\begin{align*}
& 
\left[ \begin{array}{c|c} 
\begin{matrix}
a & b & c\\
a & 2b & 4c \\
a & 3b & 9c
\end{matrix} &  
\begin{matrix}
-1 \\ 3 \\ 13
\end{matrix} 
\end{array} \right]
\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -R_1+R_2} \\
\stackrel{\longrightarrow}{\scriptstyle -R_1+R_3}
\end{array}
\left[ \begin{array}{c|c} 
\begin{matrix}
a & b & c\\
0 & b & 3c \\
0 & 2b & 8c
\end{matrix} &  
\begin{matrix}
1 \\ 4 \\ 14
\end{matrix} 
\end{array} \right]
\\ & \qquad \qquad
\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -2R_2+R_3} 
\end{array}
\left[ \begin{array}{c|c} 
\begin{matrix}
a & b & c\\
0 & b & 3c \\
0 & 0 & 2c
\end{matrix} &  
\begin{matrix}
-1 \\ 4 \\ 6
\end{matrix} \end{array} \right]
\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle \frac{1}{2}R_3} 
\end{array}
\left[ \begin{array}{c|c} 
\begin{matrix}
a & b & c\\
0 & b & 3c \\
0 & 0 & c
\end{matrix} &  
\begin{matrix}
-1 \\ 4 \\ 3
\end{matrix} 
\end{array} \right]
\\ & \qquad \qquad
\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -3R_3+R_2} 
\end{array}
\left[ \begin{array}{c|c} 
\begin{matrix}
a & b & c\\
0 & b & 0 \\
0 & 0 & c
\end{matrix} &  
\begin{matrix}
-1 \\ -5 \\ 3
\end{matrix} 
\end{array} \right]
\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -R_2+R_1} 
\end{array}
\left[ \begin{array}{c|c}
\begin{matrix}
a& 0 & c\\
0 & b & 0 \\
0 & 0 & c
\end{matrix} &  
\begin{matrix}
4 \\ -5 \\ 3
\end{matrix} 
\end{array} \right]
\\ & \qquad \qquad
\begin{array}{c}
\stackrel{\longrightarrow}{\scriptstyle -R_3+R_1} 
\end{array}
\left[ \begin{array}{c|c} 
\begin{matrix}
a & 0 & 0 \\
0 & b & 0 \\
0 & 0 & c
\end{matrix} &  
\begin{matrix}
1 \\ -5 \\ 3
\end{matrix} 
\end{array} \right].
\end{align*}
Therefore the required polynomial is $f(t)=3t^2-5t+1$.
::: 

##  Exercises

::: {#exr- } 
Verify that $(2,3,-1)$ is a solution of the linear system.
 
- $\begin{cases}x+2y+z=7 \\ x-y=-1 \\ 4x+y+z=10 \end{cases}$
- $\begin{cases} x+y=5 \\ x-z=3 \\ y+z=2 \end{cases}$
::: 


::: {#exr- } 
Verify that every triple of the form $(7-2k,8+6k,k)$ is a solution of the linear system of equations 
 
- $\begin{cases} x_1+2x_3=7 \\ x_2-6x_3=8 \end{cases}$
- $\begin{cases} 2x_1+4x_3=14 \\ x_1+3x_2-16x_3=31 \end{cases}$
::: 

::: {#exr- } 
Do the following systems have no solutions, exactly one solution, or infinitely many solutions? Justify your answer. 
 
- $\begin{cases}x+y=1\\ x+2y=1 \end{cases}$
- $\begin{cases} 3x+y=1\\ y=-2 \end{cases}$
- $\begin{cases}x+y=1\\ 2x+2y=2 \end{cases}$
- $\begin{cases}3x+y=2 \\ 6x+2y=4 \end{cases}$
::: 

::: {#exr- } 
Sketch the graph of each equation of the linear system and decide whether it has no solutions, exactly one solution, or infinitely many solutions.
 
- $\begin{cases} x+y=2\\ 2x+3y=0 \end{cases}$
- $\begin{cases} -x+3y=2\\ 2x-6y=-4 \end{cases}$
::: 

::: {#exr- } 
Find the solutions, if any, of the following linear systems of equations without using matrices. If an equation has more than one solution, write the general solution. 
 
- $\begin{cases} 3x-2y= 7 \\ 2x+y=15 \end{cases}$
- $\begin{cases} x+y=7\\ 3x+4y=12  \end{cases}$
- $\begin{cases} 2x_1-3x_2=1\\ 4x_1-6x_2=-2\\ x_1+x_2=1 \end{cases}$
- $\begin{cases} 2x_1-5x_2=12 \end{cases}$
- $\begin{cases} i x_1-3ix_2= 1\\ (2+i)x_1-x_2=-1 \end{cases}$
- $\begin{cases} (1+i)x_1-2ix_2=2\\ 2x_1-3i x_2=4-3i \end{cases}$
::: 

::: {#exr- } 
If possible, find the point(s) of intersection.

- $x-4y=11$ and $7x-2y=9$
- $x-4y+3z=11$, $7x-2y-z=-1$, $7x-2y+z=-2$
::: 

::: {#exr- } 
Let $u=(1,1,2,-1)$ and $v=(1,1,1,0)$. For which scalars $a$ and $b$ is it true that $a u+b v$ is a solution to the following system?
 
- $\begin{cases} 4x-2y-z-w=1 \\ x+3y-2z-2w=2 \end{cases}$
- $\begin{cases}x-4y-z-2w=4 \\ 7x+y-5z-2w=12 \end{cases}$
::: 



::: {#exr- } 
Find all solutions, if any exist, for the following linear systems of equations.
 
- $\begin{cases} x-y=1\\ 2x=4 \end{cases}$
- $\begin{cases} 2x+3y-z=19\\ 3x-2y+3z=7\end{cases}$
- $\begin{cases}x-3y=2 \\-2x+6y=-4\end{cases}$
- $\begin{cases}x-y+2z-2w=1\\2x+y+3w=4 \\2x+y+3w=6\end{cases}$
::: 


::: {#exr- } 
Find all solutions, if any exist, for the following linear systems of equations.
 
- $\begin{cases}2x-z=-1\\x+y-z=0\\2x-y+2z=3\end{cases}$
- $\begin{cases}2x-3y+2z=1\\x-6y+z=2\\-x-3y-z=1\end{cases}$
::: 



::: {#exr- } 
Find all solutions, if any exist, for the following linear systems of equations.
 
- $\begin{cases} x_1+2x_2-x_3=0\\ 2x_1+5x_2+5x_3=0\\ x_1+4x_2+7x_3=0\\ x_1+3x_2+3x_3=0 \end{cases}$
- $\begin{cases} x+y+z+w=1\\ 2x-2y+z+2w=3\\ 2x+6y+3z+2w=1\\ 5x-3y+3z+5w=8 \end{cases}$
::: 



::: {#exr- } 
For what values of the constant $k$ do the systems have no solution, exactly one solution, or infinity many solutions. 
 
- $\begin{cases} x+y=1\\ 3x+3y=k \end{cases}$
- $\begin{cases} x+ky=1\\ 2x-y=k \end{cases}$
::: 


::: {#exr- } 
Find $a, b, c$ such that 
$$
\frac{x^2-x+3}{(x^2+2)(2x-1)}=\frac{a x+b}{x^2+2}+\frac{c}{2x-1}.
$$
::: 

::: {#exr- } 
Let $a$ and $b$ be arbitrary constants. Find all solutions to the linear system. 

- $\begin{cases} x+2y=a\\ 3x+5y=b \end{cases}$
- $\begin{cases} a x+2y=1\\ 3x+by=4 \end{cases}$
::: 

::: {#exr- } 
Let $a$ and $b$ be arbitrary constants. Find all solutions to the linear system. 
 
- $\begin{cases}x+2y+3z=a\\ x+3y+8z=b\\ x+2y+2z=c \end{cases}$
- $\begin{cases} x-2y+4z=a\\ x-3y+5z=b\\ x-2y+6z=c \end{cases}$
::: 

::: {#exr- } 
A system of linear equations all of whose constant terms are zero is called a 
\index{homogenous system} **homogenous system**. 

- Show that a homogenous system always has at least one solution.
- Give examples to show that a homogenous system may have more than one solution or exactly one solution. 
::: 

::: {#exr- } 
Write the system corresponding to each augmented matrix.
 
- $\left[\begin{array}{ccc|c}
1 & 2 & 1 & 1 \\
0 & 4 & 0 & 1 \\
0 & 0 & 3 & 3
\end{array}\right]$
- $\left[\begin{array}{cccc|c}
1 & 0 & 0 & 0 & 0
\end{array}\right]$
- $\left[\begin{array}{ccc|c}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0
\end{array}\right]$
- $\left[\begin{array}{c|c}
2 & 1 \\
0 & 1 \\
1 & 2
\end{array}\right]$
- $\left[\begin{array}{c|c}
1 & 1 \\
1 & 0
\end{array}\right]$
- $\left[\begin{array}{cccc|c}
1 & 1 & 1 & 1 & 0
\end{array}\right]$
::: 


::: {#exr- } 
Which of the following matrices are in reduced row echelon form?
 
- $\begin{bmatrix}
0 & 1 & 2 & 0 & 3 \\
0 & 0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0 & 0
\end{bmatrix}$
- $\begin{bmatrix}
1 & 0 & 0 & 2 & 1 \\
0 & 1 & 0 & 0 & 0 \\
0 & 0 & 1 & 0 & 0
\end{bmatrix}$
- $\begin{bmatrix}
1 & 1 & 1 \\
0 & 0 & 1\\
0 & 0 & 0 \\
0 & 0 & 0 
\end{bmatrix}$
- $\begin{bmatrix}
0 & 0 & 1\\
0 & 0 & 1\\
0 & 0 & 1\\
\end{bmatrix}$
::: 


::: {#exr- } 
Which of the following matrices are in reduced row echelon form?
 
- $\begin{bmatrix}
0 & 0 & 0 & 1  \\
0 & 0 & 0 & 0\\
\end{bmatrix}$
- $\begin{bmatrix}
0 & 0 & 0 \\
0 & 0 & 0 \\
\end{bmatrix}$
- $\begin{bmatrix}
1 & 0 & 0 & 3 & 1 \\
0 & 0 & 0 & 1 & 1 \\
0 &0 & 0 & 0 & 1
\end{bmatrix}$
- $\begin{bmatrix}
1 & 0 & 2 & 1 \\
0 & 1 & 0 & 1 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 0
\end{bmatrix}$
::: 

::: {#exr- } 
Find all solutions of the linear system using the Gauss-Jordan elimination method.
 
- $\begin{cases} x_2+2x_4+3x_5=0\\ 4x_4+8x_5=0 \end{cases}$
- $\begin{cases} 3x+4y =0\\ -2x+7y=0 \end{cases}$
::: 



::: {#exr- } 
Find all solutions of the linear system using the Gauss-Jordan elimination method.
 
- $\begin{cases} x_4+2x_5-x_6=2 \\ x_1+2x_2+x_5-x_6=0\\ x_1+2x_2+2x_3-x_5+x_6=2 \end{cases}$
- $\begin{cases} 3x_1+3x_2-4x_3+x_4=2 \\ x_1+x_2-x_3-x_4=5 \end{cases}$
- $\begin{cases} 2 x_1+3x_2+4x_3=6-x_4 \\ 3x_1-2x_2-x_4=1+4x_3 \\ 3x_1+3x_3+x_4=4-x_2 \\ x_2+x_3-4x_4=-3-4x_1\end{cases}$
- $\begin{cases} 4x_1+3x_2+2x_3-x_4=4 \\ 5x_1+4x_2+3x_3-x_4=4\\ -2x_1-2x_2-x_3+2x_4=-3\\ 11x_1+6x_2+4x_3+x_4=11 \end{cases}$
- $\begin{cases} 2x_1-3x_2+x_3=5 \\ x_1+x_2-x_3=3\\ 4x_1-x_2-x_3=1 \end{cases}$
- $\begin{cases} x_1-x_2=4\\ 2x_1+x_2=7\\ 5x_1-2x_2=19 \end{cases}$
::: 

::: {#exr- } 
Find all $4\times 1$ matrices in reduced row echelon form. 
::: 


::: {#exr- } 
How many types of $3\times 2$ matrices in reduced row echelon form are there?
- How many types of $2\times 3$ matrices in reduced row echelon form are there?
::: 

::: {#exr- } 

- Describe the possible reduced row echelon forms for a matrix with two rows and two columns. 
- Describe the possible reduced row echelon forms for a matrix with three rows and three columns. 

::: 

::: {#exr- } 
Find the polynomial of degree 3 whose graph passes through the points 
$(0,1)$, $(1,0)$, $(-1,0)$, and $(2,-15)$. Sketch the graph of this cubic. 
::: 

::: {#exr- } 
Find the polynomial of degree 4 whose graph passes through the points $(1,1)$, $(2,-1)$, $(3,-59)$, and $(-2, -29)$. Sketch the graph of this quartic.
::: 

::: {#exr- } 
Find the values of $k$, if any, for which the system has

- only one solution, 
- no solutions, 
- an infinite number of solutions
 
- $\begin{cases} x_1-x_2=3 \\ 3x_1+kx_2=6 \end{cases}$
- $\begin{cases}x_1+2k x_2 =7\\ x_1+(k^2+1)x_2=3 \end{cases}$
- $\begin{cases}k x_1-x_2=1 \\ x_1+k x_2=i\end{cases}$
- $\begin{cases} k x+|k| y=1 \\ |k|x-ky=-1 \end{cases}$
::: 

::: {#exr- } 
Find values for $a$ such that the system of linear equations
$$\begin{cases} x_1-3x_2+x_3=1 \\ 2x_1+x_2-x_3=-1 \\ 5x_1-8x_2+(a^2-2)x_3=a  \end{cases}$$
will have 

- infinitely many solutions, 
- no solutions
- exactly one solution.
::: 


::: {#exr- } 
Use Gauss-Jordan elimination to solve the linear system
$$
\left\{ \begin{array}{lll}
a x_1+ b x_2 & & =r \quad (a\neq 0) \\ 
c x_1+ d x_2 & & =s \quad (e\neq 0)\\ 
& e x_3+f x_4 & =t \\
& g x_3+h x_4 & =u \\
\end{array}\right.$$
State conditions on $a, b, c, d, e, f, g$ and $h$ which guarantee a unique solution.
::: 

::: {#exr- } 
Show that the system
$$\begin{cases}a_1 x+ b_1 y+c_1 z=0 \\ a_2 x+b_2 y +c_2 z=0 \end{cases}$$
always has a solution other than $x=0$ and $y=0$.
::: 


::: {#exr- } 
What can you say about the last row of the reduced row echelon form of the following matrix?
$$
\begin{bmatrix}
1 & 2 & 3 & \cdots & n\\
n+1 & n+2 & n+3 & \cdots & 2n \\
\vdots & \vdots & \vdots & \vdots &  \vdots \\
n^2-n+1 & n^2-n+2 & n^2-n+3 & \cdots & n^2
\end{bmatrix}
$$
::: 


::: {#exr- } 
Show that the linear system of equations
$$\begin{cases} 3x_1+2x_2-4x_3=a \\ -4x_1+x_2-x_3 =b \\ 7x_1+12x_2-22x_3 =c \end{cases}$$
will have infinitely many solutions if $c=5a+2b$.
::: 


::: {#exr- } 
Find an equation of the form $x^2+y^2+a x+by+c=0$  of the circle passing through the following points. 
 
- $(-2,1)$, $(5,0)$, and $(4,1)$
- $(1,1)$, $(5,-3)$, and $(-3,-3)$
::: 


## Matrices and Vectors


The entries of an $n\times m$ \index{matrix} **matrix** $A$ are denoted by $a_{i,j}$ where $1\leq i\leq n$ and $1\leq j \leq m$.
For example, the matrix 
$$
\begin{bmatrix}a_{11} & a_{12} & a_{13} \\ a_{21} & a_{22} & a_{23}\end{bmatrix}
$$
is a $2\times 3$ matrix with $i=2$ rows and $j=3$ columns. 
If $i=j$, then the matrix is called a \index{square matrix} **square matrix** .

::: {#def- } 
Two matrices $A=[a_{ij}]$ and $B=[b_{ij}]$ of the same size are equal when $a_{ij}=b_{ij}$ for all $i,j$. 
::: 

A square matrix $A=[a_{ij}]$ is called a \index{diagonal matrix} **diagonal matrix** if $a_{ij}=0$ whenever $i\neq j$. 
A square matrix  $A=[a_{ij}]$ is called \index{upper triangular} **upper triangular** (\index{lower triangular} **lower triangular**) when $a_{ij}=0$ whenever $i>j$ ($i<j$). 
The \index{zero matrix} **zero matrix** has all entries zero and the \index{identity matrix} **identity matrix** has $a_{ii}=1$, $i=1, 2, 3...,n$ and all other entries zero. We remark that when $m$ and $n$ are understood from discussion they are usually left off.
$$
0_{m\times n}=
\begin{bmatrix}
0 & \cdots & 0 \\
0 & \cdots & 0 \\
\vdots & \cdots & \vdots \\
0 & \cdots &0 
\end{bmatrix}_{m\times n}
\qquad
I_n=
\begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}_{n\times n}
$$
 
Consider the following matrices.
$$
\begin{array}{lllll}
A=\begin{bmatrix}1 & 2 & 3 \\ 4 & 5 & 6 \end{bmatrix}
&
B=\begin{bmatrix} 2 & 0 & 0 \\ 0 & 3 & 0 \\ 0 & 0 & 0 \end{bmatrix}
&
C=\begin{bmatrix} 2 & 3 \\ 0 & 4 \end{bmatrix}
&
D=\begin{bmatrix}5 & 0 & 0 \\ 4 & 0 & 0 \\ 3 & 2 & 1\end{bmatrix}
\end{array}
$$
Notice matrices $B$, $C$ and $D$ are square and $A$ is not.
Also notice the only diagonal matrix is $B$, matrices $B$ and $C$ are upper triangular, and the only lower triangular matrix is $D$.

We will see that matrix addition has many of the same properties that scalar addition has; for example, commutativity, associativity, additive inverses, and the additive identity property.  

::: {#def- } 
Let  $A=[a_{ij}]$ and $B=[b_{ij}]$ be matrices of the same size. The \index{matrix sum} **matrix sum**  of $A$ and $B$ is the matrix $C$ defined by 
$$
C=A+B=[a_{ij}]+[b_{ij}]=[a_{ij}+b_{ij}].
$$
::: 

::: {#thm- } 
[Properties of Matrix Addition]
\label{PropertiesofMatrixAddition}
Let $A$, $B$, and $C$ denote arbitrary matrices such that the following operations are defined. 
Then the following hold. 
 
- $A+B=B+A$, 
- $A+(B+C)=(A+B)+C$, 
- for each $A$, there exists $0$ such that $0+A=A$, and
- for each $A$, there exists a matrix $-A$ such that $A+(-A)=0$.
 
::: 

::: {.proof }

- Let $A$ and $B$ denote matrices of the same size with entries $A_{ij}$ and $B_{ij}$, respectively. Then 
\begin{align*}
(A+B)_{ij} & =A_{ij}+B_{ij} & \text{definition of matrix addition}  \\
& =B_{ij}+A_{ij} & \text{scalar commutativity}  \\
& =(B+A)_{ij}  & \text{definition of matrix addition}
\end{align*}
Therefore, by the definition of matrix equality, 
$A+B=B+A$ follows. 
- If matrix $C$ is also of the same size as $A$ and $B$ with entries $C_{ij}$ then,
\begin{align*}
(A+(B+C))_{ij} & =A_{ij}+\left(B+C\right)_{ij} &  \text{definition of matrix addition}   \\
 & =A_{ij}+\left(B_{ij}+C_{ij}\right) &  \text{definition of matrix addition}   \\
 & =\left(A_{ij}+B_{ij}\right)+C_{ij} &  \text{scalar associativity}   \\
 & =\left(A+B\right)_{ij}+C_{ij} &  \text{definition of matrix addition}   \\
 & =(\left(A+B\right)+C)_{ij} &  \text{definition of matrix addition} 
\end{align*}
Therefore, by the definition of matrix equality, 
$A+(B+C)=(A+B)+C$ follows. 
- Let $A$ be an $m\times n$ matrix with entires $A_{ij}$ and let $0_{m\times n}$ denote the $m\times n$ matrix with all entires equal to the scalar additive identity denoted by $0$. Denoting the entries of $0_{m\times n}$ by $0_{ij}$ we find

\begin{align*}
(0+A)_{ij}=0_{ij}+A_{ij} = A_{ij}.
\end{align*}

Therefore, by the definition of matrix equality, 
$0+A=A$ follows. 
- The proof of this part is left for the reader as Exercise \ref{ex:PropertiesofMatrixAddition}.
::: 

The matrix properties listed in \ref{PropertiesofMatrixAddition} are called the 

- \index{commutative} **commutative** , 
- \index{associative} **associative** , 
- \index{additive identity} **additive identity** , and  
- \index{additive inverse} **additive inverse**  
properties for matrix addition, respectively. 

Just as matrix addition has several properties in common with the underlying field of scalars so too does scalar multiplication.  

::: {#def- } 
If $A$ is any matrix and $k$ is any scalar, the \index{scalar multiple} **scalar multiple** $kA$ is the matrix obtained by multiplying each entry of $A$ by $k$, that is if $A=[a_{ij}]$, then $k A=[k a_{ij}].$
::: 

The following properties say that scalers and matrices distribute over each other and are associative with each other. 

::: {#thm- } 
[Properties of Scalar Multiplication]
\label{Properties of Scalar Multiplication}
Let $A$ and $B$ denote arbitrary matrices such that the following operations are defined. Let $k$ and $p$ denote arbitrary scalars. 
Then the following hold.
 
- $k(A+B)=k A+k B$
- $(k+p)A=k A+p A$
- $(kp)A=k(pA)$
- $A(kB)=(kA)B$
::: 

::: {.proof }
- Let $A$ and $B$ be $n\times m$ matrices. Then, for $1\leq i \leq n$ and $1\leq j\leq m$ and an arbitrary scalar $k$ we find,
\begin{align*}
\left(k(A+B)\right)_{ij}& =k\left((A+B)_{ij}\right) &  \text{definition of scalar multiplication}   \\
& =k(A_{ij}+B_{ij})&  \text{definition matrix addition}   \\
& =(kA)_{ij}+(kB)_{ij}&  \text{distributive property of scalar multiplication}   \\
& =(kA+kB)_{ij}&  \text{definition of matrix addition} 
\end{align*}
Therefore, by the definition of matrix equality, 
$k(A+B)=k A+k B$ follows. 

Let $A$ be an $n\times m$ matrix and let $k$ and $p$ be scalars. Then we find,
\begin{align*}
((kp)A)_{ij}& =(kp)A_{ij}&  \text{definition of scalar multiplication}   \\
& =k(pA_{ij})&  \text{associative property of scalars}   \\
& =k(pA)_{ij}&  \text{definition of scalar multiplication}   \\
& =(k(pA))_{ij}&  \text{definition of scalar multiplication}
\end{align*}
Therefore, by the definition of matrix equality, 
$(kp)A=k(pA)$ follows. 
The proof of the remaining parts are left for the reader as Exercise \ref{ex:Properties of Scalar Multiplication}. 
::: 

::: {#exm- } 
If $A=\begin{bmatrix} 3 & -1 & 4 \\ 2 & 0 & 6 \end{bmatrix}$ and $B=\begin{bmatrix} 1 & 2 & -1\\ 0 & 3 & 2 \end{bmatrix},$ determine $5A$, $\frac{1}{2}B$, and $3A-2B$. A quick computation reveals the following.
$$
5A= 
\begin{bmatrix}
 15 & -5 & 20 \\
 10 & 0 & 30
\end{bmatrix}
\quad
\frac{1}{2}B= 
\begin{bmatrix}
 \frac{1}{2} & 1 & -\frac{1}{2} \\
 0 & \frac{3}{2} & 1
\end{bmatrix}
\quad
3A-2B=
\begin{bmatrix}
 7 & -7 & 14 \\
 6 & -6 & 14
\end{bmatrix}
$$
::: 

::: {#exm- } 
Let $A$, $B$, and $C$ be matrices of the same size. 
Simplify 
$$
2(A+3C)-3(2C-B)-3[2(2A+B-4C)-4(A-2C)].
$$ 
We obtain $2 A - 3 B$. 
::: 

::: {#exm- } 
If $kA=0$, show that either $k=0$ or $A=0$. 
Suppose $k\neq 0$ and $A=[a_{ij}]$, then $kA=[k a_{ij}]=0$ and notice each entry $k a_{ij}$ is zero if and only if $a_{ij}=0$ for every $i$ and $j$. Therefore, either $k=0$ or every entry in $A$ is zero. 
::: 

::: {#exm- } 
Find $1\times 3$ matrices $X$ and $Y$ such that 
$$
\begin{cases} 
X+2Y= \begin{bmatrix} 1 & 3 & -2 \end{bmatrix} \\
X+Y= \begin{bmatrix} 2 & 0 & 1 \end{bmatrix}.
\end{cases}
$$
Subtracting these equations together yields
$Y=\begin{bmatrix} -1 & 3 & -3 \end{bmatrix}.$ 
Then $X=\begin{bmatrix} 2 & 0 & 1 \end{bmatrix}$ and $-Y= \begin{bmatrix} 3 & -3 & 4 \end{bmatrix}.$
::: 

Let $A$ be an $m\times n$ matrix and $B$ a $n\times p$ matrix, their \index{matrix product} **matrix product** $C=AB$ is defined as the the $m\times p$ matrix whose $(i,j)$th entry is the dot product of the $i$th row of $A$ 
with the $j$th column of $B$, that is, the product of two matrices is the matrix $C$ where $c_{ij}=\sum_{k=1}^n a_{ik}b_{kj}$.
Matrix multiplication is illustrated by the following equation.
\begin{equation*}
\label{matprod}
AB=
\begin{bmatrix}
a_{ij}
\end{bmatrix}
\begin{bmatrix}
b_{ij}
\end{bmatrix}
=
\begin{bmatrix}
\displaystyle{\sum_{k=1}^n a_{1k}b_{k1}} & \cdots & \displaystyle{\sum_{k=1}^n a_{1k}}b_{kp} \\
\vdots & \displaystyle{\sum_{k=1}^n a_{ik}b_{kj}} & \vdots \\
\displaystyle{\sum_{k=1}^n a_{mk}b_{k1}} & \cdots & \displaystyle{\sum_{k=1}^n a_{mk}}b_{kn}
\end{bmatrix}
=
\begin{bmatrix}
c_{ij}
\end{bmatrix}
=C
\end{equation*}

::: {#exm- } 
\label{MatrixMult}
Let $A$ and $B$ be defined as follows.
$$
A=
\begin{bmatrix}
4 & -5 & 6 \\
1 & 8 & -9
\end{bmatrix}
\qquad 
\qquad
B=
\begin{bmatrix}
1 & 2 & -3 \\
-2 & -1 & 5 \\
-2 & 5 & -1
\end{bmatrix}
$$
If possible, find $AB$ and $BA$.
Since $A$ is a $2\times 3$ matrix and $B$ is a $3\times 3$ matrix we see that $AB$ is defined as a $2\times 3$ matrix and $BA$ is not defined. 
\begin{align*}
AB & 
=
\begin{bmatrix}
\begin{bmatrix} 4 & -5 & 6 \end{bmatrix}\cdot\vectorthree{1}{-2}{-2}
\vspace{5pt}
& 
\begin{bmatrix} 4 & -5 & 6 \end{bmatrix}\cdot\vectorthree{2}{-1}{5}
&
\begin{bmatrix} 4 & -5 & 6 \end{bmatrix}\cdot\vectorthree{-3}{5}{-1}
\hspace{5pt}
\\
\begin{bmatrix} 1 & 8 & -9 \end{bmatrix}\cdot\vectorthree{1}{-2}{-2}
& 
\begin{bmatrix} 1 & 8 & -9 \end{bmatrix}\cdot\vectorthree{2}{-1}{5}
&
\begin{bmatrix} 1 & 8 & -9 \end{bmatrix}\cdot\vectorthree{-3}{5}{-1}
\hspace{5pt}
\end{bmatrix}
&
= \begin{bmatrix}
2 & 43 & -19 \\
3  &-51 & 52
\end{bmatrix}
\end{align*}
::: 

\ref{MatrixMult} is a counterexample showing that matrix multiplication is not commutative. Even so, matrix multiplication has many useful properties such as associativity, multiplicative inverses, and distributive properties.  

::: {#thm- } [Properties of Matrix Multiplication]
\label{Properties of Matrix Multiplication}
Let $A$, $B$, and $C$ denote arbitrary matrices such that the following operations are defined. Then the following hold.
 
- $A(BC)=(AB)C$
- $A(B+C)=AB+AC$
- $(B+C)A=BA+CA$
- $AI=A$
- $IA=A$
::: 

::: {.proof }

- Suppose $B$ is an $n\times p$ matrix. Since $BC$ is defined, $C$ is a matrix with $p$ rows, and $BC$ has $n$ rows. Because $A(BC)$ is defined we may assume $A$ is an $m\times n$ matrix. Thus the product $AB$ exists and it is an $m\times p$ matrix, from which it follows that the product $(AB)C$ exists. To show that $A(BC)=A(BC)$ we must show that 
\begin{equation}
\label{matass}
[A(BC)]_{ij}=[(AB)C]_{ij}.
\end{equation}  
for arbitrary $i$ and $j$.
By definition of matrix multiplication,
\begin{align*}
[A(BC)]_{ij} & 
= \sum_t A_{it}(BC)_{tj} 
= \sum_t A_{it} \sum_s B_{t s} C_{sj} \\
& = \sum_t \sum_s A_{it} B_{ts} C_{sj}
=\sum_s \sum_t A_{it} B_{ts} C_{sj} \\
& =\sum_s \left( \sum_t A_{it} B_{ts} \right) C_{sj}
=\sum_s (AB_{i s})  C_{sj}
=[(AB)C]_{ij}.
\end{align*}
- Let $A_{ij}$, $B_{ij}$, and $C_{ij}$ denote the entries of matrices $A$, $B$ and $C$ of sizes $m\times n$, $n\times p$, and $n\times p$, respectively. 
Then $B+C=[B_{ij}+C_{ij}]$, so the $(i,j)$th entry of $A(B+C)$ is 
$$
\sum_{k=1}^n A_{ik}\left(B_{kj}+C_{kj}\right)
=\sum_{k=1}^n \left(A_{ik} B_{kj}+A_{ik} C_{kj}\right)
=\sum_{k=1}^n A_{ik} B_{kj}+\sum_{k=1}^n A_{ik} C_{kj}
$$
this is the $(i,j)$th entry of $AB+AC$ because the sums on the right are the $(i,j)$th entries of $AB$ and $AC$, respectively. Hence $A(B+C)=AB+AC$.

Let $A=[a_{ij}]_{m\times n}$. Then 
$$
AI=
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \ddots & \ddots & \vdots \\
a_{m_1} & a_{m2} & \cdots & a_{mm}
\end{bmatrix}_{m\times n}
\begin{bmatrix}
1 & 0 & 0 & \cdots & 0 \\
0 & 1 & 0 & \cdots & 0\\
0 & 0 & 1 & \cdots & 0 \\
\vdots & \ddots & \ddots & \ddots & \vdots \\
0 & 0 & 0 & \cdots & 1
\end{bmatrix}_{n\times n}.
$$
and the $(i,j)$th entry of $AI$ is the $i$th row of $A$ times the $j$th column of $I$, that is 
$$
\fbox{$\begin{matrix}
a_{i1} & a_{i2} & \cdots & a_{ij} & \cdots & a_{in}
\end{matrix}$}
\quad 
\begin{array}{rl}
\fbox{$
\begin{matrix}
0 \\ 0 \\ \vdots \\ 1 \\ \vdots \\ 0 
\end{matrix}
$} & 
\begin{matrix}
\text{ } \\ \text{ } \\ \text{ } \\ \leftarrow \text{Position $j$} \\  \text{ } \\ \text{ } 
\end{matrix}
\end{array}
$$
So the $(i,j)$th entry of $AI$ is $a_{ij}$, the $(i,j)$th entry of $A$. Thus, $AI=A$.
The proof of the remaining parts are left for the reader as Exercise \ref{ex:Properties of Matrix Multiplication}. 
::: 

The matrix properties listed in \ref{Properties of Matrix Multiplication} are called the 

- \index{associative} **associative** , 
- \index{left distributive} **left distributive** , 
- \index{right distributive} **right distributive** , 
- \index{left identity} **left identity** , and  
- \index{right identity} **right identity**  
property for matrix multiplication, respectively. 



::: {#exm- } 
Find all matrices that commute with $A=\begin{bmatrix} 1 & 2 \\ 0 & 1 \end{bmatrix}$.
Let $B=\begin{bmatrix}  a & b \\ c & d\end{bmatrix}$ be a matrix that commutes with $A$. Matrix multiplication yields  
$$
AB=\begin{bmatrix}  a+2c & b+2d \\ c & d\end{bmatrix}
\qquad \text{and} \qquad 
BA=\begin{bmatrix} a & 2a+b \\ c & 2c+d \end{bmatrix}.
$$
Since $a+2c=a$ it follows that $c=0$. Since $b+2d=2a+b$ it follows that $a=d$. Thus all matrices that commute with  $A$ have the form 
$\begin{bmatrix}  a & b \\ 0 & a  \end{bmatrix}.$
::: 

::: {#exm- } 
Write the matrix equation for the system 
$$
\begin{cases}
3x_1+x_2+7x_3+2x_4 =13 \\
2x_1-4x_2+14x_3-x_4 =-10 \\
5x_1+11x_2-7x_3+8x_4 =59 \\
2x_1+5x_2-4x_3-3x_4=39.
\end{cases}
$$
Find the reduced row echelon form using Gauss-Jordan elimination. Solve the system. 
The matrix equation is 
$$
\begin{bmatrix}
3 & 1 & 7 & 2 \\
2 & -4 & 14 & -1 \\
5 & 11 & -7 & 8 \\
2 & 5 & -4 & -3
\end{bmatrix}
\vectorfour{x_1}{x_2}{x_3}{x_4}
=\vectorfour{13}{-10}{59}{39}.
$$
The  reduced row echelon form of the coefficient matrix and the solution set are as follows
$$
\begin{bmatrix}
1 & 0 & 3 & 0 & 4 \\
0 & 1 & -2 & 0 & 5 \\
0 & 0 & 0 & 1 & -2 \\
0 & 0 & 0 & 0 & 0
\end{bmatrix}
\qquad 
%\text{and} 
\qquad 
\left\{\vectorfour{4-3t}{5+2c}{t}{-2} \mid t\in \mathbb{R}\right\}
$$
::: 

A matrix with a single column is called a \index{column vector} **column vector** or just \index{vector} and a matrix with a single row is called a \index{row vector} **row vector**. 

::: {#def- } 
A vector $\vec{b}$ is called a \index{linear combination} **linear combination**  of the vectors 
$\vlist{v}{m}$ if there exists scalars $\vlist{a}{m}$ such that 
$$
\vec{b}=\lincomb{a}{v}{m}.
$$
::: 

::: {#exm- } 
Given the following vectors, is $\vec{u}$ a linear combination of the vectors $\vec{v}_1$ and 
$\vec{v}_2$?
$$
\vec{u}=\begin{bmatrix} 7 \\8 \\9 \end{bmatrix}
\qquad
\vec{v}_1=\begin{bmatrix} 1 \\ 2 \\3 \end{bmatrix}
\qquad
\vec{v}_2=\begin{bmatrix} 4 \\ 5 \\6 \end{bmatrix}
$$
We want to find scalars $x_1$ and $x_2$ such that 
$\vec{u}=x_1\vec{v}_1+x_2\vec{v}_2.$
We solve for $x_1$ and $x_2$ using an augmented matrix and row-operations as follows. 
$$
\begin{bmatrix}
1 & 4 & 7 \\
2 & 5 & 8 \\
3 & 6 & 9
\end{bmatrix}\begin{array}{c}
\stackrel{\longrightarrow}{-2R_1+R_2}
\\
\stackrel{\longrightarrow}{-3R_1+R_3}
\end{array}
\begin{bmatrix}
1 & 4 & 7 \\
0 & -3 & -6 \\
0 & -6 & -12
\end{bmatrix}\begin{array}{c}
\stackrel{\longrightarrow}{-\frac{1}{3}R_2}
\\
\stackrel{\longrightarrow}{-\frac{1}{6}R_3}
\end{array}
\begin{bmatrix}
1 & 4 & 7 \\
0 & 1 & 2 \\
0 & 1 & 2
\end{bmatrix}
$$
$$
\stackrel{\longrightarrow}{-R_2+R_3}
\begin{bmatrix}
1 & 4 & 7 \\
0 & 1 & 2 \\
0 & 0 & 0
\end{bmatrix}\stackrel{\longrightarrow}{-4R_2+R_1}
\begin{bmatrix}
1 & 0 & -1 \\
0 & 1 & 2 \\
0 & 0 & 0
\end{bmatrix}
$$
Thus we find $x_1=-1$ and $x_2=2$. Therefore, yes $\vec{u}$ is a linear combination of $\vec{v}_1$ and $\vec{v}_2$, namely 
$\vec{u}=2\vec{v}_2-\vec{v}_1$.
::: 

::: {#exm- } 
Given the following vectors, determine whether $\vec{u}$ or $\vec{w}$ is a linear combination of the vectors $\vec v_1$ and $\vec v_2$.
$$
\vec{u}=\vectorthree{1}{1}{4}
\qquad
\vec{w}=\vectorthree{1}{5}{1}
\qquad
\vec v_1=\vectorthree{1}{2}{-1}
\qquad
\vec v_2=\vectorthree{3}{5}{2}
$$
First, $\vec{u}$  is a linear combination of $\vec v_1$ and $\vec v_2$ since 
$$
\vectorthree{1}{1}{4}
=(-2)\vectorthree{1}{2}{-1}+(1)\vectorthree{3}{5}{2}.
$$
To determine whether $\vec{w}$ is a linear combination of $\vec{v}_1$ and $\vec{v}_2$ we consider the equation 
$$
\vec{w}=x \vectorthree{1}{2}{-1}+y\vectorthree{3}{5}{2}
$$
which leads to the system
$$
\begin{cases}
x+3y= 1 \\
2x+5y =5 \\
-x+2y =1.
\end{cases}
$$
This linear system can be shown to have an empty solution set; and thus $\vec{w}$ is not a linear combination of $\vec v_1$ and $\vec v_2$. 
::: 

::: {#exm- } 
For which values of the constant $c$ is 
$\vectorthree{1}{c}{c^2}$ a linear combination of $\vectorthree{1}{a}{a^2}$ and $\vectorthree{1}{b}{b^2}$, where $a$ and $b$ are arbitrary constants?
We need to solve the following linear system 
$$
\vectorthree{1}{c}{c^2}=x\vectorthree{1}{a}{a^2}+y\vectorthree{1}{b}{b^2}
\qquad
\text{ with augmented matrix}
\qquad
\begin{bmatrix}
\begin{array}{cc|c}
1 & 1 & 1 \\
a & b & c \\
a^2 & b^2 & c^2
\end{array}
\end{bmatrix}.
$$
using row operations the augmented matrix reduces to 
$$
\begin{bmatrix}
\begin{array}{cc|c}
1 & 1 & 1 \\
0 & b-a & c-a \\
0 & 0 & (c-a)(c-b)
\end{array}
\end{bmatrix}.
$$
This system is consistent if and only if $c=a$ or $c=b$. Thus the vector is a linear combination if $c=a$ or $c=b$.
::: 

::: {#exm- } 
Express the vector $\begin{bmatrix} 7 \\11 \end{bmatrix}$ as the sum of a vector on the line $y=3x$ and a vector on the line $y=x/2$. 
We wish to find $x_1$ and $x_2$ such that 
$$
\begin{bmatrix} 7 \\ 11 \end{bmatrix}=
\begin{bmatrix} x_1 \\ 3x_1 \end{bmatrix}+
\begin{bmatrix} x_2 \\ \frac{1}{2}x_2 \end{bmatrix}.
$$
We solve this linear system using row-operations as follows.
$$
\begin{bmatrix}1 & 1 & 7 \\
3 & \frac{1}{2} & 11
\end{bmatrix}
\stackrel{\longrightarrow}{\scriptstyle -3R_1+R_2}
\begin{bmatrix}1 & 1 & 7\\
0 & \frac{-5}{2} & -10
\end{bmatrix}
%$$$$\qquad \qquad
\stackrel{\longrightarrow}{\scriptstyle(-2/5)R_2}
\begin{bmatrix}1 & 1 & 7 \\
0 & 1 & 4 
\end{bmatrix}\stackrel{\longrightarrow}{\scriptstyle-R_2+R_1}
\begin{bmatrix}1 & 0 & 7 \\
0 & 1 & 4
\end{bmatrix}
$$
The solution is $x_1=3$ and $x_2=4$. Therefore the desired sum is  
$$
\begin{bmatrix} 7 \\ 11 \end{bmatrix}=
\begin{bmatrix} 3 \\ 9 \end{bmatrix}+
\begin{bmatrix} 4 \\ 1 \end{bmatrix}
$$
since $\begin{bmatrix} 3 \\ 9 \end{bmatrix}$ is on the line $y=3x$ and $\begin{bmatrix} 4 \\ 1 \end{bmatrix}$ is on the line $y=x/2.$
::: 

::: {#thm- } 
Let $B$ be an $n\times p$ matrix and $A$ and $p \times m$ matrix with columns $\vec v_1$, ..., $\vec v_m$. Then the product $B A$ is
$$
B A = B 
\begin{bmatrix} 
\vec v_1 & \cdots & \vec v_m
\end{bmatrix}
=
\begin{bmatrix} 
B \vec v_1 & \cdots & B \vec v_m
\end{bmatrix} .
$$
::: {.proof }
Notice that $\vec v_1$, ..., $\vec v_m$ are all $p\times 1$ column vectors consisting of the columns of $A$.
So the product of $B$ and each $\vec v_i$ ($0\leq i \leq m$) are the  $n\times 1$ column vectors constituting $BA$.
::: 
::: 

::: {#thm- } 
\label{colvecmat}
If the column vectors of an $n\times m$ matrix $A$ are $\vec{v}_1,\vec{v}_2,...,\vec{v}_n$ and $\vec{x}$ is a vector with entries $\vlist{x}{m}$, then
$$
A \vec x 
%=\begin{bmatrix} | & & | \\ \vec v_1 & \cdots & \vec v_m \\ | & & | \end{bmatrix}\vec x 
=\lincomb{x}{v}{m}.
$$
::: 

::: {.proof }
The proof follows from the following equation.
\begin{align*}
A \vec x 
& =\begin{bmatrix} | & & | \\ \vec v_1 & \cdots & \vec v_m \\ | & & | \end{bmatrix}\vec x 
=
\begin{bmatrix} 
v_{11} & \cdots & v_{1m} \\ 
\vdots & \ddots & \vdots \\
v_{n1} & \cdots & v_{nm} \\ 
\end{bmatrix} 
\vectorfour{x_1}{x_2}{\vdots}{x_m} \\
& =
\begin{bmatrix} 
v_{11} x_1 + v_{12} x_2 + \cdots + v_{1m}x_m \\ 
\vdots  \\
v_{n1} x_1 + v_{n2} x_2 + \cdots + v_{nm} x_m 
\end{bmatrix}_{n\times 1} 
= \lincomb{x}{v}{m}
\end{align*}
::: 

::: {#cor- } 
Let $A$ be an $n\times m$ matrix, $\vec x$ and $\vec y$ be $m\times 1$ column vectors,
and let $k$ be a scalar. Then the following hold.
 
- $A(\vec x + \vec y)=A(\vec x) + A(\vec y)$
- $A(k \vec x )=k A(\vec x)$
 
::: 


::: {.proof }
Since the column vectors $\vec x$ and $\vec y$  are matrices and the matrices 
$A(\vec x + \vec y)$, 
$A(\vec x) + A(\vec y)$,
$A(k \vec x )$, and 
$k A(\vec x)$ 
are defined we can apply 
\ref{Properties of Matrix Multiplication} to obtain 
$A(\vec x + \vec y)=A(\vec x) + A(\vec y)$
and 
$A(k \vec x )=k A(\vec x)$.
::: 


##  Exercises

::: {#exr- } 
Let $A$ be a matrix of size $4\times 5$, let $B$ be a matrix of size $6\times 4$, let $C$ be a matrix of size $4\times 5$, and let $D$ be a matrix of size $4\times 2$. Which of the following are defined, and for those that are, what is their size?
 
- $BA$
- $DA$
- $B+A$
- $C+A$
- $C(AB)$
- $(BA)C$
- $C+DB$
- $D(C+A)$
- $A+CB$
 
::: 

::: {#exr- } 
Let 
$A=\begin{bmatrix} 2 & 1 & 3 \\ 4 & 0 & -1 \\ -1 & 1 & 0 \end{bmatrix}$
and 
$B=\begin{bmatrix} 1 & -1 & 1 \\ 0 & 2 & 0 \\ 4 & 3 & 2 \end{bmatrix}.$
Where possible, find the following matrices.
 
- $A+B$
- $B-A$
- $3B$
- $-2A$
- $AB-BA$
- $-2BA$
- $2A-4B$
- $B^2-A^2$
 
::: 

::: {#exr- } 
Find $a, b, c$, and $d$ that satisfies the equation.
 
- $\begin{bmatrix} a & b \\ c & d \end{bmatrix} = \begin{bmatrix} c-3d & -d \\ 2a+d & a+b \end{bmatrix}$
- $3 \begin{bmatrix} a \\ b \end{bmatrix} + 2  \begin{bmatrix} b \\  a  \end{bmatrix} = \begin{bmatrix} 1\\  2 \end{bmatrix}$
 
::: 

::: {#exr- } 
Let 
$A=\begin{bmatrix}2 & 1 \\ 0 & -1\end{bmatrix},$
$B=\begin{bmatrix}3 & -1 & 2\\ 0 & 1 & 4\end{bmatrix},$
$C=\begin{bmatrix}3 & -1 \\ 2 & 0\end{bmatrix},$ and
$D=\begin{bmatrix}1 & 3 \\ -1 & 0 \\1 & 4\end{bmatrix}.$
Where possible, find the following
 
- $3A-2B$
- $5C$
- $B+D$
- $2B-3D$
- $A-D$
- $A-2C$
 
::: 

::: {#exr- } 
Find $A$ in terms of $B$.
 
- $A+B=3A+2B$
- $2A-B=5(A+2B)$
 
::: 

::: {#exr- } 
Simpify the following expressions where $A$, $B$, and $C$ are matrices.

- $2[9(A-B)+7(2B-A)]-2[3(2B+A)-2(A+3B)]$
- $5[3(A-B+2C)-2(3C-B)-A]+2[3(3A-B+C)+2(B-2A)-2C]$
::: 

::: {#exr- } 
Show that if $Q+A=A$ holds for every $m\times n$ matrix $A$, then 
$Q=0_{mn}$.
::: 

::: {#exr- } 
Show that if $A$ is an $m\times n$ matrix with $A+A'=0_{mn}$, then $A'=-A$.
::: 

::: {#exr- } 
Show that if $A$ denotes an $m\times n$ matrix, then $A=-A$ if and only if $A=0$.
::: 

::: {#exr- } 
Given the following vectors, determine the values of $a$ and $b$ such that 
$\vec{u}$ 
is a linear combination of 
$\vec{v_1}$ and  
$\vec{v_2}$. 
$$
\vec{u}=\vectorfour{5}{7}{a}{b}
\qquad 
\vec{v_1}=\vectorfour{1}{1}{1}{1}
\qquad
\vec{v_2}=\vectorfour{4}{3}{2}{1}
$$ 

::: 


::: {#exr- } 
Find all solutions $x_1, x_2, x_3$ of the equation $\vec b = x_1 \vec v_1 + x_2 \vec v_2 + x_3 \vec v_3$ given the following vectors.
$$
\vec b = \vectorfour{-8}{-1}{2}{15}
\qquad
\vec v_1 = \vectorfour{1}{4}{7}{5}
\qquad
\vec v_2 = \vectorfour{2}{5}{8}{3}
\qquad
\vec v_3 = \vectorfour{4}{6}{9}{1}
$$
::: 

::: {#exr- } 
Determine the value of $a$ such that 
$\vec{u}$  is a linear combination of $\vec{v_1}$ and $\vec{v_2}$ given the following vectors. 
$$
\vec{u}=\vectorthree{1}{a}{a^2} 
\qquad
\vec{v_1}=\vectorthree{1}{2}{4}
\qquad
\vec{v_2}=\vectorthree{1}{3}{9}
$$
::: 

::: {#exr- } 
Find the product of the given matrices.
 
- $\begin{bmatrix} 1 & -1 & 2 \\ 2 & 0 & 4 \end{bmatrix}$
- $\begin{bmatrix} 2 & 3 & 1 \\ 1 & 9 & 7\\ -1 & 0 & 2 \end{bmatrix}$ 
- $\begin{bmatrix} 1 & 3 & -3 \\ \end{bmatrix}$
- $\begin{bmatrix} 3 & 0 \\ -2 & 1 \\ 0 & 6 \end{bmatrix}$
 
::: 



::: {#exr- } 
Let 
$A=\begin{bmatrix}2 & 3-i \\ 1 & i \end{bmatrix},$ 
$B=\begin{bmatrix}i & 1-1 \\ 0 & i\end{bmatrix},$ and 
$C=\begin{bmatrix} 2+1 & 1 \\ 3 & i+1 \end{bmatrix}.$
Find each of the following.
 
- $A+B+C$
- $AB+AC$
- $A+BC$
- $CB$
- $A^2 C$
- $C^2 A$
 
::: 

::: {#exr- } 
Find all matrices that commute with the given matrix.
 
- $\begin{bmatrix}0 & 1 \\ 0 & 0 \end{bmatrix}$
- $\begin{bmatrix}0 & 0 \\ 1 & 0\end{bmatrix}$
- $\begin{bmatrix}2 & 3 \\ 1 & 0\end{bmatrix}$
 
::: 

::: {#exr- } 
For what values of the constants $a, b, c$ and $d$  is  
$\vec b$ a linear combination of the vectors $\vec v_1$, $\vec v_2$, $\vec v_3$ given 
$$
\vec b = \begin{bmatrix} a \\ b  \\ c \\ d \end{bmatrix} 
\qquad
\vec v_1 =\vectorfour{0}{0}{3}{0} 
\qquad
\vec v_2 = \vectorfour{1}{0}{4}{0} 
\qquad
\vec v_3 = \vectorfour{2}{0}{5}{6}?
$$
::: 

::: {#exr- } 
If $A=\begin{bmatrix}a & b \\c & d\end{bmatrix}$  where $a\neq 0$, show that $A$ factors in the form 
$$
A=
\begin{bmatrix}
1 & 0 \\
 x & 1
\end{bmatrix}
\begin{bmatrix}
y & z \\
0 & w
\end{bmatrix}.
$$
::: 

::: {#exr- } 
Find all vectors in $\mathbb{R}^4$ that are perpendicular to the following three vectors. 
$$
u=\vectorfour{1}{1}{1}{1} \qquad 
v=\vectorfour{1}{2}{3}{4} \qquad  
w=\vectorfour{1}{9}{9}{7}
$$ 
::: 

::: {#exr- } 
Determine a scalar $t$ such that $A X=t X$ where 
$A=\begin{bmatrix}2 & 1 \\ 1 & 2 \end{bmatrix}$ and $X=\begin{bmatrix}1 \\ 1\end{bmatrix}.$
::: 

::: {#exr- } 
Show that if $A$ and $B$ commute with $C$, then so does $A+B$.
::: 

::: {#exr- } 
Show that if $A$ and $B$ are $n\times n$ matrices, then $A B=BA$ if and only if $(A+B)^2=A^2+2A B+B^2$. 
::: 

::: {#exr- } 
Let $A$ and $B$ be the $2\times 2$ matrices
$$
A=
\begin{bmatrix}
\begin{bmatrix}
1 & 3 \\ 2& 1
\end{bmatrix} 
&
\begin{bmatrix}
0 & 3 \\ 1 & 3
\end{bmatrix}
\\ \vspace{-10pt} & \\
\begin{bmatrix}
3 & 4 \\
1 & 1
\end{bmatrix}
&
\begin{bmatrix}
2 & 0 \\ 0 & 1
\end{bmatrix}
\end{bmatrix}
\quad 
B=
\begin{bmatrix}
\begin{bmatrix}
3 & 3 \\ 1 & 1
\end{bmatrix}
&
\begin{bmatrix}
2 & 6 \\
4 & 3
\end{bmatrix}
\\ \vspace{-10pt} & \\
\begin{bmatrix}
1 & 3 \\ 1 & 3
\end{bmatrix}
&
\begin{bmatrix}
2 & 1 \\ 1 & 1
\end{bmatrix}
\end{bmatrix}
$$
whose entries are themselves $2\times 2$ matrices. Where possible, find each of the following.    
 
- $A+B$
- $-2B$
- $3A$
- $2A-3B$
- $B^2$
- $AB$
- $BA$
- $AB-BA$
- $A^2-B^2$
 
::: 

::: {#exr- } 
Let $A=\begin{bmatrix}\cos \theta & \sin \theta \\ -\sin \theta & \cos \theta \end{bmatrix}$. 
Find an expression for 

- $A^2$, 
- $A^3$, and 
- $A^n$ where $n$ is a positive integer. 
::: 

::: {#exr- } 
The Pauli spin matrices 
$$
P_1
=
\begin{bmatrix}
0 & 1 \\ 1 & 0 
\end{bmatrix},
\quad 
P_2
=
\begin{bmatrix}
0 & -i \\ i & 0
\end{bmatrix},
\quad 
P_3
=
\begin{bmatrix}
1 & 0 \\ 0 &-1
\end{bmatrix}
$$
are used when studying electron spin. Find 

- $P_1 P_2, $  
- $P_2 P_1$, 
- $P_2^2$, 
- $P_1 P_3$,  
- $P_3 P_1$,  and 
- $P_1+i P_2$. 
::: 

::: {#exr- } 
Let $A$ denote an arbitrary matrix and let $k$ denote an arbitrary scalar. Prove the following properties hold.
 
- $A-A=0$
- $0A=0$
- $0A=0$
- $k I A=k A$
 
::: 

::: {#exr- } 
Find $A\vec e_1$, $A\vec e_2$, and $A\vec e_3$ given the following.
$$
\vec e_1=\vectorthree{1}{0}{0} \quad 
\vec e_2=\vectorthree{0}{1}{0} \quad 
\vec e_3=\vectorthree{0}{0}{1} \qquad
\text{and} \qquad
A=
\begin{bmatrix}
a_1 & a_2 & a_3 \\
b_1 & b_2 & b_3 \\
c_1 & c_2 & c_3
\end{bmatrix}
$$
::: 

::: {#exr- } 
Find a $3\times 3$ matrix $A$ that satisfies all of the following. 
$$
A \vectorthree{1}{0}{0}= \vectorthree{1}{2}{3} \qquad
A \vectorthree{0}{1}{0}= \vectorthree{4}{5}{6}  
\qquad \text{and} \qquad
A \vectorthree{0}{0}{1}= \vectorthree{7}{8}{9}
$$
::: 

::: {#exr- } 
Find all vectors $\vec x$ such that $A\vec x=\vec b$ given the following. 
$$
A=
\begin{bmatrix}
1 & 2  & 0\\
0 & 0 & 1 \\
0 & 0 & 0 \end{bmatrix}
\quad \text{ and } \quad 
\vec b=\vectorthree{2}{1}{0}.
$$
::: 


::: {#exr- } 
Write the system
$$
\begin{cases}
2x_1-3x_2+5x_3 =7 \\
9x_1+4x_2-6x_3 =8
\end{cases}
$$
in matrix form and write $\begin{bmatrix} 7 \\ 8 \end{bmatrix}$ as a linear combination of the columns vectors of the coefficient matrix.
::: 

::: {#exr- } 
Show that the sum of any two diagonal matrices is diagonal.   
::: 

::: {#exr- } 
Show that the sum of any two upper triangular matrices is  upper triangular.   
::: 

::: {#exr- } 
Let $A$ be an $m\times n$ matrix and let
$$
B=\vectorfour{b_1}{b_2}{\vdots}{b_n} 
\qquad \text{and} \qquad
C=\begin{bmatrix} c_1 & c_2 & \cdots & c_m\end{bmatrix}
$$
be a column vector and a row vector, respectively. Prove that $AB=\sum_{j=1}^n b_j A_j$ and  $CA=\sum_{j=1}^n c_j A_j$.
::: 

::: {#exr- } 
Let $A$ and $B$ denote $n\times n$ matrices. 
The \index{Jordan product} **Jordan product** , $A\star B$ is defined by 
$A\star B=\frac{1}{2}(AB+BA)$. Determine whether this product is commutative, associative, and/or distributive. 
::: 

::: {#exr- } 
\label{ex:PropertiesofMatrixAddition}
Complete the proof of 
\ref{PropertiesofMatrixAddition}.
::: 

::: {#exr- } 
\label{ex:Properties of Scalar Multiplication}
Complete the proof of 
\ref{Properties of Scalar Multiplication}.
::: 

::: {#exr- } 
\label{ex:Properties of Matrix Multiplication}
Complete the proof of 
\ref{Properties of Matrix Multiplication}.
::: 


## On the Solutions of Linear Systems


Recall a system of linear equations is called consistent if it has at least one solution and is called inconsistent if it has no solutions. 
In this section we completely characterize when a linear system of equations has solutions; and we do so using the notion of \textit{rank}.
Basically, the rank of a linear system is the number of leading coefficients in the reduced row echelon form of the augmented matrix of the given linear system. Our first goal will be to show the notion of rank is well-defined; that is, we wish to show that every matrix has a unique reduced row echelon form. Thus the rank of a linear system will be a unique number. 

Find a $2\times 3$ linear system whose augmented matrix has two different row echelon forms. 

::: {#def- } 
If an matrix $A$ can be obtained from another matrix $B$ by a finite number of elementary row operations, then we say $B$ is \index{row equivalent} **row equivalent** to $A$.
::: 

::: {#thm- } 
\label{rowequiv}
Every matrix is row equivalent to a matrix in row  echelon form. 
::: 

::: {.proof }
Let $A$ be an $m\times n$ nonzero matrix, with entries $a_{ij}$, say
$$
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
 & & \vdots \\
a_{m1} & a_{m2} & \cdots  & a_{mn} \\
\end{bmatrix}
$$
Either all entries in the first column of $A$ are nonzero or not. If all entries are zero then the first column of $A$ satisfies the conditions of row echelon form. 
Otherwise, assume $i$ is the least such that $a_i$ is nonzero. 
Interchanging rows 1 and $i$ (if needed) we obtain a column where the first entry is nonzero. 
Now we have a matrix of the following form.
$$
\begin{array}{ll}
\parbox{1.7cm}{\vspace{.5cm} $i$-th row $\rightarrow$} 
& 
\begin{bmatrix}
a_{i1} & a_{i2} & \cdots & a_{in} \\
a_{21} & a_{22} & \cdots & a_{2n}\\
& & \vdots \\
a_{11} & a_{12} & \cdots & a_{1n}  \\
& & \vdots \\
a_{m1} & a_{m2} & \cdots  & a_{mn}\\
\end{bmatrix}
\end{array}
$$
Next we multiply the first row by the multiplicative inverse of its nonzero entry, obtaining 1. 
For the remaining first column entries $a_{k,m}$ (where $k>1$) multiply the row by the multiplicative inverse of $a_{k,m}$ and add to the first row replacing the $k$-th row. 
These steps lead to all zero entries in the first column below the leading coefficient. Our matrix is now in the form
$$
\begin{bmatrix}
1 & a'_{i2} & \cdots & a'_{in} \\
0 & a'_{22} & \cdots & a'_{2n}\\
& & \vdots \\
0 & a'_{m2} & \cdots  & a'_{mn}\\
\end{bmatrix}
$$
where the $a'_{ij}$ are the scalars obtain from completing the row operations. 
We repeat this process on the remaining columns taking into account that applying row operations will not change the fact that the previous columns will continue to satisfy the conditions of row echelon form. 
::: 

::: {#thm- } 
\label{rowequiv}
Every matrix is row equivalent to a unique matrix in reduced row echelon form. 
::: 

::: {.proof }
Let $A$ be an $m\times n$ matrix. We apply mathematical induction on $n$ for an arbitrary $m$. 
Let $n=1$. Now $A$ is just a matrix with one column and is row equivalent to one of the following matrices. 
$$
\vectorfour{1}{0}{\vdots}{0}
\qquad \text{or} \qquad
\vectorfour{0}{0}{\vdots}{0}
$$
So the case for $n=1$ is clear. Now assume $n>1$ and let $A$ and $A'$ denote the following matrices.
$$
A=
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
 & & \vdots \\
a_{m1} & a_{m2} & \cdots  & a_{mn} \\
\end{bmatrix}
\qquad \text{and} \qquad
A'=
\begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1,n-1} \\
a_{21} & a_{22} & \cdots & a_{2,n-1} \\
 & & \vdots \\
a_{m1} & a_{m2} & \cdots  & a_{m,n-1} \\
\end{bmatrix}
$$
Notice $A'$ is just $A$ with the $n$-th column deleted. By the way $A'$ is defined, any sequence of elementary row operations that takes $A$ into reduced row echelon form also takes $A'$ into reduced row echelon form. By the induction hypothesis, any two matrices $B$ and $C$ that are reduced row echelon forms of $A$ can only differ in the $n$ column. Assume, for a contradiction, that $B$ and $C$ differ only in the $n$-th column. Then there exists an integer $j$ such that the $j$-th row of $B$ is not equal to the $j$-th row of $C$. 
::: 

Due to \ref{rowequiv} the following definition is well-defined; meaning if a matrix $A$ is reduced to the unique matrix in reduced row echelon form 

::: {#def- } 
The \index{rank} **rank**  of a matrix $A$ is the number of leading coefficients in $\text{rref}(A)$.
::: 

::: {#exm- } 
Let $a, d, f$ be nonzero constants and let $b, c, e$ be arbitrary constants.  Find the rank of the following matrices. 
$$
A=\begin{bmatrix} 0 & 0 & a \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix} \qquad 
B=\begin{bmatrix} 0 & a & 0 \\ 0 & 0 & d \\ 0 & 0 & 0 \end{bmatrix} \qquad 
C=\begin{bmatrix} a & b & c \\ 0 & d & e \\ 0 & 0 & f \end{bmatrix}
$$
Since $a$, $f$, and $d$ are all nonzero, 
$$
\text{rref}(A)=\begin{bmatrix} 0 & 0 & 1 \\ 0 & 0 & 0 \\ 0 & 0 & 0 \end{bmatrix}, \qquad 
\text{rref}(B)=\begin{bmatrix} 0 & 1 & 0 \\ 0 & 0 & 1 \\ 0 & 0 & 0 \end{bmatrix}, \qquad 
\text{rref}(C)=\begin{bmatrix} 1 & 0 & 0 \\ 0 & 1 & 0 \\ 0 & 0 & 1 \end{bmatrix}.
$$
Thus the rank of $A$, $B$, and $C$ is 1, 2, and 3, respectively. 

::: 

::: {#lem- } 
Let $A\vec{x}=\vec{b}$ and $C\vec{x}=\vec{d}$ be two linear systems of the same size. If the augmented matrices
 $\begin{bmatrix} A \, | \, \vec{b} \end{bmatrix}$ 
and 
$\begin{bmatrix}C \, | \, \vec{d} \end{bmatrix}$ 
are row equivalent, then the linear systems are equivalent. 
::: 

::: {.proof }
The proof follows immediately from 
\ref{proprowoperations}.
::: 

::: {#thm- } 
[Fundamental Theorem of Linear Systems]
\label{systemchar}
Let A be the coefficient matrix of an $n\times m$ system. Then
 
- If $\text{rank}(A)=n$, then the system is consistent.
- If $\text{rank}(A)=m$, then the system has at most one solution.
- If $\text{rank}(A)<m$, then the system has either infinitely many solutions or none.
 
::: 

::: {.proof }
First let's make two observations. 
By definition of the reduced row echelon form of a matrix, there is at most one leading 1 in each of the $m$ columns and in each of the $n$ columns. 
Let A be the coefficient matrix of an $n$ by $m$ system. 
Then 
$$
\text{rank}(A) \leq n \qquad \text{and}\qquad \text{rank}(A)\leq m.
$$
A linear system is inconsistent if and only if the reduced row echelon form of its augmented matrix has a row of the form 
\begin{equation}
\label{inconsirow}
\left[
\begin{array}{cccc|c}
0 & 0 & \cdots & 0 & 1 
\end{array}
\right].
\end{equation}
Moreover, for any linear system with $m$ variables,
\begin{equation*}
\begin{tabular}{ccccccc} \small
$\left(\parbox[c]{1.55cm}{\begin{center}
number of free variables
\end{center}}\right)$ & $=$  & 
$\left(\parbox[c]{1.55cm}{\begin{center}
total number of variables
\end{center}}\right)$ & $-$  & 
$\left(\parbox[c]{1.55cm}{\begin{center}
number of leading variables
\end{center}}\right)$ & $=$  
& $\parbox[c]{2.15cm}{\begin{center}
$m-\text{rank}(A)$
\end{center}}$
\end{tabular} 
\end{equation*}
 
- If $\text{rank} A=n$ then each row must contain a leading 1. Thus there is no row of the form in \ref{inconsirow} and so the system is consistent. 
- If $\text{rank}(A)=m$ then there are no free variables. So the only possible choice is for there to be no solutions or exactly one solution.  
- If $\text{rank}(A)=t<m$ then there $m-t$ free variables. So the only possible choice is for there to be no solutions or infinitely many solutions.    
 
 
::: 

The following two corollaries are immediate consequences of \ref{systemchar}.

::: {#cor- } 
\label{cor:linsystmecor1}
A linear system with fewer equations than unknowns has either no solutions or infinity many solutions.
::: 

::: {.proof }
The proof is left for the reader as Exercise \ref{ex:linsystmecor1}.
::: 

::: {#cor- } 
\label{cor:linsystmecor2}
A linear system of $n$ equations in $n$ variables has a unique solution if and only if the rank of its coefficients matrix $A$ is $n$, and in this case 
$\text{rref}(A)=I_n$.
::: 

::: {.proof }
The proof is left for the reader as Exercise \ref{ex:linsystmecor2}.
::: 

::: {#exm- } 
Find the rank of the coefficient matrix and solve the linear system of equations
$$
\begin{cases}
x_1-x_2+x_3=4\\
3x_1+4x_2-x_3=8\\
5x_1+9x_2-4x_3=13.
\end{cases}
$$
We use Gaussian elimination with the augmented matrix to find the rank of the coefficient matrix. 
\begin{align*}
&
\begin{bmatrix}
\begin{array}{ccc|c}
1 & -1 & 2 & 4\\
3 & 4 & -1 & 8\\
5 & 9 & -4 & 13
\end{array}
\end{bmatrix}
\begin{array}{c}
\stackrel{\longrightarrow}{-3R_1+R_2}
\\
\stackrel{\longrightarrow}{-5R_1+R_3}
\end{array}
\begin{bmatrix}
\begin{array}{ccc|c}
1 & -1 & 2 & 4 \\
0 & 7 & -7 & -4 \\
0 & 14 & -14 & -7
\end{array}
\end{bmatrix}
\\
& 
\begin{array}{c}
\stackrel{\longrightarrow}{-\frac{1}{7}R_2} \\
\stackrel{\longrightarrow}{-14R_2+R_3}
\end{array}
\begin{bmatrix}
1 & -1 & 2 & 4 \\
0 & 1 & -1 & -\frac{4}{7} \\
0 & 0 & 0 & 1
\end{bmatrix}
\begin{array}{c}
\stackrel{\longrightarrow}{\frac{4}{7}R_3+R_2}
\\
\stackrel{\longrightarrow}{-4R_3+R_2}
\\
\stackrel{\longrightarrow}{R_2+R_1}
\end{array}
\begin{bmatrix}
1 & 0 & 1 & 0 \\
0 & 1 & -1 & 0 \\
0 & 0 & 0 & 1
\end{bmatrix}
\end{align*}
The number of leading 1's is 2 and thus $\text{rank}(A)=2$. By \ref{systemchar} the system either has no solutions or infinitely many solutions. 
The last row represents $0=1$, which means that the system has no solution. 

::: 

::: {#exm- } 
Consider the system 
$$
\begin{cases}
y+2k z =0 \\ 
x+2y+6z  =2 \\
k x+2 z  =1 
\end{cases}
$$
where $k$ is a constant.

- For which values of $k$ does the system have a unique solution?
- When are there no solutions?
- When are there infinitely  many solutions?

Using Gaussian elimination:
\begin{align*}
\stackrel{\longrightarrow}{R_1 \leftrightarrow R_2}
\left[
\begin{array}{ccc|c}
1 & 2 & 6 & 2 \\
0 & 1 & 2k & 0 \\
k & 0 & 2 & 1
\end{array}
\right]
& 
\stackrel{\longrightarrow}{-k R_1+R_2}
\left[
\begin{array}{ccc|c}
1 & 2 & 6 & 2 \\
0 & 1 & 2k & 0 \\
0 & -2k & -6k+2 & -2k+1
\end{array}
\right]
\\
& 
\begin{array}{c}
\stackrel{\longrightarrow}{-2R_2+R_1} \\
\stackrel{\longrightarrow}{2kR_2+R_3} 
\end{array}
\left[
\begin{array}{ccc|c}
1 & 0 & -4k+6 & 2 \\
0 & 1 & 2k & 0 \\
0 & 0 & 4k^2-6k+2 & -2k+1
\end{array}
\right]
\end{align*}
Notice 
$
4k^2-6k+2=-2(-2k+1)(k-1)=0
$ 
when $k=1/2$ and $k=1$. 
(a) When $k\neq 1/2$ and $k\neq 1$ there is a unique solution. 
(b) When $k \neq 1/2$ and $k=1$, this system has no solutions. 
(c) When $k=1/2$ this system has infinitely many solutions.

::: 

::: {#exm- } 
Solve the linear system of equations
$$
\begin{cases}
(3+i)x_1+(1+i)x_2=4+4i\\
x_1-x_2=2i
\end{cases}
$$
where $x_1$ and $x_2$ are complex variables.
We convert the system into a linear system with real variables. Let 
$x_1=y_1+i z_1$ and $x_2=y_2+i z_2$. Now substation into the original system leads to the system
$$
\begin{cases}
3y_1-z_1+(y_1+3z_1)i+(y_2-z_2)+(y_2+z_2)i=4+4i \\
(y_1-y_2)+(z_1-z_2)i=0+2i
\end{cases}$$
Equating real and imaginary parts leads to the system
$$
\begin{cases}
3y_1+y_2-z_1-z_2=4\\
y_1+y_2+3z_1+z_2 =4\\
y_1-y_2=0\\
z_1-z_2=2
\end{cases}
$$
The solutions are $y_1=1$, $y_2=1$, $z_1=1$, and $z_2=-1$. Thus the solutions to the original system are $x_1=1+i$ and $x_2=1-i$.

::: 

::: {#def- } 
A linear system of the form $A \vec x = \vec 0$ is called a \index{homogeneous} **homogeneous**  linear system; that is, if all of the constant terms in the linear system are zero. 
::: 

In particular, if $A$ and $B$ are row equivalent $m \times n$ matrices, then the homogenous systems $A\vec{x}=\vec{0}$ and $B\vec{x}=\vec{0}$ are equivalent. 

::: {#lem- } 
If $A$ is an $n\times n$ matrix and the system $A \vec{x}=\vec{0}$ has no nontrivial solution, then $A$ is row equivalent to $I_n$. 
::: 

::: {.proof }
If $A\vec{x}=\vec{0}$ has no nontrivial solutions, then the trivial solution is its unique solution. By \ref{cor:linsystmecor2}, $\text{rref}(A)=I_n$ and by 
\ref{rowequiv}, 
$A$ is row equivalent to a unique matrix in reduced row echelon form, thus $A$ is row equivalent to $I_n$.
::: 

::: {#lem- } 
\label{lem:homosys}
Let $A \vec{x} = \vec{0}$ be a linear homogeneous system. 
 
- All homogeneous systems are consistent.
- A homogeneous system with fewer equations than unknowns has infinitely many solutions.
- If $\vec{x}_1$ and $\vec{x}_2$ are solutions, then $\vec{x}_1+\vec{x}_2$ is also a solution.
- If $\vec{x}_1$ is a solution, then $k \vec{x}_1$ is also a solution.
 
::: 

::: {.proof }
The proof is left for the reader as Exercise \ref{ex:homosys}.
::: 

There is a strong relationship between the solutions to a linear system 
$A\vec{x} = \vec{b}$ and the solutions to the corresponding homogeneous system, $A\vec{x}= \vec{0}$. 

::: {#thm- } 
If $\vec{u}$ is any specific solution to the linear system  $A\vec{x}=\vec{b}$, 
then the entire solution set of $A\vec{x}=\vec{b}$ can be described as
$$
\{\vec{u}+\vec{v} \, | \, \vec{v} 
\text{ is any solution to } A \vec{x}=\vec{0}\}.
$$
::: 

::: {.proof }
Let $\vec{u}$ be any particular solution to the system $A\vec{x}=\vec{b}$ and let $\vec{v}$ be any solution to the system $A\vec{x}=\vec{0}$. 
Then 
$$
A(\vec{u}+\vec{v})
=A\vec{u}+A\vec{v}
=\vec{b}+\vec{0}
=\vec{b},
$$
which shows that every vector of the form $\vec{u}+\vec{v}$ is a solution to the system $A\vec{x}=\vec{b}$. 
Conversely, let $\vec{w}$ be an arbitrary solution to the system $A\vec{x}=\vec{b}$. Notice 
$$
A(\vec{w}-\vec{u})
=A\vec{w} - A\vec{u}
=\vec{b}-\vec{b}
=\vec{0},
$$ 
which shows $\vec{w}-\vec{u}$ is a solution to the system $A\vec{x}=\vec{0}$. Set $\vec{v}=\vec{w}-\vec{u}$, then $\vec{w}=\vec{u}+\vec{w}-\vec{u}=\vec{u}+\vec{v}$ where $\vec{v}$ is a solution to the system  $A\vec{x}=\vec{0}$.
::: 

::: {#cor- } 
\label{cor:unsol}
If $A$ is an $n\times n$ and $A\vec{x}=\vec{0}$ has no nontrivial solutions, then the system $A\vec{x}=\vec{b}$ has a unique solution.
::: 

::: {.proof }
The proof is left for the reader as Exercise \ref{ex:unsol}.
::: 

##  Exercises

::: {#exr- } 
Find an inconsistent system of two linear equations in three unknowns. Describe the situation geometrically. 
::: 

::: {#exr- } 
Find the rank of the matrix. 
 
- $\begin{bmatrix} 1 & 2 & 3 \\ 0 & 0 & 3 \\ 0 & 0 & 1 \end{bmatrix}$
- $\begin{bmatrix} 1 & 2 & 3 \\ 0 & 0 & 3 \\ 0 & 0 & 3 \end{bmatrix}$
- $\begin{bmatrix} 1 & 2 & 3 \\ 0 & 1 & 3 \\ 0 & 0 & 1 \end{bmatrix}$
- $\begin{bmatrix} 2 & 2 & 2 \\ 2 & 2 & 2 \\ 2 & 2 & 2 \\ 2 & 2 & 2 \\ \end{bmatrix}$
- $\begin{bmatrix} 1 & 4 & 7 \\ 2 & 5 & 8 \\ 3 & 6 & 9 \end{bmatrix}$
- $\begin{bmatrix} 1 & 4 & 7 \\ 2 & 5 & 8 \\ -3 & -6 & -9\end{bmatrix}$
 
::: 

::: {#exr- } 

- If the rank of a $4\times 4$ matrix $A$ is 4, what is $\text{rref}(A)$?
- If the rank of a $5\times 3$ matrix $A$ is 3, what is $\text{rref}(A)$?
- If the rank of a $2\times 5$ matrix $A$ is 1, what is $\text{rref}(A)$?
::: 

::: {#exr- } 
Find the rank of the following matrices.
 
- $\begin{bmatrix} a & 0 & 0 \\ 0 & c & 0\\ 0 & 0 & f\end{bmatrix}$
- $\begin{bmatrix}a & b & c \\ 0 & c & d \\ 0 & 0 & f \end{bmatrix}$
- $\begin{bmatrix} a & 0 & 0 \\ b & c & 0\\ d & e & f\end{bmatrix}$
 
where $a$, $d$, $f$ are nonzero and $b$, $c,$ and $e$ are arbitrary scalars. 
::: 

::: {#exr- } 
Find the rank of the system of equations. 
 
- $\begin{cases} x+2y+3z=0 \\ 2x+3y+4z=0 \\ 3x+4y+6z=0 \end{cases}$
- $\begin{cases} x+2y+3z=2\\ 2x+3y+4z=-2\\ 3x+4y+6z=2 \end{cases}$
- $\begin{cases} x+2y+3z=0 \\ 2x+3y+4z=1\\ 3x+4y+6z=3 \end{cases}$
- $\begin{cases} x+2y+3z=a\\ 2x+3y+4z=b \\ 3x+4y+6z=c \end{cases}$
 
::: 

::: {#exr- } 
Find all solutions to the homogenous system.
 
- $\begin{cases} 4x_1-x_2=0 \\ 7x_1+3x_2=0\\ -8x_1+6x_2=0 \end{cases}$
- $\begin{cases}x_1-2x_2+x_3=0\\3x_1+2x_3+x_4=0\\ 4x_2-x_3-x_4=0\\ 5x_1+3x_3-x_4=0\end{cases}$
- $\begin{cases} x_1-3x_2=0\\ -2x_1+6x_2=0\\ 4x_1-12x_2=0 \end{cases}$
- $\begin{cases}x_1+x_2-x_3=0\\ 4x_1-x_2+5x_3=0\\ 2x_1-x_2-2x_3=0\\ 3x_1+2x_2-x_3=0\end{cases}$
 
::: 

::: {#exr- } 
Show that the homogenous system of linear equations 
$$
\begin{cases}
a x+by=0 \\
cx+dy =0
\end{cases}
$$
has an infinite number of solutions if and only if $ad-bc=0$.
::: 

::: {#exr- } 
For any positive integer $n$, find a system of $n$ equations in two variables that has infinitely many solutions. 
::: 

::: {#exr- } 
If possible, find a condition of the coefficients of the homogenous system of linear equations so that 
(i) the system only has the zero solution, and 
(ii)  the system has infinitely many solutions. 
 
- $\begin{cases} a_{11}x_1+a_{12}x_2+a_{13}x_3=0 \\ a_{21}x_1+a_{22}x_2+a_{23}x_3=0 \end{cases}$
- $\begin{cases} a_{11}x_1+a_{12}x_2+a_{13}x_3=0 \\ a_{21}x_1+a_{22}x_2+a_{23}x_3=0 \\ a_{31}x_1+a_{32}x_2+a_{33}x_3=0\end{cases}$
 
::: 

::: {#exr- } 
Show that if a system of linear equations is inconsistent, then its reduced row echelon form has a row of the form  
$$
\left[
\begin{array}{cccc|c}
0 & 0 & \cdots & 0 & 1 
\end{array}
\right].
$$
::: 

::: {#exr- } 
\label{ex:linsystmecor1}
Prove
\ref{cor:linsystmecor1}.
::: 

::: {#exr- } 
\label{ex:linsystmecor2}
Prove
\ref{cor:linsystmecor2}. 
::: 

::: {#exr- } 
Determine the values of $k$ for which the system has nontrivial solutions.
 
- $\begin{cases} 2x_1-3x_2+5x_3=0\\ -x_1+7x_2-x_3=0\\ 4x_1-11x_2+k x_3=0 \end{cases}$
- $\begin{cases} x_1-2x_2-5x_3=0\\ 2x_1+3x_2+x_3=0\\ x_1-7x_2-k x_3=0\end{cases}$
 
::: 

::: {#exr- } 
Show that if $AX=\vec 0$ is a homogenous system with an infinite number of solutions and the system $AX=B$ has a solution, then $AX=B$ must have an infinite number of solutions. 
::: 

::: {#exr- } 
Find an example, where possible, for each of the following.

- A linear system of four equations in four unknowns that has a line as a solution set.
- A linear system of four equations in four unknowns that has a plane as a solution set.
- A linear system of four equations in three unknowns that has a line as a solution set.
- A linear system of four equations in two unknowns that has a plane as a solution set.
 
::: 

::: {#exr- } 
Determine whether or not the following system is consistent.
$$
\begin{bmatrix}
0 & i & 1-i \\
-i & 0 & i\\
1-i & -i & 0
\end{bmatrix}
\cdot
\begin{bmatrix}
x_1 \\ x_2 \\ x_3
\end{bmatrix}
=
\begin{bmatrix}
-1 \\ 0 \\ 1
\end{bmatrix}
$$
::: 

::: {#exr- } 
Show that if $AX=0$ for all vectors $X$, then $A=0$.
::: 

::: {#exr- } 
Under what conditions will $k$ planes 
$a_j x +b_j y+c_j z=d_j$ for $j=1, 2, ..., k$ intersect in exactly one point?
::: 

::: {#exr- } 
Given that $AX=B$ is consistent and of rank $r$, for what sets of $r$ unknowns can one solve?   
::: 

::: {#exr- } 
If possible, write the matrix $A$
as a linear combination of the matrices 
$$
\begin{bmatrix}
1 & 0 \\
0 & 1
\end{bmatrix}
\qquad \text{and} \qquad
\begin{bmatrix}
1 & 0 \\
0 & 0 
\end{bmatrix}.
$$
 
- $A=\begin{bmatrix}3 & 0 \\ 0 & 2\end{bmatrix}$
- $A=\begin{bmatrix} 4 & 1 \\ 0 & -3 \end{bmatrix}$
- $A=\begin{bmatrix}11 & 2 \\ 0 & -4\end{bmatrix}$
 
::: 

::: {#exr- } 
Let $A Z=B$ be a given system of linear equations where $A_{n\times n}$ and $B_{n\times 1}$ are complex matrices.
Let $Z=X+i Y$. Show that the original complex $n\times n$ system is equivalent to the $2n\times 2n$ real system 
$$
\begin{cases}
CX-DY=S \\
CX+DY=T
\end{cases}
$$
where $B=S+iT$.
::: 

::: {#exr- } 
\label{ex:homosys}
Prove
\ref{lem:homosys}. 
::: 

::: {#exr- } 
\label{ex:unsol}
Prove
\ref{cor:unsol}. 
::: 







